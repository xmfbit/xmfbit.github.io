<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[使用IPDB调试Python代码]]></title>
      <url>%2F2017%2F08%2F21%2Fdebugging-with-ipdb%2F</url>
      <content type="text"><![CDATA[IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。 安装与使用IPDB以Python第三方库的形式给出，使用pip install ipdb即可轻松安装。 在使用时，有两种常见方式。 集成到源代码中通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：123456import ipdb# some codex = 10ipdb.set_trace()y = 20# other code 则程序会在执行完x = 10这条语句之后停止，展开Ipython环境，就可以自由地调试了。 命令式上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：1python -m ipdb your_code.py 常用命令IPDB调试环境提供的常见命令有： 帮助帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？ 使用h即可调出IPDB的帮助。可以使用help command的方法查询特定命令的具体用法。 下一条语句使用n(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？ 进入函数内部使用s(step into)进入函数调用的内部。 打断点使用b line_number(break)的方式给指定的行号位置加上断点。使用b file_name:line_number的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。 另外，打断点还支持指定条件下进入，可以查询帮助文档。 一直执行直到遇到下一个断点使用c(continue)执行代码直到遇到某个断点或程序执行完毕。 一直执行直到返回使用r(return)执行代码直到当前所在的这个函数返回。 跳过某段代码使用j line_number(jump)可以跳过某段代码，直接执行指定行号所在的代码。 更多上下文在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用l first[, second](list)命令。 其中first指示向上最多显示的行号，second指示向下最多显示的行号（可以省略）。当second小于first时，second指的是从first开始的向下的行数（相对值vs绝对值）。 根据SO上的这个问题，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。 我在哪里调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用w或者where可以打印出目前所在的行号位置以及上下文信息。 这是啥我们可以使用whatis variable_name的方法，查看变量的类别（感觉有点鸡肋，用type也可以办到）。 列出当前函数的全部参数当你身处一个函数内部的时候，可以使用a(argument)打印出传入函数的所有参数的值。 打印使用p(print)和pp(pretty print)可以打印表达式的值。 清除断点使用cl或者clear file:line_number清除断点。如果没有参数，则清除所有断点。 再来一次使用restart重新启动调试器，断点等信息都会保留。restart实际是run的别名，使用run args的方式传入参数。 退出使用q退出调试，并清除所有信息。 当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Focal Loss论文阅读 - Focal Loss for Dense Object Detection]]></title>
      <url>%2F2017%2F08%2F14%2Ffocal-loss-paper%2F</url>
      <content type="text"><![CDATA[Focal Loss这篇文章是He Kaiming和Ross发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的讨论。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。 下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势。 为什么要有Focal Loss？目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。与之相反，前者是输出一个稠密的proposal，]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[DELL 游匣7559安装Ubuntu和CUDA记录]]></title>
      <url>%2F2017%2F08%2F10%2Finstall-ubuntu-in-dell%2F</url>
      <content type="text"><![CDATA[虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考这篇文章成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。 蛋疼的过往之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。 安装Ubuntu我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。 选择“Install Ubuntu”选项，按e，找到包含有quiet splash的那行脚本，将quiet splash替换为以下内容： 1nomodeset i915.modeset=1 quiet splash 之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按e，按照上述方法替换quiet splash。确定可以进入Ubuntu系统并登陆。 GRUB设置下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：sudo vi /etc/default/grub，找到包含GRUB_CMDLINE_LINUX_DEFAULT的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）： 1GRUB_CMDLINE_LINUX_DEFAULT="nomodeset i915.modeset=1 quiet splash" 更新系统软件配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新， 1234sudo apt-get updatesudo apt-get upgradesudo apt-get dist-upgradesudo apt-get autoremove 参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？ 由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。1sudo apt-get install vim 更新完之后，重启，确认可以正常登陆系统。 移除原有的Nvidia和Nouveau驱动按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。1sudo service lightdm stop 之后，执行卸载操作：123sudo apt-get remove --purge nvidia*sudo apt-get remove --purge bumblebee*sudo apt-get --purge remove xserver-xorg-video-nouveau* 编辑配置文件，/etc/modprobe.d/blacklist.conf，将Nouveau加入到黑名单中：12345blacklist nouveaublacklist lbm-nouveaualias nouveau offalias lbm-nouveau offoptions nouveau modeset=0 编辑/etc/init/gpu-manager.conf文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：1234567# Comment these start on settings ; GPU Manager ruins our work#start on (starting lightdm# or starting kdm# or starting xdm# or starting lxdm)taskexec gpu-manager --log /var/log/gpu-manager.log 之后，更新initramfs并重启。1sudo update-initramfs -u -k all 重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：12# 正常情况下，下面的命令应该不产生任何输出lsmod | grep nouveau 并确定关闭了gpu-manager服务：1sudo service gpu-manager stop 至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。 安装CUDA鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。 按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。123sudo service lightdm stopsudo apt-get install linux-headers-$(uname -r)sudo apt-get install mesa-utils 安装CUDA包：1234sudo dpkg -i YOUR_CUDA_DEB_PATHsudo apt-get updatesudo apt-get install cuda-8-0sudo apt-get autoremove 安装完毕之后使用sudo reboot重启，确定能够正常登陆系统。 在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！ 测试CUDA我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到~/.bashrc或者~/.zshrc中去（不过我相信在经历完这些安装之后应该还没有闲心去搞oh-my-zsh。。。）。12export PATH="$PATH:/usr/local/cuda-8.0/bin"export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib" 接下来，我们将使用CUDA自带的example进行测试：123456789# 导入我们刚加入的环境变量source ~/.bashrccd /usr/local/cuda-8.0/bin# 将CUDA example拷贝到$HOME下./cuda-install-samples-8.0.sh ~# 进入拷贝到的那个目录 buildcd ~/NVIDIA_CUDA-8.0_Samplesmake -j12# 自己挑选几个目录进去运行编译生成的可执行文件测试吧~ Last But Not Least安装玩CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 8 定制new和delete]]></title>
      <url>%2F2017%2F07%2F03%2Feffective-cpp-08%2F</url>
      <content type="text"><![CDATA[手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是operator new和operator delete，配角是new_handler，它在当operator new无法满足客户内存需求时候被调用。 另外，operator new和operator delete只用于分配单一对象内存。对于数组，应使用operator new[]，并通过operator delete[]归还。除非特别指定，本章中的各项既适用于单一operator new，也适用于operator new[]。 最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。 49 了解new-handler的行为什么是new-handler？当operator new无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。 为了指定new-handler，必须调用位于标准库&lt;new&gt;的函数set_new_handler。其声明如下：1234namespace std &#123; typedef void (*new_handler) (); new_handler set_new_handler(new_handler p) throw();&#125; 其中，传入参数p是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。throw()表示该函数不抛出任何异常。 当operator new无法满足内存需求时，会不断调用set_new_handler()，直到找到足够的内存。更加具体的介绍见条款51. 一个设计良好的new_handler函数可以是以下的设计策略： 设法找到更多的内存可供使用，以便使得下一次的operator new成功。 安装另一个new_handler函数。即在其中再次调用set_new_handler，找到其他的擦屁股函数接盘。 卸载new_handler函数。即将NULL指针传进set_new_handler()中去。这样，operator new会抛出异常。 抛出bad_alloc（或其派生类）异常。 不返回（放弃治疗），直接告诉程序exit或abort。 有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的set_new_handler()函数和operator new。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。 123456789101112131415class A &#123;public: static std::new_handler set_new_handler(std::new_handler p) throw(); static void* operator new(std::size_t size) throw(std::bad_alloc);private: static std::new_handler current_handler;&#125;;// 实现文件std::new_handler A::set_new_handler(std::new_handler p) throw() &#123; std::new_hanlder old = current_handler; current_handler = p; return old;&#125; 静态成员变量必须在类外进行定义（除非是const且为整数型），所以需要在类外定义：12// 实现文件std::new_handler A::current_handler = 0; 在实现自定义的operator new的时候，首先调用set_new_handler()将自己的擦屁股函数安装为默认，然后调用global的operator new进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理： 1234567891011121314151617// 这个类实现了自动安装与恢复new_handlerclass Helper &#123;public: explicit Helper(std::new_handler p): handler(p) &#123;&#125; ~Helper() &#123;std::set_new_handler(handler); &#125;private: std::new_handler handler; // 禁止拷贝构造与赋值 Helper(const Helper&amp;); Helper&amp; operator= (const Helper&amp;);&#125;;// 实现类A自定义的operator newvoid* A::operator new(std::size_t size) throw(std::bad_alloc) &#123; // 存储了函数返回值，也就是原始的 new_handler Helper h(std::set_new_handler(current_handler)); return ::operator new(size);&#125; 新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的current_handler变量，我们把这个基类做成模板。 12345678910template &lt;typename T&gt;class HandlerHelper &#123;public: static std::new_handler set_new_handler(std::new_handler p) throw(); static void* operator new(std::size_t size) throw(std::bad_alloc); ... // 其他的new版本，见条款52private: static std::new_handler current_handler;&#125;;// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样 这样，我们只要让类A继承自HandlerHelper&lt;A&gt;即可（看上去很怪异。。。）：123class A: public HandlerHelper&lt;A&gt; &#123; ...&#125;; 50 了解替换new和delete的合适时机最常见的理由（替换之后你能得到什么好处）： 检测运用上的错误。比如缓冲区越界，我们可以在delete的时候进行检查。 强化效能。编译器实现的operator new是为了普适性的功能，改成自定义版本可能提升效能。 收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的operator new和delete能够收集到这些信息。 但是，写出能正常工作的new却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的Pool对分配大量小型对象很有帮助。 51 编写new和delete时候需要遵守常规自定义的operator new需要满足以下几点： 如果有足够的内存，则返回其指针；否则，遵循条款49的约定。 具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为NULL时，才抛出异常bad_alloc。 C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。 还要避免掩盖正常的operator new。 下面就是一个自定义operator new的例子：123456789101112131415161718192021222324void* operator new(size_t size) throw(bad_alloc) &#123; // 你的operator new也可能接受额外参数 using namespace std; if(size == 0) &#123; size = 1; // 处理0byte申请 &#125; while(true) &#123; // ... try to alloc memory if(success) &#123; return the pointer; &#125; // 处理分配失败，找出当前的handler // 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄 // 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数 new_handler globalHandler = set_new_handler(0); set_new_handler(globalHandler); if(globalHandler) &#123; (*globalHandler)(); &#125; else &#123; throw bad_alloc(); &#125; &#125;&#125; 在自定义operator delete时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。1234void operator delete(void* memory) throw() &#123; if(memory == 0) return; // ...&#125; 52 写了placement new也要写placement delete如果operator new接受的参数除了一定会有的那个size_t之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：1void* operator new(size_t size, void* memory) throw(); 上述placement new已经被纳入C++规范（可以在头文件&lt;new&gt;中找到它。）这个函数常用来在vector的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。 本条款主要探讨与placement new使用不当相关的内存泄漏问题。当你写一个new表达式时，共有两个函数被调用： 分配内存的operator new 该类的构造函数 假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的operator new对应的delete版本。 如果目前面对的是正常签名的operator new delete，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。1234// 某个类Wedget内部有自定义的placement new如下static void* operator new(size_t size, ostream&amp; logStream) throw (bad_alloc);Widget* pw = new (std::cerr) Widget; 如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。123static void operator delete(void* memory, ostream&amp; logStream) throw();// 这样，即使下式抛出异常，也能正确处理Widget* pw = new (std::cerr) Widget; 然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：1delete pw; 那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。 一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。12345678910111213141516171819202122232425class StdNewDeleteForms &#123;public: // 正常的new和delete static void* operator new(std::size_t size) throw std::bad_alloc) &#123; return ::operator new(size); &#125; static void operator delete(void* memory) throw() &#123; ::operator delete(memory); &#125; // placement new 和 delete static void* operator new(std::size_t size, void* p) throw() &#123; ::operator new(size, p); &#125; static void operator delete(void* memory, void* p) throw() &#123; ::operator delete(memory, p); &#125; // nothrow new 和 delete static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123; return ::operator new(size, nt); &#125; static void operator delete(void* memory, const std::nothrow_t&amp;) throw() &#123; ::operator delete(mempry); &#125;&#125;; 上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和using声明（见条款39），取得标准形式。12345678910class Widget: public StdNewDeleteForms &#123;public: // 使用标准new 和 delete using StdNewDeleteForms::operator new; using StdNetDeleteForms::operator delete; // 添加自定义的placement new 和 delete static void* operator new(std::size_t size, std::ostream&amp; logStream) throw(std::bad_alloc); static void operator delete(void* memory, std::ostream&amp; logStream) throw();&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法]]></title>
      <url>%2F2017%2F06%2F25%2Fhinton-nnml-06%2F</url>
      <content type="text"><![CDATA[第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的博客和对应的论文以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。 下图即来自上面的这篇博客。 Momentum我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。 具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\epsilon$为学习率，$\alpha$为momentum系数。 \Delta w_t = v_t = \alpha v_{t-1} - \epsilon g_t = \Delta w_t - \epsilon g_t为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）： v_t = \alpha(v_{t-1} + \frac{\epsilon g}{1-\alpha}) - \frac{\epsilon g}{1-\alpha}由于$\alpha &lt; 0$，所以当$t = \infty$时，只剩下了后面的常数项，即： v_\infty = -\frac{\epsilon}{1-\alpha}g也就是说，权重更新的幅度变成了原来的$\frac{1}{1-\alpha}$倍。若取$\alpha=0.99$，则加速$100$倍。 Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。 一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。 Adaptive Learning Rate这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。 例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。 下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。 RMSProprprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。 我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。 \text{MeanSquare}(w, t) = 0.9 \text{MeanSquare}(w, t-1) + 0.1g_t^2然后，将梯度除以上面的得到的Mean Square值。 RMSProp还有一些变种，列举如下： 课程总结 对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。 对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。 如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。 “Modern” SGD从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。 学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。 学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。 学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。 高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面） AdagradAdagrad对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即： g_{i} = \bigtriangledown_{\theta_i} J(\theta)Adagrad使用一个系数来为不同的参数修正学习率，如下： \hat{g_i} = \frac{1}{\sqrt{G_i+\epsilon}}g_i其中，$G_i$是截止到当前时间步$t$时，参数$\theta_i$对应梯度$g_i$的平方和。 我们可以把上面的式子写成矩阵形式。其中，$\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \odot g_t$。 \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t我们再来看PyTorch中的相关实现： 123456# for each gradient of parameters:# addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t# let epsilon = 1E-10state['sum'].addcmul_(1, grad, grad) # 计算 Gstd = state['sum'].sqrt().add_(1e-10) # 计算 \sqrt(G)p.data.addcdiv_(-clr, grad, std) # 更新 由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。 Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。 AdadeltaAdadelta给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。 E[g_t^2] = \gamma E[g_{t-1}^2] + (1-\gamma)g_t^2由于$\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。 \hat{g}_t = \frac{1}{\text{RMS}[g]}g_t作者还观察到，这样更新的话，其实$\theta$和$\Delta \theta$的单位是不一样的（此时$\Delta \theta$是无量纲数）。所以，作者提出再乘上一个$\text{RMS}[\Delta \theta]$来平衡（同时去掉了学习率$\eta$），所以，最终的参数更新如下： \theta_{t+1} = \theta_t - \frac{\text{RMS}[\Delta \theta]}{\text{RMS}[g]}g_t这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率lr这一参数设定，默认值为$1.0$。代码注释中，我使用MS来指代$E[x^2]$。即，$\text{RMS}[x] = \sqrt{\text{MS}[x]+\epsilon}$。12345678910# update: MS[g] = MS[g]*\rho + g*g*(1-\rho)square_avg.mul_(rho).addcmul_(1 - rho, grad, grad)# current RMS[g] = sqrt(MS[g] + \epsilon)std = square_avg.add(eps).sqrt_()# \Delta \theta = RMS[\Delta \theta] / RMS[g]) * gdelta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)# update parameter: \theta -= lr * \Delta \thetap.data.add_(-group['lr'], delta)# update MS[\Delta \theta] = MS[\Delta \theta] * \rho + \Delta \theta^2 * (1-\rho)acc_delta.mul_(rho).addcmul_(1 - rho, delta, delta) AdamAdaptive momen Estimation（Adam，自适应矩估计），是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。 \begin{aligned}m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t\\v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2\end{aligned}作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正： \begin{aligned}\hat{m} &= \frac{m}{1-\beta_1}\\ \hat{v}&=\frac{v}{1-\beta_2}\end{aligned}参数的更新如下： \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t} + \epsilon}}\hat{m_t}作者给出$\beta_1 = 0.9$，$\beta_2=0.999$，$\epsilon=10^{-8}$。 为了更好地理解PyTorch中的实现方式，需要对上式进行变形： \Delta \theta = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta \frac{m_t}{\sqrt{v_t}}代码中令$\text{step_size} = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta$。同时，$\beta$也要以指数规律衰减，即：$\beta_t = \beta_0^t$。 12345678910111213141516# exp_avg is `m`: expected average of gexp_avg.mul_(beta1).add_(1 - beta1, grad)# exp_avg_sq is `v`: expected average of g's squareexp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)# \sqrt&#123;v_t + \epsilon&#125;denom = exp_avg_sq.sqrt().add_(group['eps'])# 1 - \beta_1^tbias_correction1 = 1 - beta1 ** state['step']# 1 - \beta_2^tbias_correction2 = 1 - beta2 ** state['step']# get step_sizestep_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1# delta = -step_size * m / sqrt(v)p.data.addcdiv_(-step_size, exp_avg, denom) AdaMax上面Adam中，实际上我们是用梯度$g$的$2$范数（$\sqrt{\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。 由于无穷范数就是求绝对值最大的分量，所以这种方法叫做AdaMax。其对应的$\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）： u_t = \beta_2^\infty u_{t-1} + (1-\beta_2^\infty) g_t^\infty我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。 相应的更新权重操作为： \theta_{t+1} = \theta_t -\frac{\eta}{u_t}\hat{m}_t在PyTorch中的实现如下：12345678910111213141516# Update biased first moment estimate, which is \hat&#123;m&#125;_texp_avg.mul_(beta1).add_(1 - beta1, grad)# 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个# Update the exponentially weighted infinity norm.norm_buf = torch.cat([ exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))## beta1 correctionbias_correction = 1 - beta1 ** state['step']clr = group['lr'] / bias_correctionp.data.addcdiv_(-clr, exp_avg, exp_inf)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 7 模板与泛型编程]]></title>
      <url>%2F2017%2F06%2F23%2Feffective-cpp-07%2F</url>
      <content type="text"><![CDATA[模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。 41 了解隐式接口和编译器多态OOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。 而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型T支持操作符&lt;。隐式接口不基于函数的声明，而是由有效表达式组成。 同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。12template &lt;typename T&gt;bool fun(const T&amp; a, const T&amp; b) &#123;return a &lt; b;&#125; 42 了解typename的双重意义typename和class一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：12template &lt;typename/class T&gt;void fun(T&amp; a) &#123;...&#125; 但是有一个地方只能用typename，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：1234567// 打印容器内的第二个元素template &lt;typename C&gt;void print_2nd_element(const C&amp; container) &#123; // 嵌套类型 typename C::const_iterator it(container.begin()); cout &lt;&lt; *++it;&#125; 不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如： 12345678template &lt;typename T&gt;class Derived: public Base&lt;T&gt;::Nested &#123; //即使Nest是嵌套类型，这里也不用typenamepublic: explicit Derived(int x) :Base&lt;T&gt;::Nested(x) &#123; // 成员初始化列表也不用 typename Base&lt;T&gt;::Nested tmp; //这时候要使用 &#125;&#125;; 43 学习处理模板化基类内的名称这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子： 123456789101112131415161718192021222324252627class TypeA &#123;public: void fun();&#125;;class TypeB &#123;public: void fun();&#125;;template &lt;typename Type&gt;class Base &#123;public: void do_something() &#123; Type x; x.fun(); &#125;&#125;;template &lt;typename Type&gt;class Derived: public Base&lt;Type&gt; &#123;public: void do_something_too() &#123; // ... do_something(); // 调用基类的函数，这里无法编译 &#125;&#125;; 这是因为存在如下可能，Base&lt;Tyep&gt;对某种Type进行了特化。12345template &lt;&gt;class Base&lt;TypeC&gt; &#123;public: // 这里没有实现 dom_somthing 函数&#125;; 所以存在这样的可能：class Derived&lt;TypeC&gt;: public Base&lt;TypeC&gt;，而这里面是没有do_something函数的。 为了解决这个问题，有三种办法： 在基类调用方法前面加上this-&gt;。 1234void do_something_too() &#123; // ... this-&gt;do_something(); // 调用基类的函数，这里无法编译&#125; 使用using声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。 12345using Base&lt;Type&gt;::do_something;void do_something_too() &#123; // ... this-&gt;do_something(); // 调用基类的函数，这里无法编译&#125; 明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。 1234void do_something_too() &#123; // ... Base&lt;Type&gt;::do_something(); // 调用基类的函数，这里无法编译&#125; 44 将与参数无关的代码抽离模板由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。 如下所示是一个$N$阶方阵，其中n是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。 12345template &lt;typename T, size_t n&gt;class Matrix &#123;public: void invert();&#125;; 一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。 1234567891011template &lt;typename T&gt;class MatrixBase &#123;protected: MatrixBase(size_t n, T* pMem) // 存储矩阵大小和指针 :size(n), pData(pMem) &#123;&#125; void setDataPtr(T* ptr) &#123;pData = ptr;&#125; // 设置指针 void invert(); // 实现求逆private: size_t size; T* pData;&#125;; 而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用private继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。123456789template &lt;typename T, size_t n&gt;class Matrix: private MatrixBase&lt;T&gt; &#123;public: Matrix(): MatrixBase&lt;T&gt;(n, 0), pData(new T[n*n]) &#123; this-&gt;setDataPtr(pData.get()); // 将指针副本传给基类 &#125;private: boost::scoped_array&lt;T&gt; pData;&#125;; 然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量n是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。 上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上int和long有相同的二进制表述。那么vector&lt;int&gt;和vector&lt;long&gt;的成员函数可能完全相同，也会造成代码膨胀。 在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如vector&lt;int*&gt;, list&lt;const int*&gt;等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即T*）时，你应该让它调用另一个无类型指针void*的函数，由后者完成实际工作。 45 运用成员函数模板接受所有兼容类型使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。12Base* p = new Base;Base* p = new Derived; 12345678910111213// 一个通用的智能指针模板template &lt;typename T&gt;class SmartPointer &#123;public: // 为了兼容类型，需要再引入一个模板参数 U template &lt;typename U&gt; SmartPointer(const SmartPointer&lt;U&gt;&amp; other) // 这里可能会发生指针之间的隐式类型转换 :ptr(other.get()) &#123;...&#125; T* get() const &#123; return ptr; &#125;private: T* ptr;&#125;; 成员函数模板还可以用来作为赋值操作。1234567891011template &lt;typename T&gt;class shared_ptr &#123;public: ... // 接受任意兼容的shared_ptr赋值 template &lt;typename Y&gt; shared_ptr&amp; operator = (shared_ptr&lt;Y&gt; const&amp; r); // 接受任意兼容的auto_ptr赋值 template &lt;typename Y&gt; shared_ptr&amp; operator = (auto_ptr&lt;Y&gt; const&amp; r);&#125;; 不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：1shared_ptr&amp; operator = (shared_ptr const&amp; r); 46 需要类型转换时请为模板定义（friend）非成员函数回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类Rational声明为模板。 12345678910111213template &lt;typename T&gt;class Rational &#123;public: Rational(const T&amp; numerator=0, const T&amp; denominator=1);&#125;;template &lt;typename T&gt;const Rational&lt;T&gt; operator*(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs)&#123;...&#125;Rational&lt;int&gt; onehalf(1, 2);Rational&lt;int&gt; res = onehalf * 2; // 改成模板后便会编译错误！ 这是因为在进行模板类型推导时，并未将2进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。 解决方法是将这个运算符重载函数声明为Rational&lt;T&gt;的友元函数。这样，在onehalf被声明时，Rational&lt;int&gt;类被具现化，则该友元函数也被声明出来了。 然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。123456789101112131415template &lt;typename T&gt; class Rational; // 前向声明template &lt;typename T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123;&#125;;template &lt;typename T&gt;class Rational &#123;public: Rational(const T&amp; numerator=0, const T&amp; denominator=1) &#123; &#125; // ... friend const Rational&lt;T&gt; operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123;return doMultiply(lhs, rhs); &#125;&#125;; 47 使用trait表现类型信息STL中的advance函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。 输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如istream_iterator。 输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如ostream_iterator。 前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。 双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。 随机迭代器。可以随意跳转任意距离，例如vector或原始指针。 为了对它们进行分类，C++有对应的tag标签。12345struct input_iterator_tag &#123;&#125;;struct output_iterator_tag &#123;&#125;;struct forward_iterator_tag: public input_iterator_tag &#123;&#125;;struct bidirectional_iterator_tag: public forward_iterator_tag &#123;&#125;;struct random_access_iterator_tag: public bidirectional_iterator_tag &#123;&#125;; 所以我们可以在advance的代码中，对迭代器的类型进行判断，从而采取不同的操作。trait就是能够让你在编译器获得类型信息。 我们希望trait也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的trait（而且C++11加入了更多），其中针对迭代器的是iterator_traits。 为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过typedef来实现）。如队列的迭代器支持随机访问，则：123456789template &lt;typename T&gt;class deque &#123;public: class iterator &#123; public: typedef random_access_iterator_tag iterator_category; // ... &#125;&#125;; 这样，我们就能在iterator_traits内部通过访问迭代器的iterator_category来获得其类型信息啦~如下所示，iterator_traits只是鹦鹉学舌般地表现IterT说自己是什么。 1234template &lt;typename IterT&gt;struct iterator_traits &#123; typedef typename IterT::iterator_category iterator_category;&#125;; 如何支持原始指针呢？用模板特化就好了~ 1234template &lt;typename T&gt;struct iterator_traits&lt;T*&gt; &#123; typedef random_access_iterator_tag iterator_category;&#125;; 总结起来，如何设计并实现一个traits呢？ 确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。 为该信息取一个名称，如iterator_category 提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。 好了，下面我们可以实现advance了。12345678template &lt;typename IterT, typename DistT&gt;void advance(IterT&amp; iter, DistT d) &#123; if(typeid(typename std::iterator_traits&lt;IterT&gt;::iterator_category == typeid(std::random_access_iterator_tag) &#123; // ... &#125; // ...&#125; 然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的if-else功能。 我们为不同类型的迭代器实现不同的移动方法。1234567891011121314template &lt;typename IterT, typename DistT&gt;void doAdvance(IterT&amp; iter, Dist d, std::random_access_iterator_tag) &#123; iter += d;&#125;// ...其他类型的迭代器对应的 doadvance// 用advance函数包装这些重载函数template &lt;typename Iter, typename DistT&gt;void advance(IterT&amp; iter, Dist d) &#123; doAdvance(iter, d, typename std::iterator_traits&lt;IterT&gt;::iterator_category()); // 注意 typename // 注意传入的是对象实例，所以要 iterator_category()&#125; 也就是说 首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在trait参数。 建立包装函数（包工头），调用上述劳工函数并传递trait信息。 48 认识模板元编程模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。 条款47介绍了选择分支结构如何借由trait实现。这里介绍循环由递归模板具现化实现的方法。 为了生成斐波那契数列，我们首先定义一个模板参数为n的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。 123456789template &lt;unsigned n&gt;struct F &#123; enum &#123;value = n * F&lt;n-1&gt;::value &#125;;&#125;;template &lt;&gt;struct F&lt;0&gt; &#123; enum &#123;value = 1 &#125;;&#125;; TMP博大精深，想要深入学习，还是要参考相关书籍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 6 继承与面向对象设计]]></title>
      <url>%2F2017%2F06%2F17%2Feffective-cpp-06%2F</url>
      <content type="text"><![CDATA[C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？ 真•面向对象编程！ 32 确定public继承塑模出Is-a的关系请把这条规则记在心中：public继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。 不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了fly()这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。 33 避免遮掩继承而来的名称这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。 一种解决办法是使用using声明。如下所示：1234567891011121314151617181920class Base &#123;public: virtual void f1() = 0; void f3(); void f3(double);&#125;;class Derived: public Base &#123;public: using Base::f3; virtual void f1(); void f3();&#125;;Derived d;d.f1(); // 没问题，调用了Derived中的f1d.f3(); // 没问题，调用了Derived中的f3double x;d.f3(x); // 没问题，调用了Base中的f3。// 但是如果没有using声明的话，Base::f3会被冲掉。 34 区分接口继承和实现继承表面上直截了当的public继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明： 12345678class Shape &#123;public: virtual void draw() const = 0; virtual void error(const std::string&amp; msg); int getID() const;&#125;;class Rect: public Shape &#123;...&#125;;class Circle: public Shape &#123;...&#125;; 纯虚函数声明纯虚函数（如draw()函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。不过，你仍然可以给纯虚函数提供函数定义。 虚函数非纯虚函数（如error()函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。 12345678910111213class Base &#123;public: virtual void fun() = 0; // 注意，我们改写成了纯虚函数protected: void default_fun() &#123;...&#125;; // 缺省实现&#125;;// 此时，若想使用缺省实现，就必须显式地调用class Derived: public Base &#123;public: virtual void fun() &#123; default_fun(); &#125;&#125;; 不过这样导致一个多余的default_fun()函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。 123456789101112131415161718192021class Base &#123;public: virtual void fun() = 0;&#125;;// 为纯虚函数提供定义void Base::fun() &#123; // 缺省行为&#125;class Derived: public Base &#123;public: // 显式调用基类的纯虚函数，实现缺省行为 virtual void fun() &#123;Base::fun(); &#125;&#125;;class Derived2: public Base &#123;public: // 实现自定义的行为 virtual void fun() &#123; // ... &#125;&#125;; 非虚函数这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。 35 考虑virtual函数之外的其他选择虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。 在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。1234class GameCharacter &#123;public: virtual int healthValue() const;&#125;; 使用non-virtual interface实现template method模式这种流派主张virtual函数应该几乎总是私有的。较好的设计时将healthValue()函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。 1234567891011class GameCharacter &#123;public: int healthValue() const &#123; // ... 前期准备 int ret_val = doHealthValue(); // ... 后续清理 return ret_val &#125;private: virtual int doHealthValue() const &#123;...&#125;&#125;; 这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写doHealthValue()函数，针对自身的特点计算血量。 使用函数指针实现策略模式上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。 123456789101112131415class GameCharacter; // 前置声明// 计算血量的缺省方法int defaultHealthValue(const GameCharacter&amp;);class GameCharacter &#123;public: typedef int (*HealthCalcFun) (const GameCharacter&amp;); explicit GameCharacter(HealthCalcFun f=defaultHealthValue :healthFunc(f)&#123; // ... &#125;private: HealthCalcFun healthFunc;&#125;; 这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。或者我们可以在运行时候，通过设定healthFunc，来实现动态血量计算方法的变化。 借由std::function实现策略模式作为上面的改进，我们可以使用std::function（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于std::function的大致介绍，可以看这里）。 我们只需将上面的typedef改掉即可。不再使用函数指针，而是更加高级更加通用的std::function。 1typedef std::function&lt;int(const GameCharacter&amp;)&gt; HealthCalcFun; 使用古典的策略模式如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。GameCharacter类中则含有一个指向HealthCalcFun类实例的指针。 12345678910111213141516171819//我们首先定义HealthCalcFunc基类class GameCharacter; // 前向声明class HealthCalcFunc &#123;public: virtual int calc(const GameCharacter&amp; gc) const &#123;...&#125;&#125;;HealthCalcFunc defaultCalcFunc;class GameCharacter &#123;private: HealthCalcFunc* pfun;public: explicit GameCharacter(HealthCalcFunc* p=&amp;defaultCalcFunc): pfun(p) &#123;&#125; int healthValue() const &#123; return pfun-&gt;calc(*this); &#125;&#125;; 该条款给出了虚函数的若干替代方案。 36 绝不重新定义继承而来的非虚函数在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。 如下所示。123456789101112131415class B &#123;public: void mf() &#123;...&#125;&#125;;class D: public B &#123;public: void mf() &#123;...&#125;&#125;;D d;B* pb = &amp;d;D* pd = &amp;d;pb-&gt;mf(); // 调用的是B::mf()pd-&gt;mf(); // 调用的是D::mf() 这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于pb被声明为一个指向B的指针，所以其调用的是B的成员函数mf()。 为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。 37 绝不重新定义继承而来的缺省参数值由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。 静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。123// Circle是Shape的派生类Shape* ps;Shape* pc = new Circle; // 静态类型都是Shape 动态类型是指当前所指对象的类型。就上例来说，pc的动态类型是Circle*，而ps没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。1ps = new Circle; // 现在ps的动态类型是Circle* 虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。 不过遵守此项条款，有时又会造成不便。看下例： 12345678910class Shape &#123;public: enum ShapeColor &#123;RED, GREEN&#125;; virtual void draw(ShapeColor c=RED) const=0;&#125;;class Circle: public Shape &#123;public: virtual void draw(ShapeColor c=RED) const;&#125;; 第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成GREEN为默认参数，需要在基类和派生类中同时修改。 一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。 1234567891011121314class Shape &#123;public: void draw(ShapeColor c=RED) const &#123; doDraw(c); // 调用私有的虚函数 &#125;private: //真正的工作在此完成 virtual void doDraw(ShapeColor c) const = 0; &#125;;class Circle: public Shape &#123;private: virtual void doDraw(ShapeColor c) const; // 派生类重写这个真正的实现&#125;; 38 通过复合塑模has-a或“根据某物实现出”复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。 39 明智而审慎地使用private继承私有继承意味着条款38中的“根据某物实现出”。例如D私有继承自B，不是说D是某种B，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。B的每样东西在D中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用B来实现D的功能而已。 但是复合也能达到相同的效果啊~我在D中加入一个B的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。 40 明智而审慎地使用多重继承使用多重继承有可能造成歧义。例如，C继承自A和B，而两个基类中都含有成员函数mf()。那么当d.mf()的时候，究竟是在调用哪个呢？你必须明确地指出,d.A::mf()。 使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将File定为虚基类，所有直接继承自它的类采用虚继承。 1234class File &#123;...&#125;;class InputFile: virtual public File &#123;...&#125;;class OutputFile: virtual public File &#123;...&#125;;class IOFile: public InputFile, public OutputFile &#123;...&#125;; 从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。 所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface） 附注 std::function的基本使用std::function的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为std::function。如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;functional&gt;#include &lt;iostream&gt;struct Foo &#123; Foo(int num) : num_(num) &#123;&#125; void print_add(int i) const &#123; std::cout &lt;&lt; num_+i &lt;&lt; '\n'; &#125; int num_;&#125;;void print_num(int i)&#123; std::cout &lt;&lt; i &lt;&lt; '\n';&#125;struct PrintNum &#123; void operator()(int i) const &#123; std::cout &lt;&lt; i &lt;&lt; '\n'; &#125;&#125;;int main()&#123; // store a free function // 函数指针 std::function&lt;void(int)&gt; f_display = print_num; f_display(-9); // lambda表达式 // store a lambda std::function&lt;void()&gt; f_display_42 = []() &#123; print_num(42); &#125;; f_display_42(); // store the result of a call to std::bind // 绑定之后的函数对象 std::function&lt;void()&gt; f_display_31337 = std::bind(print_num, 31337); f_display_31337(); // store a call to a member function // 类中的成员函数，第一个参数为类实例的const reference std::function&lt;void(const Foo&amp;, int)&gt; f_add_display = &amp;Foo::print_add; const Foo foo(314159); f_add_display(foo, 1); f_add_display(314159, 1); // store a call to a data member accessor std::function&lt;int(Foo const&amp;)&gt; f_num = &amp;Foo::num_; std::cout &lt;&lt; "num_: " &lt;&lt; f_num(foo) &lt;&lt; '\n'; // store a call to a member function and object using std::placeholders::_1; std::function&lt;void(int)&gt; f_add_display2 = std::bind( &amp;Foo::print_add, foo, _1 ); f_add_display2(2); // store a call to a member function and object ptr std::function&lt;void(int)&gt; f_add_display3 = std::bind( &amp;Foo::print_add, &amp;foo, _1 ); f_add_display3(3); // store a call to a function object std::function&lt;void(int)&gt; f_display_obj = PrintNum(); f_display_obj(18);&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Silver RL课程 - DP Planning]]></title>
      <url>%2F2017%2F06%2F06%2Fsilver-rl-dp%2F</url>
      <content type="text"><![CDATA[上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。 动态规划动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件： 最优子结构。能够分解为若干子问题。 子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。 MDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\prime$，而在考虑状态$s^\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。 迭代策略估计给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）： v_{k+1}(s) = \sum_{a\in \mathcal{A}}\pi(a|s)(R_s^a+\gamma\sum_{s^\prime \in \mathcal{S}} P_{ss^\prime}^av_k(s^\prime))我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\in \mathcal{S}$，使用上式利用上轮中的$v(s^\prime)$更新$v_{k+1}(s)$，直到收敛。 给出下面的算例。$4\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\pi(s)$。 这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npv = [0 for _ in xrange(16)]line1 = range(1, 4)line4 = range(12, 15)col1 = [4, 8, 12]col4 = [3, 7, 11]# the environment simulatordef get_new_loc(idx, action): if idx == 0 or idx == 15: ret = idx reward = 0 return ret, reward if action == 0: # up if idx in line1: ret = idx else: ret = idx-4 elif action == 1: # down if idx in line4: ret = idx else: ret = idx+4 elif action == 2: # left if idx in col1: ret = idx else: ret = idx-1 elif action == 3: # right if idx in col4: ret = idx else: ret = idx+1 reward = -1 return ret, rewardgamma = 1.K = [1, 2, 3, 10, 100]for k in xrange(1, 101): # in each iteration, update v(s) via: # v(s) = \sum_a \pi(a|s) + \gamma \sum_s^\prime P_&#123;ss^\prime&#125;^a v(s^\prime) v_aux = v[:] for i in xrange(16): v_aux[i] = 0. for action in range(4): j, r = get_new_loc(i, action) v_aux[i] += 0.25*(r+gamma*v[j]) v = v_aux if k in K: print 'k = &#123;&#125; '.format(k), print ', '.join(map(lambda x: '&#123;:.1f&#125;'.format(x), v)) 策略的改进评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示： \pi^\prime = arg\max_{a\in \mathcal{A}}q_\pi(s,a)以上小节中给出的算例为例，最终值函数结果为： 那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。 为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。 当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。 值迭代首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\prime$，都有，该策略对$s^\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。 v^\ast(s) = \max_{a\in \mathcal{A}}R_s^a+\gamma\sum_{s^\prime\in \mathcal{S}}P_{ss^\prime}^av^\ast(s^\prime)通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）： 每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。 注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。 异步DP上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\prime$的旧值来计算$s$的新值。如下图所示：上面讨论了同步迭代的三个主要问题： 我们也可以使用异步方法。主要包括以下三种： 就地（in-place） DP就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\text{new}}(s^\orime)$来更新$v(s)$。 带有优先级的状态扫描（Prioritized Sweeping）根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即 |\max\_{a\in A}(R\_s^a+\gamma\sum\_{s^\prime\in S}P\_{ss^\prime}^a v(s^\prime)-v(s)|实现的具体细节如下： 实时DP使用智能体与环境交互的经验（experience）来挑选状态。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Silver RL课程 - MDP]]></title>
      <url>%2F2017%2F05%2F31%2Fsilver-rl-mdp%2F</url>
      <content type="text"><![CDATA[Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。 马尔科夫性质不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立： P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \dots, S_t]定义状态转移概率（State Transition Probability）如下： P_{ss^\prime} = P[S_{t+1}=s^\prime|S_t=s]前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。 下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。 马尔科夫过程马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中， $S$是一个有限状态集合。 $P$是状态转移矩阵，定义如上。 马尔科夫奖赏过程马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \gamma)$来表示。其中， $R$代表奖励函数，$R_s = E[R_{t+1}|S_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。 $\gamma$是折旧率（discount），$\gamma \in [0,1]$ 定义回报（Return）为当前时刻往后得到的折旧总奖励，即： G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}折旧率的引入，有以下几点考虑： 在有环存在的马氏过程中，避免了无穷大回报的出现。 未来的不确定性对当前的影响较小。 事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。 人类行为倾向于即时奖励。 如果马氏过程是存在终止的，有的时候也可以使用$\gamma=1$，也就是不打折。 值函数值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下： v(s) = E[G_t|S_t=s]值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示： 最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面： \begin{aligned} E[G_{t+1}|S_t = s] &= \sum_{s^\prime\in S}E[G_{t+1}|S_{t+1}=s^\prime]P(S_{t+1}=s^\prime|S_t=s)\\ &=\sum_{s^\prime}v(S_{t+1}=s^\prime)P(S_{t+1}=s^\prime|S_t=s)\\ &=E[v(S_{t+1})|S_t=s] \end{aligned}上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\prime$过程中收到的奖励$r$，和从新的状态$s^\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。 把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。 v(s) = R_s + \gamma\sum_{s^\prime \in S}P_{ss^\prime}v(s^\prime)或者写成下面的矩阵形式，更加紧凑： v = R+\gamma Pv 当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下： 对于含有$n$个状态的系统，求解复杂度是$\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有： 动态规划 DP 蒙特卡洛仿真（Monte-Carlo evaluation） 时间差分学习（Temporal Difference Learning） 马尔科夫决策过程马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\gamma)$描述。其中， $A$是一个有限决策集合。 $P_{ss^\prime}^a = P(S_{t+1}=s^\prime|S_t=s, A_t=a)$是状态转移概率矩阵。 $R_{s}^a = E[R_{t+1}|S_t=s, A_t=a]$是奖励函数（与动作也挂钩） 策略策略（Policy）$\pi$是指在给定状态情况下，采取动作的概率分布，如下： \pi(a|s)=P(A_t=a|S_t=s)对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。 当我们给定一个MDP和相应的策略$\pi$时，状态转移过程$S_1,S_2,\dots$是一个马氏过程$(S, P^\pi)$（上标$\pi$表示$P$由$\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\dots$是一个马氏奖赏过程$(S, P^\pi, R^\pi, \gamma)$。具体来说，如下（就是全概率公式）： \begin{aligned} P_{ss^\prime}^\pi &= \sum_{a\in A}\pi(a|s)P_{ss^\prime}^a\\ R_s^\pi &=\sum_{a\in A}\pi(a|s)R_s^a \end{aligned}值函数MDP的值函数$v_\pi(s)$是指在当前状态$s$出发，使用策略$\pi$得到的回报期望，即， v_\pi(s) = E_\pi[G_t|S_t=s]引入“动作-值”函数（action-value function）$q_\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\pi$得到的回报期望，即， q_\pi(s,a) = E_\pi[G_t|S_t=s,A_t=a]贝尔曼方程两者的关系如下如所示（通过全概率公式联系）： 注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\prime$的值函数$v(s^\prime)$之间的关系。同样是一个全概率公式： 综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\prime)$的关系： 同样，相邻时刻Q函数的关系： 写成紧凑的矩阵形式： v_\pi = R^\pi + \gamma P^\pi v_\pi这个方程的解是： v_\pi = (1-\gamma P^\pi)^{-1}R^\pi和上面对策略函数的分解类似，我们有下面两式成立： v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(sS_{t+1})]最优值函数最优的值函数是指在所有的策略中，使得$v_\pi(s)$取得最大值的那个，即： v_\ast(s) = \max_\pi v_\pi(s)最优Q函数的定义同理： q_\ast(s,a) = \max_\pi q_\pi(s,a)定义策略$\pi$集合上的一个偏序为 \pi > \pi^\prime \quad \text{if} \quad v_\pi(s) > v_{\pi^\prime}(s), \forall s如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是： 同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。 常用的求解方法包括： 值迭代（Value Iteration） 策略迭代（Policy Iteration） Q Learning Sarsa]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 02]]></title>
      <url>%2F2017%2F05%2F25%2Fhinton-nnml-02%2F</url>
      <content type="text"><![CDATA[这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。 Different neural network archsFeed-forward neural network 前馈神经网络前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。 Recurrent networkRNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。 RNN常用来对序列进行建模（modeling squence）。这里有一篇不错的介绍。 Symmetrically connected network这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。 Percetron训练最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是： 将原始输入向量转化为feature activation。 寻找合适的权重对feature进行加权，得到某个标量。 如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。 对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。 所以，感知机的数学模型可以描述如下： y=\begin{cases}1, \quad \text{if} \quad \sum w_ix_i+b>0 \\ 0 \quad \text{otherwise}\end{cases}训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法： 若预测值与真实值相同，则不作调整； 否则，若错输出为$0$，则将输入的$x$加到权重$w$上去； 若错输出为1，则从权重$w$上将输入的$x$减去。 当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。 原理下面从几何角度分析一下感知机。 权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。 让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。 当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。 所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。 下面用刚才的这种思考方式证明上述训练方法的正确性。 首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。 我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。 我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。） 每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。） 感知机的局限感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu Cannot Mount exfat格式硬盘的解决办法]]></title>
      <url>%2F2017%2F05%2F04%2Fubuntu-cannot-mount-exfat-disk%2F</url>
      <content type="text"><![CDATA[我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示unable to mount。这篇文章是对解决办法的记录。 解决方法参见页面，运行以下命令： 123456$ sudo -i # 获取root权限# apt-get update# apt-get install --reinstall exfat-fuse exfat-utils# mkdir -p /media/user/exfat# chmod -Rf 777 /media/user/exfat# fdisk -l 之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。 在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 01]]></title>
      <url>%2F2017%2F05%2F03%2Fhinton-nnml-01%2F</url>
      <content type="text"><![CDATA[Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。 Why do we need ML?从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。 What are neural network?为什么要研究神经网络？ 理解人脑机理的一个途径； 受到神经系统启发的并行计算 新的学习算法来解决现实问题（本课所关心的） （这里总结的不是很科学，勉强概括了讲义的内容）神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。 神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。 大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。 而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。 Simple models of different neurons我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。 Linear neuron顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示： y = b+\sum_{i=1}^{n}w_ix_iBinary threshold neuron这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述： y = \begin{cases}1 \quad \text{if} \quad z\ge \theta\\ 0\quad \text{otherwise}\end{cases}其中，$z$是输入信号的线性组合，$z=\sum_{i}w_ix_i$ 或者， y = \begin{cases}1 \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\sum_{i}w_ix_i$ Rectified linear neuron和上面的二值化神经元对比，有： y = \begin{cases}z \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}Sigmoid neuron这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示： y = \frac{1}{1+\exp(-z)} 由于Logistic函数将$(-\infty, +\infty)$的值压缩为S型，所以得名Sigmoid。 Stochastic binary neuron这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是： P(y=1) = \frac{1}{1+\exp(-z)}对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。 Three types of learning即有监督学习，无监督学习和强化学习。 有监督学习：给定输入向量，预测输出。 无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。 强化学习：学习如何决策达到最大期望奖赏。 有监督学习有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。 所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。 强化学习在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。 强化学习的难点如下： 奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。 奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。 无监督学习无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-光流估计]]></title>
      <url>%2F2017%2F05%2F03%2Fcs131-opticalflow%2F</url>
      <content type="text"><![CDATA[光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。 是你让我的世界从那刻变成粉红色 划掉。。。 光流的计算光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。 光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设： 前后两帧点的位移不大（泰勒展开） 外界光强保持恒定。 空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开） 其中第二条外界光强保持恒定，可以从下面的等式来理解。 在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下： I(x,y,t-1) = I(x+u, y+v, t)从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。 I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t上面两式联立，可以得到， I_xu+I_yv+I_t=0上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。 这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。 上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法 L-K方法上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\times 5$的时候，建立的方程。 使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。 上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？ $\mathbf{A}^\dagger \mathbf{A}$是可逆的。 $\mathbf{A}^\dagger \mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\lambda_1$, $\lambda_2$不应该太小。 $\mathbf{A}^\dagger \mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\lambda_1/\lambda_2$不应该太大。 而我们在Harris角点检测的时候已经讨论过$\mathbf{A}^\dagger \mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。 下面这张图就是当时的讨论结果。 上面就是使用L-K方法估计光流的一般思路。 金字塔方法在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。 作业：基于光流法的帧间插值问题描述假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0&lt;t&lt;1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。 简单粗暴法我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说： I_t = (1-t)I_0+tI_1这种方法称为”cross-fading”。效果如下。可以看到有较多的模糊抖动。 基于光流法使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示： I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)用MATLAB实现如下：1234567for y =1:height for x = 1:width dy = min(max(round(y+v0(y,x)*t), 1), height); dx = min(max(round(x+u0(y,x)*t), 1), width); img(dy,dx,:) = img0(y,x,:); endend 改进上面的式子假定光流不变，但是实际上光流很可能也是改变的。考虑到光流的变化，下式的计算应该更加接近： I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))然而，我们并不能得到$u_t$和$v_t$的准确值。我们使用下面的方法近似求解。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 5 实现]]></title>
      <url>%2F2017%2F05%2F01%2Feffective-cpp-05%2F</url>
      <content type="text"><![CDATA[第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。 26 尽可能延后变量定义式的出现时间变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。 27 尽量少做转型动作第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。 第二，效率问题。这里展开说。 C++中的转型动作有以下三种： (T)expression，C时代的风格 T(expression)，函数风格 新式风格，包括static_cast,const_cast,dynamic_cast和reinterpret_cast四种。 作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。 对于 const_cast，用于脱掉某对象的const属性。 对于static_cast，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将double类型转换为int等。 对于dynamic_cast，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，dynamic_cast的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。 很多时候，之所以使用dynamic_cast是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。 对于reinterpret_cast，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为int，此转换是在bit层面的转换，更详细的信息可以参见cpp reference的介绍。 在它们之中，reinterpret_cast和const_cast完全是编译器层面的东西。static_cast会导致编译器生成对应CPU指令，但是是在编译器就能决定的。dynamic_cast是在运行时多态的一种手段。 28 避免返回指向对象内部成分的句柄（handle）当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为const，但是实际上并不能避免通过此句柄修改资源的情况发生。 例如在自定义的string类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合const约定的。 所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回const handle&amp;。 即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。 29 为“异常安全”努力是值得的异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的delete操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。 让我们先看一个不满足异常安全的函数例子：12345678// 自定义`Menu`类中修改背景图片的成员函数void Menu::changeBg(std::istream&amp; imgSrc) &#123; lock(&amp;this-&gt;mutex); // 互斥锁 1 delete this-&gt;bg; // 释放本已有的bg2 ++this-&gt;imgChangeCnt; // 计数器 3 bg = new Image(imgSrc); // 新图片 4 unlock(&amp;this-&gt;mutex); // 释放锁 5&#125; 上面的代码存在以下问题，使得它不满足异常安全性。 资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。 数据被破坏。还是上面的情况，则bg此时的资源已经被析构，而且计数器的值也增加了。· 解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。 我们将异常安全分为以下三类： 基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟bg是何值） 强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。 在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。 绝不抛出异常。作用于内置类型（int或指针等）身上的所有操作都提供了nothrow保证。 30 透彻了解内联的方方面面inline函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用inline替换函数宏定义。 然而，inline不要乱用。首先，inline会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。 另一方面，inline只是对编译器的申请，不是真的一定内联。 inline函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入inline关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。 有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。 作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。 31 将文件间的编译依存关系降至最低C++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。 首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。 另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。 从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。 使用这种思虑，定义的包含有Date类型对象（指明这个人的生日）的Person类如下： 1234567891011121314151617181920212223#include &lt;string&gt; // for string#include &lt;memory&gt; // for shared_ptrclass PersonImpl; // Person实现类的前置声明class Date; // Person接口用到的类的前置声明class Person &#123;public: Person(const std::string&amp; name, const Date&amp; birthday); std::string name() const;private: std::shared_ptr&lt;PersonImpl&gt; pImpl; // 指向实现类&#125;;/*****************实现文件****************/#include "Person.h"#include "PersonImpl.h"Person::Person(const std::string&amp; name, const Date&amp; birthday): pImpl(new PersonImpl(name, birthday)) &#123;&#125;std::string Person::name() const &#123; return pImpl-&gt;name();&#125; 在上面的代码中，通过构造handle类PersonImpl，在Person中我们只需要前置声明Date，而无需包含头文件date.hpp。这样，即使Date或者Person有修改，影响也仅限于Date的实现文件和PersonImpl而已，不会传导到Person和使用了Person的其他代码文件。通过这种做法，实际上Person成为了一个单纯的接口，具体的实现在PersonImpl中完成，实现了“接口与实现的分离”。 综上： 如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。 如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。 为声明式和定义式提供不同的头文件（Person本身和PersonImpl）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。 除了上面的方法，也可以将Person定义为抽象基类（Caffe中的Layer就是类似的模式）。为了达成这一目标，Person需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示： 12345class Person &#123;public: virtual ~Person(); virtual string name() const = 0;&#125;; 客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为static。 12345class Person &#123;public: static shared_ptr&lt;Person&gt; create(const string&amp; name, const Date&amp; birthday);// ... 刚才的其他代码&#125;; 当然，要想使用，我们还必须定义派生类实现相应的接口。 123456789class RealPerson: public Person &#123;public: RealPerson(const string&amp; name, const Date&amp; birthday): name(name), birthDate(birthday) &#123;&#125; virtual ~RealPerson() &#123;&#125; string name() const &#123; return this-&gt;name; &#125;private: string name; Date birthDate;&#125;; 上面的工厂函数create()的实现： 123shared_ptr&lt;Person&gt; Person::create(const string&amp; name, const Date&amp; date) &#123; return shared_ptr&lt;Person&gt;(new RealPerson(name, date));&#125; 实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。 当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 4 设计与声明]]></title>
      <url>%2F2017%2F04%2F29%2Feffective-cpp-04%2F</url>
      <content type="text"><![CDATA[良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。 18 让接口容易被使用，不易被误用首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入const限定修饰符。 其次，尽量使接口与内建类型等保持一致。例如STL中统一使用size()方法获取容器的大小。 任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。 19 设计class犹如设计type设计自定义的class要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑： 新的对象如何创建和销毁？这关系到构造和析构函数。 对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。 新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。 什么是新类型的合法值？可能需要对setter()函数进行参数检查。 新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。 新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。 什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。 什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。 谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为friend。 什么事新类型的未声明接口？ 新类型有多一般化？是否建立template class更好？ 新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。 20 宁以pass-by-reference-to-const替换pass-by-value对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。 总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。 21 必须返回对象时，不要妄想返回其reference函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！ C++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。 22 将成员变量声明为private封装，封装，还是封装！ 而且，请记住，其实只有两种访问权限：private（提供了封装）和其他（包括protected，不提供封装）。 23 宁以non-member和non-friend函数替换member函数对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三： 封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。 代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。 编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。 24 若所有参数均需要类型转换，请为此采用non-member函数作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为explicit，可以完成整形到有理数类的隐式类型转换。 重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：123456class Rational &#123;// ...public: Rational(int numerator=0, int denominator=1); const Rational operator*(const Rational&amp; rhs) const;&#125;; 然而，这样做的话，auto res = 2*Rational(4,5)就无法通过编译，因为int并没有实现operator*(const Rational&amp;)操作。 更好的方法是将其作为non-member函数，123const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; //...&#125; 25 考虑写出一个不抛出异常的swap()函数这一条款更像是模板特化规则的大杂烩。 STL中的swap()函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：12345678namespace std &#123;template &lt;typename T&gt;void swap(T&amp; a, T&amp;b) &#123; T tmp(a); a = b; b = tmp;&#125;&#125; 但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。 如何对我们的对象Widget实现特化？ 如果Widget不是模板类，那么我们需要进行全特化。加入以下：123456namespace std &#123;template &lt;&gt;void swap&lt;Widget&gt;(Widget&amp; a, Widget&amp; b) &#123; swap(pImpl, b.pImpl);&#125;&#125; 更好的解决方法是先将swap()定义为Widget类的公共成员函数，然后再全特化标准库的swap()方法时调用。这样与STL的约定保持一致。STL中vector等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了std名字空间的swap()方法。 当Widget是模板类时，需要进行偏特化。也许看上去是这样：123456namespace std &#123;template &lt;typename T&gt;void swap(Widget&lt;T&gt;&amp; a, Widget&lt;T&gt;&amp; b) &#123; a.swap(b);&#125;&#125; 但是程序员可以全特化std中的模板，却不能加入新的类或函数进入std中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。 所以，可以在Widget存在的名字空间内定义swap()（而不是加入std），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则： 一般使用标准库中的swap()即可。 如果自己实现，首先提供一个public的swap()成员函数，注意这儿函数决不能抛出异常。 在类或者模板在的名字空间中提供一个non-member的swap()函数，并令它调用上述的swap()成员函数。 如果是类，而不是模板，那么特化std::swap()，并令它调用上述swap()成员函数。 在客户端代码调用swap()时，确定包含一个using声明式，以便让std::swap()在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用swap()。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP阅读 - Chapter 3 资源管理]]></title>
      <url>%2F2017%2F04%2F25%2Feffective-cpp-03%2F</url>
      <content type="text"><![CDATA[C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。 13 以对象管理资源书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。 本条款可以（较浅显地）归纳为： 不要使用裸指针！要使用智能指针！ 我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。 许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了weak_ptr和shared_ptr，请使用它们。如果没有C++11，请使用boost）。 以对象管理资源的两个关键想法： 获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！ 管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。 14 在资源管理类中小心coping行为有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。 如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。 12345678910// Lock是互斥锁的资源管理类class Lock &#123;public: explicit Lock(Mutex* pm):mutexPtr(pm) &#123; // 获得资源 lock(mutexPtr); &#125; ~Lock() &#123;unlock(mutexPtr); &#125; //释放资源private: Mutex* mutexPtr;&#125;; 这样，我们期望能够使用Lock对象实现对互斥锁的自动管理。 123456Mutex m; // 互斥锁// ...&#123;Lock ml(&amp;m);// ...&#125; // 在区块外，ml自动析构，实现解锁 然而，如何处理Lock对象的拷贝？ 情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。 情景二，对底层资源进行引用计数。也许我们可以利用shared_ptr，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看shared_ptr部分文档。 情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。 情景四，移除底层资源所有权。将所有权移至新的对象。 15 在资源管理类中提供对原始资源的访问许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。 对于这种情况，智能指针提供了get()函数用来获取其原始指针的拷贝。同时，它们也重载了-&gt;和*操作符，允许隐式转换为原始指针。 我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型T可以通过定义operator T()实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。 16 成对使用new和delete时采取相同的形式这项条款是说如果动态分配内存时候使用了new T()得到了单个对象的内存空间，那么销毁时应该使用delete销毁；如果当初使用了new T[]得到了对象数组空间，那么销毁时应该使用delete []。两者不能混用，否则会导致未定义行为。 另外，除非必要，不要使用原始数组。STL中的vector和string是替代数组的不错选择。 17 以独立语句将newed对象置于智能指针以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。 书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。 如下面的函数声明： 12int priority() &#123; /*some code*/&#125;void process(shared_ptr&lt;Widget&gt; pw, int priority) &#123; /*some code*/&#125; 在使用时，也许你会这样调用process函数。 1process(new Widget(), priority()); 首先，这样是不能通过编译器的。因为shared_ptr的构造函数是explicit的，不能够隐式将原始指针转换为shared_ptr对象。但是改为下面的代码就没问题了吗？ 1process(shared_ptr&lt;Widget&gt;(new Widget()), priority()); 由于C++中函数参数的核算顺序是不确定的，所以可能发生： new出来一个Widget资源 调用priority()函数，注意此时可能引发异常，使得Widget资源无法回收 构造shared_ptr对象 问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。 12auto pw = shared_ptr&lt;Widget&gt;(new Widget());process(pw, priority());]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 2 构造拷贝和赋值函数]]></title>
      <url>%2F2017%2F04%2F24%2Feffective-cpp-02%2F</url>
      <content type="text"><![CDATA[在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。 05 了解C++默默编写并调用了哪些函数C++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。 在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。 在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。 06 若不想使用编译器自动生成的函数，那就明确拒绝有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为private，并且不提供函数的实现。 07 为多态基类声明虚析构函数多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。 而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为virtual也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。 由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下： 那些有虚函数的类，几乎确定都应该有一个虚析构函数。 对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括vector，string等。 有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。 12345class ABC &#123;public: virtual ~ABC() = 0; &#125;;ABC::~ABC() &#123;&#125; 这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用~ABC()，所以必须为这个函数提供一份定义。 08 别让异常逃离析构函数如果在析构函数中抛出异常，会导致未定义的行为。 析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。 09 绝不在构造函数和析构函数中调用虚函数你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。 为什么呢？ 如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。 直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。 10 令operator=返回一个*this的引用为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。 所谓连锁赋值，是指下面这样的情况： 1a = b = c; 这一条款不仅适用于赋值运算符，也适用于+=等。 1234567class A &#123;public: A&amp; operator=(const A&amp; rhs) &#123; // ... return *this; &#125;&#125;; 11 在operator=中处理自我赋值所谓自我赋值，是指： 123Widget w;// ...w = w; 自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。 一种方法是进行“证同测试”，如下： 1234A&amp; operator=(const A&amp; rhs) &#123; if(&amp;rhs == this) return *this; //证同测试 // ...&#125; 不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。 还有一种方法，使用copy-swap方法，首先拷贝构造一个rhs的拷贝，然后交换该拷贝和*this， 毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。 12 复制对象时勿忘每一个成分这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子： 12345678910class Derived: public Base&#123;private: int a;public: Derived(const Derived&amp; rhs):a(rhs.a) &#123;&#125; Derived&amp; operator=(const Derived&amp; rhs) &#123; a = rhs.a; return *this; &#125;&#125;; 上面的代码只是拷贝了派生类新加入的成员a，而对基类中已有的成员未作处理。要记住， 任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为private，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为： 123456Derived(const Derived&amp; rhs):Base(rhs), a(rhs.a) &#123;&#125;Derived&amp; operator=(const Derived&amp; rhs) &#123; Base::operator=(rhs); a = rhs.a; return *this;&#125; 此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的init()函数是一个更好的选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python中的迭代器和生成器]]></title>
      <url>%2F2017%2F04%2F21%2Fpython-iter-generator%2F</url>
      <content type="text"><![CDATA[在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程Iterators &amp; Generators。 迭代器使用for循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。 12for i in range(100): # do something 100 times 在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的key。 同样，也有很多函数接受的参数为可迭代对象。例如list()和tuple()，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如 1list(&#123;'x':1, 'y':2&#125;) # =&gt; ['x', 'y'] 为什么list或者str这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用for x in XXX这种方法来遍历了呢？ 在Python中，有内建的函数iter()和next()。一般用法时，iter()方法接受一个可迭代对象，会调用这个对象的__iter__()方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现next()方法（Python3中改为了__next__()方法）。 如下面的例子，yrange_iter是yrange的一个迭代器。yrange实现了__iter__()方法，是一个可迭代对象。调用iter(yrange object)的结果就是返回一个yrange_iter的对象实例。 1234567891011121314151617# Version 1.0 使用迭代器类class yrange_iter(object): def __init__(self, yrange): self.n = yrange.n self.i = 0 def next(self): v = self.i self.i += 1 return vclass yrange(object): def __init__(self, n): self.n = n def __iter__(self): return yrange_iter(self)print type(iter(yrange(5))) # &lt;class '__main__.yrange_iter'&gt; 而不停地调用迭代器的next()方法，就能够不断输出迭代序列。如下所示： 12345678910In [3]: yiter = iter(yrange(5))In [4]: yiter.next()Out[4]: 0In [5]: yiter.next()Out[5]: 1In [6]: yiter.next()Out[6]: 2 其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉yrange_iter，直接让yrange.__iter__()方法返回其自身实例。这样，我们只需要在yrange类中实现__iter__()方法和next()方法即可。如下所示： 12345678910111213141516171819202122# Version2.0 简化版，迭代器是本身class yrange(object): def __init__(self, n): self.n = n def __iter__(self): self.i = 0 return self def next(self): v = self.i self.i += 1 return vIn [8]: yiter = iter(yrange(5))In [9]: yiter.next()Out[9]: 0In [10]: yiter.next()Out[10]: 1In [11]: yiter.next()Out[11]: 2 然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是self.n实际上并没有用到。如果我只想产生0到10以内的序列呢？ 我们只需要加入判断条件，当超出序列边界时，抛出Python内建的StopIteration异常即可。 12345678910111213141516# Version3.0 加入边界判断，生成有限长度序列class yrange(object): def __init__(self, n): self.n = n def __iter__(self): self.i = 0 return self def next(self): if self.i == self.n: raise StopIteration v = self.i self.i += 1 return vfor i in yrange(5): print i Problem 1Write an iterator class reverse_iter, that takes a list and iterates it from the reverse direction. 1234567891011class reverse_iter(object): def __init__(self, alist): self.container = alist self.i = len(alist) def next(self): if self.i == 0: raise StopIteration self.i -= 1 return self.container[self.i]it = reverse_iter([1, 2, 3, 4]) 生成器生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的yield语句。此外，生成器函数是懒惰求值，只有当调用next()方法时，生成器才开始顺序执行，直到遇到yield语句。yield语句就像return，但是并未退出，而是打上断电，等待下一次next()方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def foo(): print "begin" for i in range(3): print "before yield", i yield i print "after yield", i print "end"&gt;&gt;&gt; f = foo()&gt;&gt;&gt; f.next()beginbefore yield 00&gt;&gt;&gt; f.next()after yield 0before yield 11&gt;&gt;&gt; f.next()after yield 1before yield 22&gt;&gt;&gt; f.next()after yield 2endTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 生成器表达式生成器表达式和列表相似，将[]换为()即可。如下所示： 123for i in (x**2 for x in [1,2,3,4]): print i# print 1 4 9 16 生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。 下面的代码使用生成器得到前10组勾股数。通过在调用take()方法时修改传入实参n的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边x的取值为$[0, \infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上break配合才可以达到相同的效果。 1234567891011121314151617181920212223def integer(start, end=None): """Generate integer sequence [start, end) If `end` is not given, then [start, \infty] """ i = start while True: if end is not None and i == end: raise StopIteration yield i i += 1def take(n, g): i = 0 while True: if i &lt; n: yield g.next() i += 1 else: raise StopIteration# 假定 x&gt;y&gt;z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6tup = ((x,y,z) for x in integer(0) for y in integer(0, x) for z in integer(0, y) if x*x==y*y+z*z)list(take(10, tup)) Problem 2Write a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters. 12345678910111213141516def readfiles(filenames): for f in filenames: for line in open(f): yield linedef grep(lines): return (line for line in lines if len(line)&gt;40)def printlines(lines): for line in lines: print line,def main(filenames): lines = readfiles(filenames) lines = grep(lines) printlines(lines) Problem 3Write a function findfiles that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree. 注意get_all_file()方法中递归中生成器的写法，见SO的这个帖子。 1234567891011121314import osdef generate_all_file(root): for item in os.listdir(root): item = os.path.join(root, item) if os.path.isfile(item): yield os.path.abspath(item) else: for item in generate_all_file(item): yield itemdef findfiles(root): for item in generate_all_file(root): print item Problem 4Write a function to compute the number of python files (.py extension) in a specified directory recursively. 1234def generate_all_py_file(root): return (file for file in generate_all_file(root) if os.path.splitext(file)[-1] == '.py')print len(list(generate_all_py_file('./'))) Problem 5Write a function to compute the total number of lines of code in all python files in the specified directory recursively. 123def generate_all_line(root): return (line for f in generate_all_py_file(root) for line in open(f))print len(list(generate_all_line('./'))) Problem 6Write a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively. 1234def generate_all_no_empty_and_comment_line(root): return (line for line in generate_all_line(root) if not (line=='' or line.startswith('#')))print len(list(generate_all_no_empty_and_comment_line('./'))) Problem 7Write a program split.py, that takes an integer n and a filename as command line arguments and splits the file into multiple small files with each having n lines. 1234567891011121314151617def get_numbered_line(filename): i = 0 for line in open(filename): yield i, line i += 1def split(file_name, n): i = 0 f = open('output-%d.txt' %i, 'w') for idx, line in get_numbered_line(file_name): f.write(line) if (idx+1) % n == 0: f.close() i += 1 f = open('output-%d.txt' %i, 'w') f.close() Problem 9The built-in function enumerate takes an iteratable and returns an iterator over pairs (index, value) for each value in the source. Write a function my_enumerate that works like enumerate. 1234567def my_enumerate(iterable): i = 0 seq = iter(iterable) while True: val = seq.next() yield i, val i += 1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 1 让自己习惯C++]]></title>
      <url>%2F2017%2F04%2F20%2Feffective-cpp-01%2F</url>
      <content type="text"><![CDATA[本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。 01 将C++视作语言联邦C++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。 本条款中，将C++概括为四个次语言组成的联邦： 传统C：面向过程，也规定了C++基本的语法。 OOP：面向对象，带类的C，加入了继承，虚函数等概念。 Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。 STL：标准模板库。使用STL要遵守它的约定。 想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。 02 尽量使用const, enum, inline替换 #define（以编译器替换预处理器）#define是C时代遗留下来的预编译指令。 当#define用来定义某个常量时，通常const是一个更好的选择。 12#define PI 3.14const double PI = 3.14; 当此常量为整数类型（int, char, bool）等时，也可以使用enum定义常量。这种做法常常用在模板元编程中。 1enum &#123;K = 1&#125;; 对于const常量，你可以获取变量的地址，但是对于enum来说，无法获取变量的地址。对于这一点来说，enum和#define相类似。 另一种可能使用#define的场景是宏定义。这种情形可以使用inline声明内联函数解决。 总之，尽可能相信编译器的力量。使用#define将遮蔽编译器的视野，带来奇怪的问题。 03 尽可能使用constconst不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。 所以，如果某一变量满足const的要求，那么请加上const，和编译器签订一份契约，保护你的行为。 这里不再讨论const的寻常用法。提示一下：当修饰指针变量时，const在星号左边，是指指针所指物是常量；当const在星号右边，是指指针本身是常量。如下所示： 123456const int* p = &amp;a;*p = 5; // 非法p = &amp;b; // 合法int* const p = &amp;a;p = &amp;b; // 非法*p = 5; // 合法 STL中，如果声明某个迭代器为const，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用const_iterator。 const更丰富的用法是用于函数声明中， 当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误： 1if (fun(a, b) = c) // 这里错把 == 打成了 = 当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。 当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。 这种情况下，可能会有const重载现象。 12345678class my_string&#123; const char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125; char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125;&#125;; 实际调用时，根据调用该函数的对象是否是const的来决定究竟调用哪个版本。 上面的实现未免过于复杂，我们还可以改成下面的形式： 123456789class my_string&#123; const char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125; char&amp; operator[](size_t pos) &#123; return const_cast&lt;char&amp;&gt;( static_cast&lt;const my_string&amp;&gt;(*this)[pos]); &#125;&#125;; 注意上面的代码进行了两次类型转换。由non-const reference转为const reference是类型安全的，使用static_cast进行。最后我们要脱掉const char&amp;的const属性，使用了const_cast。 对于const成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为mutable。 04 确保对象在使用前已经被初始化使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。 对于内建类型，需要手动初始化。 对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下： 1234public A(name, age) &#123; this-&gt;name = name; // 这是赋值，不是初始化！ this-&gt;age = age;&#125; 如果在类A的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用copy-construct实现。 需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。 讨论完上述情况，再来看一种特殊变量：不同编译单元non-local static变量，是指不在某个函数scope下的static变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe中的底层数学计算函数]]></title>
      <url>%2F2017%2F03%2F08%2Fmathfunctions-in-caffe%2F</url>
      <content type="text"><![CDATA[Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对mathfunction.hpp 文件中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。 BLAS介绍以下内容参考BLAS wiki页面整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。 BLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。 不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如AMD的ACML（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。 在BLAS中，实现了矩阵与矩阵相乘的函数gemm（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数gemv，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。 可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。 对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章Why GEMM is at the heart of deep learning的分析。上面的图也都来源于这篇博客。 矩阵运算函数矩阵运算函数在文件math_functions.hpp中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。 矩阵与矩阵，矩阵与向量的乘法函数caffe_cpu_gemm()是对BLAS中矩阵与矩阵相乘函数gemm的包装。与之对应的caffe_cpu_gemv()是对矩阵与向量相乘gemv函数的包装。以前者为例，其实现代码如下： 12345678910template&lt;&gt;void caffe_cpu_gemm&lt;float&gt;(const CBLAS_TRANSPOSE TransA, const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K, const float alpha, const float* A, const float* B, const float beta, float* C) &#123; int lda = (TransA == CblasNoTrans) ? K : M; int ldb = (TransB == CblasNoTrans) ? N : K; cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, N);&#125; 可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的cblas_sgemm()函数。其功能是计算C = alpha * A * B + beta * C。参数的具体含义可以查看BLAS的相关文档。 矩阵/向量的加减下面的函数都是将X指针所指的数据作为src，将Y指针所指的数据为dst。同时，第一个参数统一是向量的长度。 caffe_axpy(N, alpha, x, mutable y)实现向量加法Y = alpha * X + Y。 caffe_axpby(N, alpha, x, beta, mutable y)实现向量加法Y = alpha * X + beta * Y 这两个函数的用法可以参见欧氏距离loss函数中的梯度计算： 1234567caffe_cpu_axpby( bottom[i]-&gt;count(), // count alpha, // alpha diff_.cpu_data(), // a Dtype(0), // beta bottom[i]-&gt;mutable_cpu_diff()); // b&#125; 其中，bottom[i]-&gt;count()给定了blob的大小，也就是向量的长度。alpha实际是由顶层top_blob传来的loss_weight，也即是*top_blob-&gt;cpu_diff()/batch_size。由于是直接将加权后的diff直接赋给bottom_blob的cpu_diff，所以，将beta赋值为0。 内存相关和C语言中的memset()和memcpy()类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似： caffe_copy(N, x, mutable y)实现向量拷贝。源地址和目标地址服从上小节的约定。 caffe_set(N, alpha, mutable x)实现向量的置位，将向量分量填充为值alpha。 查看其实现可以知道，这里Caffe中直接调用了memset()完成任务。123456789101112131415template &lt;typename Dtype&gt;void caffe_set(const int N, const Dtype alpha, Dtype* Y) &#123; if (alpha == 0) &#123; memset(Y, 0, sizeof(Dtype) * N); // NOLINT(caffe/alt_fn) return; &#125; for (int i = 0; i &lt; N; ++i) &#123; Y[i] = alpha; &#125;&#125;// 模板的特化template void caffe_set&lt;int&gt;(const int N, const int alpha, int* Y);template void caffe_set&lt;float&gt;(const int N, const float alpha, float* Y);template void caffe_set&lt;double&gt;(const int N, const double alpha, double* Y); 而caffe_copy()中则是直接实现了CPU和GPU的功能。注意到下面代码中调用cudaMemcpy()的时候，使用了参数cudaMemcpyDefault。通过查阅文档，这个变量的含义是cudaMemcpyDefault: Default based unified virtual address space。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。 123456789101112131415template &lt;typename Dtype&gt;void caffe_copy(const int N, const Dtype* X, Dtype* Y) &#123; if (X != Y) &#123; if (Caffe::mode() == Caffe::GPU) &#123;#ifndef CPU_ONLY // NOLINT_NEXT_LINE(caffe/alt_fn) CUDA_CHECK(cudaMemcpy(Y, X, sizeof(Dtype) * N, cudaMemcpyDefault));#else NO_GPU;#endif &#125; else &#123; memcpy(Y, X, sizeof(Dtype) * N); // NOLINT(caffe/alt_fn) &#125; &#125;&#125; 所谓的unified virtual address（UVA）就是下图这个意思（见P2P&amp;UVA）。 有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示： 其使用条件如下： 向量逐元素运算 caffe_add(N, a, b, y)函数实现Y[i] = a[i] + b[i]。 caffe_sub, caffe_div, caffe_mul同理。 caffe_exp, caffe_powx, caffe_abs, caffe_sqr, caffe_log相似，这里只将caffe_exp()的实现复制如下： 12345// 又是模板特化template &lt;&gt;void caffe_exp&lt;float&gt;(const int n, const float* a, float* y) &#123; vsExp(n, a, y); // 返回 y[i] = exp(a[i])&#125; caffe_scal实现向量的数乘。这个函数常常用在loss_layer中计算反传的梯度，常常要乘上一个标量loss_weight。 caffe_add_scalar实现向量每个分量与标量相加。 GPU版本相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了math_functions.hpp中，而相应的实现代码在math_functions.cu中。 随机数产生器Caffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。 这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数caffe_rng_distribution_name即可。 参考资料【1】seven-first 的博客]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Residual Net论文阅读 - Identity Mapping in Deep Residual Networks]]></title>
      <url>%2F2017%2F03%2F07%2Fresidualnet-paper2-identitymapping%2F</url>
      <content type="text"><![CDATA[这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。 从残差到残差在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出，$\mathcal{F}$为残差函数。 y_l = h(x_l) + \mathcal{F}(x_l, W_l)x_{l+1} = f(y_l)在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即， h(x_l) = x_l在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。 作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。 从右图可以看到，本单元的输入$x_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y_l$，直接和$x_l$相加，得到该层的输出$x_{l+1}$。 利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。 Identity Mapping: Why Always me?作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。 在使用了这种新结构之后，我们有， x_{l+1} = x_l + \mathcal{F}(x_l, W_l)如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则： x_L = x_l +\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i)从上面的式子可以看出， 深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\mathcal{F}$。 我们有，$x_L=x_0+\sum_{i=0}^{L-1}\mathcal{F}(x_i, W_i)$，而普通网络$x_L$和$x_l$的关系比较复杂，$x_L = \prod_{i=0}^{L-1}W_ix_0$。看上去，前者的优化应该更加简单。 计算bp的时候，有， \frac{\partial \epsilon}{\partial x_l} = \frac{\partial \epsilon} {\partial x_L}\frac{\partial x_L}{\partial x_l} = \frac{\partial \epsilon}{\partial x_L}(1+\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i))上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。 如果不做Identity Mapping，而是乘上一个系数$\lambda$呢？作者发现这会在上面的式子上出现$\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。 所以，Identity Mapping是坠吼的！ 花式跑实验论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO网络参数的解析与存储]]></title>
      <url>%2F2017%2F03%2F06%2Fyolo-cfg-parser%2F</url>
      <content type="text"><![CDATA[YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。 YOLO网络结构定义的CFG文件YOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。 CFG文件的格式可以归纳如下（可以打开某个CFG文件进行对照）：123456789[net]# 这里会对net的参数进行配置# 同时YOLO将对net的求解器的参数也放在了这里[conv]# 一些conv层的参数描述[maxpool]# 一些池化层的参数描述# 顺序堆叠的其他layer描述 在Darknet的代码中，将每个[]符号导引的参数列表叫做section。 网络结构解析器 Parser具体的解析实现参见parser.c文件。我们先以convolutional_layer parse_convolutional(list *options, size_params params)函数为例，看一下Darknet是如何完成对卷积层参数的解析的。 从函数签名可以看出，这个函数接受一个list的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而size_params类型的变量params指示了该层上一层的参数情况，其具体定义如下：12345678910typedef struct size_params&#123; int batch; int inputs; int h; int w; int c; int index; int time_steps; network net;&#125; size_params; 这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。 进入函数内部，会发现频繁出现option_find_int这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。 我们首先来看一下这个函数的定义吧~这个函数并不在parser.c中，而是在option_list.c 文件中。 12345678910111213// l: data pointer to the list// key: the key to find, example: "filters", "padding"// def: default valueint option_find_int(list *l, char *key, int def)&#123; // 去找到该key对应的数值，使用atoi转换为整型数 char *v = option_find(l, key); if(v) return atoi(v); // 使用XXX_quiet版本可以不打印此信息 fprintf(stderr, "%s: Using default '%d'\n", key, def); // 没有找到key，返回默认值 return def;&#125; 而其中的option_find函数则是逐项顺序查找，匹配字符串来实现的。 12345678910111213char *option_find(list *l, char *key)&#123; node *n = l-&gt;front; while(n)&#123; kvp *p = (kvp *)n-&gt;val; if(strcmp(p-&gt;key, key) == 0)&#123; p-&gt;used = 1; return p-&gt;val; &#125; n = n-&gt;next; &#125; return 0;&#125; 构建conv层由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。 123456789101112131415161718192021222324 // 首先得到参数 int n = option_find_int(options, "filters",1); int size = option_find_int(options, "size",1); int stride = option_find_int(options, "stride",1); int pad = option_find_int_quiet(options, "pad",0); int padding = option_find_int_quiet(options, "padding",0); if(pad) padding = size/2;// 激活函数是通过匹配其名称的方法得到的 char *activation_s = option_find_str(options, "activation", "logistic"); ACTIVATION activation = get_activation(activation_s); // 通过上层的信息得到batch size，做参数检查 int batch,h,w,c; h = params.h; w = params.w; c = params.c; batch=params.batch; if(!(h &amp;&amp; w &amp;&amp; c)) error("Layer before convolutional layer must output image."); int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0); int binary = option_find_int_quiet(options, "binary", 0); int xnor = option_find_int_quiet(options, "xnor", 0); // 调用初始化函数 convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam); layer.flipped = option_find_int_quiet(options, "flipped", 0); layer.dot = option_find_float_quiet(options, "dot", 0); 所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。 构建网络有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。 Darknet中对应的函数为network parse_network_cfg(char *filename)，这个函数接受文件名为参数，进行网络结构的解析。 首先，调用read_cfg(filename)得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是[net] section，要特殊对待。这里不再多说了。 保存参数信息Darknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示： 12345678910111213141516171819202122232425void save_convolutional_weights(layer l, FILE *fp)&#123; if(l.binary)&#123; //save_convolutional_weights_binary(l, fp); //return; &#125;#ifdef GPU if(gpu_index &gt;= 0)&#123; pull_convolutional_layer(l); &#125;#endif int num = l.n*l.c*l.size*l.size; fwrite(l.biases, sizeof(float), l.n, fp); // 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论 if (l.batch_normalize)&#123; fwrite(l.scales, sizeof(float), l.n, fp); fwrite(l.rolling_mean, sizeof(float), l.n, fp); fwrite(l.rolling_variance, sizeof(float), l.n, fp); &#125; fwrite(l.weights, sizeof(float), num, fp); if(l.adam)&#123; fwrite(l.m, sizeof(float), num, fp); fwrite(l.v, sizeof(float), num, fp); &#125;&#125; 在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Residual Net论文阅读 - Deep Residual Learning for Image Recongnition]]></title>
      <url>%2F2017%2F03%2F05%2Fresidualnet-paper%2F</url>
      <content type="text"><![CDATA[Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。 更深的网络 -&gt; 更好的性能在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。 在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？ Is learning better networks as easy as stacking more layers? 很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。 另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。 个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。 残差单元也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。 使用残差单元时，我们不再让网络去直接学映射$\mathcal{H}(x)$，而是学习映射$\mathcal{F}(x) = \mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。 应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。 残差学习从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\mathcal{H}$，而是这个映射和输入的残差$\mathcal{F}$。这里叫做残差学习（Residual Learning）。 神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。 上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。 y = \mathcal{F(x,\lbrace W_i\rbrace)}+x上面的式子要求$\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。 同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\mathcal{F}$只剩一个线性映射可以学了。。。）。 ImageNet实验和比较由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。 这里着重介绍作者在ImageNet上的实验结果。 作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。 另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。 同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。 方案A使用zero-padding的方法 方案B使用乘上权重矩阵的方法 方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式 实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。 Bottleneck结构为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\times 1$的卷积核，来进行channel的变形。通过前面的$1\times 1$卷积核，将channel降下来。和$3\times 3$卷积核作用后，再用最后的$1\times 1$卷积核升上去。 使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。 附录在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[toy demo - PyTorch + MNIST]]></title>
      <url>%2F2017%2F03%2F04%2Fpytorch-mnist-example%2F</url>
      <content type="text"><![CDATA[本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。 加载MNIST数据集PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。MNIST是torchvision.datasets包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将download参数设置为True，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过root传入即可。 在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在torchvision.transforms包中找到对应的操作。在下面的代码中，通过使用transforms.Compose()，我们构造了对数据进行预处理的复合操作序列，ToTensor负责将PIL图像转换为Tensor数据（RGB通道从[0, 255]范围变为[0, 1]）， Normalize负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入tuple。 之后，我们通过DataLoader返回一个数据集上的可迭代对象。一会我们通过for循环，就可以遍历数据集了。 1234567891011121314151617181920212223import torchimport torch.nn as nnfrom torch.autograd import Variableimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torch.nn.functional as Fimport torch.optim as optimtrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)test_set = dset.MNIST(root=root, train=False, transform=trans)batch_size = 128kwargs = &#123;'num_workers': 1, 'pin_memory': True&#125;train_loader = torch.utils.data.DataLoader( dataset=train_set, batch_size=batch_size, shuffle=True, **kwargs)test_loader = torch.utils.data.DataLoader( dataset=test_set, batch_size=batch_size, shuffle=False, **kwargs) 网络构建在进行网络构建时，主要通过torch.nn包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。nn.Linear负责构建全连接层，需要提供输入和输出的通道数，也就是y = wx+b中x和y的维度。 1234567891011121314class MLPNet(nn.Module): def __init__(self): super(MLPNet, self).__init__() self.fc1 = nn.Linear(28*28, 500) self.fc2 = nn.Linear(500, 256) self.fc3 = nn.Linear(256, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) loss = self.ceriation(x, target) return x, loss 由于PyTorch可以实现自动求导，所以我们只需实现forward过程即可。这里由于池化层和非线性变换都没有参数，所以使用了nn.functionals中的对应操作实现。通过看文档，可以发现，一般nn里面的各种层，都会在nn.functionals里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。 1234# With square kernels and equal stridefilters = autograd.Variable(torch.randn(8,4,3,3))inputs = autograd.Variable(torch.randn(1,4,5,5))F.conv2d(inputs, filters, padding=1) 同样地，我们可以实现LeNet的结构如下。 12345678910111213141516171819202122232425262728293031323334353637class MLPNet(nn.Module): def __init__(self): super(MLPNet, self).__init__() self.fc1 = nn.Linear(28*28, 500) self.fc2 = nn.Linear(500, 256) self.fc3 = nn.Linear(256, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = F.relu(self.fc3(x)) loss = self.ceriation(x, target) return x, loss def name(self): return 'mlpnet'class LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5, 1) self.conv2 = nn.Conv2d(20, 50, 5, 1) self.fc1 = nn.Linear(4*4*50, 500) self.fc2 = nn.Linear(500, 10) self.ceriation = nn.CrossEntropyLoss() def forward(self, x, target): x = self.conv1(x) x = F.max_pool2d(x, 2, 2) x = F.relu(x) x = self.conv2(x) x = F.max_pool2d(x, 2, 2) x = F.relu(x) x = x.view(-1, 4*4*50) x = self.fc1(x) x = self.fc2(x) loss = self.ceriation(x, target) return x, loss 训练与测试在训练时，我们首先应确定优化方法。这里我们使用带动量的SGD方法。下面代码中的optim.SGD初始化需要接受网络中待优化的Parameter列表（或是迭代器），以及学习率lr，动量momentum。 1optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。 123456789101112131415161718192021model = MLPNet().cuda() # 以MLP为例for epoch in xrange(10): # trainning for batch_idx, (x, target) in enumerate(train_loader): optimizer.zero_grad() #每次都要清空上一步中参数的grad，否则会出错的~ x, target = Variable(x.cuda()), Variable(target.cuda()) _, loss = model(x, target) #得到loss loss.backward() #bp optimizer.step() #优化器迭代 if batch_idx % 100 == 0: print '==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'.format(epoch, batch_idx, loss.data[0]) # testing correct_cnt, ave_loss = 0, 0 for batch_idx, (x, target) in enumerate(test_loader): x, target = Variable(x.cuda(), volatile=True), Variable(target.cuda(), volatile=True) score, loss = model(x, target) _, pred_label = torch.max(score.data, 1) correct_cnt += (pred_label == target.data).sum() ave_loss += loss.data[0] accuracy = correct_cnt*1.0/len(test_loader)/batch_size ave_loss /= len(test_loader) 当优化完毕后，需要保存模型。这里官方文档给出了推荐的方法，如下所示：123torch.save(model.state_dict(), PATH) #保存网络参数the_model = TheModelClass(*args, **kwargs)the_model.load_state_dict(torch.load(PATH)) #读取网络参数 该博客的完整代码可以见：PyTorch MNIST demo。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[远程登录Jupyter笔记本]]></title>
      <url>%2F2017%2F02%2F26%2Fjupyternotebook-remote-useage%2F</url>
      <content type="text"><![CDATA[Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用jupyter notebook命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。 配置jupter notebook登录远程服务器后，使用如下命令生成配置文件。 1jupyter notebook --generate-config 并对其内容进行修改。我主要修改了两处地方： c.NotebookApp.ip=&#39;*&#39;，即不限制ip访问 c.NotebookApp.password = u&#39;hash_value&#39; 上面的hash_value是由用户给定的密码生成的。可以使用ipython中的命令轻松搞定。 12345from notebook.auth import passwdpasswd()"""这里会要求用户输入密码并确认，之后生成的hash值就是要填写到上面的""" 启动notebook之后，在远程服务器上启动笔记本jupyter notebook。接着，在本地机器上访问远程服务器ip:8888（默认端口为8888，也可以在配置文件中修改），输入密码即可访问远程笔记本。 本篇内容参考自博客远程访问jupyter notebook。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyTorch简介]]></title>
      <url>%2F2017%2F02%2F25%2Fpytorch-tutor-01%2F</url>
      <content type="text"><![CDATA[这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于GitHub repo。 PyTorch简介PyTorch是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的numpy，另一方面，PyTorch也是强大的深度学习框架。 目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的prototxt进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。 TensorsTensor，即numpy中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的Tensor可以与numpy中的array很方便地进行互相转换。 通过Tensor(shape)便可以创建所需要大小的tensor。如下所示。 12345x = torch.Tensor(5, 3) # construct a 5x3 matrix, uninitialized# 或者随机填充y = torch.rand(5, 3) # construct a randomly initialized matrix# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuplex.size() # out: torch.Size([5, 3]) PyTorch中已经实现了很多常用的op，如下所示。 1234567891011121314151617# addition: syntax 1x + y # out: [torch.FloatTensor of size 5x3]# addition: syntax 2torch.add(x, y) # 或者使用torch包中的显式的op名称# addition: giving an output tensorresult = torch.Tensor(5, 3) # 预先定义sizetorch.add(x, y, out=result) # 结果被填充到变量result# 对于加法运算，其实没必要这么复杂out = x + y # 无需预先定义size# torch包中带有下划线的op说明是就地进行的，如下所示# addition: in-placey.add_(x) # 将x加到y上# 其他的例子: x.copy_(y), x.t_(). PyTorch中的元素索引方式和numpy相同。 12# standard numpy-like indexing with all bells and whistlesx[:,1] # out: [torch.FloatTensor of size 5] 对于更多的op，可以参见PyTorch的文档页面。 Tensor可以和numpy中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。 1234567891011121314151617# Tensor 转为 np.arraya = torch.ones(5) # out: [torch.FloatTensor of size 5]# 使用 numpy方法即可实现转换b = a.numpy() # out: array([ 1., 1., 1., 1., 1.], dtype=float32)# 注意！a的值的变化同样引起b的变化a.add_(1)print(a)print(b) # a b的值都变成2# np.array 转为Tensorimport numpy as npa = np.ones(5)# 使用torch.from_numpy即可实现转换b = torch.from_numpy(a) # out: [torch.DoubleTensor of size 5]np.add(a, 1, out=a)print(a)print(b) # a b的值都变为2 PyTorch中使用GPU计算很简单，通过调用.cuda()方法，很容易实现GPU支持。 123456# let us run this cell only if CUDA is availableif torch.cuda.is_available(): print('cuda is avaliable') x = x.cuda() y = y.cuda() x + y # 在GPU上进行计算 Neural Network说完了数据类型Tensor，下一步便是如何实现一个神经网络。首先，对自动求导做一说明。 我们需要关注的是autograd.Variable。这个东西包装了Tensor。一旦你完成了计算，就可以使用.backward()方法自动得到（以该Variable为叶子节点的那个）网络中参数的梯度。Variable有一个名叫data的字段，可以通过它获得被包装起来的那个原始的Tensor数据。同时，使用grad字段，可以获取梯度（也是一个Variable）。 Variable是计算图的节点，同时Function实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个Variable有一个creator的字段，表明了它是由哪个Function创建的（除了用户自己显式创建的那些，这时候creator是None）。 当进行反向传播计算梯度时，如果Variable是标量（比如最终的loss是欧氏距离或者交叉熵），那么backward()函数不需要参数。然而如果Variable有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和Variableshape匹配的Tensor）。看下面的说明代码。 1234567891011121314151617181920212223242526272829from torch.autograd import Variablex = Variable(torch.ones(2, 2), requires_grad = True)x # x 包装了一个2x2的Tensor"""Variable containing: 1 1 1 1[torch.FloatTensor of size 2x2]"""# Variable进行计算# y was created as a result of an operation,# so it has a creatory = x + 2y.creator # out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;z = y * y * 3 out = z.mean() # out: Variable containing: 27 [torch.FloatTensor of size 1]# let's backprop nowout.backward() # 其实相当于 out.backward(torch.Tensor([1.0]))# print gradients d(out)/dxx.grad"""Variable containing: 4.5000 4.5000 4.5000 4.5000[torch.FloatTensor of size 2x2]""" 下面的代码就是结果不是标量，而是普通的Tensor的例子。12345678910111213141516171819# 也可以通过Tensor显式地创建Variablex = torch.randn(3)x = Variable(x, requires_grad = True)# 一个更复杂的 op例子y = x * 2while y.data.norm() &lt; 1000: y = y * 2# 计算 dy/dxgradients = torch.FloatTensor([0.1, 1.0, 0.0001])y.backward(gradients)x.grad"""Variable containing: 204.8000 2048.0000 0.2048[torch.FloatTensor of size 3]""" 说完了NN的构成元素Variable，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了torch.nn包。我们自定义的网络结构是由若干的layer组成的，我们将其设置为 nn.Module的子类，只要使用方法forward(input)就可以返回网络的output。下面的代码展示了如何建立一个包含有conv和max-pooling和fc层的简单CNN网络。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import torch.nn as nn # 以我的理解，貌似有参数的都在nn里面import torch.nn.functional as F # 没有参数的（如pooling和relu）都在functional里面？class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel self.conv2 = nn.Conv2d(6, 16, 5) # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。 # 所以fc层的第一个参数是 16x5x5 self.fc1 = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构 # 同时，我们无需实现 backward，这是被自动求导实现的 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number x = x.view(-1, self.num_flat_features(x)) # 把它拉直 x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features# 实例化Net对象net = Net()net # 给出了网络结构"""Net ( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear (400 -&gt; 120) (fc2): Linear (120 -&gt; 84) (fc3): Linear (84 -&gt; 10))""" 我们可以列出网络中的所有参数。 1234params = list(net.parameters())print(len(params)) # out: 10, 5个权重，5个biasprint(params[0].size()) # conv1's weight out: torch.Size([6, 1, 5, 5])print(params[1].size()) # conv1's bias, out: torch.Size([6]) 给出网络的输入，得到网络的输出。并进行反向传播梯度。 1234input = Variable(torch.randn(1, 1, 32, 32))out = net(input) # 重载了()运算符？net.zero_grad() # bp前，把所有参数的grad buffer清零out.backward(torch.randn(1, 10)) 注意一点，torch.nn只支持mini-batch。所以如果你的输入只有一个样例的时候，使用input.unsqueeze(0)人为给它加上一个维度，让它变成一个4-D的Tensor。 网络训练给定target和网络的output，就可以计算loss函数了。在torch.nn中已经实现好了一些loss函数。 1234567891011output = net(input)target = Variable(torch.range(1, 10)) # a dummy target, for example# 使用平均平方误差，即欧几里得距离criterion = nn.MSELoss()loss = criterion(output, target)loss"""Variable containing: 38.6049[torch.FloatTensor of size 1]""" 网络的整体结构如下所示。 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 我们可以使用previous_functions来获得该节点前面Function的信息。 123456789# For illustration, let us follow a few steps backwardprint(loss.creator) # MSELossprint(loss.creator.previous_functions[0][0]) # Linearprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU&quot;&quot;&quot;&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;&quot;&quot;&quot; 进行反向传播后，让我们查看一下参数的变化。 1234567# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.net.zero_grad() # zeroes the gradient buffers of all parametersprint('conv1.bias.grad before backward')print(net.conv1.bias.grad)loss.backward()print('conv1.bias.grad after backward')print(net.conv1.bias.grad) 计算梯度后，自然需要更新参数了。简单的方法可以自己手写： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 不过，torch.optim中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。 123456789import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr = 0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 数据载入由于PyTorch的Python接口和np.array之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了torchvision包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。 123456789101112131415161718192021import torchvisionimport torchvision.transforms as transforms# The output of torchvision datasets are PILImage images of range [0, 1].# We transform them to Tensors of normalized range [-1, 1]# Compose: Composes several transforms together.# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transformstransform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) # torchvision.transforms.Normalize(mean, std)# 读取CIFAR10数据集 trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)# 使用DataLoadertrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)# Test集，设置train = Falsetestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 接下来，我们对上面部分的CNN网络进行小修，设置第一个conv层接受3通道的输入。并使用交叉熵定义loss。 1234567891011121314151617181920212223class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2,2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16*5*5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()# use a Classification Cross-Entropy losscriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。 12345678910111213141516171819202122232425for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # wrap them in Variable inputs, labels = Variable(inputs), Variable(labels) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.data[0] if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0print('Finished Training') 我们在测试集上选取一个mini-batch（也就是4张，见上面testloader的定义），进行测试。 1234567891011dataiter = iter(testloader)images, labels = dataiter.next() # 得到image和对应的labeloutputs = net(Variable(images))# the outputs are energies for the 10 classes.# Higher the energy for a class, the more the network# thinks that the image is of the particular class# So, let's get the index of the highest energy_, predicted = torch.max(outputs.data, 1) # 找出分数最高的对应的channel，即为top-1类别print('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4))) 测试一下整个测试集合上的表现。 12345678910correct = 0total = 0for data in testloader: # 每一个test mini-batch images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum()print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) 对哪一类的预测精度更高呢？ 1234567891011class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i] class_total[label] += 1 上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用.cuda()方法就行了。 1net.cuda() 不过记得在每次训练测试的迭代中，images和label也要传送到GPU上才可以。 1inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) 更多的例子和教程更多的例子更多的教程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Caffe中使用Baidu warpctc实现CTC Loss的计算]]></title>
      <url>%2F2017%2F02%2F22%2Fwarpctc-caffe%2F</url>
      <content type="text"><![CDATA[CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的warp-ctc，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的项目页面。本文介绍内容的相关代码可以参见我的GitHub项目warpctc-caffe 移植warp-ctc本节介绍了如何将warp-ctc的源码在Caffe中进行编译。 首先，我们将warp-ctc的项目代码从GitHub上clone下来。在Caffe的include/caffe和src/caffe下分别创建名为3rdparty的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。 由于warp-ctc中使用了C++11的相关技术，所以需要修改Caffe的Makefile文件，添加C++11支持，可以参见Makefile。 对Caffe的修改就是这么简单，之后我们需要修改warp-ctc中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。 warp-ctc提供了CPU多线程的计算，这里我直接将相应的openmp并行化语句删掉了。 另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为cuh，这样才能够通过编译。否则编译器会给出找不到__host__和__device__等等关键字的错误。 对于详细的修改配置，还请参见GitHub相应的代码文件。 实现CTC Loss计算编译没有问题后，我们可以编写ctc_loss_layer实现CTC Loss的计算。在实现时，注意参考文件ctc.h。这个文件中给出了使用warp-ctc进行CTC Loss计算的全部API接口。 ctc_loss_layer继承自loss_layer，主要是前向和反向计算的实现。由于warp-ctc中只对单精度浮点数float进行支持，所以，对于双精度网络参数，直接将其设置为NOT_IMPLEMENTED，如下所示。 1234567891011template &lt;&gt;void CtcLossLayer&lt;double&gt;::Forward_cpu( const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;double&gt;*&gt;&amp; top) &#123; NOT_IMPLEMENTED;&#125;template &lt;&gt;void CtcLossLayer&lt;double&gt;::Backward_cpu(const vector&lt;Blob&lt;double&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom) &#123; NOT_IMPLEMENTED;&#125; 使用warp-ctc相关接口进行CTC Loss计算的步骤如下： 设置ctcOptions，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。 调用get_workspace_size()函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。 调用compute_ctc_loss()函数，计算loss和gradient。 其中，在第三步中计算gradient时，可以直接将对应blob的cpu/gpu_diff指针传入，作为gradient。 这部分的实现代码分别位于include/caffe/layers和src/caffe/layers/下。 验证码数字识别本部分相关代码位于examples/warpctc文件夹下。实验方案如下。 使用Python中的capycha进行包含0-9数字的验证码图片的产生，图片中数字个数从1到MAX_LEN不等。 使用10作为blank_label，将所有的标签序列在后面补blank_label以达到同样的长度MAX_LEN。 将图像的每一列看做一个time step，网络模型使用image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss，简单粗暴。 模型训练过程中，数据输入使用HDF5格式。 数据产生使用captcha生成验证码图片。这里是一个简单的API demo。默认生成的图片大小为160x60。我们将其长宽缩小一半，使用80x30的彩色图片作为输入。 使用python中的h5py模块生成HDF5格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。 LSTM的输入在Caffe中已经有了lstm_layer的实现。lstm_layer要求输入的序列blob为TxNx...，也就是说我们需要将输入的image进行转置。 Caffe中Batch的内存布局顺序为NxCxHxW。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了liuwei的SSD工作中实现的permute_layer进行转置，将W维度放到最前方。与之对应的参数定义如下： 123456789101112layer &#123; name: &quot;permuted_data&quot; type: &quot;Permute&quot; bottom: &quot;data&quot; top: &quot;permuted_data&quot; permute_param &#123; order: 3 # W order: 0 # N order: 1 # C order: 2 # H &#125;&#125; 另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为ContinuationIndicator的layer，产生对应的time indicator序列。 训练在某次试验中，迭代50,000次，实验过程中的损失函数变化如下： 在验证集上的精度变化如下： 最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。 至于该实验的具体细节，可以参考repo的相关具体代码实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-MeanShift]]></title>
      <url>%2F2017%2F02%2F12%2Fcs131-mean-shift%2F</url>
      <content type="text"><![CDATA[MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。 MeanShift是一种用来寻找特征空间内模态的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。 核密度估计上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了这篇博客和这篇讲义。 注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\mathbb{R}^n\rightarrow \mathbb{R}$的函数满足以下条件，就能将其作为核函数。 比如高斯核函数： K(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{x^2}{2\sigma^2})核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。 f(x) = \frac{1}{nh^d}\sum_{i=1}^{n}K(\frac{x-x_i}{h})如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。 K(x) = c_{k,d}k(\Arrowvert x\Arrowvert ^2)mean shift向量那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\prime(s)$。 观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\Arrowvert x\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。 m_h(x) = \frac{\sum_{i=1}^{n}x_i g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}{\sum_{i=1}^{n}g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}-x所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？ 算法流程所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。 123456789101112131415161718192021222324252627282930313233343536373839%% generate datamu = [1 2];Sigma = [1 0; 0 2]; R = chol(Sigma);N = 250;data = repmat(mu, N, 1) + randn(N, 2)*R;figurehold onscatter(data(:, 1), data(:, 2), 50, 'filled');%% meanshiftmu0 = rand(1,2) * 5;mu = mean_shift(mu0, 10, data);function out = gaussian_kernel(x, sigma)% gauss kernel, g(x) = \exp(-x^2/2\sigma^2)out = exp(-x.*x/(2*sigma*sigma));endfunction mu = mean_shift(mu0, h, data)% implementation of meanshift algorithm% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \frac&#123;\sum_i=1^n xg&#125;&#123;\sum_i=1^n g&#125;mu = mu0;sigma = 1; % parameter for gaussian kernel functionfor iter = 1:20 fprintf('iter = %d, mu = [%f, %f]\n', iter, mu(1), mu(2)); scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled'); offset = bsxfun(@minus, mu, data); % offset = x-x_i dis = sum(offset.^2, 2); % dis = ||x-x_i||^2 x = data(dis &lt; h, :); % neighborhood with bandwidth = h g = gaussian_kernel(offset(dis &lt; h), sigma); xg = x.*g; mu_prev = mu; mu = sum(xg, 1) / sum(g, 1); if norm(mu_prev - mu, 2) &lt; 1E-2 break; end plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);endscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');end 同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。 K(x) = \frac{1}{e^x+e^{-x}+2}123function out = logistic_kernel(x)out = 1./(exp(x) + exp(-x) + 2);end]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Ubuntu14.04构建Caffe]]></title>
      <url>%2F2017%2F02%2F09%2Fbuild-caffe-ubuntu%2F</url>
      <content type="text"><![CDATA[Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。 修改Makefile.config当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).USE_CUDNN := 1 # 这里我们使用cudnn加速# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3# OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.# 这里可以去掉sm_20和21，因为实在是已经太老了# 如果保留的话，编译时nvcc会给出警告CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=sm_50 \ -gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS := atlas# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.# 这里我们使用AnacondaANACONDA_HOME := $(HOME)/anaconda2 PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ $(ANACONDA_HOME)/include/python2.7 \ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include# Uncomment to use Python 3 (default is Python 2)# PYTHON_LIBRARIES := boost_python3 python3.5m# PYTHON_INCLUDE := /usr/include/python3.5m \# /usr/lib/python3.5/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.#PYTHON_LIB := /usr/lib PYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# NCCL acceleration switch (uncomment to build with NCCL)# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)# USE_NCCL := 1# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1# N.B. both build and distribute dirs are cleared on `make clean`BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ 对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将include中的头文件放于/usr/local/cuda-8.0/include下，将lib中的库文件放于/usr/loca/cuda-8.0/lib64文件夹下即可。 构建使用make -j8进行编译，并使用make pycaffe生成python接口。并在.bashrc中添加内容：1export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH 结果在import caffe时出现问题如下：1ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory 解决方法如下，详见GitHub issue讨论。1sudo ldconfig /usr/local/cuda/lib64 然而仍有问题，如下：1ImportError: No module named google.protobuf.internal 解决方法如下，详见G+ caffe-user group的帖子。1pip install protobuf 不过仍然存在的问题是远程SSH登录时，不能在ipython环境下导入caffe，不知为何。 使用make test; make runtest进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下： 1error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory 解决方法为手动添加符号链接，详见GitHub讨论帖。 123cd /usr/lib/x86_64-linux-gnusudo ln -s libhdf5.so.7 libhdf5.so.10sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10 另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录/opt/intel/mkl下。使用sudo权限，在目录/etc/ld.so.conf.d/下建立一个名为intel_mkl_setttings.conf的文件，将MKL安装位置下的链接库目录添加进去，如下所示：1/opt/intel/mkl/lib/intel64_lin/ 接着，运行sudo ldconfig命令，就可以了。 测试首先，通过make runtest看是否全部test可以通过。其次，可以试运行example下的LeNet训练。1234cd $CAFFE_ROOT./data/mnist/get_mnist.sh./examples/mnist/create_mnist.sh./examples/mnist/train_lenet.sh]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙]]></title>
      <url>%2F2017%2F02%2F08%2Fdigitalocean-shadowsocks%2F</url>
      <content type="text"><![CDATA[身在天朝，GFW是一个很蛋疼的东西。有人说GFW挡住了天朝与世界其他地方的交流，尤其给科研造成了很多不便；也有人认为GFW挡住了美利坚的“坚船利炮”（Google，Facebook，Tweet等），让中国的互联网企业高速发展。这两种思路都很有道理，然而GFW对计算机相关行业人员造成不便倒也是千真万确，让人在无数次404之后，脱口而出F*。也有人打趣，会不会FQ，已经成为了CS入行的第一道关卡，正好用来刷掉一批人。之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。 之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。 申请机器在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。 申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。 安装ss远程登录后，我们需要安装ss。安装命令很简单。12apt-get install python-pippip install shadowsocks 然而，在安装时，我遇到了一个奇怪的问题，提示我unsupported locale setting，后来搜索得知，是语言配置的问题，见这篇博文，解决办法如下：1export LC_ALL=C 编辑配置文件之后，进入/etc目录，建立一个名叫shadowsocks.json的文件（文件名任意，一会对应即可），文件配置内容如下：123456789&#123;&quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8388,&quot;local_address&quot;: &quot;127.0.0.1&quot;,&quot;local_port&quot;: 1080,&quot;password&quot;:&quot;your_password（任写）&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 其中第一行写成::即是为了IPV6连接。 编辑启动项，设置自动启动之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。 编辑/etc/rc.local文件，在exit 0之前添加如下命令。1ssserver -c /etc/shadowsocks.json -d start # 这里的json文件名要相对应 之后，使用reboot命令重启即可。 客户端配置客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。 IOS平台设置终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。 IPsec VPN 服务器一键安装脚本 配置 IPsec/L2TP VPN 客户端 首先，使用如下命令在VPN服务器上搭建IPsec服务： 1wget https://git.io/vpnsetup -O vpnsetup.sh &amp;&amp; sudo sh vpnsetup.sh 然后按照下面的步骤在IOS平台上进行设置。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-KMeans聚类]]></title>
      <url>%2F2017%2F02%2F05%2Fcs131-kmeans%2F</url>
      <content type="text"><![CDATA[K-Means聚类是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。 \text{SSD} = \sum_{i=1}^{k}\sum_{x\in c_i}(x-c_i)^2 目标函数K-Means方法实际上需要确定两个参数，$c^\ast$和$\delta^\ast$。其中$c_{i}^\ast$代表各个聚类中心的位置，$\delta_{ij}^\ast$的取值为$\lbrace 0,1\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。 那么，目标函数可以写成如下的形式。 c^\ast, \delta^\ast = \arg\min_{c,\delta} \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{k}\delta_{i,j}(c_i-x_j)^2然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。 实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。 算法流程K-Means算法的流程如下所示。 假设我们有$N$个样本点，$\lbrace x_1, \dots, x_N\rbrace, x_i\in\mathbb{R}^D$，并给出聚类数目$k$。 首先，随机选取一系列的聚类中心点$\mu_i, i = 1,\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。 算法细节初始化上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。 kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \omega(x-c_i)^2$选取其他的聚类中心点。其中$\omega$是归一化系数。 多次初始化，保留最好的结果。 K值的选取在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？ 我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。 距离的度量目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。 欧几里得距离（最为常用） 余弦距离（向量的夹角） 核函数（Kernel K-Means） 迭代终止条件当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下： 达到了预先给定的最大迭代次数 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛） 目标函数（平均的距离）下降小于阈值 基于K-Means的图像分割图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。 然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。 在2012年PAMI上有一篇文章SLIC Superpixels Compared to State-of-the-art Superpixel Methods介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。 优点和不足作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。 它的缺点主要有： 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。 针对K-Means，也有不少相关改进工作，参考下面这幅图吧。 MATLAB实验下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用scatter函数做出散点图。 代码中的主要部分为my_kmeans函数的实现（为了不与内建的kmeans函数重名，故加上了my前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。 注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119%% generate dataK = 3; % number of clusterspos = [-5, 5; 0, 1; 3, 6]; % position of cluster centersN = 20; % number of data pointsR = 3; % radius of clustersdata = zeros(N, 2); % dataclass = zeros(N, 1); % index of clusterfor i = 1:N idx = randi(3, 1); dr = R*rand(); data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)]; class(i) = idx;end%% visualization data pointsfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];for i = 1:K x = data(class == i, 1); y = data(class == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');end%% K-Meansbest_J = 1E100;best_idx = 0;for times = 1:5 % 5 times experiments to choose the best result [mu, assignment, J] = my_kmeans(data, K); if best_J &gt; J best_idx = times; best_J = J; end fprintf('%d experiment: J = %f\n', times, J); disp(mu);endfprintf('best: %d experiment: J = %f\n', best_idx, best_J);%% basic functionsfunction J = ssd(X, mu, assignment)% sum of square distance% X -- data, N*D matrix% mu -- centers of clusters, K*D matrix% assignment -- current assignment of data to clustersJ = 0;K = size(mu, 1);for k = 1:K x_k = X(assignment == k, :); mu_k = mu(k, :); err2 = bsxfun(@minus, x_k, mu_k).^2; J = J + sum(err2(:));endJ = J / size(X, 1);endfunction mu = compute_mu(X, assignment, K)mu = zeros(K, size(X, 2));for k = 1:K x_k = X(assignment == k, :); mu(k, :) = mean(x_k, 1);endendfunction assignment = assign(X, mu)% assign data points to clustersN = size(X, 1);assignment = zeros(N, 1);for i = 1:N x = X(i, :); err2 = bsxfun(@minus, x, mu).^2; dis = sum(err2, 2); [~, idx] = min(dis); assignment(i) = idx;endendfunction [mu, assignment, J] = my_kmeans(X, K)N = size(X, 1);assignment = zeros(N, 1);idx = randsample(N, K);mu = X(idx, :);% for i = 1:K% for j = 1:N% if assignment_gt(j) == i% mu(i,:) = X(j,:);% break;% end% end% endfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];scatter(mu(:,1), mu(:,2), 200, color, 'd');for iter = 1:20 assignment_prev = assignment; assignment = assign(X, mu); if assignment == assignment_prev break; end mu_prev = mu; mu = compute_mu(X, assignment, K); scatter(mu(:, 1), mu(:, 2), 200, color, 'd'); MU = zeros(2*K, 2); MU(1:2:end, :) = mu_prev; MU(2:2:end, :) = mu; mu_x = reshape(MU(:, 1), [], K); mu_y = reshape(MU(:, 2), [], K); plot(mu_x, mu_y, 'k-.');endfor i = 1:K x = X(assignment == i, 1); y = X(assignment == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');endJ = ssd(X, mu, assignment);end 在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。 下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。 再换个大点的数据集来做，效果貌似还不错~ PS这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：1$c_&#123;i&#125;^\ast$ XXX $\delta_&#123;ij&#125;^\ast$ 它的显示效果为$c{i}^\ast$ XXX $\delta{ij}^\ast$。 这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：1$c\_&#123;i&#125;^\ast$ XXX $\delta\_&#123;ij&#125;^\ast$ 它的显示效果为$c_{i}^\ast$ XXX $\delta_{ij}^\ast$。 具体分析可以参见博客。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[B站视频“线性代数的本质”观后感]]></title>
      <url>%2F2017%2F02%2F05%2Fvideo-linear-alg-essential-property%2F</url>
      <content type="text"><![CDATA[线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：线性代数的本质。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。 从线性空间和线性变换讲起BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。 而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的线性变换。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件： 变换前后原点不动 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。 线性变换与矩阵的关系视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。 在由一组基向量$\alpha_i, i = 1,2,\dots,n$张成的线性空间$\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是 v = \sum_{i=1}^{n}k_i\alpha_i则线性变换$\mathcal{T}$对$v$作用之后，有， u = \mathcal{T}(v) = \mathcal{T}(\sum_{i=1}^{n}k_i\alpha_i)根据线性变换的叠加性，有， u = \sum_{i=1}^{n}k_i\mathcal{T}(\alpha_i)设$\alpha_i$经过线性变换$\mathcal{T}$作用后，变换为$\beta_i$，那么， u = \sum_{i=1}^{n}k_i\beta_i也就是说， u = \begin{bmatrix}\mathcal{T}(\alpha_1), \mathcal{T}(\alpha_2), \cdots, \mathcal{T}(\alpha_n)\end{bmatrix} \begin{bmatrix}k_1\\\\ k_2\\\\ \vdots\\\\ k_n\end{bmatrix}上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。 举个例子，旋转变换。如果旋转$\frac{\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$和$(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为 A = \begin{bmatrix}\frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}& \frac{\sqrt{2}}{2}\end{bmatrix}矩阵$A$的两列分别为变换后的基向量坐标。 矩阵乘法那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \begin{bmatrix}-1\\ 0\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以， Ax = -1\begin{bmatrix}\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}\end{bmatrix} + 0\begin{bmatrix}-\frac{\sqrt{2}}{2}\\\\\frac{\sqrt{2}}{2}\end{bmatrix}而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。 所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。 而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。 矩阵的秩的意义就是矩阵列空间的维数。 同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示： 既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。 行列式仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。 我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况： 当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。 当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。 你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。 点积叉积和对偶性这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道， \langle v, u \rangle = \sum_{i=1}^{n}v_iu_i从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。 按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\mathbb{R}^2$到$\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO 论文阅读]]></title>
      <url>%2F2017%2F02%2F04%2Fyolo-paper%2F</url>
      <content type="text"><![CDATA[YOLO(You Only Look Once)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为YOLO V1和YOLO V2。YOLO V2的代码目前作为Darknet的一部分开源在GitHub。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。 YOLO V1这里不妨把YOLO V1论文“You Only Look Once: Unitied, Real-Time Object Detection”的摘要部分意译如下： 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。 和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。 基本思路 网格划分：将输入image划分为$S \times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示： \text{confidence} = P(\text{Object})\times \text{IoU}_{\text{pred}}^{\text{truth}} 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\text{Class}_i|\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。 \text{confidence}\times P(\text{Class}_i|\text{Object}) = P(\text{Class}_i)\times \text{IoU}_{\text{pred}}^{\text{truth}} 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\times 7 \times 30$ 网络模型结构Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。 另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。 训练同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。 由于Ren的论文提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\times 224$提升到了$448 \times 448$。 在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示： f(x)= \begin{cases} x, &\text{if}\ x > 0 \\\\ 0.1x, &\text{otherwise} \end{cases}很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明： loss的形式采用误差平方和的形式（真是把回归进行到底了。。。） 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\lambda_{\text{coord}} = 5，\lambda_{\text{noobj}} = 0.5 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\sqrt{w}$和$\sqrt{h}$。 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。 loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。 $\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。 在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中detection_layer.c中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121if(state.train)&#123; float avg_iou = 0; float avg_cat = 0; float avg_allcat = 0; float avg_obj = 0; float avg_anyobj = 0; int count = 0; *(l.cost) = 0; int size = l.inputs * l.batch; memset(l.delta, 0, size * sizeof(float)); for (b = 0; b &lt; l.batch; ++b)&#123; int index = b*l.inputs; // for each grid cell for (i = 0; i &lt; locations; ++i) &#123; // locations = S * S = 49 int truth_index = (b*locations + i)*(1+l.coords+l.classes); int is_obj = state.truth[truth_index]; // for each bbox for (j = 0; j &lt; l.n; ++j) &#123; // l.n = B = 2 int p_index = index + locations*l.classes + i*l.n + j; l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]); // 因为no obj对应的bbox很多，而responsible的只有一个 // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去 *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2); avg_anyobj += l.output[p_index]; &#125; int best_index = -1; float best_iou = 0; float best_rmse = 20; // 该grid cell没有目标，直接返回 if (!is_obj)&#123; continue; &#125; // 否则，找出responsible的bounding box，计算其他几项的loss int class_index = index + i*l.classes; for(j = 0; j &lt; l.classes; ++j) &#123; l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]); *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2); if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j]; avg_allcat += l.output[class_index+j]; &#125; box truth = float_to_box(state.truth + truth_index + 1 + l.classes); truth.x /= l.side; truth.y /= l.side; // 找到最好的IoU，对应的bbox是responsible的，记录其index for(j = 0; j &lt; l.n; ++j)&#123; int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt)&#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //iou = 0; float rmse = box_rmse(out, truth); if(best_iou &gt; 0 || iou &gt; 0)&#123; if(iou &gt; best_iou)&#123; best_iou = iou; best_index = j; &#125; &#125;else&#123; if(rmse &lt; best_rmse)&#123; best_rmse = rmse; best_index = j; &#125; &#125; &#125; if(l.forced)&#123; if(truth.w*truth.h &lt; .1)&#123; best_index = 1; &#125;else&#123; best_index = 0; &#125; &#125; if(l.random &amp;&amp; *(state.net.seen) &lt; 64000)&#123; best_index = rand()%l.n; &#125; int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords; int tbox_index = truth_index + 1 + l.classes; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt) &#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //printf("%d,", best_index); int p_index = index + locations*l.classes + i*l.n + best_index; *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2); // 还记得我们曾经统一加过吗？这里需要减去了 *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2); avg_obj += l.output[p_index]; l.delta[p_index] = l.object_scale * (1.-l.output[p_index]); if(l.rescore)&#123; l.delta[p_index] = l.object_scale * (iou - l.output[p_index]); &#125; l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]); l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]); l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]); if(l.sqrt)&#123; l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]); &#125; *(l.cost) += pow(1-iou, 2); avg_iou += iou; ++count; &#125; &#125; YOLO V2YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中； 修改了网络结构，去掉了全连接层，改成了全卷积结构； 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。 下面，还是先把论文的摘要意译如下： 我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。 根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。 Better在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。 改进1：引入BN层（Batch Normalization）Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。 改进2：高分辨率分类器（High Resolution Classifier）YOLO V1首先在ImageNet上以$224\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。 改进3：引入Anchor BoxYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。 作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。 与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。 使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。 改进4：Dimension Cluster在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。 这里对作者使用的方法不再过多赘述，强调以下两点： 作者使用的聚类方法是K-Means； 相似性度量不用欧氏距离，而是用IoU，定义如下：d(\text{box}, \text{centroid}) = 1-\text{IoU}(\text{box}, \text{centroid}) 使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。 改进5：直接位置预测（Direct Location Prediction）我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。 在output的feature map上，对于每个cell（共计$13\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。 设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。 Darknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）： 1234567891011121314151617181920212223242526272829// get bounding box// x: data pointer of feature map// biases: data pointer of anchor box data// biases[2*n] = width of anchor box// biases[2*n+1] = height of anchor box// n: output bounding box for each cell in the feature map// index: output bounding box index in the cell// i: `cx` in the paper// j: 'cy' in the paper// (cx, cy) is the offset from the left top corner of the feature map// (w, h) is the size of feature map (do normalization in the code)box get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h)&#123; box b; // i &lt;- cx, j &lt;- cy // index + 0: tx // index + 1: ty // index + 2: tw // index + 3: th // index + 4: to // not used here // index + 5, +6, ..., +(C+4) // confidence of P(class c|Object), not used here b.x = (i + logistic_activate(x[index + 0])) / w; // bx = cx+sigmoid(tx) b.y = (j + logistic_activate(x[index + 1])) / h; // by = cy+sigmoid(ty) b.w = exp(x[index + 2]) * biases[2*n] / w; // bw = exp(tw) * pw b.h = exp(x[index + 3]) * biases[2*n+1] / h; // bh = exp(th) * ph // 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到 // 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值 return b;&#125; 顺便说一下对bounding box的bp实现。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637// truth: ground truth// x: data pointer of feature map// biases: data pointer of anchor box data// n, index, i, j, w, h: same meaning with `get_region_box`// delta: data pointer of gradient// scale: just a weight, given by userfloat delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale)&#123; box pred = get_region_box(x, biases, n, index, i, j, w, h); // get iou of the bbox and truth float iou = box_iou(pred, truth); // ground truth of the parameters (tx, ty, tw, th) float tx = (truth.x*w - i); float ty = (truth.y*h - j); float tw = log(truth.w*w / biases[2*n]); float th = log(truth.h*h / biases[2*n + 1]); // 这里是欧式距离损失的梯度回传 // 以tx为例。 // loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx) // d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx) // 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx) // 也就是(bx^hat-bx) * d(bx)/d(tx) // 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的） // 后面相当于是求sigmoid函数对输入自变量的梯度。 // 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里 delta[index + 0] = scale * (tx - logistic_activate(x[index + 0])) * logistic_gradient(logistic_activate(x[index + 0])); delta[index + 1] = scale * (ty - logistic_activate(x[index + 1])) * logistic_gradient(logistic_activate(x[index + 1])); // tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw delta[index + 2] = scale * (tw - x[index + 2]); delta[index + 3] = scale * (th - x[index + 3]); return iou;&#125; 接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。 1234567891011// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个// 首先，我们认为当前bounding box没有responsible for any groundtruth，// 那么，loss = 1/2*(0-sigmoid(to))^2// =&gt; d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*yl.delta[index + 4] = l.noobject_scale * ((0 - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));// 如果best iou &gt; thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可if (best_iou &gt; l.thresh) &#123; l.delta[index + 4] = 0;&#125; 这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。12345678// for each classfor(n = 0; n &lt; classes; ++n)&#123; // P_i = \frac&#123;exp^out_i&#125;&#123;sum of exp^out_j&#125; // SoftmaxLoss = -logP(class) // ∂SoftmaxLoss/∂output = -(1(n==class)-P) delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]); if(n == class) *avg_cat += output[index + n];&#125; 改进6：Fine-Gained Features这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\times 26$大小的feature map加进来。 在具体实现时，是将higher resolution（也就是$26\times 26$）的feature map stacking在一起。比如，原大小为$26\times 26 \times 512$的feature map，因为我们要将其变为$13\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中reorg_layer的实现。 使用这一扩展之后的feature map，提高了1%的性能提升。 改进7：多尺度训练（Multi-Scale Training）在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。 具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\lbrace 320, 352, \dots, 608\rbrace$。 在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。 总结在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。 Faster这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。 在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。 在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\times 224$大小的图像进行训练，再使用$448\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。 然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\times(5+20)=125$。从YOLO V2的yolo_voc.cfg文件中，我们也可以看到如下的对应结构： 1234567891011121314[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[convolutional]size=1stride=1pad=1filters=125activation=linear 同时，加上上文提到的pass-through结构。 Stronger未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-立体视觉基础]]></title>
      <url>%2F2017%2F02%2F02%2Fcs131-camera%2F</url>
      <content type="text"><![CDATA[数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。 针孔相机模型（Pinhole Camera）针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。 投影几何的重要性质在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。 在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。 另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。 针孔相机模型如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\Pi^\prime$的点$P^\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。 由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\prime$坐标之间的数量关系为： \left\{\begin{matrix} x^\prime = fx/z \\ y^\prime = fy/z \end{matrix}\right.可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。 这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。 上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设： 内假设（和相机本身有关） 不同方向上焦距相同； 光学中心在相平面的坐标原点$(0, 0)$ 没有倾斜（no skew） 外假设（和相机位姿有关，和相机本身参数无关） 相机没有旋转（坐标轴与世界坐标系方向重合） 相机没有平移（相机中心与世界坐标系中心重合） 其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。 下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。 理想情况理想情况以上假设全部满足，矩阵$M$如下所示。 光学中心不在像平面的坐标原点假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为： 像素非正方形由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下： no skew这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下： 相机的旋转和平移相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。 所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\in \mathbb{R}^{3\times 4}$。 P^\prime = MHP首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\mathbb{0}$矩阵变为了一个平移向量。 进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示： 将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下： 最终形式综上所示，变换矩阵的最终形式为： 其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。 上面的内容总结起来，如下图所示。 对极几何基础概念如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。 极点：$e$和$e^\prime$点分别是$OO^\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。 极平面：点$O$，$O^\prime$，$P$点共同确定的平面（灰色） 极线：极平面与两个成像平面的交线，即$pe$和$p^\prime e^\prime$（蓝色） 基线：两个相机中心的连线（黄色） 极线约束从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？ 如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\prime$有向线段，表明相机中心的位移。 （下面的推导参考了博客：计算机视觉基础4——对极几何）。在下面的推导中，我们使用$p^\prime$表示在相机$O^\prime$下的向量$O\prime P$，符号$p$同理。那么，有如下关系成立：$R(p-T) = p^\prime$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(SIFT)]]></title>
      <url>%2F2017%2F01%2F30%2Fcs131-sift%2F</url>
      <content type="text"><![CDATA[SIFT(尺度不变特征变换，Scale Invariant Feature Transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下： scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。 interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。 SIFT介绍上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。 而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。 而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。 Lowe的论文中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。 这篇博客主要是Lowe上述论文的读书笔记，按照SIFT特征的计算步骤进行组织。 尺度空间极值的检测方法前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\sigma)$的卷积结果。如下式所示： L(x,y,\sigma) = G(x,y,\sigma)\ast I(x,y)其中，$G(x,y, \sigma) = \frac{1}{2\pi\sigma^2}\exp(-(x^2+y^2)/2\sigma^2)$。不同的$\sigma$代表不同的尺度。 DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即， D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\sigma$最终变成了2倍（即$\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。 为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\sigma^2\Delta G$提供了足够的近似。其中前面的$\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\sigma \Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。 对于高斯核函数，有以下性质： \frac{\partial G}{\partial \sigma} = \sigma \Delta G我们将式子左侧的微分变成差分，得到了下式： \sigma\Delta G \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}也就是： G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \Delta G当$k=1$时，上式的近似误差为0（即上面的$s=\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。 构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。 另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。 此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。 128维feature的获取我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引pyramid{scale}(y, x)就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。 我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量patch_mag和patch_theta分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。 123patch_mag = sqrt(patch_dx.^2 + patch_dy.^2);patch_theta = atan2(patch_dy, patch_dx); % atan2的返回结果在区间[-pi, pi]上。patch_theta = mod(patch_theta, 2*pi); % 这里我们要将其转换为[0, 2pi] 之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。 所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将[0, 2pi]区间划分为若干个bin，并将patch内的每个点使用其梯度大小向对应的bin内投票即可。如下所示： 12345678910111213141516171819202122232425262728293031323334function [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)% Compute a gradient histogram using gradient magnitudes and directions.% Each point is assigned to one of num_bins depending on its gradient% direction; the gradient magnitude of that point is added to its bin.%% INPUT% num_bins: The number of bins to which points should be assigned.% gradient_magnitudes, gradient angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient% for the ith point. gradient_angles ranges from 0 to 2*pi% % OUTPUT% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is% the sum of entries in gradient_magnitudes whose corresponding% gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for% angles between angle_step and 2*angle_step. Angle_step is calculated as% 2*pi/num_bins.% angles: A 1 x num_bins array which holds the histogram bin lower bounds.% In other words, histogram(i) contains the sum of the% gradient magnitudes of all points whose gradient directions fall% in the range [angles(i), angles(i + 1)) angle_step = 2 * pi / num_bins; angles = 0 : angle_step : (2*pi-angle_step); histogram = zeros(1, num_bins); num = numel(gradient_angles); for n = 1:num index = floor(gradient_angles(n) / angle_step) + 1; histogram(index) = histogram(index) + gradient_magnitudes(n); end end Lowe论文中推荐的bin数目为36个，计算主方向的函数如下： 12345678910111213141516171819202122232425function direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)% Computes the dominant gradient direction for the region around a keypoint% given the scale of the keypoint and the gradient magnitudes and gradient% angles of the pixels in the region surrounding the keypoint.%% INPUT% gradient_magnitudes, gradient_angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient for% the ith point. % Compute a gradient histogram using the weighted gradient magnitudes. % In David Lowe's paper he suggests using 36 bins for this histogram. num_bins = 36; % Step 1: % compute the 36-bin histogram of angles using ComputeGradientHistogram() [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles); % Step 2: % Find the maximum value of the gradient histogram, and set "direction" % to the angle corresponding to the maximum. (To match our solutions, % just use the lower-bound angle of the max histogram bin. (E.g. return % 0 radians if it's bin 1.) [~, max_index] = max(histogram); direction = angle_bound(max_index);end 之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。 123patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;patch_theta = mod(patch_theta, 2*pi);patch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16 遍历cell，计算feature如下： 123456789101112131415feature = [];row_iter = 1;for y = 1:num_histograms col_iter = 1; for x = 1:num_histograms cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta); feature = [feature, histogram]; col_iter = col_iter + pixelsPerHistogram; end row_iter = row_iter + pixelsPerHistogram;end 最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。 这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。 应用：图像特征点匹配和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中descriptor是两幅图像的SIFT特征向量。阈值默认为取做0.7。 123456789101112131415161718192021222324252627282930313233343536373839function match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)% SIFTSimpleMatcher% Match one set of SIFT descriptors (descriptor1) to another set of% descriptors (decriptor2). Each descriptor from descriptor1 can at% most be matched to one member of descriptor2, but descriptors from% descriptor2 can be matched more than once.% % Matches are determined as follows:% For each descriptor vector in descriptor1, find the Euclidean distance% between it and each descriptor vector in descriptor2. If the smallest% distance is less than thresh*(the next smallest distance), we say that% the two vectors are a match, and we add the row [d1 index, d2 index] to% the "match" array.% % INPUT:% descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.% descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.% thresh: a given threshold of ratio. Typically 0.7%% OUTPUT:% Match: N * 2 matrix, each row is a match.% For example, Match(k, :) = [i, j] means i-th descriptor in% descriptor1 is matched to j-th descriptor in descriptor2. if ~exist('thresh', 'var'), thresh = 0.7; end match = []; [N1, ~] = size(descriptor1); for i = 1:N1 fea = descriptor1(i, :); err = bsxfun(@minus, fea, descriptor2); dis = sqrt(sum(err.^2, 2)); [sorted_dis, ind] = sort(dis, 1); if sorted_dis(1) &lt; thresh * sorted_dis(2) match = [match; [i, ind(1)]]; end endend 接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足： Hp_{\text{before}} = p_{\text{after}}其中 p = \begin{bmatrix}x \\\\ y \\\\ 1\end{bmatrix}对上式稍作变形，有 p_{\text{before}}^\dagger H^\dagger = p_{\text{after}}\dagger就可以使用标准的最小二乘正则方程进行求解了。代码如下： 1234567891011121314151617181920212223242526272829303132function H = ComputeAffineMatrix( Pt1, Pt2 )%ComputeAffineMatrix% Computes the transformation matrix that transforms a point from% coordinate frame 1 to coordinate frame 2%Input:% Pt1: N * 2 matrix, each row is a point in image 1% (N must be at least 3)% Pt2: N * 2 matrix, each row is the point in image 2 that% matches the same point in image 1 (N should be more than 3)%Output:% H: 3 * 3 affine transformation matrix,% such that H*pt1(i,:) = pt2(i,:) N = size(Pt1,1); if size(Pt1, 1) ~= size(Pt2, 1), error('Dimensions unmatched.'); elseif N&lt;3 error('At least 3 points are required.'); end % Convert the input points to homogeneous coordintes. P1 = [Pt1';ones(1,N)]; P2 = [Pt2';ones(1,N)]; H = P1*P1'\P1*P2'; H = H'; % Sometimes numerical issues cause least-squares to produce a bottom % row which is not exactly [0 0 1], which confuses some of the later % code. So we'll ensure the bottom row is exactly [0 0 1]. H(3,:) = [0 0 1];end 作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(Harris 角点)]]></title>
      <url>%2F2017%2F01%2F25%2Fcs131-finding-features%2F</url>
      <content type="text"><![CDATA[feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。 那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。 Harris角点角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。 Harris角点得名于其发明者Harris，是一种常见的角点检测方法。给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。 E(u,v) = \sum_x\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。 使用泰勒级数展开，并忽略非线性项，我们有 I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v所以上式可以写成（线性二次型写成了矩阵形式）， E(u,v) = \sum_{x,y}w(I_xu+I_yv)^2 = \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\\\v\end{bmatrix}其中， M = w\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y&I_y^2\end{bmatrix}当使用门限函数时，权值$w_{i,j} = 1$，则， M = \begin{bmatrix}\sum I_xI_x& \sum I_xI_y\\\\\sum I_xI_y&\sum I_yI_y\end{bmatrix} = \sum \begin{bmatrix}I_x \\\\I_y\end{bmatrix}\begin{bmatrix}I_x &I_y\end{bmatrix}当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵 M = \begin{bmatrix}\lambda_1 & 0 \\\\ 0&\lambda_2 \end{bmatrix} 当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。 M = R^{-1}\Sigma R, \text{其中}\Sigma = \begin{bmatrix}\lambda_1&0\\\\0&\lambda_2\end{bmatrix}所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\lambda_1$和$\lambda_2$）。 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。 \theta = \det(M)-\alpha\text{trace}(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2 为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示： w(x,y) = \exp(-(x^2+y^2)/2\sigma^2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-边缘检测]]></title>
      <url>%2F2017%2F01%2F24%2Fcs131-edge-detection%2F</url>
      <content type="text"><![CDATA[边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。 边缘的产生若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点： 物体表面不平造成灰度值的不连续； 深度值不同造成灰度值不连续； 物体表面颜色的突变造成灰度值不连续 朴素思想利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。 问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。 在$x$方向上，令$g_x = \frac{\partial f}{\partial x}$；在$y$方向上，令$g_y = \frac{\partial f}{\partial y}$。梯度的大小和方向为 g = \lbrack g_x, g_y\rbrack, \theta = \arctan(g_y/g_x)通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。 只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。 改进1：先平滑改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有： \frac{d}{dx}(f\ast g) = f\ast\frac{d}{dx}g所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。 进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。 改进2：Canny检测子改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下： 使用DoG计算梯度幅值和方向。 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。 同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定low和high两个阈值，来判定某个点是否属于强或弱边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比low还要小，则在此停止。 改进3：RANSAC方法有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。 RANSAC方法的思想在于，认为已有的feature大部分都是好的。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。 以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。 上述RANSAC方法进行直线拟合的过程可以总结如下： 按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。 而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：12least square: a = 3.319566, b = -1.446528ransac method: a = 1.899640, b= 1.298608 实验使用的MATLAB代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162%% generate datax = 0:1:10;y_gt = 2*x+1;y = y_gt + randn(size(y_gt));scatter(x, y, [], [1,0,0]);hold onout_x = 0:1:10;out_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));scatter(out_x, out_y, [], [0,0,1]);X = [x, out_x]';Y = [y, out_y]';X = [X, ones(length(X), 1)];[a, b] = ls_fit(X, Y);plot(x, a*x+b, 'linestyle', '--', 'color', 'r');[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);plot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');fprintf('least square: a = %f, b = %f\n',a, b);fprintf('ransac method: a = %f, b= %f\n', ra, rb)function [a, b] = ransac_fit(X, Y, k, n, t ,d)% ransac fit% k -- maximum iteration number% n -- smallest point numer required% t -- threshold to identify a point is fit well% d -- the number of nearby points to assert a model is finedata = [X, Y];N = size(data, 1);best_good_cnt = -1;best_a = 0;best_b = 0;for i = 1:k % sample point idx = randsample(N, n); data_sampled = data(idx, :); % fit with least square [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3)); % test model not_sampled = ones(N, 1); not_sampled(idx) = 0; not_sampled_data = data(not_sampled == 1, :); distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1); inner_flag = distance &lt; t; good_cnt = sum(inner_flag); if good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt best_good_cnt = good_cnt; data_refine = data(find(inner_flag), :); [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3)); best_a = a; best_b = b; end fprintf('iteration %d, best_a = %f, best_b = %f\n', i, best_a, best_b);enda = best_a;b = best_b;endfunction [a, b] = ls_fit(X, Y)% least square fitA = X'*X\X'*Y;a = A(1);b = A(2);end 我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。 仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。 RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线性滤波器和矩阵的SVD分解]]></title>
      <url>%2F2017%2F01%2F23%2Fcs131-filter-svd%2F</url>
      <content type="text"><![CDATA[数字图像可以看做$\mathbb{R}^2 \rightarrow \mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。 卷积卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自博客《图像卷积与滤波的一些知识点》） 在卷积操作时，常常需要对图像做padding，常用的padding方法有： zero padding，也就是填充0值。 edge replication，也就是复制边缘值进行填充。 mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。 作业1调整图像灰度值为0到255计算相应的k和offset值即可。另外MATLAB中的uint8函数可以将结果削顶与截底为0到255之间。123scale_ratio = 255.0 / (max_val - min_val);offset = -min_val * scale_ratio;fixedimg = scale_ratio * dark + offset; SVD图像压缩使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。 MATLAB实现分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。 MATLAB代码如下：12345678910111213141516171819202122232425%% read imageim = imread('./flower.bmp');im_gray = double(rgb2gray(im));[u, s, v] = svd(im_gray);%% get sigular valuesigma = diag(s);top_k = sigma(1:10);figureplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');figuresubplot(2, 2, 1);imshow(uint8(im_gray));title('flower.bmp')index = 2;for k = [10, 50, 100] uk = u(:, 1:k); sk = s(1:k, 1:k); vk = v(:, 1:k); im_rec = uk * sk * vk'; subplot(2, 2, index); index = index + 1; imshow(uint8(im_rec)); title(sprintf('k = %d', k));end 图像SVD压缩中的误差分析完全是个人随手推导，不严格的说明： 将矩阵分块。由SVD分解公式$\mathbf{U}\mathbf{\Sigma} \mathbf{V^\dagger} = \mathbf{A}$，把$\mathbf{U}$按列分块，$\mathbf{V^\dagger}$按行分块，有下式成立： \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} \begin{bmatrix} v_1^\dagger\\\\ v_2^\dagger\\\\ \dots\\\\ v_m^\dagger \end{bmatrix}=\mathbf{A}由于 \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2u_2 &\vdots &\sigma_nu_n \end{bmatrix}所以， \mathbf{A} = \sum_{i = 1}^{r}\sigma_iu_iv_i^\dagger上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\sigma_k = 0$。 所以\mathbf{A} - \hat{\mathbf{A}} = \sum_{i = k+1}^{r}\sigma_iu_iv_i^\dagger 根绝矩阵范数的性质，我们有， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i\left\lVert u_i\right\rVert\left\lVert v_i^\dagger\right\rVert由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有： e \le \sum_{i=k+1}^{r}\sigma_iSVD与矩阵范数如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。 $f(\mathbf{A}) = \mathbf{0} \Leftrightarrow \mathbf{A} = \mathbf{0}$ $f(c\mathbf{A}) = c f(\mathbf{A}), \forall c \in \mathbb{R}$ $f(\mathbf{A+b}) \le f(\mathbf{A}) + f(\mathbf{B})$ 其中，矩阵的2范数可以定义为 \left\lVert\mathbf{A}\right\rVert_2 = \max{\sqrt{(\mathbf{A}x)^\dagger\mathbf{A}x}}其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。 下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。 对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）： (Ax)^\dagger Ax = x^\dagger V \Sigma^\dagger \Sigma V^\dagger x其中，$U^\dagger U = I$，已经被消去了。 进一步化简，我们将$V^\dagger x$看做一个整体，令$\omega = V\dagger x$，那么有， (Ax)^\dagger Ax = (\Sigma \omega)^\dagger \Sigma \omega也就是说，矩阵的2范转换为了$\Sigma \omega$的幅值的最大值。由于$\omega$是酉矩阵和一个单位向量的乘积，所以$\omega$仍然是单位阵。 由于$\Sigma$是对角阵，所以$\omega$与其相乘后，相当于每个分量分别被放大了$\sigma_i$倍。即 \Sigma \omega = \begin{bmatrix} \sigma_1 \omega_1\\\\ \sigma_2 \omega_2\\\\ \cdots\\\\ \sigma_n \omega_n \end{bmatrix}它的幅值平方为 \left\lVert \Sigma \omega \right \rVert ^2 = \sum_{i=1}^{n}\sigma_i^2 \omega_i^2 \le \sigma_{1} \sum_{i=1}^{n}\omega_i^2 = \sigma_1^2当且仅当，$\omega_1 = 1$, $\omega_k = 0, k &gt; 1$时取得等号。 综上所述，矩阵2范数的值等于其最大的奇异值。 矩阵的另一种范数定义方法Frobenius norm定义如下： \left\lVert A \right\rVert_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}\left\vert a_{i,j}\right\rvert}如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式： \left\lVert A\right \rVert_F^2 = \text{trace}(A^\dagger A)利用矩阵的SVD分解，可以很容易得出，$\text{trace}(A^\dagger A) = \sum_{i=1}^{r}\sigma_i^2$ 说明如下： \text{trace}(A^\dagger A) = \text{trace}(V\Sigma^\dagger\Sigma V^\dagger)由于$V^\dagger = V^{-1}$，而且$\text{trace}(BAB^{-1}) = \text{trace}(A)$，所以， \text{trace}(A^\dagger A) = \text{trace}(\Sigma^\dagger \Sigma) = \sum_{i=1}^{r}\sigma_i^2也就是说，矩阵的F范数等于它的奇异值平方和的平方根。 \left\lVert A\right\rVert_F= \sqrt{\sum_{i=1}^{r}\sigma_i^2}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线代基础]]></title>
      <url>%2F2017%2F01%2F22%2Fcs131-linear-alg%2F</url>
      <content type="text"><![CDATA[CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，该课程目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。 由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前线代基础的复习与整理。 向量与矩阵数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。 矩阵作为线性变换通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。 scale变换对角阵可以用来表示放缩变换。 \begin{bmatrix} s_x & 0\\\\ 0 & s_y \end{bmatrix}\begin{bmatrix} x\\\\ y \end{bmatrix} = \begin{bmatrix} s_xx\\\\ s_yy \end{bmatrix}旋转变换如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为： \mathbf{R} = \begin{bmatrix} \cos\theta &-\sin\theta \\\\ \sin\theta &\cos\theta \end{bmatrix}旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$. 齐次变换(Homogeneous Transform)只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。 \mathbf{H} =\begin{bmatrix} a & b & t_x\\\\ c & d & t_y\\\\ 0 & 0 & 1 \end{bmatrix},\mathbf{H}\begin{bmatrix} x\\\\ y\\\\ 1\\\\ \end{bmatrix}=\begin{bmatrix} ax+by+t_x\\\\ cx+dy+t_y\\\\ 1 \end{bmatrix}SVD分解可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积： \mathbf{U}\mathbf{\Sigma}\mathbf{V^\dagger} = \mathbf{A}其中矩阵$\mathbf{A}$大小为$m\times n$，矩阵$\mathbf{U}$是大小为$m\times m$的酉矩阵，$\mathbf{V}$是大小为$n \times n$的酉矩阵，$\mathbf{\Sigma}$是大小为$m \times n$的旋转矩阵，即只有主对角元素不为0. SVD分解在主成分分析中年很有用。由于矩阵$\mathbf{\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。 如下图，是使用前10个分量对原图片进行压缩的效果。 12345678910im = imread('./superman.png');im_gray = rbg2gray(im);[u, s, v] = svd(double(im_gray));k = 10;uk = u(:, 1:k);sigma = diag(s);sk = diag(sigma(1:k));vk = v(:, 1:k);im_k = uk*sk*vk';imshow(uint8(im_k))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用 Visual Studio 编译 GSL 科学计算库]]></title>
      <url>%2F2016%2F12%2F16%2Fgsl-with-vs%2F</url>
      <content type="text"><![CDATA[GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。 GSL 的项目主页提供的说明来看，GSL支持如下的科学计算： （下面的这张表格的HTML使用的是No-Cruft Excel to HTML Table Converter生成的） Complex Numbers Roots of Polynomials Special Functions Vectors and Matrices Permutations Sorting BLAS Support Linear Algebra Eigensystems Fast Fourier Transforms Quadrature Random Numbers Quasi-Random Sequences Random Distributions Statistics Histograms N-Tuples Monte Carlo Integration Simulated Annealing Differential Equations Interpolation Numerical Differentiation Chebyshev Approximation Series Acceleration Discrete Hankel Transforms Root-Finding Minimization Least-Squares Fitting Physical Constants IEEE Floating-Point Discrete Wavelet Transforms Basis splines GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO! 1234./configuremakemake installmake clean 同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。 使用CMAKE编译成.SLN文件打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。 使用Visual Studio生成解决方案使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。 当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\bin，\gsl，\Debug和\Release。 加入环境变量修改环境变量的Path，将\GSL_Build_Path\bin\Debug加入，这主要是为了\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。 这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。 建立Visual Studio属性表Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)。 配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。 12345678910111213141516171819&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;ImportGroup Label="PropertySheets" /&gt; &lt;PropertyGroup Label="UserMacros" /&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;$(OPENCV249)\include;E:\GSLCode\gsl-build\;$(IncludePath)&lt;/IncludePath&gt; &lt;LibraryPath Condition="'$(Platform)'=='Win32'"&gt;$(OPENCV249)\x86\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;LibraryPath Condition="'$(Platform)'=='X64'"&gt;$(OPENCV249)\x64\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link Condition="'$(Configuration)'=='Debug'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;Link Condition="'$(Configuration)'=='Release'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; &lt;ItemGroup /&gt;&lt;/Project&gt; 在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！ 测试在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。 123456789#include &lt;stdio.h&gt;#include &lt;gsl/gsl_sf_bessel.h&gt;int main(void)&#123; double x = 5.0; double y = gsl_sf_bessel_J0(x); printf("J0(%g) = %.18e\n", x, y); return 0;&#125; 控制台输出正确：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows环境下使用Doxygen生成注释文档]]></title>
      <url>%2F2016%2F12%2F16%2Fuse-doxygen%2F</url>
      <content type="text"><![CDATA[Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。 安装 DoxygenDoxygen 在Windows平台下的安装比较简单，Doxygen的项目主页提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。 安装成功后，使用命令行命令 1doxygen --help 就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。 使用命令， 1doxygen -g doxygen_filename 就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。 使用命令， 1doxygen doxygen_filename 就可以生成注释文档了。 下面就来说一说对中文的支持。 生成 HTML 格式文档中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。 我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。 这样一来，编译出来的 HTML 页面就不会有中文乱码了。 生成Latex 格式文档生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。 可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。 打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 \begin{document}一行，将其改为 12\begin&#123;document&#125;\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125; 也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。 相应的，我们要将结尾的 \end{document)改为：12\end&#123;CJK&#125;\end&#123;document&#125; 这样，运行make命令之后，就可以看到中文的注释文档了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Code highlightHello World! 1234#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "HelloWorld\n";&#125; 1print 'HelloWorld' Latex Support by MathjaxMass-energy equation by Einstein: $E = mc^2$ a linear equation: \mathbf{A}\mathbf{v} = \mathbf{y}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python Regular Expressions （Python 正则表达式)]]></title>
      <url>%2F2014%2F07%2F17%2Fpython-reg-exp%2F</url>
      <content type="text"><![CDATA[本文来自于Google Developers中对于Python的介绍。https://developers.google.com/edu/python/regular-expressions。 认识正则表达式Python的正则表达式是使用 re 模块的。 12345match = re.search(pattern,str)if match: print 'found',match.group()else: print 'NOT Found!' 正则表达式的规则基本规则 a, x, 9 都是普通字符 (ordinary characters) . (一个点)可以匹配任何单个字符（除了’\n’） \w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\W （大写的W）可以匹配非单词里的这些元素 \b 匹配单词与非单词的分界 \s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\n\r\t\f)；\S（大写的S）匹配一个非 whitespace character \d 匹配十进制数字 [0-9] ^=start，$=end 用来匹配字符串的开始和结束 \ 是转义字符，用 . 来匹配串里的’.’，等一些基本的例子 12345678910## 在字符串'piiig'中查找'iii'match = re.search(r'iii', 'piiig') # found, match.group() == "iii"match = re.search(r'igs', 'piiig') # not found, match == None## . 匹配除了\n的任意字符match = re.search(r'..g', 'piiig') # found, match.group() == "iig"## \d 匹配0-9的数字字符, \w 匹配单词里的字符match = re.search(r'\d\d\d', 'p123g') # found, match.group() == "123"match = re.search(r'\w\w\w', '@@abcd!!') # found, match.group() == "abc" 重复可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。 ‘+’ 用来匹配1个或者多个字符 ‘*’ 用来匹配0个或者多个字符 ‘?’ 用来匹配0个或1个字符 注意，’+’和’*’会匹配尽可能多的字符。 一些重复字符的例子12345678910111213141516## i+ 匹配1个或者多个'i'match = re.search(r'pi+', 'piiig') # found, match.group() == "piii"## 找到字符串中最左边尽可能长的模式。## 注意，并没有匹配到第二个 'i+'match = re.search(r'i+', 'piigiiii') # found, match.group() == "ii"## \s* 匹配0个或1个空白字符 whitespacematch = re.search(r'\d\s*\d\s*\d', 'xx1 2 3xx') # found, match.group() == "1 2 3"match = re.search(r'\d\s*\d\s*\d', 'xx12 3xx') # found, match.group() == "12 3"match = re.search(r'\d\s*\d\s*\d', 'xx123xx') # found, match.group() == "123"## ^ 匹配字符串的第一个字符match = re.search(r'^b\w+', 'foobar') # not found, match == None## 与上例对比match = re.search(r'b\w+', 'foobar') # found, match.group() == "bar" Email考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配： 1match = re.search(r'\w+@\w+',str) 但是，对于这种Email地址 xyz alice-b@google.com purple monkey则不能奏效。 使用方括号方括号里面的字符表示一个字符集合。[abc]可以被用来匹配’a’或者’b’或者’c’。\w \s等都可以用在方括号里，除了’.’以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下： 1match = re.search('r[\w.-]+@[\w.-]+',str) 你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如ab表示除了’a’和’b’之外的其他字符。 操作以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？可以在模式中用圆括号指定。 123456str = 'purple alice-b@google.com monkey dishwasher'match = re.search('([\w.-]+)@([\w.-]+)', str) #用圆括号指定分割if match: print match.group() ## 'alice-b@google.com' (the whole match) print match.group(1) ## 'alice-b' (the username, group 1) print match.group(2) ## 'google.com' (the host, group 2) findall 函数与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。 12345str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## findall返回一个包含所有匹配结果的 listemails = re.findall(r'[\w\.-]+@[\w\.-]+', str) ## ['alice@google.com', 'bob@abc.com']for email in emails: print email 在文件中使用findall当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？ 12f = open(filename.txt,'r')matches = re.findall(pattern,f.read()) findall 和分组和group的用法相似，也可以指定分组。 12345678str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## 返回了一个listtuples = re.findall(r'([\w\.-]+)@([\w\.-]+)', str)print tuples ## [('alice', 'google.com'), ('bob', 'abc.com')]## list中的元素是tuplefor tuple in tuples: print tuple[0] ## username print tuple[1] ## host 调试正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。 其他选项正则表达式还可以设置“选项”。 1match = re.search(pat,str,opt) 这些可选项如下： IGNORECASE 忽视大小写 DOTALL 允许’.’匹配’\n’ MULTILINE 在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束]]></content>
    </entry>

    
  
  
</search>
