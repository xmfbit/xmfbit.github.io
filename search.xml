<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>论文阅读 - TResNet High Performance GPU-Dedicated Architecture</title>
    <url>/2020/05/02/paper-tresnet/</url>
    <content><![CDATA[<p>作者在摘要中所想的我们在工作中也观察到，尽管最近两年关于CNN网络的设计仍然有各式各样论文出现，比如Mobilenet / ShuffleNet，又或者是NAS搜索的网络结构如EfficientNet等，实际在GPU上使用起来并没有设想的那么high performance（latency / throughput），反而是ResNet系列历久弥新，真正经受住了工业界的考验，仍然是最常用的模型（可以很肯定没有之一），尤其是50，在速度和精度上达到了很好的trade off。</p>
<blockquote>
<p>vanilla ResNet50 is usually significantly faster than its recent competitors, of- fering better throughput-accuracy trade-off.</p>
</blockquote>
<p>深度学习技术现在早已经走出了学术象牙塔，在工业界广泛铺开。在精度已经没有太多提升空间的现在，网络的计算资源消耗 / latency / QPS等越来越成为大家关注的热点。这篇文章就试图在维持网络high performance的前提，提升网络的精度。这在现实问题中很有意义。</p>
<p><img src="/img/paper_tresnet_sota_benchmark.png" alt="SOTA模型的benchmark"></p>
<a id="more"></a>
<h1 id="你们呐，还是Too-young-too-simple"><a href="#你们呐，还是Too-young-too-simple" class="headerlink" title="你们呐，还是Too young, too simple"></a>你们呐，还是Too young, too simple</h1><p>见题图，作者比较了几种SOTA模型和ResNet-50在train和inference的速度（其实体现的是大批量时候的吞吐），可以看到最过分的是EfficientNet和MixNet，FLOPS比ResNet低了这么多，吞吐反而不如（如果没有Google的TPU加持，还是不要挑战EfficientNet了，我们的实测也是发现很坑）。</p>
<p>GPU的计算力越来越强，很多时候其实并不是FLOPS限制了网络的能力，而是访存。</p>
<ul>
<li>EfficientNet / ResNext / MixNet：使用分离卷积降低了FLOPS，但是实际考虑GPU吞吐量时，更重要的是访存，而不是节省的那一点FLOPS。就像这个帖子里面所讨论的那样<a href="https://github.com/pytorch/pytorch/issues/18631" target="_blank" rel="noopener">FP32 depthwise convolution is slow in GPU #18631</a></li>
</ul>
<blockquote>
<p>PS: Depthwise and group convolution is slower due to lower arithmetic intensity i.e. reduced data reuse (both leads to fragmented memory-accesses). Its a feature not a bug. Only specialized implementation can make it fast.</p>
</blockquote>
<ul>
<li>multi-path的使用。在训练的时候，需要为这些路径上的激活都储存相应的grad，造成显存占用上升，不利于大batch size，导致吞吐下降。</li>
</ul>
<p>这里作者对访存的diss之前也看过其他人的分析，我并不是做体系结构的。不过据我所知，很多神经网络加速器也是在解决这个问题。随着芯片的计算能力越来越强，XXTFLOPS的能力，却会被mem访问速度限制。IC设计和半导体产业就这样，不断地在实际中发现问题解决问题，让我们从五十年前（现在看来）孱弱的计算力，一步步发展到今天便捷的手机和强大的GPU。而计算能力的提升，又不断地催生新的技术应用。提出新的问题。电气革命依靠的是对化石能源的利用，而信息革命离不开计算能力的不断发掘。摩尔定律万岁~</p>
<p><img src="/img/paper_tresnet_our_result.png" alt="结果"></p>
<h1 id="TResNet的设计"><a href="#TResNet的设计" class="headerlink" title="TResNet的设计"></a>TResNet的设计</h1><p>怎么说呢，这里的设计好像并没有什么太深入的东西。也是读到这里，让我对这篇文章的价值觉得没这么大了。</p>
<h2 id="Stem设计"><a href="#Stem设计" class="headerlink" title="Stem设计"></a>Stem设计</h2><p>Stem指的是data输入到ResNet连读堆叠block之间的那个部分，起到的作用是迅速downsample输入。例如ResNet使用$7\times 7$，stride为2的conv和max pooling串联，将输入从224缩小到56。其他网络也都有类似的设计。在<a href="https://arxiv.org/abs/1812.01187" target="_blank" rel="noopener">Bag of tricks</a>这篇文章中，ResNet-D是将$7\times 7$的conv分解为两个$3\times 3$的conv，</p>
<p>这里TResNet使用了一个“Space-To-Depth” layer，将spatial转到depth维度上去，达到缩小尺寸的目的，再接一个$1\times 1$的conv，得到想要的channel数量。</p>
<p><img src="/img/paper_tresnet_stem_design.png" alt="stem design"></p>
<p>代码中有这个layer的具体实现方式。类似ShuffleNet，以H为例，会将其分为若干组，即<code>bs</code>，然后重组。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SpaceToDepth</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block_size=<span class="number">4</span>)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        <span class="keyword">assert</span> block_size == <span class="number">4</span></span><br><span class="line">        self.bs = block_size</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        N, C, H, W = x.size()</span><br><span class="line">        <span class="comment"># reshape NCHW -&gt; NCH'BW'B</span></span><br><span class="line">        x = x.view(N, C, H // self.bs, self.bs, W // self.bs, self.bs)  <span class="comment"># (N, C, H//bs, bs, W//bs, bs)</span></span><br><span class="line">        <span class="comment"># transpose: NBBCH'W'</span></span><br><span class="line">        x = x.permute(<span class="number">0</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">4</span>).contiguous()  <span class="comment"># (N, bs, bs, C, H//bs, W//bs)</span></span><br><span class="line">        <span class="comment"># reshape -&gt; NC'H'W'</span></span><br><span class="line">        x = x.view(N, C * (self.bs ** <span class="number">2</span>), H // self.bs, W // self.bs)  <span class="comment"># (N, C*bs^2, H//bs, W//bs)</span></span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>这个操作来自于这篇文章<a href="https://arxiv.org/abs/1909.03205" target="_blank" rel="noopener">Non-discriminative data or weak model? On the relative importance of data and model resolution</a>。核心观点是“是网络内部的feature map的resolution影响网络acc，而不是输入”</p>
<blockquote>
<p>In this paper, we show that up to a point, the input resolution alone plays little role in the network performance, and it is the internal resolution that is the critical driver of model quality. We then build on these insights to develop novel neural network architectures that we call \emph{Isometric Neural Networks}. These models maintain a fixed internal resolution throughout their entire depth. </p>
</blockquote>
<h2 id="抗混叠（anti-alias）下采样-AA"><a href="#抗混叠（anti-alias）下采样-AA" class="headerlink" title="抗混叠（anti-alias）下采样 - AA"></a>抗混叠（anti-alias）下采样 - AA</h2><p>将ResNet中的下采样换成一种比较经济的AA：stride为2的conv被替换为stride为1的conv，再接上stride为2的blur $3\times 3$的conv kernel</p>
<p><img src="/img/paper_tresnet_aa.png" alt="AA"></p>
<p>具体实现代码（只展示了blur的$3\times 3$conv）如下。可以看到这里直接使用了$3\times 3$的<a href="https://en.wikipedia.org/wiki/Gaussian_blur" target="_blank" rel="noopener">高斯模糊kernel</a>：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Downsample</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filt_size=<span class="number">3</span>, stride=<span class="number">2</span>, channels=None)</span>:</span></span><br><span class="line">        super(Downsample, self).__init__()</span><br><span class="line">        self.filt_size = filt_size</span><br><span class="line">        self.stride = stride</span><br><span class="line">        self.channels = channels</span><br><span class="line"></span><br><span class="line">        <span class="keyword">assert</span> self.filt_size == <span class="number">3</span></span><br><span class="line">        a = torch.tensor([<span class="number">1.</span>, <span class="number">2.</span>, <span class="number">1.</span>])</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">In [2]: a = torch.tensor([1., 2., 1.])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [3]: filt = (a[:, None] * a[None, :])</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">In [4]: filt</span></span><br><span class="line"><span class="string">Out[4]:</span></span><br><span class="line"><span class="string">tensor([[1., 2., 1.],</span></span><br><span class="line"><span class="string">        [2., 4., 2.],</span></span><br><span class="line"><span class="string">        [1., 2., 1.]])</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        filt = (a[:, <span class="literal">None</span>] * a[<span class="literal">None</span>, :])</span><br><span class="line">        filt = filt / torch.sum(filt)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># self.filt = filt[None, None, :, :].repeat((self.channels, 1, 1, 1))</span></span><br><span class="line">        self.register_buffer(<span class="string">'filt'</span>, filt[<span class="literal">None</span>, <span class="literal">None</span>, :, :].repeat((self.channels, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input)</span>:</span></span><br><span class="line">        input_pad = F.pad(input, (<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>), <span class="string">'reflect'</span>)</span><br><span class="line">        <span class="keyword">return</span> F.conv2d(input_pad, self.filt, stride=self.stride, padding=<span class="number">0</span>, groups=input.shape[<span class="number">1</span>])</span><br></pre></td></tr></table></figure>
<h2 id="Inplace-Activated-BN-Inplace-ABN"><a href="#Inplace-Activated-BN-Inplace-ABN" class="headerlink" title="Inplace Activated BN (Inplace ABN)"></a>Inplace Activated BN (Inplace ABN)</h2><p>把所有的BN+ReLU结构换成了Inplace Activated BN，节省训练时候的显存消耗，并使用Leaky ReLU替换了plain ReLU。使用Inplace ABN增大了少许计算量，不过大大增加了batch size，从而增大了网络的吞吐。</p>
<h2 id="Block-选择"><a href="#Block-选择" class="headerlink" title="Block 选择"></a>Block 选择</h2><p>ResNet论文中，对于不同深度的网络，采取了两种不同的Block构造方法，plain是指bypass直接堆叠$3\times 3$的两个conv。bottleneck指bypass首尾使用$1\times 1$来reduce depth，中间使用单个$3\times 3$的conv。对于18和34层网络，使用plain；对于50及以上使用bottleneck。</p>
<p>这里作者认为plain结构有更大的感受野，所以放在网络浅层（前两个stage）；bottleneck放在网络深层（后两个stage）。具体网络结构见下图：</p>
<p><img src="/img/paper_tresnet_arch_overall.png" alt="arch overall"></p>
<h2 id="SE-op"><a href="#SE-op" class="headerlink" title="SE op"></a>SE op</h2><p>SENet提出的SE改进用的比较多了。这里作者加进来主要是为了提高网络的acc。具体见下吧，没什么好说的：</p>
<p><img src="/img/paper_tresnet_se_layer.png" alt="SE使用"></p>
<p>最后，TResNet的单个block进化成了这个样子：</p>
<p><img src="/img/paper_tresnet_single_block.png" alt="Block in TResNet"></p>
<h2 id="Code-optimization"><a href="#Code-optimization" class="headerlink" title="Code optimization"></a>Code optimization</h2><p>作者这里花了不少的篇幅讲如何使用jit等trick在PyTorch中加速TResNet。据我们的使用经验，jit是有用，但是会被TRT落下一大截。所以用PyTorch native模型去部署并没有什么意思。TResNet中的操作也都是可以TensorRT化的，然而我对它TensorRT的速度持怀疑态度。</p>
<p>所以这里其实我并没有看。有兴趣的话可以对照代码学习下，包括jit的使用。</p>
<h1 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h1><p>实验结果这里贴一下。作者构造了M / L / XL三个系列（大杯，超大杯？），其实M是用来和ResNet-50打擂台的。</p>
<p><img src="/img/paper_tresnet_comparison_with_resnet.png" alt="比较"></p>
<p>下面的消融实验我觉得还是有一定意义的。</p>
<p><img src="/img/paper_tresnet_ablation_study.png" alt="消融实验"></p>
<p>此外，增大input的分辨率一般也能提升网络acc，当然也会拖慢网络。这里作者进行了测试。发现TResNet-M在输入size为448情况下，也能有很大的提升。具体数据这里不贴了。</p>
<p>后面和EfficientNet的比较不多说了。不用TPU，EfficientNet并没有多实用。</p>
<h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>这篇文章读下来，并没有标题和摘要那般有意义。又是ResNet-50被拉出来打，最后其实速度也没提升多少。训练速度其实我们并不太care；inference速度的话和ResNet-50不相上下，而acc其实也高了一点而已。据我们实际工作观察，有TRT加持的ResNet-50的速度更是起飞（nv对ResNet做了很多对应的trick加速，甚至有block级别的plugin加速支持。。见<a href="https://github.com/mlperf/inference_results_v0.5/tree/master/closed/NVIDIA/code/resnet/tensorrt" target="_blank" rel="noopener">ResNet50 Benchmark</a>）。所以其实如果用TRT部署，目测这篇文章的模型结构是拼不过ResNet的。</p>
<p>TResNet把前人的工作做了一个杂烩。又想快又想acc高怎么办？增大吞吐。然而对于GPU这种已经定型的硬件，其实就是增大batch size。。。简单总结下：</p>
<ul>
<li>提升acc：auti-alias downsampling，leaky-relu，se</li>
<li>提升吞吐：spatial-to-depth，inplace-ABN，se不全用，plain / bottleneck 混用</li>
</ul>
<p>当然，上面两个具体内容是有交叉的。</p>
<p><img src="/img/paper_tresnet_main_work.png" alt="main work"></p>
]]></content>
      <tags>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
    <url>/2020/04/15/paper-tf-training-aweare-quantization/</url>
    <content><![CDATA[<p>Google比较早的关于training-aware-quantization的模型量化的paper，不过提供了很多模型量化的基本知识。后面不管是TFLite还是TensorRT，都能在这篇文章中找到对应的基础知识。Arxiv: <a href="https://arxiv.org/abs/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></p>
<a id="more"></a>
<h2 id="Quantization-Scheme-如何量化"><a href="#Quantization-Scheme-如何量化" class="headerlink" title="Quantization Scheme - 如何量化"></a>Quantization Scheme - 如何量化</h2><p>在这个小节，我们更考虑如何在数学上去设计量化的conv操作。在下一个小节，会从更实际的角度来考虑一个CNN的量化。</p>
<p>把量化值$q$映射到浮点数$r$的式子很简单，其中$S$和$Z$是量化参数，分别为零点（zero-point）和scaling factor：</p>
<script type="math/tex; mode=display">r = S(q - Z)</script><p>$S$和$Z$作为量化参数，每个weight array或者activation array是共享的，不同的array之间不共享。</p>
<p>这里的$S$决定了分辨率，也就是最小量化误差。而$Z$可以用来平衡掉数据偏离$0$的bias。</p>
<p>有了上述变换规则， CNN中常见的矩阵相乘可以表示为下图。其中，$\alpha$的值为$1,2,3$，分别表示输入矩阵1，输入矩阵2，结果矩阵3。式4给出了在量化后的$Q$上计算原始浮点$R$的矩阵的方法。$\Sigma$内都是整数运算，只需要最后乘上一个scaling factor $M$即可，从而加速了计算。</p>
<p><img src="/img/paper_tf_taq_matmul.png" alt="矩阵乘法"></p>
<h2 id="CNN的量化-以Conv为例"><a href="#CNN的量化-以Conv为例" class="headerlink" title="CNN的量化 - 以Conv为例"></a>CNN的量化 - 以Conv为例</h2><p>以卷积op为例，说明在实际的CNN模型中量化是如何做的。</p>
<h3 id="layer-fusion"><a href="#layer-fusion" class="headerlink" title="layer fusion"></a>layer fusion</h3><p>首先要做的是layer fusion，例如把常见的conv + bn + relu的3个op简化为一个op。relu这种op的fusion不多说，BN的fusion需要考虑权重，参见下个小节。</p>
<p>卷积实际上还是在进行矩阵乘法。在相乘的时候，用<code>uint8</code>来存储两个操作数，用<code>int32</code>存储结果，以防止相加的时候溢出。如：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">int32 += uint8 * uint8</span><br></pre></td></tr></table></figure>
<p>对于bias，也使用<code>int32</code>存储。作者在这里指出，使用较高精度存储bias，是很有必要的。因为bias的每个值都要加到对应channel的所有activation上去，所以它的比较小的量化误差也会对结果造成比较大的影响。综合起来，对于bias，我们使用$S = S_1S_2$（和conv的结果的scaling factor相同），且zero-point为$0$。</p>
<p>如下图所示，当conv / +bias / relu等操作做完之后，再进行量化。最终完成了fusion之后的conv op的计算。</p>
<p><img src="/img/paper_tf_taq_conv_quantization.png" alt="conv的计算"></p>
<h2 id="Training-aware-Quantization"><a href="#Training-aware-Quantization" class="headerlink" title="Training-aware Quantization"></a>Training-aware Quantization</h2><p>在训练时，是使用float精度模拟量化模型，并使用float更新梯度。在inference的时候，直接在支持INT8的硬件上跑inference。这时候就是原生的量化模型在前向计算了。如下图所示：</p>
<p><img src="/img/paper_tf_taq_train_inference.png" alt="train &amp;&amp; inference"></p>
<p>这里作者首先分析了量化模型相对于原始精度模型可能的掉点原因：</p>
<ul>
<li>同一组weight或activation的不同channel之间的差异较大。因为我们上面已经说过，它们会共享同一个scaling factor。所以如果某个channel的weight特别小，就会造成相对误差很大。</li>
<li>某些离群点(outlier)影响，把整个分布带偏。</li>
</ul>
<p>要使用float模拟定点量化模型前向传播，要特别注意量化究竟是在哪里发生的。要注意的是下面两种情况：</p>
<ul>
<li>对于weight来说，如果有BN的话，要先把BN和conv的weight做fusion，再量化，再前向计算，这里下面会详细说明一下</li>
<li>对于activation来说，一般是激活函数之后，还有bypass的相加或concat之后（对ResNet这种结构）</li>
</ul>
<h3 id="BN的fusion"><a href="#BN的fusion" class="headerlink" title="BN的fusion"></a>BN的fusion</h3><p>前面提到，BN要和conv fusion到一起。按channel做如下操作即可：</p>
<script type="math/tex; mode=display">w_{\text{fold}} = \frac{\gamma w}{\sqrt{\sigma^2 + \epsilon}}</script><p>再加上量化，训练时候的整个计算图如下所示：</p>
<p><img src="/img/paper_tf_taq_conv_bn_folding_in_taining.png" alt="conv/bn folding"></p>
<h3 id="带饱和的量化"><a href="#带饱和的量化" class="headerlink" title="带饱和的量化"></a>带饱和的量化</h3><p>为了避免outlier的影响，在量化之前要先进行一波饱和操作，然后将值域均匀地映射到定点数表示的范围，例如8bit量化为256个stage。</p>
<p><img src="/img/tf_paper_taq_quantize_with_clamp.png" alt="quantization_with_clamp"></p>
<p>不过新的问题出现了。对于weight，可以很容易地找到这样的$a$和$b$，但是对于activation，只有模型跑起来才能知道其范围。文章指出可以使用指数滑动平均来做（就是BN在训练时更新moving_mean和moving_variance的方式），要注意的点在于：</p>
<ul>
<li>滑动平均的系数应该定的很接近$1$，有利于变化比较平滑</li>
<li>开始的若干步训练，可以暂时去掉activation的quantization，有利于网络稳定</li>
</ul>
]]></content>
      <tags>
        <tag>paper</tag>
        <tag>model compression</tag>
        <tag>model quantization</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - DARTS</title>
    <url>/2020/04/14/paper-darts/</url>
    <content><![CDATA[<p>NAS的文章很多了，这篇介绍DARTS：<a href="https://arxiv.org/abs/1806.09055" target="_blank" rel="noopener">DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH</a></p>
<p><img src="/img/paper_darts_basic_idea.png" alt="darts的基本思路"></p>
<a id="more"></a>
<h1 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h1><p>基本沿用前人工作的基本设定：</p>
<ul>
<li>每个cell是由$N$个有序的节点组成的DAG。其中，每个节点$x^{(i)}$表示一个feature map，节点之间的有向弧$E(i,j)$表示由$x^{(i)}$到$x^{(j)}$的某种操作（operation）$o^{(i,j)}$</li>
<li>每个cell有两个输入，一个输出。两个输入分别是第$i-1$个和$i-2$个cell的输出（假设当前cell是第$i$个）。输出是cell内所有节点应用某种Reduction操作（如concat）得到的</li>
<li>每个节点的值是由它前面的所有节点决定的：</li>
</ul>
<script type="math/tex; mode=display">x^{(j)} = \sum_{i<j}o^{(i,j)}x^{(i)}</script><ul>
<li>特殊op“Zero”，表示两个节点之间其实没有连接。</li>
</ul>
<p>遵循上面的设定，网络结构的搜索，就转换为了搜索节点之间operation的问题。下面，我们就对这个问题建模，将它转换为一个可微分用梯度下降搜索的优化问题。</p>
<h1 id="松弛"><a href="#松弛" class="headerlink" title="松弛"></a>松弛</h1><p>“松弛”是一种优化中常用的技巧。</p>
<p>设搜索空间内的所有可能op的集合为$\mathcal{O}$。本来$x^{(i)}$到$x^{(j)}$的op是在这个集合中离散地取值，但是现在我们把它松弛为一个连续问题：</p>
<script type="math/tex; mode=display">\bar{o} = \sum_{o\in\mathcal{O}}\frac{\exp(\alpha_o)}{\sum_{o^{\prime}\in\mathcal{O}}\exp(\alpha_{o^{\prime}})}o</script><p>其中，$\alpha$是一个长度为$|\mathcal{O}|$的向量，$\alpha_o$是里面对应于操作$o$的权重。</p>
<p>当搜索过程结束后，选取$\mathcal{O}$中能够使得$\alpha$中分量最大的那个元素$o^\ast$就是最终两个节点的连接op：</p>
<script type="math/tex; mode=display">o^{\ast} =\underset{o\in\mathcal{O}}{\operatorname{argmax}} \alpha_o</script><p>当然，除了网络结构，我们还需要去学习网络的权重参数$w$。所以整个问题是一个<a href="https://en.wikipedia.org/wiki/Bilevel_optimization" target="_blank" rel="noopener">bi-level</a>的优化问题，$\alpha$是upper-level变量，$w$是lower-level变量。PS：超参数搜索也有相关工作将其建模为bi-level的优化问题求解。</p>
<p>也就是说，给定某个$\alpha$（也就是某个确定的网络结构），在训练集上得到最优的$w$，并将当前的网络结构和权重在验证集上做评估。那个在验证集上得到最好的结果对应的网络结构，就是我们要找的$\alpha$，而网络的权重$w$也对应得出。</p>
<p><img src="/img/paper_darts_optimization_goal.png" alt="优化目标"></p>
<p>也就是说，我们需要最小化模型在验证集上的损失函数；其中，$w$是$\alpha$的某个函数（在这里，$\alpha$是和某个网络结构一一对应的），需要满足训练集上的损失函数最小。</p>
<h1 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h1><p>我们已经把DARTS抽象成了一个优化问题，下面考虑如何高效求解。</p>
<p>显然，按照上面的想法，给定网络结构后，在训练集上得到最优的$w$，再去验证集上跑评估，是不现实的。一是搜索空间巨大，耗时太长；二是仍然无法根据当前的$\alpha$，得到下一步该向哪里走，难道仍然要用启发式或诸如进化算法等方法？如果是求导，那这个链路也太长了，根本不现实。这里作者指出，可以用如下的方式近似梯度：</p>
<p><img src="/img/paper_darts_approximate_gd.png" alt="论文中使用的approximate gradient descent"></p>
<p>看起来括号里面的内容是把求解$w^\ast$的$N$步迭代只取了一步。</p>
<script type="math/tex; mode=display">w^\ast = w - \sum_{N}\xi\frac{\partial\mathcal{L}_{\text{train}}(w,\alpha)}{\partial w}|_{w_i}</script><p>为什么能这样近似？似乎没有什么严密的理论支撑，作者对这个“瑕疵”的处理方法是：</p>
<ul>
<li>拿出CIFAR10和ImageNet以及PTB等数据集上的结果，证明算法在实际上是可以work的，而且效果很好</li>
<li>后面给出了一个在简单优化问题上的讨论</li>
<li>给出了关于超参数设置的经验技巧，最重要的还放出了源码</li>
</ul>
<p>这无疑让整个文章的可信度大大增强。</p>
<p>算法迭代步骤可以描述如下：</p>
<p><img src="/img/paper_darts_alg_precedure.png" alt="迭代"></p>
<p>不过上面的算法描述在实际中并不好用，因为$\alpha$的那一坨梯度一看就很不好求。我们可以通过链式求导法则将其展开。</p>
<p>考虑函数$f(x, g(x))$对$x$的导数$\frac{df}{dx}$。首先令$y = g(x)$，有全微分：</p>
<script type="math/tex; mode=display">df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy</script><p>而且，有$dy = \frac{dg}{dx} dx$。所以，</p>
<script type="math/tex; mode=display">\frac{df}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{dg}{dx}</script><p>回归我们的问题，令$x = \alpha$，$w^\prime = g(\alpha) = w - \xi\nabla_w\mathcal{L}_{train}(w, \alpha)$。</p>
<p>有</p>
<script type="math/tex; mode=display">\frac{dw^\prime}{d\alpha} = -\xi \nabla^2_{w,\alpha}\mathcal{L}_{train}(w,\alpha)</script><p>将它代回上面$\frac{df}{dx}$，就得到了论文里面的形式：</p>
<p><img src="/img/paper_darts_apply_chain_rule.png" alt="论文给出的形式"></p>
<p>化简还没有结束。考虑到$\nabla_w\mathcal{L}_{train}$已经是一个$\mathbb{R}^n$的向量，再对$\alpha$求导，就是一个雅克比矩阵。再和后面那个梯度向量相乘，导致计算量很大。这里作者采用了差分近似微分的方法：</p>
<p><img src="/img/paper_darts_diff_as_gradient.png" alt="差分近似微分"></p>
<p>下面是作者的具体计算代码：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 更新 \alpha</span></span><br><span class="line">architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)</span><br><span class="line"><span class="comment"># 更新w</span></span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure>
<p>下面进入<code>Architect</code>类的内部看下<code>step</code>的实现。当令$\xi=0$时，$w$不用前进一步，<code>architect.step</code>比较简单（对应于<code>unrolled=False</code>）:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_backward_step</span><span class="params">(self, input_valid, target_valid)</span>:</span></span><br><span class="line">  <span class="comment"># loss = L_val(w, alpha)</span></span><br><span class="line">  loss = self.model._loss(input_valid, target_valid)</span><br><span class="line">  loss.backward()</span><br></pre></td></tr></table></figure>
<p>当$\xi\neq 0$时，$w$要在train集合上前进一步，对应于<code>unrolled=True</code>：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_backward_step_unrolled</span><span class="params">(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer)</span>:</span></span><br><span class="line">  <span class="comment"># 在train上更新w = w - \xi * dL_train(w, alpha) / dw</span></span><br><span class="line">  unrolled_model = self._compute_unrolled_model(input_train, target_train, eta, network_optimizer)</span><br><span class="line">  <span class="comment"># 在target上计算loss，然后对alpha求导</span></span><br><span class="line">  unrolled_loss = unrolled_model._loss(input_valid, target_valid)</span><br><span class="line"></span><br><span class="line">  unrolled_loss.backward()</span><br><span class="line">  dalpha = [v.grad <span class="keyword">for</span> v <span class="keyword">in</span> unrolled_model.arch_parameters()]</span><br><span class="line">  vector = [v.grad.data <span class="keyword">for</span> v <span class="keyword">in</span> unrolled_model.parameters()]</span><br><span class="line">  <span class="comment"># 就是那个差分替代微分的式子</span></span><br><span class="line">  implicit_grads = self._hessian_vector_product(vector, input_train, target_train)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> g, ig <span class="keyword">in</span> zip(dalpha, implicit_grads):</span><br><span class="line">    g.data.sub_(eta, ig.data)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 更新alpha</span></span><br><span class="line">  <span class="keyword">for</span> v, g <span class="keyword">in</span> zip(self.model.arch_parameters(), dalpha):</span><br><span class="line">    <span class="keyword">if</span> v.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      v.grad = Variable(g.data)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      v.grad.data.copy_(g.data)</span><br></pre></td></tr></table></figure>
<p>差分的计算具体是这里。注意到作者提到了两个超参数的经验值：</p>
<p><img src="/img/paper_darts_hyper_param_exp_value.png" alt="经验值设置"></p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_hessian_vector_product</span><span class="params">(self, vector, input, target, r=<span class="number">1e-2</span>)</span>:</span></span><br><span class="line">  <span class="comment"># R = \epsilon，按照上面的经验值公式求取</span></span><br><span class="line">  R = r / _concat(vector).norm()</span><br><span class="line">  <span class="comment"># 这是前面那一项</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.add_(R, v)</span><br><span class="line">  loss = self.model._loss(input, target)</span><br><span class="line">  grads_p = torch.autograd.grad(loss, self.model.arch_parameters())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 这是后面那一项</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.sub_(<span class="number">2</span>*R, v)</span><br><span class="line">  loss = self.model._loss(input, target)</span><br><span class="line">  grads_n = torch.autograd.grad(loss, self.model.arch_parameters())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 别忘把w复原</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.add_(R, v)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 最后的近似结果</span></span><br><span class="line">  <span class="keyword">return</span> [(x-y).div_(<span class="number">2</span>*R) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(grads_p, grads_n)]</span><br></pre></td></tr></table></figure>
<h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="算法收敛的讨论"><a href="#算法收敛的讨论" class="headerlink" title="算法收敛的讨论"></a>算法收敛的讨论</h2><p>这里并没有数学上的证明保证方法的收敛性。作者也没有回避这一点，并指出，\xi 的选取对于是否收敛很重要。总之，实验效果很好，说明这个近似是可以work的。</p>
<blockquote>
<p>While we are not currently aware of the convergence guarantees for our optimization algorithm, in practice it is able to reach a fixed point with a suitable choice of ξ</p>
</blockquote>
<p>作者对其做在简单问题下的收敛做了讨论。</p>
<p><img src="/img/paper_darts_convergence_discussion.png" alt="简单问题上的讨论"></p>
<h2 id="CNN-CIFAR10"><a href="#CNN-CIFAR10" class="headerlink" title="CNN @ CIFAR10"></a>CNN @ CIFAR10</h2><h3 id="operation-set"><a href="#operation-set" class="headerlink" title="operation set"></a>operation set</h3><p>选择$3\times 3$，$5\times 5$的kernel size大小的分离卷积和pooling等，再加上identity和zero。具体可以参考代码：<a href="https://github.com/quark0/darts/blob/master/cnn/operations.py" target="_blank" rel="noopener">darts/cnn/operations.py</a>。例如，$3\times 3$的分离卷积如下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 给定input channel和stride，生成3x3分离卷积</span></span><br><span class="line"><span class="comment"># 'sep_conv_3x3' : lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到是如下更小op的串联：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># relu -&gt; 3x3 seperable conv -&gt; bn -&gt; relu -&gt; 3x3 seperable conv(stride=1) -&gt; bn</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SepConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, C_in, C_out, kernel_size, stride, padding, affine=True)</span>:</span></span><br><span class="line">    super(SepConv, self).__init__()</span><br><span class="line">    self.op = nn.Sequential(</span><br><span class="line">      nn.ReLU(inplace=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.BatchNorm2d(C_in, affine=affine),</span><br><span class="line">      nn.ReLU(inplace=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=<span class="number">1</span>, padding=padding, groups=C_in, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_out, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.BatchNorm2d(C_out, affine=affine),</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.op(x)</span><br></pre></td></tr></table></figure>
<p>主要特点是：</p>
<ul>
<li>顺序为relu -&gt; conv -&gt; bn</li>
<li>可分离卷积重复两次</li>
</ul>
<p>这也是前面NAS文章的惯常操作。</p>
<h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>在Cell尺度上，每个cell由$N=7$个node组成。输入节点如前所述（有时候可能需要$1\times 1$节点），输出节点是该cell的所有中间节点（不包括input节点）在channel上的concat。</p>
<p>在网络宏观尺度上，cell堆叠形成最后的网络。Cell也被分为<code>normal</code>和<code>reduce</code>两种。后者会对输入节点取stride为$2$，从而downsampling。在网络的$1/3$和$2/3$深度处为reduce cell，其他为normal cell。normal和reduce cell分别有一套共享的$\alpha$参数。从而，整个网络的结构可以被两组$\alpha_{\text{normal}}$和$\alpha_{\text{reduce}}$完全描述。</p>
<p>下图直观地展示了在CIFAR10上搜索出来的cell结构：</p>
<ul>
<li>两个输入，一个输出，四个中间节点，它们通过concat操作成了输出</li>
<li>每个中间节点入度都是2，也就是我们选取的是$\alpha$中top $K=2$的op</li>
</ul>
<p><img src="/img/paper_darts_net_arch_on_cifar10.png" alt="DARTS在CIFAR10上搜出的cell"></p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在CIFAR10上，搜出的网络性能和之前基于RL或进化算法的SOTA方法是可比的，而且GPU小时数明显缩短。DARTS方法和ENAS是少数能够在&lt;10 GPU*days的计算资源下做出比较好结果的方法。其中DARTS又比ENAS有较好的TestError。作者对此也做了说明：</p>
<blockquote>
<p>DARTS outperformed ENAS (Pham et al., 2018b) by discovering cells with comparable error rates but lessparameters. The longer search time is due to the fact that we have repeated the search process fourtimes for cell selection. This practice is less important for convolutional cells however, because theperformance of discovered architectures does not strongly depend on initialization</p>
</blockquote>
<p><img src="/img/paper_darts_sota_comparision_cnn.png" alt="CNN搜索与SOTA比较"></p>
<h2 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h2><p><img src="/img/paper_darts_result_imagenet.png" alt="在ImageNet上的结果"></p>
<h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>文章的主要工作：</p>
<ul>
<li>将NAS问题通过松弛建模为一个关于模型结构的优化问题</li>
<li>提出了一个不错的解决该优化问题的梯度下降解法</li>
<li>在相关任务上证明了方法的有效性</li>
<li>给大家在堆GPU资源用强化学习 / 进化算法之外，指出了一条可行的NAS求解之路</li>
</ul>
<p>文章的作者应该是有比较多的数学优化方面的知识。引入权重向量并softmax求取top K op应该是还算让人容易想到，但后面的优化求解就很容易出错劝退。</p>
<h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul>
<li>PyTorch实现的DARTS：<a href="https://github.com/quark0/darts" target="_blank" rel="noopener">quark0/darts</a></li>
</ul>
]]></content>
      <tags>
        <tag>paper</tag>
        <tag>nas</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT Missing Semester - Command-line Environment</title>
    <url>/2020/04/06/mit-missing-semester-05-commandline-env/</url>
    <content><![CDATA[<p>这是<a href="https://missing.csail.mit.edu/2020/command-line/" target="_blank" rel="noopener">MIT Missing Semester系列</a>的第五讲，主要关于shell下对于进程（Process)的控制。</p>
<a id="more"></a>
<h1 id="Job-Control"><a href="#Job-Control" class="headerlink" title="Job Control"></a>Job Control</h1><h2 id="Kill-Process"><a href="#Kill-Process" class="headerlink" title="Kill Process"></a>Kill Process</h2><p>最简单的，要想结束当前正在进行的进程，使用<code>Ctrl+c</code>即可。而实际上，这是通过向其发送了<code>SIGINT</code>（INT是interrupt的意思）的信号量实现的。例如，执行如下脚本时候，如果按下<code>Ctrl+c</code>，并不会退出，而只是会执行<code>handler</code>函数。更多信号量的文档可以参考<code>man signal</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> signal, time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handler</span><span class="params">(signum, time)</span>:</span></span><br><span class="line">    <span class="comment"># 中断会进这里</span></span><br><span class="line">    print(<span class="string">"\nI got a SIGINT, but I am not stopping"</span>)</span><br><span class="line"></span><br><span class="line">signal.signal(signal.SIGINT, handler)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    time.sleep(<span class="number">.1</span>)</span><br><span class="line">    print(<span class="string">"\r&#123;&#125;"</span>.format(i), end=<span class="string">""</span>)</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h2 id="休眠-suspend"><a href="#休眠-suspend" class="headerlink" title="休眠 (suspend)"></a>休眠 (suspend)</h2><p>使用<code>Ctrl-z</code>发送<code>SIGSTOP</code>信号可以使得进程暂时休眠，并可以后续继续运行。我们可以用下面的代码做实验。在<code>Ctrl+z</code>后，程序暂时休眠停止运行，计数器的值也不再更新，直到使用<code>fg</code>命令，才重新开始。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 每隔1s打印当前计数器的值</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> count(<span class="number">0</span>):</span><br><span class="line">        print(<span class="string">'current time: &#123;&#125;'</span>.format(i))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure>
<p><img src="/img/mit_missing_semester_05_example_suspend_process.png" alt="运行实例"></p>
<p>除了<code>fg</code>以外，还可以使用<code>bg</code>来开始被休眠的进程。只不过<code>bg</code>会让重新开始的进程在后台运行。</p>
<p>这里直接贴出讲师的一个例子：</p>
<p><img src="/img/mit_missing_semester_05_example_job_control.png" alt="job control"></p>
<p>需要注意的是：</p>
<ul>
<li><code>bg</code>的使用：将进程状态从suspending转到running</li>
<li><code>jobs</code>可以列出当前所有未完成的进程</li>
<li>可以使用<code>%number</code>的形式引用<code>jobs</code>列出的进程</li>
<li>除了快捷键，也可以使用<code>kill</code>向指定进程发送信号量，具体可以参考<code>man kill</code></li>
</ul>
<h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><p>远程ssh到开发机是常见的操作，这里有一些比较零散的知识点记录。</p>
<ul>
<li>使用<code>ssh-copy-id $host_name</code>将当前机器的ssh public key拷贝到给定的远程host机器（之前我都是通过手动copy）</li>
<li>将文件拷贝到远程机器的N种方法：<ul>
<li>使用<code>ssh + tee</code>：<code>cat local_file | ssh remote_server tee server_file</code>，这是利用了<code>ssh remote_server</code>可以后接shell命令</li>
<li>使用<code>scp</code>，这也是我最常用的命令了</li>
<li>使用<code>rsync</code>，更强大的<code>scp</code>，可以跳过重复文件等</li>
</ul>
</li>
<li>端口转发，例如在远程服务器的<code>8888</code>端口启动了jupyter notebook，可以使用<code>ssh -L 9999:localhost:8888 foobar@remote_server</code>将其转发到本机的<code>9999</code>端口，这样在本机浏览<code>localhost:9999</code>即可访问笔记本</li>
</ul>
<p><img src="/img/mit_missing_semester_05_local_port_forward.png" alt="local port forwarding"><br><img src="/img/mit_missing_semester_05_remote_port_forward.png" alt="remote port forwarding"></p>
<h1 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h1><h2 id="创建alias"><a href="#创建alias" class="headerlink" title="创建alias"></a>创建alias</h2><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">alias</span> dc=<span class="string">"cd"</span></span><br><span class="line"><span class="comment"># 列出最常用的10个命令</span></span><br><span class="line"><span class="comment">## 将第一列（表示序号）设为空字符串</span></span><br><span class="line"><span class="built_in">history</span> | awk <span class="string">'&#123;$1="";print substr($0,2)&#125;'</span> | sort | uniq -c | sort -n | tail -n 10</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>TFRecord 简介</title>
    <url>/2020/04/03/tfrecord-introduction/</url>
    <content><![CDATA[<p>TFRecord是TensorFlow中常用的数据打包格式。通过将训练数据或测试数据打包成TFRecord文件，就可以配合TF中相关的DataLoader / Transformer等API实现数据的加载和处理，便于高效地训练和评估模型。</p>
<p>TF官方tutorial：<a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord and tf.Example</a></p>
<p><img src="/img/tfrecord_logo.jpeg" alt="TFRecord好！"></p>
<a id="more"></a>
<h1 id="组成TFReocrd的砖石：tf-Example"><a href="#组成TFReocrd的砖石：tf-Example" class="headerlink" title="组成TFReocrd的砖石：tf.Example"></a>组成TFReocrd的砖石：<code>tf.Example</code></h1><p><code>tf.Example</code>是一个Protobuffer定义的message，表达了一组string到bytes value的映射。TFRecord文件里面其实就是存储的序列化的<code>tf.Example</code>。如果对Protobuffer不熟悉，可以去看下Google的<a href="https://developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="noopener">文档</a>和<a href="https://developers.google.com/protocol-buffers/docs/pythontutorial" target="_blank" rel="noopener">教程</a>。</p>
<h2 id="Example-是什么"><a href="#Example-是什么" class="headerlink" title="Example 是什么"></a>Example 是什么</h2><p>我们可以具体到相关代码去详细地看下<code>tf.Example</code>的构成。作为一个Protobuffer message，它被定义在文件<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/core/example/example.proto#L88" target="_blank" rel="noopener">core/example/example.proto</a>中：</p>
<figure class="highlight protobuf"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line">  Features features = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>好吧，原来只是包了一层<code>Features</code>的message。我们还需要进一步去查找<code>Features</code>的message<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/core/example/feature.proto#L85-L88" target="_blank" rel="noopener">定义</a>：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">message Features &#123;</span><br><span class="line">  // Map from feature name to feature.</span><br><span class="line">  map&lt;string, Feature&gt; feature = 1;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>到这里，我们可以看出，<code>tf.Example</code>确实表达了一组string到Feature的映射。其中，这个string表示feature name，后面的Feature又是一个message。继续寻找：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// Containers for non-sequential data.</span><br><span class="line">message Feature &#123;</span><br><span class="line">  // Each feature can be exactly one kind.</span><br><span class="line">  oneof kind &#123;</span><br><span class="line">    BytesList bytes_list = 1;</span><br><span class="line">    FloatList float_list = 2;</span><br><span class="line">    Int64List int64_list = 3;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 这里摘一个 Int64List 的定义如下，float/bytes同理</span><br><span class="line">message Int64List &#123;</span><br><span class="line">  // 可以看到，如其名所示，表示的是int64数值的列表</span><br><span class="line">  repeated int64 value = 1 [packed = true];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>看起来，是描述了一组各种数据类型的list，包括二进制字节流，float或者int64的数值列表。</p>
<h2 id="属于自己的Example"><a href="#属于自己的Example" class="headerlink" title="属于自己的Example"></a>属于自己的Example</h2><p>有了上面的分解，要想构造自己数据集的<code>tf.Example</code>，就可以一步步组合起来。</p>
<p>首先用下面的几个帮助函数，将给定的Python类型数据转换为对应的Feature。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># The following functions can be used to convert a value to a type compatible</span></span><br><span class="line"><span class="comment"># with tf.Example.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">  <span class="keyword">if</span> isinstance(value, type(tf.constant(<span class="number">0</span>))):</span><br><span class="line">    value = value.numpy() <span class="comment"># BytesList won't unpack a string from an EagerTensor.</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们直接认为value是个标量，如果是tf.Tensor，可以使用</span></span><br><span class="line"><span class="comment"># `tf.io.serialize_tensor`将其序列化为bytes</span></span><br><span class="line"><span class="comment"># `tf.io.parse_tensor`可以反序列化为tf.Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_float_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a float_list from a float / double."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns an int64_list from a bool / enum / int / uint."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br></pre></td></tr></table></figure>
<p>有了<code>Feature</code>，就可以组成<code>Features</code>，只要把对应的名字作为string传进去就行了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">features_dict = &#123;<span class="string">'image'</span>: _bytes_feature(image_data), <span class="string">'label'</span>: _int64_feature(label)&#125;</span><br><span class="line">features = tf.train.Features(feature=features_dict)</span><br></pre></td></tr></table></figure>
<p><code>Example</code>自然也就有了：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">example = tf.train.Example(features=features)</span><br></pre></td></tr></table></figure>
<h1 id="TFRecord"><a href="#TFRecord" class="headerlink" title="TFRecord"></a>TFRecord</h1><p>TFRecord是一个二进制文件，只能顺序读取。它的数据打包格式如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uint64 length</span><br><span class="line">uint32 masked_crc32_of_length</span><br><span class="line">byte   data[length]</span><br><span class="line">uint32 masked_crc32_of_data</span><br></pre></td></tr></table></figure>
<p>其中，<code>data[length]</code>通常是一个<code>Example</code>序列化之后的数据。</p>
<h2 id="将Example写入TFRecord"><a href="#将Example写入TFRecord" class="headerlink" title="将Example写入TFRecord"></a>将<code>Example</code>写入TFRecord</h2><p>可以使用python API，将<code>Example</code>proto写入TFRecord文件。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.io.TFRecordWriter(filename) <span class="keyword">as</span> writer:</span><br><span class="line">    <span class="keyword">for</span> image_file <span class="keyword">in</span> image_files: </span><br><span class="line">        image_data = open(image_file, <span class="string">'rb'</span>).read()</span><br><span class="line">        features = tf.train.Features(feature=&#123;<span class="string">'image'</span>: _bytes_feature(image_Data)&#125;)</span><br><span class="line">        <span class="comment"># 得到 example</span></span><br><span class="line">        example = tf.train.Example(features=features)</span><br><span class="line">        <span class="comment"># 通过调用message.SerializeToString() 将其序列化</span></span><br><span class="line">        writer.write(example.SerializeToString())</span><br></pre></td></tr></table></figure>
<h2 id="读取TFRecord中的Example"><a href="#读取TFRecord中的Example" class="headerlink" title="读取TFRecord中的Example"></a>读取TFRecord中的<code>Example</code></h2><p>通过<code>tf.data.TFRecordDataset</code>得到<code>Dataset</code>，然后遍历它，并反序列化，就可以得到原始数据。下面的代码段从TFRecord文件中读取刚刚写入的image：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_from_single_example</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="string">""" 从example message反序列化得到当初写入的内容 """</span></span><br><span class="line">    <span class="comment"># 描述features</span></span><br><span class="line">    desc = &#123;<span class="string">'image'</span>: tf.io.FixedLenFeature([], dtype=tf.string)&#125;</span><br><span class="line">    <span class="comment"># 使用tf.io.parse_single_example反序列化</span></span><br><span class="line">    <span class="keyword">return</span> tf.io.parse_single_example(example_proto, desc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_image_from_bytes</span><span class="params">(image_data)</span>:</span></span><br><span class="line">    <span class="string">""" use cv2.imdecode decode image from raw binary data """</span></span><br><span class="line">    bytes_array = np.array(bytearray(image_data))</span><br><span class="line">    <span class="keyword">return</span> cv2.imdecode(bytes_array, cv2.IMREAD_COLOR)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_from_single_example</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="string">""" get image fom example serialized data """</span></span><br><span class="line">    data = parse_from_single_example(example_proto)</span><br><span class="line">    image_data = data[<span class="string">'image'</span>].numpy()</span><br><span class="line">    <span class="comment"># the image_data is str</span></span><br><span class="line">    <span class="comment"># decode the binary bytes to get the image</span></span><br><span class="line">    <span class="keyword">return</span> decode_image_from_bytes(image_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = tf.data.TFRecordDataset(tfrecord_file)</span><br><span class="line">data_iter = iter(dataset)</span><br><span class="line">first_example = next(data_iter)</span><br><span class="line"></span><br><span class="line">first_image = get_image_from_single_example(first_example)</span><br></pre></td></tr></table></figure>
<p>或者可以用<code>map</code>来将parser的pipeline应用于原dataset：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意这里不能用get_image_from_single_example</span></span><br><span class="line"><span class="comment"># 因为 `.numpy()` 不能用于静态 Map</span></span><br><span class="line">image_data = dataset.map(parse_from_single_example)</span><br><span class="line"></span><br><span class="line">first_image_data = next(iter(image_data))</span><br><span class="line">image = decode_image_from_bytes(first_image_data[<span class="string">'image'</span>].numpy())</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tf</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 travis 发布博客</title>
    <url>/2020/03/21/publish-blog-with-travis/</url>
    <content><![CDATA[<p>我的博客之前是通过手动调用<code>hexo generate</code>生成<code>public</code>，并更新到<code>master</code>分支的。最近试了下使用travis直接发布，发现省事了不少。官方的文档其实还是挺全的，不过也碰到了一些坑，记录在这里。</p>
<p><img src="/img/travis_logo.png" alt="travis logo"><br><a id="more"></a></p>
<h2 id="Travis-简介"><a href="#Travis-简介" class="headerlink" title="Travis 简介"></a>Travis 简介</h2><p>Travis（/‘trævis/）是一款CI（持续集成）工具。<a href="https://www.zhihu.com/question/23444990" target="_blank" rel="noopener">这里</a>有一篇关于持续集成的知乎问答。</p>
<blockquote>
<p>持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。</p>
</blockquote>
<p>在公司内团队内开发某个项目的时候，我们常常也会使用jenkins等工具作为CI的工具。比如当某位同学试图向<code>master</code>分支merge代码时，就会触发测试。机器人会在相关MR下评论，通知build和test的结果。</p>
<p><img src="/img/what_is_ci.jpg" alt="什么是持续集成"></p>
<p>在GitHub的很多项目中，都有CI的身影。如下图所示，在caffe项目的README页面中就显示了该项目目前的CI状态。至于如何在项目中添加这个功能，可以参考<a href="https://developer.github.com/v3/repos/statuses/" target="_blank" rel="noopener">这个页面</a>，这里暂时不多说。</p>
<p><img src="/img/caffe_build_status.png" alt="caffe example"></p>
<p>而如果我们想在Github自己的项目中使用CI，就可以考虑<a href="https://travis-ci.org/" target="_blank" rel="noopener">Travis</a>。</p>
<p>上面介绍了什么是CI以及travis可以帮助我们进行CI。那为什么可以使用这个功能发布博客吗？因为我们的博客本身是一个依托于Github page功能的静态网站。首先我们有了<code>username.github.io</code>这个repo，然后在其<code>master</code>分支下放置了hexo生成的静态HTML，就可以看到博客了。想一下之前发布博客的步骤：</p>
<ul>
<li>编写内容</li>
<li>使用<code>hexo generate</code>生成HTML等（会放在一个<code>public/</code>文件夹下）</li>
<li>将<code>public/</code>发布到repo的master分支下</li>
</ul>
<p>现在我们就可以把后两步使用travis完成。当我们编写好内容后，将其推送到repo的非master分支，并触发CI的构建，就可以自动完成后两步。</p>
<h2 id="配置-Travis"><a href="#配置-Travis" class="headerlink" title="配置 Travis"></a>配置 Travis</h2><p>阮一峰的博客里面有一篇<a href="http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html" target="_blank" rel="noopener">travis教程</a>可以参考。我们这里因为只是一个比较简单的博客发布功能，所以不再展开。</p>
<p>如果你还没有自己的博客，可以去搜索如何使用hexo搭建博客系统，首先确保本地能够跑起来，成功访问自己的博客页面。如果已经有了博客，可以参考<a href="https://github.com/hexojs/hexo-starter" target="_blank" rel="noopener">这个repo</a>，整理下自己的目录结构，尤其是<code>.gitignore</code>。注意，这个包含原始网页内容的分支不能是<code>master</code>。你可以建立一个叫<code>hexo</code>的分支来做这件事。</p>
<p>接下来，在你的账户内添加<code>Travis CI</code>：<a href="https://github.com/marketplace/travis-ci" target="_blank" rel="noopener">Travis CI</a>。并在<a href="https://github.com/settings/installations" target="_blank" rel="noopener">Applications settings</a>页面确认Travis可以访问你的repo。这时候应该会重定向到Travis的页面。</p>
<p>打开新窗口，去往<a href="https://github.com/settings/tokens" target="_blank" rel="noopener">Github token</a>生成new token。</p>
<p>在Travis中，找到repo setting，并在<code>Environment Variables</code>中，设置name为<code>GH_TOKEN</code>，并将上面的token加入。</p>
<p>在你的repo中，checkout到<code>hexo</code>分支。并添加<code>.travis.yml</code>文件，内容如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo: false</span><br><span class="line">language: node_js</span><br><span class="line">node_js:</span><br><span class="line">  - 10 # use nodejs v10 LTS</span><br><span class="line">cache: npm</span><br><span class="line">branches:</span><br><span class="line">  only:</span><br><span class="line">    - hexo # build hexo branch only</span><br><span class="line">script:</span><br><span class="line">  - hexo generate # generate static files</span><br><span class="line">deploy:</span><br><span class="line">  provider: pages</span><br><span class="line">  skip-cleanup: true</span><br><span class="line">  github-token: $GH_TOKEN</span><br><span class="line">  keep-history: true</span><br><span class="line">  on:</span><br><span class="line">    branch: hexo</span><br><span class="line">  target_branch: master  # 这行很重要</span><br><span class="line">  local-dir: public</span><br></pre></td></tr></table></figure>
<p>注意上面一定要设置<code>target_branch</code>一项，因为我们需要生成的内容写入<code>master</code>分支。关于这些选项的含义，<a href="https://docs.travis-ci.com/user/deployment/pages/" target="_blank" rel="noopener">Travis</a>有相关介绍。不过我是看的<a href="https://bookdown.org/yihui/blogdown/travis-github.html" target="_blank" rel="noopener">这个</a>。这里面的解释更加针对博客部署的场景，建议读一下。</p>
<p>接下来，我们往<code>hexo</code>分支上推送内容，就会触发CI并生成网页了！</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>你可以前往<a href="https://travis-ci.com/dashboard" target="_blank" rel="noopener">Travis Dashboard</a>查看自己项目的构建情况。</p>
<p><img src="/img/travis_pane.png" alt="travis dashboard"></p>
<p>另外，如果你在hexo中也用了<code>landscape</code>主题，可能会报fail。解决方法很粗暴，直接删除这个文件就行了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">rm themes/landscape/README.md</span><br></pre></td></tr></table></figure>
<h2 id="注意"><a href="#注意" class="headerlink" title="注意"></a>注意</h2><p>我发现travis有两个网站，分别是travis-ci.com和travis-ci.org。这两个网站有什么区别？一句话解释：这两个网站都是可用的travis-ci网站，但目前要用.com那个。具体解释见这个帖子：<a href="https://devops.stackexchange.com/questions/1201/whats-the-difference-between-travis-ci-org-and-travis-ci-com" target="_blank" rel="noopener">What’s the difference between travis-ci.org and travis-ci.com?</a></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li><a href="https://hexo.io/docs/github-pages" target="_blank" rel="noopener">官方教程</a></li>
</ul>
<p>上面的官方教程已经说的比较详细了，但是有坑，就是需要设置<code>target_branch</code>没有说明。</p>
]]></content>
      <tags>
        <tag>tool</tag>
        <tag>travis</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT Missing Semester - Data Wrangling</title>
    <url>/2020/03/15/mit-missing-semester-04-data-wrangling/</url>
    <content><![CDATA[<p>这是<a href="https://missing.csail.mit.edu/2020/data-wrangling/" target="_blank" rel="noopener">MIT Missing Semester系列</a>的第四讲。关于vim的第三讲跳过。Data Wrangling在这里的意思是对数据做变换（Transformation）。例如将一个MP4格式的视频转换为AVI，或或者是从日志中提取所需要的结构化文本信息。具体到本课，主要是处理文本信息：如何匹配到我们感兴趣的信息，如果构建一个处理的pipeline等。</p>
<a id="more"></a>
<p>`</p>
<h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>在很久以前，总结了一篇关于python中的正则表达式的常用用法，竟然也是博客的第一篇文章：<a href="https://xmfbit.github.io/2014/07/17/python-reg-exp/">python正则表达式</a>。</p>
<p>推荐一个<a href="https://regexone.com/" target="_blank" rel="noopener">交互式的正则表达式学习网站</a>。这里有一些简单的规则：</p>
<ul>
<li><code>.</code>匹配任意字符，除了<code>\n</code></li>
<li><code>*</code>匹配前缀的任意个，包括0个</li>
<li><code>+</code>匹配前缀的任意个，不包括0个</li>
<li><code>?</code>匹配前缀的0个或1个</li>
<li><code>[abc]</code>匹配给定集合里面的元素，例如这里匹配<code>a</code>或<code>b</code>或<code>c</code></li>
<li><code>(ab)</code>匹配给定的组合，例如这里匹配<code>ab</code></li>
<li><code>(exp1|exp2)</code>匹配<code>exp1</code>或<code>exp2</code></li>
<li><code>^</code>指示一行的开头</li>
<li><code>$</code>指示一行的结尾</li>
</ul>
<p>要注意的是，<code>(</code>在下面的sed中如果没有特殊说明，被视为普通字符，需要加上<code>-E</code>选项。</p>
<p>如果我们想要指定具体的次数呢？可以使用<code>.{n}</code>的形式。例如，<code>a{3}</code>表明匹配3个<code>a</code>；<code>[ab]{4}</code>匹配4个<code>a</code>或<code>b</code>。使用range表达式，表明在某个范围内：<code>.{2,5}</code>表示2到5个任意字符。</p>
<h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed(Stream Editor)可以帮助我们变换文本。sed每次从输入流中读入一行，作相应变换，并输出。</p>
<blockquote>
<p>A stream editor is used to perform basic text transformations on an input stream (a file or input from a pipeline)</p>
</blockquote>
<p><img src="/img/sed_workflow.png" alt="sed workflow"></p>
<p><a href="https://www.tutorialspoint.com/sed/index.htm" target="_blank" rel="noopener">这里</a>是一个sed的教程，下面结合该教程和讲师的实例，对sed使用做一说明。</p>
<h3 id="sed的其他用法"><a href="#sed的其他用法" class="headerlink" title="sed的其他用法"></a>sed的其他用法</h3><p>这里首先对sed的其他用法做一说明。</p>
<p>使用<code>-e</code>可以传入一些命令，例如<code>1d</code>就是删除第一行。通过串联，可以删除多行。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 删除第一行和第五行</span></span><br><span class="line">sed -e <span class="string">'1d'</span> -e `5d` input</span><br></pre></td></tr></table></figure>
<p>还可以使用<code>-f</code>指示从某个文件内读取命令，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1d\n2d"</span> &gt; arg.txt</span><br><span class="line">sed -f arg.txt input</span><br></pre></td></tr></table></figure>
<h3 id="sed对文本进行查找替换"><a href="#sed对文本进行查找替换" class="headerlink" title="sed对文本进行查找替换"></a>sed对文本进行查找替换</h3><p>sed最常用的场景之一，使用如下命令，将文本文件中的<code>pattern</code>（一个正则表达式）替换为<code>new</code>。当<code>new</code>为空时，将直接删去<code>pattern</code>。最后的<code>g</code>如果不加，则只匹配一次，加上<code>g</code>表示全局。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sed <span class="string">'s/pattern/new/ filename/g'</span></span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># .* 表示任意多个任意字符，包括0个。所以下面会把 says hello以及它前面的内容都删掉</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to dog"</span> | sed <span class="string">'s/.*says hello//'</span></span><br><span class="line"><span class="comment"># output: to dog</span></span><br></pre></td></tr></table></figure>
<p>注意，<code>.*</code>组合是greedy的。这意味着它会尽可能多地去匹配任意字符。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to says hello to dog"</span> | sed <span class="string">'s/.*says hello//'</span></span><br><span class="line"><span class="comment">#output: to dog</span></span><br></pre></td></tr></table></figure>
<h3 id="capture-group"><a href="#capture-group" class="headerlink" title="capture group"></a>capture group</h3><p>capture group是指我希望记住匹配到的值。在正则表达式中，使用<code>()</code>括起来的就是capture group。我们可以使用<code>\1</code>，<code>\2</code>来引用它们。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 想知道cat对谁打招呼了</span></span><br><span class="line"><span class="comment"># 我们使用`(.*)`来匹配任意多的字符，也就是dog</span></span><br><span class="line"><span class="comment"># 并将整行替换为`\1`，也就是capture group</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to dog"</span> | sed -E <span class="string">'s/.*says hello to (.*)/\1/'</span></span><br><span class="line"><span class="comment"># output：dog</span></span><br></pre></td></tr></table></figure>
<h2 id="sort和uniq"><a href="#sort和uniq" class="headerlink" title="sort和uniq"></a>sort和uniq</h2><p>sort，顾名思义，读入input，排序，再将它们输出。uniq，可以将<strong>紧邻的</strong>相同行进行合并。因为uniq只能合并紧邻的向同行，所以常常和sort配合使用。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># -c 会在每一行前面加上一列，显示重复出现的次数</span></span><br><span class="line"><span class="comment"># 继续排序，并输出出现频率最高的10个 （sort升序排列，使用tail找到末尾，也就是最大的）</span></span><br><span class="line"><span class="comment"># sort -k        -k, --key=KEYDEF</span></span><br><span class="line"><span class="comment">#                sort via a key; KEYDEF gives location and type</span></span><br><span class="line"><span class="comment">#      -n        numeric sort</span></span><br><span class="line">xxx | sort | uniq -c | sort -nk1,1 | tail -10</span><br></pre></td></tr></table></figure>
<h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>awk是另一种stream editor。与sed相比，它更针对于成列的数据。例如，我们可以打印文本文件的第一列：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> input</span><br></pre></td></tr></table></figure>
<p>awk的一般用法还会加上<code>pattern</code>，例如<code>awk &#39;pattern {action}&#39; input</code>。例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $0表示非特定列，而是整行</span></span><br><span class="line"><span class="comment"># 找出第一列符合给定pattern的那些行，并打印这个整行</span></span><br><span class="line">awk <span class="string">'$1 ~ /pattern/ &#123;print $0&#125;'</span> input</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world\nsmile world"</span> | awk <span class="string">'$1 ~ /^h.*o$/ &#123;print $1&#125;'</span></span><br><span class="line"><span class="comment"># output: hello</span></span><br><span class="line"><span class="comment"># 这里smile这行因为不符合pattern，所以被filter掉了</span></span><br></pre></td></tr></table></figure>
<p>awk的功能还远不止此。awk中可以使用循环，分支等语句构成更复杂的逻辑。</p>
<p>paste命令可以用来合并多行为一行。下面的命令将输入文件的第一列顺序拼接为一行，并使用逗号分隔。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> input | paste -sd,</span><br></pre></td></tr></table></figure>
<blockquote>
<p>The paste utility concatenates the corresponding lines of the given input files, replacing all but the last file’s newline characters with a single tab character, and writes the resulting lines to standard output.</p>
</blockquote>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本课主要介绍了一些常用的文本处理命令，包括sed, awk, sort, uniq, paste等。下面使用tldr命令给出这些命令的常用用法供参考。</p>
<h3 id="sed-1"><a href="#sed-1" class="headerlink" title="sed"></a>sed</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  ~ tldr sed</span><br><span class="line"><span class="comment"># sed</span></span><br><span class="line"></span><br><span class="line">  Edit text <span class="keyword">in</span> a scriptable manner.</span><br><span class="line"></span><br><span class="line">- Replace the first occurrence of a regular expression <span class="keyword">in</span> each line of a file, and <span class="built_in">print</span> the result:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'s/regex/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace all occurrences of an extended regular expression <span class="keyword">in</span> a file, and <span class="built_in">print</span> the result:</span><br><span class="line"></span><br><span class="line">  sed -r <span class="string">'s/regex/replace/g'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace all occurrences of a string <span class="keyword">in</span> a file, overwriting the file (i.e. <span class="keyword">in</span>-place):</span><br><span class="line"></span><br><span class="line">  sed -i <span class="string">'s/find/replace/g'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace only on lines matching the line pattern:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'/line_pattern/s/find/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Delete lines matching the line pattern:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'/line_pattern/d'</span> filename</span><br><span class="line"></span><br><span class="line">- Print only text between n-th line till the next empty line:</span><br><span class="line"></span><br><span class="line">  sed -n <span class="string">'n,/^$/p'</span> filename</span><br><span class="line"></span><br><span class="line">- Apply multiple find-replace expressions to a file:</span><br><span class="line"></span><br><span class="line">  sed -e <span class="string">'s/find/replace/'</span> -e <span class="string">'s/find/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace separator / by any other character not used <span class="keyword">in</span> the find or replace patterns, e.g., <span class="comment">#:</span></span><br><span class="line"></span><br><span class="line">  sed <span class="string">'s#find#replace#'</span> filename</span><br><span class="line"></span><br><span class="line">- Print only the n-th line of a file:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'nq;d'</span> filename</span><br></pre></td></tr></table></figure>
<h3 id="awk-1"><a href="#awk-1" class="headerlink" title="awk"></a>awk</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  ~ tldr awk</span><br><span class="line"><span class="comment"># awk</span></span><br><span class="line"></span><br><span class="line">  A versatile programming language <span class="keyword">for</span> working on files.</span><br><span class="line"></span><br><span class="line">- Print the fifth column (a.k.a. field) <span class="keyword">in</span> a space-separated file:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;print $5&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print the second column of the lines containing <span class="string">"something"</span> <span class="keyword">in</span> a space-separated file:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'/something/ &#123;print $2&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print the last column of each line <span class="keyword">in</span> a file, using a comma (instead of space) as a field separator:</span><br><span class="line"></span><br><span class="line">  awk -F <span class="string">','</span> <span class="string">'&#123;print $NF&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Sum the values <span class="keyword">in</span> the first column of a file and <span class="built_in">print</span> the total:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;s+=$1&#125; END &#123;print s&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Sum the values <span class="keyword">in</span> the first column and pretty-print the values and <span class="keyword">then</span> the total:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;s+=$1; print $1&#125; END &#123;print "--------"; print s&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print every third line starting from the first line:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'NR%3==1'</span> filename</span><br></pre></td></tr></table></figure>
<h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  ~ tldr tldr</span><br><span class="line"><span class="comment"># sort</span></span><br><span class="line"></span><br><span class="line">  Sort lines of text files.</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> ascending order:</span><br><span class="line"></span><br><span class="line">  sort filename</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> descending order:</span><br><span class="line"></span><br><span class="line">  sort -r filename</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> <span class="keyword">case</span>-insensitive way:</span><br><span class="line"></span><br><span class="line">  sort --ignore-case filename</span><br><span class="line"></span><br><span class="line">- Sort a file using numeric rather than alphabetic order:</span><br><span class="line"></span><br><span class="line">  sort -n filename</span><br><span class="line"></span><br><span class="line">- Sort the passwd file by the 3rd field, numerically:</span><br><span class="line"></span><br><span class="line">  sort -t: -k 3n /etc/passwd</span><br><span class="line"></span><br><span class="line">- Sort a file preserving only unique lines:</span><br><span class="line"></span><br><span class="line">  sort -u filename</span><br><span class="line"></span><br><span class="line">- Sort human-readable numbers (<span class="keyword">in</span> this <span class="keyword">case</span> the 5th field of `ls -lh`):</span><br><span class="line"></span><br><span class="line">  ls -lh | sort -h -k 5</span><br></pre></td></tr></table></figure>
<h3 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">➜  ~ tldr uniq</span><br><span class="line"><span class="comment"># uniq</span></span><br><span class="line"></span><br><span class="line">  Output the unique lines from the given input or file.</span><br><span class="line">  Since it does not detect repeated lines unless they are adjacent, we need to sort them first.</span><br><span class="line"></span><br><span class="line">- Display each line once:</span><br><span class="line"></span><br><span class="line">  sort file | uniq</span><br><span class="line"></span><br><span class="line">- Display only unique lines:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -u</span><br><span class="line"></span><br><span class="line">- Display only duplicate lines:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -d</span><br><span class="line"></span><br><span class="line">- Display number of occurrences of each line along with that line:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -c</span><br><span class="line"></span><br><span class="line">- Display number of occurrences of each line, sorted by the most frequent:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -c | sort -nr</span><br></pre></td></tr></table></figure>
<h3 id="paste"><a href="#paste" class="headerlink" title="paste"></a>paste</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">➜  ~ tldr paste</span><br><span class="line"># paste</span><br><span class="line"></span><br><span class="line">  Merge lines of files.</span><br><span class="line"></span><br><span class="line">- Join all the lines into a single line, using TAB as delimiter:</span><br><span class="line"></span><br><span class="line">  paste -s file</span><br><span class="line"></span><br><span class="line">- Join all the lines into a single line, using the specified delimiter:</span><br><span class="line"></span><br><span class="line">  paste -s -d delimiter file</span><br><span class="line"></span><br><span class="line">- Merge two files side by side, each in its column, using TAB as delimiter:</span><br><span class="line"></span><br><span class="line">  paste file1 file2</span><br><span class="line"></span><br><span class="line">- Merge two files side by side, each in its column, using the specified delimiter:</span><br><span class="line"></span><br><span class="line">  paste -d delimiter file1 file2</span><br><span class="line"></span><br><span class="line">- Merge two files, with lines added alternatively:</span><br><span class="line"></span><br><span class="line">  paste -d &apos;\n&apos; file1 file2</span><br></pre></td></tr></table></figure>
<h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="关于正则匹配的应用"><a href="#关于正则匹配的应用" class="headerlink" title="关于正则匹配的应用"></a>关于正则匹配的应用</h3><ul>
<li>Find the number of words (in <code>/usr/share/dict/words</code>) that contain at least three <code>a</code>s and don’t have a <code>&#39;s</code> ending.</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat /usr/share/dict/words | grep -E <span class="string">'(.*a)&#123;3&#125;'</span> | grep -Ev <span class="string">"\'s$"</span> | wc -l</span><br><span class="line"><span class="comment">#    7047</span></span><br></pre></td></tr></table></figure>
<h3 id="sed-使用"><a href="#sed-使用" class="headerlink" title="sed 使用"></a>sed 使用</h3><p>就地修改文件，使用<code>-i</code>。最好在后面指定backup文件的后缀名，否则将不会做backup。有丢失原始数据的风险。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">-i extension</span><br><span class="line">         Edit files <span class="keyword">in</span>-place, saving backups with the specified extension.  If a zero-length extension is given, no </span><br><span class="line">         backup will be saved. It is not recommended to give a zero-length extension when <span class="keyword">in</span>-place editing files, as </span><br><span class="line">         you risk corruption or partial content <span class="keyword">in</span> situations <span class="built_in">where</span> disk space is exhausted, etc.</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>MIT Missing Semester - Shell</title>
    <url>/2020/03/13/mit-missing-semester-02-shell/</url>
    <content><![CDATA[<p>工欲善其事，必先利其器。<a href="https://missing.csail.mit.edu/" target="_blank" rel="noopener">MIT Missing Semester</a>就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！</p>
<p>这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。</p>
<p>这里我跳过了第一节课，直接从bash shell开始。</p>
<a id="more"></a>
<h2 id="一些零散的点"><a href="#一些零散的点" class="headerlink" title="一些零散的点"></a>一些零散的点</h2><ul>
<li>bash中双引号和单引号的区别</li>
</ul>
<p>双引号会发生变量替换，单引号不会。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">foo=bar</span><br><span class="line"></span><br><span class="line"><span class="comment"># output: hello, bar</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello, <span class="variable">$foo</span>"</span></span><br><span class="line"><span class="comment"># output: hello, $foo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'hello, $foo'</span></span><br></pre></td></tr></table></figure>
<ul>
<li>bangbang</li>
</ul>
<p>使用<code>!!</code>可以执行上一条命令。</p>
<ul>
<li>bash 中的特殊变量</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$? <span class="comment"># 上一条命令的返回值，正常退出是0，否则是非0</span></span><br><span class="line"><span class="variable">$@</span> <span class="comment"># 所有输入的参数</span></span><br><span class="line"><span class="variable">$#</span> <span class="comment"># 输入参数的个数</span></span><br><span class="line">$$ <span class="comment"># pid of current script</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查上条命令是否正常退出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"fail"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"success"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<ul>
<li>如何忽略命令的输出</li>
</ul>
<p>有的时候，我们只想要命令的返回值。例如使用<code>grep foo bar</code>来查看文件<code>bar</code>中是否含有字符串<code>foo</code>，可以将标准输出和标准错误重定向到<code>/dev/null</code>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 第一个是标准输出，第二个是标准错误</span></span><br><span class="line">grep <span class="string">"foo"</span> bar &gt; /dev/null 2&gt; /dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者可以这样：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># grep "name" test_lazy.cpp 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$?"</span> -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"found foo in bar"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"not found foo in bar"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<h2 id="globbing"><a href="#globbing" class="headerlink" title="globbing"></a>globbing</h2><ul>
<li>任意字符：<code>*</code></li>
<li>单个字符：<code>?</code></li>
<li>使用<code>{}</code>给定可选元素的集合。</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a.&#123;hpp,cpp&#125;  =&gt; a.hpp a.cpp</span><br><span class="line">a&#123;,0,1,2&#125;   =&gt; a a0 a1 a2</span><br><span class="line"><span class="comment"># 支持多层级</span></span><br><span class="line">touch proj&#123;1,2&#125;/&#123;a,b&#125;.txt</span><br><span class="line"><span class="comment"># 还支持range</span></span><br><span class="line">touch proj&#123;1,2&#125;/&#123;a..g&#125;.txt</span><br></pre></td></tr></table></figure>
<h2 id="bash-中的函数"><a href="#bash-中的函数" class="headerlink" title="bash 中的函数"></a>bash 中的函数</h2><ul>
<li>如何写函数</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="title">mycd</span></span> () &#123;</span><br><span class="line">    <span class="built_in">cd</span> <span class="variable">$1</span></span><br><span class="line">    <span class="built_in">pwd</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>如何在bash中导入脚本中的函数</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> your_bash_script.sh</span><br><span class="line"><span class="comment"># then use the function defined in the bash script</span></span><br><span class="line"><span class="comment"># 你可以这样理解：from your_bash_script import *</span></span><br></pre></td></tr></table></figure>
<h2 id="for-loop"><a href="#for-loop" class="headerlink" title="for-loop"></a>for-loop</h2><h3 id="遍历给定的元素序列"><a href="#遍历给定的元素序列" class="headerlink" title="遍历给定的元素序列"></a>遍历给定的元素序列</h3><p>使用<code>for item in xxx; do yyy; done</code>来遍历给定的序列，并施加具体操作于序列元素：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1 2 3</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$i</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment"># welcome 1</span></span><br><span class="line"><span class="comment"># welcome 2</span></span><br><span class="line"><span class="comment"># welcome 3</span></span><br></pre></td></tr></table></figure>
<p>注意，列表元素是通过空格来隔离的。如果这样写</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1, 2, 3</span><br></pre></td></tr></table></figure>
<p>那么最终输出也是<code>welcome 1, welcome 2, welcome 3</code></p>
<p>还可以使用for-range的方法：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br></pre></td></tr></table></figure>
<h3 id="c-style-for-loop"><a href="#c-style-for-loop" class="headerlink" title="c-style for-loop"></a>c-style for-loop</h3><p>也可以像C语言那样使用for-loop：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (( Exp1; Exp2; Exp3)); <span class="keyword">do</span> xxx; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (( c=1; c&lt;=3; c++ )); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<p>还可以使用这种风格构造无穷循环，<code>for (( ; ; )); do xxx; done</code>。</p>
<h3 id="break-continue"><a href="#break-continue" class="headerlink" title="break / continue"></a>break / continue</h3><p>当满足一定条件时，使用<code>break</code>退出循环，或使用<code>continue</code>继续循环。</p>
<h3 id="while"><a href="#while" class="headerlink" title="while"></a>while</h3><p>除了for-loop，还可以使用<code>while</code>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">while</span> CONDITION; <span class="keyword">do</span> xxx; <span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="until"><a href="#until" class="headerlink" title="until"></a>until</h3><p><code>until</code>和<code>while</code>的用法一致，不同点在于：</p>
<ul>
<li><code>while</code>是CONDITION为<code>true</code>执行，当<code>false</code>是退出循环</li>
<li><code>until</code>是CONDITION为<code>false</code>执行，当<code>true</code>时退出循环</li>
</ul>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">c=1</span><br><span class="line">until [ <span class="variable">$c</span> -gt 3 ]; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span></span><br><span class="line">  ((c++))</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h2 id="数学表达式"><a href="#数学表达式" class="headerlink" title="数学表达式"></a>数学表达式</h2><p>在上面for-loop中，已经看到了我们使用<code>((exp))</code>的形式进行数学表达式运算。一般来说，在bash shell中进行数学表达式的运算可以采用：</p>
<ul>
<li>使用<code>expr</code>，如<code>c=$(expr 1 + 1); echo $c</code>，注意操作数与操作符之间都是有空格的。</li>
<li>使用<code>let</code>，如<code>c=1; let c=$c+1; echo $c</code>，注意操作数与操作符之间没有空格。</li>
<li>使用双括号<code>(())</code>，就像上面看到的那样：<code>c=1; echo $((c += 1)); echo $c</code>。这时候，操作数与操作符之间的空格可有可无。</li>
</ul>
<p>最后一种双括号可能更为常用，支持的操作符：<code>+/-/++/--/*/%/**</code>，也支持逻辑运算符：<code>&gt;=/&lt;=/&gt;/&lt;/==/!=/!/||/&amp;&amp;</code>。</p>
<p>如果希望进行浮点数运算，bash本身是不支持的，可以使用<code>bc</code>命令，将表达式作为字符串传入就可以了：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1.0+2.0"</span> | bc</span><br><span class="line">c=$(r=1.5;<span class="built_in">echo</span> <span class="string">"<span class="variable">$r</span> + 2.5"</span>|bc); <span class="built_in">echo</span> <span class="variable">$c</span></span><br></pre></td></tr></table></figure>
<h2 id="调试工具"><a href="#调试工具" class="headerlink" title="调试工具"></a>调试工具</h2><p>shellcheck可以用来帮助静态分析shell脚本。用法：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">shellcheck your_bash_script.sh</span><br></pre></td></tr></table></figure>
<p>可以去网站上试用：<a href="https://www.shellcheck.net/#" target="_blank" rel="noopener">Shellcheck</a></p>
<h2 id="几个有用的命令"><a href="#几个有用的命令" class="headerlink" title="几个有用的命令"></a>几个有用的命令</h2><p>这里列出一些常用的命令工具，都是和查找有关。更多内容，可以通过<code>man</code>或者<code>tldr</code>查看。</p>
<h3 id="查找文件-find"><a href="#查找文件-find" class="headerlink" title="查找文件 find"></a>查找文件 find</h3><p>最常用的用法：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 递归地查找当前目录及子目录下所有的python文件</span></span><br><span class="line">find . -name=<span class="string">"*.py"</span></span><br><span class="line"><span class="comment"># -type d 表示过滤结果为所有目录</span></span><br><span class="line"><span class="comment"># -type f 表示过滤结果为所有文件</span></span><br><span class="line">find . -name=<span class="string">"test"</span> -<span class="built_in">type</span> d</span><br><span class="line"><span class="comment"># Find all files modified in the last day</span></span><br><span class="line">find . -mtime -1</span><br><span class="line"><span class="comment"># Find all zip files with size in range 500k to 10M</span></span><br><span class="line">find . -size +500k -size -10M -name <span class="string">'*.tar.gz'</span></span><br></pre></td></tr></table></figure>
<p><code>find</code>还可以通过<code>-exec</code>来接后续处理，如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Delete all files with .tmp extension</span></span><br><span class="line"><span class="comment"># 注意最后的 \</span></span><br><span class="line">find . -name <span class="string">'*.tmp'</span> -<span class="built_in">exec</span> rm &#123;&#125; \;</span><br><span class="line"><span class="comment"># Find all PNG files and convert them to JPG</span></span><br><span class="line">find . -name <span class="string">'*.png'</span> -<span class="built_in">exec</span> convert &#123;&#125; &#123;.&#125;.jpg \;</span><br></pre></td></tr></table></figure>
<p>你也可以用<code>fd</code>作为<code>find</code>的改进版。具体用法可以参考<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">fd</a>，这里不再多说了。</p>
<h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>如果你想按照名字去查找文件，还可以试试<code>locate</code>。一个简单的比较：</p>
<ul>
<li><code>locate</code>只支持按名字查找，<code>find</code>可以更加多样</li>
<li><code>locate</code>通过周期性更新的database来查找，时效性不如<code>find</code></li>
<li><code>locate</code>使用更简单，默认会查找所有符合要求的文件，<code>find</code>一般是查找给定路径下的文件</li>
</ul>
<p>由于上述原因，我一般是使用<code>locate</code>查找系统自带的某个lib等文件。比如有时候我可能不知道<code>libcudart.so</code>在哪里，这时候就可以通过<code>locate libcudart.so</code>来查找。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">locate libcudart.so | grep <span class="string">"/usr"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7</span></span><br><span class="line"><span class="comment"># /usr/local/cuda-10.0/lib64/libcudart.so</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure>
<h3 id="在文件中查找字符串-grep"><a href="#在文件中查找字符串-grep" class="headerlink" title="在文件中查找字符串 grep"></a>在文件中查找字符串 grep</h3><p><code>grep</code> 用来在文件中正则匹配字符串，比如某个变量或函数定义啥的。<code>grep</code>命令很有用，在后续课程中还会着重介绍。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在文件中查找xxx，并打印其所在的行</span></span><br><span class="line">grep xxx file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在所有文件中递归地查找</span></span><br><span class="line">grep -R xxx .</span><br></pre></td></tr></table></figure>
<p>常用的一些flag，可以是<code>-C +number</code>（用来显示match的context，number是行数），<code>-v</code>是反转（不包含所给pattern的行）</p>
<p>和<code>find</code>一样，<code>grep</code>也有一些更好用的替代品，如<code>rg</code>，<code>ag</code>，<code>ack</code>等。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Find all python files where I used the requests library</span></span><br><span class="line">rg -t py <span class="string">'import requests'</span></span><br><span class="line"><span class="comment"># Find all files (including hidden files) without a shebang line</span></span><br><span class="line">rg -u --files-without-match <span class="string">"^#!"</span></span><br><span class="line"><span class="comment"># Find all matches of foo and print the following 5 lines</span></span><br><span class="line">rg foo -A 5</span><br><span class="line"><span class="comment"># Print statistics of matches (# of matched lines and files )</span></span><br><span class="line">rg --stats PATTERN</span><br></pre></td></tr></table></figure>
<h3 id="查找历史命令-history"><a href="#查找历史命令-history" class="headerlink" title="查找历史命令 history"></a>查找历史命令 history</h3><p><code>history</code>可以打印历史的shell命令，和<code>grep</code>配合能够找到历史上曾经用过的某给定命令。不过这个我在使用<code>zsh</code>的时候，一般是通过光标上下键来联想查找的。</p>
<p>一个有用的工具：<code>fzf</code>（which means 模糊查找）。</p>
<p>另外，这里讲师推荐了一款基于历史命令的自动补全（看lecture时候觉得很酷）。如果你和我一样使用<code>zsh</code>，可以参考这个插件：<a href="https://github.com/zsh-users/zsh-autosuggestions" target="_blank" rel="noopener">zsh-autosuggestions</a>。</p>
<h3 id="关于目录"><a href="#关于目录" class="headerlink" title="关于目录"></a>关于目录</h3><p>因为shell环境下没有GUI，所以查看一个目录内的内容，包括跳转目录都很不方便。对此也有一些好用的工具：</p>
<ul>
<li>查看目录内容：<code>tree</code>（最经典），<code>broot</code>，<code>nnn</code>，<code>ranger</code></li>
<li>跳转目录：<code>autojump</code>（在用），<code>fasd</code></li>
</ul>
<h2 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h2><h3 id="关于ls的用法"><a href="#关于ls的用法" class="headerlink" title="关于ls的用法"></a>关于ls的用法</h3><ul>
<li>Includes all files, including hidden files：<code>ls -al</code></li>
<li>Sizes are listed in human readable format (e.g. 454M instead of 454279954)：<code>ls -lh</code></li>
<li>Files are ordered by recency：<code>ls -lt</code></li>
<li>Output is colorized：<code>ls -l --color</code> （zsh自动colorized，所以这个没有验证）</li>
</ul>
<h3 id="bash函数"><a href="#bash函数" class="headerlink" title="bash函数"></a>bash函数</h3><p>Write bash functions <code>marco</code> and <code>polo</code> that do the following. Whenever you execute <code>marco</code> the current working directory should be saved in some manner, then when you execute <code>polo</code>, no matter what directory you are in, <code>polo</code> should cd you back to the directory where you executed <code>marco</code>. For ease of debugging you can write the code in a file <code>marco.sh</code> and (re)load the definitions to your shell by executing <code>source marco.sh</code>.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用文件存储要cd的path</span></span><br><span class="line"><span class="function"><span class="title">macro</span></span> () &#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$(pwd)</span>"</span> &gt; <span class="variable">$HOME</span>/.macro.history</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">polo</span></span> () &#123;</span><br><span class="line">    <span class="built_in">cd</span> <span class="string">"<span class="variable">$(cat "$HOME/.macro.history")</span>"</span> || <span class="built_in">exit</span> 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="循环和程序返回值判断"><a href="#循环和程序返回值判断" class="headerlink" title="循环和程序返回值判断"></a>循环和程序返回值判断</h3><p>Say you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail.</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=1; ; i++)); <span class="keyword">do</span></span><br><span class="line">  <span class="comment"># save the script to `fail_rarely.sh`</span></span><br><span class="line">  ./fail_rarely.sh 2&amp;&gt; out.log</span><br><span class="line">  <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"fail after run <span class="variable">$i</span> times"</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"stdout and stderr message:"</span></span><br><span class="line">    cat out.log</span><br><span class="line">    <span class="built_in">break</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure>
<h3 id="xargs和管道"><a href="#xargs和管道" class="headerlink" title="xargs和管道"></a>xargs和管道</h3><p>As we covered in lecture find’s <code>-exec</code> can be very powerful for performing operations over the files we are searching for. However, what if we want to do something with all the files, like creating a zip file? As you have seen so far commands will take input from both arguments and STDIN. When piping commands, we are connecting STDOUT to STDIN, but some commands like tar take inputs from arguments. To bridge this disconnect there’s the <code>xargs</code> command which will execute a command using STDIN as arguments. For example <code>ls | xargs rm</code> will delete the files in the current directory.</p>
<p>Your task is to write a command that recursively finds all HTML files in the folder and makes a zip with them. Note that your command should work even if the files have spaces (hint: check <code>-d</code> flag for <code>xargs</code>)</p>
<h4 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a>xargs</h4><p>先来看下<code>xargs</code>和管道的区别。这里已经给了一个例子：<code>ls | xargs rm</code>。由于<code>rm</code>命令比较危险，所以下面会换成<code>cat</code>（删除文件变成了打印文件内容）。</p>
<p>为什么不能用管道呢，比如<code>ls | cat</code>。我们先建立一个空目录作为playground：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"simgple test"</span> &gt; a.txt</span><br></pre></td></tr></table></figure>
<p>执行<code>ls | cat</code>，会发现它只是把当前目录下的所有文件名打印了出来，并没有打印<code>a.txt</code>的内容：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls | cat</span><br><span class="line"><span class="comment"># a.txt</span></span><br></pre></td></tr></table></figure>
<p>这是因为管道只是把STDOUT作为<code>cat</code>的STDIN。在linux中，STDOUT和STDIN是两个特殊的文件，<code>ls</code>将把它的输出结果写入到STDOUT中，同时我们就会在屏幕上看到对应输出。而<code>cat</code>从STDIN中接受输入。当没有管道时，由用户输入并写入STDIN。由于管道，<code>cat</code>将直接从STDOUT中读取。也就是<code>ls</code>的输出，也就是当前目录下的文件列表。拆解后想当于下面：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls &gt; stdout_ls</span><br><span class="line">cat &lt; stdout_ls</span><br></pre></td></tr></table></figure>
<p>所以，如果我们想要打印<code>a.txt</code>的内容，管道就不够用了。也就是上面说的，我们要把<code>ls</code>的输出作为<code>cat</code>的参数。这时候需要使用<code>xargs</code>：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">ls | xargs cat</span><br><span class="line"><span class="comment"># simple test</span></span><br></pre></td></tr></table></figure>
<h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>先准备一些测试文件</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">mkdir htmls</span><br><span class="line"><span class="built_in">cd</span> htmls</span><br><span class="line">mkdir htmls/&#123;1..3&#125;</span><br><span class="line">touch htmls/1/1.html</span><br><span class="line">touch htmls/2/2\ 2.html</span><br><span class="line">touch htmls/root.html</span><br></pre></td></tr></table></figure>
<h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>题目说明中的<code>-d</code>没找到，在<a href="https://www.tecmint.com/xargs-command-examples/" target="_blank" rel="noopener">12 Practical Examples of Linux Xargs Command for Beginners</a>找到了如下用法，使用<br><code>-print0</code>和<code>-0</code>（是数字<code>0</code>）配合，具体可以参考<code>man xargs</code>中的内容。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">#-0      Change xargs to expect NUL (``\0'') characters as separators, instead of spaces and newlines.  </span></span><br><span class="line"><span class="comment">#        This is expected to be used in concert with the -print0 function in find(1).</span></span><br><span class="line"></span><br><span class="line">find htmls -name <span class="string">"*.html"</span> -print0 | xargs -0 tar vcf html.zip</span><br></pre></td></tr></table></figure>
<h3 id="命令组合"><a href="#命令组合" class="headerlink" title="命令组合"></a>命令组合</h3><p>Write a command or script to recursively find the most recently modified file in a directory. More generally, can you list all files by recency?</p>
<p>首先递归地列出当前目录下的所有文件，再使用<code>ls -lt</code>将其按照时间排序。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">find -L . -<span class="built_in">type</span> f -print0 | xargs -0 ls -lt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果只需要最新的那个，使用 head命令只打印第一行</span></span><br><span class="line">find -L . -<span class="built_in">type</span> f -print0 | xargs -0 ls -lt | head -1</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>tool</tag>
        <tag>bash</tag>
      </tags>
  </entry>
  <entry>
    <title>Debian9 编译Caffe的一个坑</title>
    <url>/2019/11/24/caffe-compiling-debian9/</url>
    <content><![CDATA[<p>记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">In file included from /usr/local/cuda/include/cuda_runtime.h:120:0,</span><br><span class="line">                 from &lt;command-line&gt;:0:</span><br><span class="line">/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &quot;&quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER</span><br><span class="line">_BUILD__ instead.&quot;&quot; is not valid in preprocessor expressions</span><br><span class="line"> #define __CUDACC_VER__ &quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&quot;</span><br></pre></td></tr></table></figure>
<p>如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。</p>
<p>解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个<a href="https://github.com/NVIDIA/caffe/issues/408" target="_blank" rel="noopener">github issue</a>，修改<code>include/caffe/common.hpp</code>即可。</p>
<a id="more"></a>
<p><img src="/img/fix_caffe_for_boost_CUDACC_VER_error.png" alt="diff修改"></p>
]]></content>
  </entry>
  <entry>
    <title>论文 - MetaPruning：Meta Learning for Automatic Neural Network Channel Pruning</title>
    <url>/2019/10/26/paper-meta-pruning/</url>
    <content><![CDATA[<p>这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：<a href="https://github.com/liuzechun/MetaPruning" target="_blank" rel="noopener">MetaPruning</a>。</p>
<p>这篇文章首发于<a href="https://wemp.app/accounts/fd027dce-bcd1-4eaf-9e64-88bffd7ca8a2" target="_blank" rel="noopener">Paper Weekly公众号</a>，欢迎关注。</p>
<a id="more"></a>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>模型剪枝是一种能够减少模型大小和计算量的方法。模型剪枝一般可以分为三个步骤：</p>
<ul>
<li>训练一个参数量较多的大网络</li>
<li>将不重要的权重参数剪掉</li>
<li>剪枝后的小网络做fine tune</li>
</ul>
<p>其中第二步是模型剪枝中的关键。有很多paper围绕“怎么判断权重是否重要”以及“如何剪枝”等问题进行讨论。困扰模型剪枝落地的一个问题就是剪枝比例的确定。传统的剪枝方法常常需要人工layer by layer地去确定每层的剪枝比例，然后进行fine tune，用起来很耗时，而且很不方便。不过最近的<a href="https://arxiv.org/abs/1810.05270" target="_blank" rel="noopener">Rethinking the Value of Network Pruning</a>指出，剪枝后的权重并不重要，对于channel pruning来说，更重要的是找到剪枝后的网络结构，具体来说就是每层留下的channel数量。受这个发现启发，文章提出可以用一个PruningNet，对于给定的剪枝网络，自动生成weight，无需进行retrain，然后评测剪枝网络在验证集上的性能，从而选出最优的网络结构。</p>
<p>具体来说，PruningNet的输入是剪枝后的网络结构，必须首先对网络结构进行编码，转换为coding vector。这里可以直接用剪枝后网络每层的channel数来编码。在搜索剪枝网络的时候，我们可以尝试各种coding vector，用PruningNet生成剪枝后的网络权重。网络结构和权重都有了，就可以去评测网络的性能。进而用进化算法搜索最优的coding vector，也就是最优的剪枝结构。在用进化算法搜索的时候，可以使用自定义的目标函数，包括将网络的accuracy，latency，FLOPS等考虑进来。</p>
<p><img src="/img/paper_metapruning_pruningnet.jpg" alt="PruningNet的训练和使用"></p>
<h2 id="PruningNet"><a href="#PruningNet" class="headerlink" title="PruningNet"></a>PruningNet</h2><p>从上一小节已经可以知道，PruningNet是整个算法的关键。那么怎么才能找到这样一个“神奇网络”呢？</p>
<p>先做一下符号约定，使用$c_i$表示剪枝之后第$i$层的channel数量，$l$为网络的层数，$W$表示剪枝后网络的权重。那么PruningNet的输入输出如下所示：</p>
<p>$W = \text{PruningNet}(c_1, c_2, \dots, c_l)$</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>先结合下图看一下forward部分。PruningNet是由$l$个PruningBlock组成的，每个PruningBlock是一个两层的MLP。首先看图b，编码着网络结构信息的coding vector输入到当前block后，输出经过Reshape，成了一个Weight Matrix。注意哦，这里的WeightMatrix是固定大小的（也就是未剪枝的原始Weight shape大小），和剪枝网络结构无关。再看图a，因为要对网络进行剪枝，所以WeightMatrix要进行Crop。对应到图b，可以看到，Crop是在两个维度上进行的。首先，由于上一层也进行了剪枝，所以input channel数变少了；其次，由于当前层进行了剪枝，所以output channel数变少了。这样经过Crop，就生成了剪枝后的网络weight。我们再输入一个mini batch的训练图片，就可以得到剪枝后的网络的loss。</p>
<p><img src="/img/paper_metapruning_pruningnet_forward.jpg" alt="PruningNet train forward"></p>
<p>在backward部分，我们不更新剪枝后网络的权重，而是更新PruningNet的权重。由于上面的操作都是可微分的，所以直接用链式法则传过去就行。如果你使用PyTorch等支持自动微分的框架，这是很容易的。</p>
<p>下图所示是训练过程的整个PruningNet（左侧）和剪枝后网络（右侧，即PrunedNet）。训练过程中的coding vector在状态空间里随机采样，随机选取每层的channel数量。</p>
<p>PS：和原始论文相比，下图和上图顺序是颠倒的。这里从底向上介绍了PruningNet的训练，而论文则是自顶向下。</p>
<p><img src="/img/paper_metapruning_whole_meta_learning.jpg" alt="整个PruningNet"></p>
<h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>训练好PruningNet后，就可以用它来进行搜索了！我们只需要输入某个coding vector，PruningNet就会为我们生成对应每层的WeightMatrix。别忘了coding vector是编码的网络结构，现在又有了weight，我们就可以在验证集上测试网络的性能了。进而，可以使用进化算法等优化方法去搜索最优的coding vector。当我们得到了最优结构的剪枝网络后，再from scratch地训练它。</p>
<p>进化算法这里不再赘述，很多优化的书中包括网上都有资料。这里把整个算法流程贴出来：</p>
<p><img src="/img/paper_metapruning_evaluation_algorithm.jpg" alt="进化算法流程"></p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在ImageNet上用MobileNet和ResNet进行了实验。训练PruningNet用了$\frac{1}{4}$的原模型的epochs。数据增强使用常见的标准流程，输入image大小为$224\times 224$。</p>
<p>将原始ImageNet的训练集做分割，每个类别选50张组成sub-validation（共计50000），其余作为sub-training。在训练时，我们使用sub-training训练PruningNet。在搜索时，使用sub-validation评估剪枝网络的性能。不过，还要注意，在搜索时，使用20000张sub-training中的图片重新计算BatchNorm layer中的running mean和running variance。</p>
<h3 id="shortcut剪枝"><a href="#shortcut剪枝" class="headerlink" title="shortcut剪枝"></a>shortcut剪枝</h3><p>在进行模型剪枝时，一个比较难处理的问题是ResNet中的shortcut结构。因为最后有一个element-wise的相加操作，必须保证两路feature map是严格shape相同的，所以不能随意剪枝，否则会造成channel不匹配。下面对几种论文中用到的网络结构分别讨论。</p>
<h4 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet-v1"></a>MobileNet-v1</h4><p>MobileNet-v1是没有shortcut结构的。我们为每个conv layer都配上相应的PruningBlock——一个两层的MLP。PruningNet的输入coding vector中的元素是剪枝后每层的channel数量。而输入第$i$个PruningBlock的是一个2D vector，由归一化的第$i-1$层和第$i$层的剪枝比例构成。这部分可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv1/training/mobilenet_v1.py#L15" target="_blank" rel="noopener">MetaPruning</a>来看。注意第$1$个conv layer的输入是1D vector，因为它是第一个被剪枝的layer。在训练时，coding vector的搜索空间被以一定步长划分为grid，采样就是在这些格点上进行的。</p>
<h4 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet-v2"></a>MobileNet-v2</h4><p>MobileNet-v2引入了类似ResNet的shortcut结构，这种resnet block必须统一看待。具体来说，对于没有在resnet block中的conv，处理方法如MobileNet-v1。对每个resnet block，配上一个相应的PruningBlock。由于每个resnet block中只有一个中间层（$3\times 3$的conv），所以输出第$i$个PruningBlock的是一个3D vector，由归一化的第$i-1$个resnet block，第$i$个resnet block和中间conv层的剪枝比例构成。其他设置和MobileNet-v1相同。这里可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv2/training/mobilenet_v2.py#L109" target="_blank" rel="noopener">MetaPruning</a>来看。</p>
<h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>处理方法如MobileNet-v2所示。可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/resnet/training/resnet.py#L75" target="_blank" rel="noopener">MetaPruning</a>来看。</p>
<h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在相近FLOPS情况下，和MobileNet论文中改变ratio参数得到的模型比较，MetaPruning得到的模型accuracy更高。尤其是压缩比例更大时，该方法更有优势。</p>
<p><img src="/img/paper_metapruning_compare_with_mobilenet_baseline.jpg" alt="MobileNet baseline比较"></p>
<p>和其他剪枝方法（如<a href="https://arxiv.org/abs/1802.03494" target="_blank" rel="noopener">AMC</a>）等方法比较，该方法也得到了SOTA的结果。MetaPruning方法能够以一种统一的方法处理ResNet中的shortcut结构，并且不需要人工调整太多的参数。</p>
<p><img src="/img/paper_metapruning_compare_with_other_pruning_automl.jpg" alt="和其他剪枝方法比较"></p>
<p>上面的比较都是基于理论FLOPS，现在更多人在关注网络在实际硬件上的latency怎么样。文章对此也进行了讨论。如何测试网络的latency？当然可以每个网络都实际跑一下，不过有些麻烦。基于每个layer的inference时间是互相独立的这个假设，作者首先构造了各个layer inference latency的查找表（参见论文<a href="https://arxiv.org/abs/1812.03443" target="_blank" rel="noopener">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</a>），以此来估计实际网络的latency。作者这里和MobileNet baseline做了比较，结果也证明了该方法更优。</p>
<p><img src="/img/paper_metapruning_latency_compare_with_mobilenet_baseline.jpg" alt="latency比较"></p>
<h3 id="PruningNet结果分析"><a href="#PruningNet结果分析" class="headerlink" title="PruningNet结果分析"></a>PruningNet结果分析</h3><p>此外，作者还对PruningNet的预测结果进行可视化，试图找出一些可解释性，并找出剪枝参数的一些规律。</p>
<ul>
<li>down-sampling的部分PruningNet倾向于保留更多的channel，如MobileNet-v2 block中间的那个conv</li>
<li>优先剪浅层layer的channel，FLOPS约束太强剪深层的channel，但可能会造成网络accuracy下降比较多</li>
</ul>
<h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章从“剪枝后的weight作用不大”的现象出发，将剪枝和NAS结合，提出了PruningNet为剪枝后的网络预测weight，避免了网络的retrain，从而可以快速衡量剪枝网络的性能。并在编码网络信息的coding vector状态空间进行搜索，找到给定约束条件下的最优网络结构，在ImageNet数据集和ResNet/MobileNet-v1/v2上取得了比之前剪枝算法更好的效果。</p>
]]></content>
      <tags>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
    <url>/2019/07/06/bag-of-tricks-for-image-cls/</url>
    <content><![CDATA[<p>这是<a href="https://arxiv.org/abs/1812.01187" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。</p>
<p><img src="/img/bag_of_tricks_no_silver_bullet.jpeg" alt="软工里面没有银弹，数据科学同样这样"></p>
<a id="more"></a>
<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇文章主要讨论了训练图片分类模型的tricks，包括data augmentation，（lr，batch size等）超参设置，模型架构微调和模型蒸馏等技术。可以在增加少许计算量的情况下，把ResNet-50的top 1 acc提升4个点，从而打败许多后起之秀。Talk is cheap, show me the code. 论文讨论的方法对应代码，都已经在GluonCV中开源，所以建议在阅读论文的时候，对照<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py" target="_blank" rel="noopener">代码</a>进行学习。</p>
<p><img src="/img/bag_of_tricks_resnet50_overperform_others.jpg" alt="ResNet-50的效果提升"></p>
<h2 id="Baseline-Training"><a href="#Baseline-Training" class="headerlink" title="Baseline Training"></a>Baseline Training</h2><p>这里介绍了一些（已经不算trick的）训练ResNet-50可以注意的地方。使用这些方法，应该可以复现论文中给出的结果。</p>
<h3 id="Data-Argumentation"><a href="#Data-Argumentation" class="headerlink" title="Data Argumentation"></a>Data Argumentation</h3><p>这里都是老生常谈了，可以直接参看代码<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py#L203" target="_blank" rel="noopener">gluon cv/image classification</a>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">jitter_param = <span class="number">0.4</span></span><br><span class="line">lighting_param = <span class="number">0.1</span></span><br><span class="line">mean_rgb = [<span class="number">123.68</span>, <span class="number">116.779</span>, <span class="number">103.939</span>]</span><br><span class="line">std_rgb = [<span class="number">58.393</span>, <span class="number">57.12</span>, <span class="number">57.375</span>]</span><br><span class="line">train_data = mx.io.ImageRecordIter(</span><br><span class="line">    path_imgrec         = rec_train,</span><br><span class="line">    path_imgidx         = rec_train_idx,</span><br><span class="line">    preprocess_threads  = num_workers,</span><br><span class="line">    shuffle             = <span class="literal">True</span>,</span><br><span class="line">    batch_size          = batch_size,</span><br><span class="line">    data_shape          = (<span class="number">3</span>, input_size, input_size),</span><br><span class="line">    mean_r              = mean_rgb[<span class="number">0</span>],</span><br><span class="line">    mean_g              = mean_rgb[<span class="number">1</span>],</span><br><span class="line">    mean_b              = mean_rgb[<span class="number">2</span>],</span><br><span class="line">    std_r               = std_rgb[<span class="number">0</span>],</span><br><span class="line">    std_g               = std_rgb[<span class="number">1</span>],</span><br><span class="line">    std_b               = std_rgb[<span class="number">2</span>],</span><br><span class="line">    rand_mirror         = <span class="literal">True</span>,</span><br><span class="line">    random_resized_crop = <span class="literal">True</span>,</span><br><span class="line">    max_aspect_ratio    = <span class="number">4.</span> / <span class="number">3.</span>,</span><br><span class="line">    min_aspect_ratio    = <span class="number">3.</span> / <span class="number">4.</span>,</span><br><span class="line">    max_random_area     = <span class="number">1</span>,</span><br><span class="line">    min_random_area     = <span class="number">0.08</span>,</span><br><span class="line">    brightness          = jitter_param,</span><br><span class="line">    saturation          = jitter_param,</span><br><span class="line">    contrast            = jitter_param,</span><br><span class="line">    pca_noise           = lighting_param,</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><ul>
<li>使用Xavier初始化卷积层和全连接层的权重，也就是$w\sim \mathcal{U}(-a, a)$，其中$a = \sqrt{6/(d_{in} + d_{out})}$，$d$是输入和输出的channel size。偏置项初始化为$0$。</li>
<li>BatchNorm的$\gamma$初始化为$1$，偏置项为$0$。</li>
</ul>
<h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>8卡V100，batch size = 256，使用NAG梯度下降，lr从0.1，在30，60，90epoch处除以10。</p>
<p>使用上述设置，得到的ResNet-50模型比原始论文更好，不过Inception-V3（输入为$229\times 229$大小）和MobileNet稍差于原始论文。</p>
<h2 id="更快地训练"><a href="#更快地训练" class="headerlink" title="更快地训练"></a>更快地训练</h2><p>主要讨论使用低精度（FP16）和大batch size对训练的影响。</p>
<h3 id="大batch-size"><a href="#大batch-size" class="headerlink" title="大batch size"></a>大batch size</h3><p>大的batch size经常会导致模型的val acc降低（一个简单的解释是，大batch size造成iteration次数减少，导致模型效果变差。当然，实际训练中，大batch size常常搭配较大的lr，所以问题并不是这么简单），可以考虑使用下面的方法解决这个问题。</p>
<h4 id="（成比例）提高lr"><a href="#（成比例）提高lr" class="headerlink" title="（成比例）提高lr"></a>（成比例）提高lr</h4><p>上面说的iteration次数减少是一个方面。另一个考虑是大的batch size会造成对梯度的估计方差变小，我们可以乘上一个较大的lr，让方差的不确定性增大一些。一个经验之谈是，lr随着batch size成比例扩大。比如在训练ResNet-50的时候，He给出的在$B = 256$时，lr取为$0.1$。那么如果$B = 512$，那么lr也相应扩大为$0.2$。</p>
<h4 id="lr-warm-up"><a href="#lr-warm-up" class="headerlink" title="lr warm up"></a>lr warm up</h4><p>如果lr初始设置的很大，可能会带来数值不稳定。因为刚开始的时候权重是随机初始化的，gradient也比较大。可以给lr做warm up，也就是开始若干个迭代用较小的lr，等训练稳定了再用回那个大的lr。一种方法是线性warm up，也就是在warm up阶段，lr是线性地从0涨到给定的那个大lr。</p>
<h4 id="设置-gamma-0"><a href="#设置-gamma-0" class="headerlink" title="设置$\gamma = 0$"></a>设置$\gamma = 0$</h4><p>这个操作比较新奇，在初始阶段，BN的$\beta$参数是设置为$0$的。如果我们再设置$\gamma = 0$，说明BN的输出就是$0$了。这是什么操作？！</p>
<p>作者指出，可以在ResNet这种有by-pass的结构中使用这个trick。在ResNet block的最后一层，我们经常做$y = x + res(x)$，可以考虑将res这一路的最后一个BN层的$\gamma$参数设置为0。这时候，相当于只有输入$x$传到后面，相当于减少了网络的层深。之后的训练中，$\gamma$会逐渐变大，也就逐渐恢复了res通路。</p>
<p>这种方法也是试图解决网络训练初始阶段不稳定的问题。不过这个操作还是挺骚的。。。类似的方法（利用BN层的$\gamma$参数）也见到过被用在模型剪枝上，如Net Sliming等方法。可以参见博客中的相关文章讨论。</p>
<h4 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h4><p>给weight加上L2 norm来做weight decay，是缓解网络过拟合的标准解决办法之一。不过，最好只对conv和fc的kernel做，而不要对它们的bias，BN的$\gamma$和$\beta$做。</p>
<p>上面的方法，在batch size不大于2K的时候，应该是够用了的。</p>
<h3 id="低精度"><a href="#低精度" class="headerlink" title="低精度"></a>低精度</h3><p>很多新GPU都加入了FP16的硬件支持，例如V100上使用FP16比FP32，训练能够加速$2$到$3$倍。FP16的问题是表示范围变小了，同时分辨率变小。对应地会造成两个问题，溢出和无法更新（梯度过小，不到FP16的最小表示）。一种解决办法是使用FP16来做forward和backward，但是在FP32上更新梯度（防止梯度过小）。同时给loss乘上一个系数，让它更好地契合FP16能表示的数据范围。</p>
<p>这里简要介绍下FP16精度的相关内容。关于Nvidia GPU FP16的更多信息，可以参考<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Nvidia文档混合精度训练</a>。</p>
<h4 id="FP16数据表示"><a href="#FP16数据表示" class="headerlink" title="FP16数据表示"></a>FP16数据表示</h4><p>FP16，顾名思义，就是使用16个bit表示浮点数。具体编码方式上，和FP32基本一致，只不过位数有了缩水。</p>
<blockquote>
<p>IEEE 754 standard defines the following 16-bit half-precision floating point format: 1 sign bit, 5 exponent bits, and 10 fractional bits.</p>
</blockquote>
<p>TODO: FP32和FP16的比较</p>
<h4 id="FP16-in-MXNet"><a href="#FP16-in-MXNet" class="headerlink" title="FP16 in MXNet"></a>FP16 in MXNet</h4><p>在MXNet中，使用混合精度训练还是挺简单的。具体可以参考<a href="https://mxnet.incubator.apache.org/versions/master/faq/float16.html" target="_blank" rel="noopener">Mixed precision training using float16</a></p>
<p>下面是使用gluon训练时候要注意的几个地方：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## optimizer 开启混合精度选项</span></span><br><span class="line"><span class="comment">## 这会使optimizer为参数保存一份FP32拷贝，在上面进行梯度的更新，</span></span><br><span class="line"><span class="comment">## 防止梯度过小无法更新FP16</span></span><br><span class="line"><span class="keyword">if</span> opt.dtype != <span class="string">'float32'</span>:</span><br><span class="line">    optimizer_params[<span class="string">'multi_precision'</span>] = <span class="literal">True</span></span><br><span class="line"><span class="comment">## net cast到给定的数值精度</span></span><br><span class="line">net = get_model(model_name, **kwargs)</span><br><span class="line">net.cast(opt.dtype)</span><br><span class="line"><span class="comment">## 训练过程中，将输入也cast到指定精度</span></span><br><span class="line"><span class="keyword">while</span> in_training:</span><br><span class="line">    <span class="comment">## blablabla</span></span><br><span class="line">    outputs = [net(X.astype(opt.dtype, copy=<span class="literal">False</span>)) <span class="keyword">for</span> X <span class="keyword">in</span> data]</span><br><span class="line">    <span class="comment">## 计算loss也把label cast到指定精度</span></span><br></pre></td></tr></table></figure>
<p>使用MXNet老的symbolic接口时候，因为静态图一旦写好就固定了，所以我们需要在建图的时候，考虑FP16精度。</p>
<ul>
<li>在原始输入node后面接一个<code>cast</code> op，将FP32转成FP16。</li>
<li>最好在<code>SoftmaxOutput</code>之前，插入一个<code>cast</code> op，将FP16转回FP32，以便有更高的精度。</li>
<li><code>optimizer</code>打开<code>multi_precision</code>开关，这里和上面gluon是一致的。</li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 建图</span></span><br><span class="line">data = mx.sym.Variable(name=<span class="string">"data"</span>)</span><br><span class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</span><br><span class="line">    data = mx.sym.Cast(data=data, dtype=np.float16)</span><br><span class="line"><span class="comment"># ... the rest of the network</span></span><br><span class="line">net_out = net(data)</span><br><span class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</span><br><span class="line">    net_out = mx.sym.Cast(data=net_out, dtype=np.float32)</span><br><span class="line">output = mx.sym.SoftmaxOutput(data=net_out, name=<span class="string">'softmax'</span>)</span><br><span class="line"><span class="comment">## 优化器设置</span></span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>, multi_precision=<span class="literal">True</span>, lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure>
<p>下面有几条额外的建议：</p>
<ul>
<li>FP16加速主要来源于新GPU上的Tensor Core计算$D = A * B + C$这种运算，且它们的维度是$8$的倍数。所以如果不满足$8$倍数这个条件，FP16的计算速度可能不会很快，或者说和FP32相比没多少优势。尤其是当你在CIFAR10这种输入图片size比较小的数据集上训练的时候。</li>
<li>针对上面这种情况，你可以使用<code>nvprof</code>工具来check是否Tensor Core被使用了，那些名字里面带有<code>s884cudnn</code>的操作就是了。</li>
<li>确保data io和preprocessing不要成为瓶颈，不然面对这些扯后腿的地方，FP16男默女泪。</li>
<li>batch size最好设置为8的倍数，2的幂次是坠吼的。</li>
<li>如果GPU memory还算充足，可以设置<code>MXNET_CUDNN_AUTOTUNE_DEFAULT = 2</code>，来让MXNet有更多的测试来选用最快的卷积算法，代价就是更多的显存占用。</li>
<li>最好为BatchNorm和SoftmaxOutput使用FP32精度。Gluon里面这些都是自动的，MXNet中BN层是自动的，但是SoftmaxOutput需要自己设置一下，见上。</li>
</ul>
<h4 id="loss-scaling"><a href="#loss-scaling" class="headerlink" title="loss scaling"></a>loss scaling</h4><p>再说一下上面提到的loss scaling。</p>
<p>为啥要做loss scaling呢？主要是由于FP16的精度比较差，而能够表示的较大的数对于CNN网络来说又基本用不到（虽然说FP16的表示范围相比FP32已经缩水不少了），所以可能出现这样一种情形，loss对FP16 weight或activation求梯度，梯度太小，以至于FP16无法表示。那其实我们可以给loss乘上一个系数，放大gradient，以便FP16能够表示。在梯度更新之前，再把这个梯度scale回去，就可以了。如下图所示。</p>
<p><img src="/img/bag_of_tricks_fp16_range_dismatch.jpg" alt="FP16和FP32的range不匹配"></p>
<p>使用gluon或MXNet设置loss scaling的方法如下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## gluon</span></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss(weight=<span class="number">128</span>)</span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</span><br><span class="line">                                multi_precision=<span class="literal">True</span>,</span><br><span class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</span><br><span class="line"><span class="comment">## mxnet</span></span><br><span class="line">mxnet.sym.SoftmaxOutput(other_args, grad_scale=<span class="number">128.0</span>)</span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</span><br><span class="line">                                multi_precision=<span class="literal">True</span>,</span><br><span class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</span><br></pre></td></tr></table></figure>
<p>经验来看，对于Multibox SSD, R-CNN, bigLSTM and Seq2seq这些任务，loss scaling是比较有必要的。这里有个疑问，loss scaling应该是在训练过程中不断变化的，但上面的使用都是直接把loss scaling写死了（gluon还好，再手动给loss乘上一个因子），那如何修改loss scaling呢？后面指出可以使用constant的loss scaling（一般取2的幂次64，128等），但是不知道实际训练会不会有问题。<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Nvidia guide</a>中给出的建议是：</p>
<blockquote>
<p>If you encounter precision problems, it is beneficial to scale the loss up by 128, and scale the application of the gradients down by 128.</p>
</blockquote>
<p>当然，最好的办法是自己看一下FP32 gradient的分布。</p>
<p>当当当。。。说了这么多，那么具体加速效果如何呢？使用batch size = $1024$，和batch size = $256$的baseline相比，从下表可知，三种不同的网络结构，分别加速了$1.6X$到$3X$，而且acc还涨了一些。</p>
<p><img src="/img/bag_of_tricks_accelarate_training.jpg" alt="加速效果"></p>
<p>具体的acc影响的ablation实验如下。可以看到，只是使用lr线性增大的情况下，大（batch size的）网络稍逊于小（batch size的）网络。不过当使用上面几个技术综合来看的时候，大小网络的性能差异已经抹去了，而且大网络的训练速度更快。</p>
<p><img src="/img/bag_of_tricks_ablation_of_accelarate_train.jpg" alt="ablation实验结果"></p>
<h2 id="更好的网络"><a href="#更好的网络" class="headerlink" title="更好的网络"></a>更好的网络</h2><p>TODO: 未完待续</p>
]]></content>
  </entry>
  <entry>
    <title>Hello TVM</title>
    <url>/2019/06/29/tvm-helloworld/</url>
    <content><![CDATA[<p>TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！)</p>
<blockquote>
<p>TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.</p>
</blockquote>
<p>compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。</p>
<p>我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM!</p>
<p><img src="/img/tvm_introduction.jpg" alt="TVM概念图"></p>
<a id="more"></a>
<h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>随着深度学习逐渐从研究所的“伊甸园”迅速在工业界的铺开，摆在大家面前的问题是如何将深度学习模型部署到目标硬件平台上，能够多快好省地完成前向计算，从而提供更好的用户体验，<del>同时为老板省钱，还能减少碳排放来造福子孙</del>。</p>
<p>和单纯做研究相比，在工业界我们主要遇到了两个问题：</p>
<ul>
<li>深度学习框架实在是太$^{\text{TM}}$多了。caffe / mxnet / tensorflow / pytorch训练出来的模型都彼此有不同的分发格式。如果你和我一样，做过不同框架的TensorRT的部署，我想你会懂的。。。</li>
<li>GPU实在是太$^{\text{TM}}$贵了。深度学习春风吹满地，老黄股票真争气。另一方面，一些嵌入式平台没有使用GPU的条件。同时一些人也开始在做FPGA/ASIC的深度学习加速卡。如何将深度学习模型部署适配到多样的硬件平台上？</li>
</ul>
<p>为了解决第一个问题，TVM内部实现了自己的IR，可以将上面这些主流深度学习框架的模型转换为统一的内部表示，以便后续处理。若想要详细了解，可以看下NNVM这篇博客：<a href="https://tvm.ai/2017/10/06/nnvm-compiler-announcement" target="_blank" rel="noopener">NNVM Compiler: Open Compiler for AI Frameworks</a>。这张图应该能够说明NNVM在TVM中起到的作用。</p>
<p><img src="/img/tvm_hello_nnvm_as_a_bridge.jpg" alt="NNVM在TVM中的作用"></p>
<p>为了解决第二个问题，TVM内部有多重机制来做优化。其中一个特点是，使用机器学习（结合专家知识）的方法，通过在目标硬件上跑大量trial，来获得该硬件上相关运算（例如卷积）的最优实现。这使得TVM能够做到快速为新型硬件或新的op做优化。我们知道，在GPU上我们站在Nvidia内部专家的肩膀上，使用CUDA / CUDNN / CUBLAS编程。但相比于Conv / Pooling等Nvidia已经优化的很好了的op，我们自己写的op很可能效率不高。或者在新的硬件上，没有类似CUDA的生态，如何对网络进行调优？TVM这种基于机器学习的方法给出了一个可行的方案。我们只需给定参数的搜索空间（少量的人类专家知识），就可以将剩下的工作交给TVM。如果对此感兴趣，可以阅读TVM中关于AutoTuner的介绍和tutorial：<a href="https://docs.tvm.ai/tutorials/autotvm/tune_nnvm_arm.html" target="_blank" rel="noopener">Auto-tuning a convolutional network for ARM CPU</a>。</p>
<h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>我的环境为Debian 8，CUDA 9。</p>
<h3 id="准备代码"><a href="#准备代码" class="headerlink" title="准备代码"></a>准备代码</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/dmlc/tvm.git</span><br><span class="line"><span class="built_in">cd</span> tvm</span><br><span class="line">git checkout e22b5802</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure>
<h3 id="config文件"><a href="#config文件" class="headerlink" title="config文件"></a>config文件</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> tvm</span><br><span class="line">mkdir build</span><br><span class="line">cp ../cmake/config.cmake ./build</span><br><span class="line"><span class="built_in">cd</span> build</span><br></pre></td></tr></table></figure>
<p>编辑config文件，打开CUDA / BLAS / cuBLAS / CUDNN的开关。注意下LLVM的开关。LLVM可以从这个页面<a href="http://releases.llvm.org/download.html" target="_blank" rel="noopener">LLVM Download</a>下载，我之前就已经下载好，版本为7.0。如果你像我一样是Debian8，可以使用for Ubuntu14.04的那个版本。由于是已经编译好的二进制包，下载之后解压即可。</p>
<p>找到这一行，改成<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">set(USE_LLVM /path/to/llvm/bin/llvm-config)</span><br></pre></td></tr></table></figure></p>
<h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>这里有个坑，因为我们使用了LLVM，最好使用LLVM中的clang。否则可能导致tvm生成的代码无法二次导入。见这个讨论帖：<a href="https://discuss.tvm.ai/t/runtime-llvm-cc-create-shared-error-while-run-tune-simple-template/1037" target="_blank" rel="noopener">_cc.create_shared error while run tune_simple_template</a>。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> LLVM=/path/to/llvm</span><br><span class="line">cmake -DCMAKE_C_COMPILER=<span class="variable">$LLVM</span>/bin/clang -DCMAKE_CXX_COMPILER=<span class="variable">$LLVM</span>/bin/clang++ ..</span><br><span class="line"><span class="comment"># 火力全开，let's rock</span></span><br><span class="line">make -j$(nproc)</span><br></pre></td></tr></table></figure>
<h3 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">cd</span> /path/to/tvm</span><br><span class="line"><span class="comment"># 我一般用清华的镜像，你呢。。。</span></span><br><span class="line"><span class="built_in">export</span> THU_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install tornado tornado psutil xgboost numpy decorator attrs  --user -i <span class="variable">$THU_MIRROR</span></span><br><span class="line"><span class="built_in">cd</span> python; python setup.py install --user; <span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> topi/python; python setup.py install --user; <span class="built_in">cd</span> ../..</span><br><span class="line"><span class="built_in">cd</span> nnvm/python; python setup.py install --user; <span class="built_in">cd</span> ../..</span><br></pre></td></tr></table></figure>
<h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><p>使用tvm为mxnet symbol计算图生成CUDA代码，并进行前向计算。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> relay</span><br><span class="line"><span class="keyword">from</span> tvm.relay <span class="keyword">import</span> testing</span><br><span class="line"><span class="keyword">from</span> tvm.contrib <span class="keyword">import</span> graph_runtime</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="comment">## load mxnet model</span></span><br><span class="line">prefix = <span class="string">'/your/mxnet/checkpoint/prefix'</span></span><br><span class="line">epoch = <span class="number">0</span></span><br><span class="line">mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment">## import model into tvm from mxnet</span></span><br><span class="line">shape_dict = &#123;<span class="string">'data'</span>: (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)&#125;</span><br><span class="line"><span class="comment">## tvm提供了 frontend.from_XXX 接口，从不同的框架中导入模型</span></span><br><span class="line">relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict,</span><br><span class="line">        arg_params=arg_params, aux_params=aux_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定目标硬件为 GPU，生成TVM模型</span></span><br><span class="line"><span class="comment">## ---------------------------- </span></span><br><span class="line"><span class="comment"># graph：execution graph in json format</span></span><br><span class="line"><span class="comment"># lib: tvm module library of compiled functions for the graph on the target hardware</span></span><br><span class="line"><span class="comment"># params: parameter blobs</span></span><br><span class="line"><span class="comment">## ---------------------------</span></span><br><span class="line">target = <span class="string">'cuda'</span></span><br><span class="line"><span class="keyword">with</span> relay.build_config(opt_level=<span class="number">3</span>):</span><br><span class="line">    graph, lib, params = relay.build(relay_func, target, params=relay_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run forward</span></span><br><span class="line"><span class="comment">## 直接使用tvm提供的cat示例图片</span></span><br><span class="line"><span class="keyword">from</span> tvm.contrib.download <span class="keyword">import</span> download_testdata</span><br><span class="line">img_url = <span class="string">'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'</span></span><br><span class="line">img_path = download_testdata(img_url, <span class="string">'cat.png'</span>, module=<span class="string">'data'</span>)</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.open(img_path).resize((<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_image</span><span class="params">(im)</span>:</span></span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.transpose(im, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    im = im[np.newaxis, :]</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line">x = transform_image(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># let's go</span></span><br><span class="line">ctx = tvm.gpu(<span class="number">0</span>)</span><br><span class="line">dtype = <span class="string">'float32'</span></span><br><span class="line"><span class="comment">## 加载模型</span></span><br><span class="line">m = graph_runtime.create(graph, lib, ctx)</span><br><span class="line"><span class="comment">## set input data</span></span><br><span class="line">m.set_input(<span class="string">'data'</span>, tvm.nd.array(x.astype(dtype)))</span><br><span class="line"><span class="comment">## set input params</span></span><br><span class="line">m.set_input(**params)</span><br><span class="line">m.run()</span><br><span class="line"><span class="comment"># get output</span></span><br><span class="line">outputs = m.get_output(<span class="number">0</span>)</span><br><span class="line">top1 = np.argmax(outputs.asnumpy()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># save model</span></span><br><span class="line"><span class="comment">## lib存为tar包文件，解压后可以发现，就是打包了动态链接库</span></span><br><span class="line">path_lib = <span class="string">'./deploy_resnet50_v2_lib.tar'</span></span><br><span class="line">lib.export_library(path_lib)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算图存为json文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(graph)</span><br><span class="line"><span class="comment">## 权重存为二进制文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_params'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(relay.save_param_dict(params))</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model back</span></span><br><span class="line">loaded_json = open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>).read()</span><br><span class="line">loaded_lib = tvm.module.load(path_lib)</span><br><span class="line">loaded_params = bytearray(open(<span class="string">'./deploy_params'</span>, <span class="string">'rb'</span>).read())</span><br><span class="line">module = graph_runtime.create(loaded_json, loaded_lib, ctx)</span><br><span class="line"><span class="comment">## 好了，剩下的就都一样了</span></span><br></pre></td></tr></table></figure>
<h2 id="最后的话"><a href="#最后的话" class="headerlink" title="最后的话"></a>最后的话</h2><p>我个人的观点，TVM是一个很有意思的项目。在深度学习模型的优化和部署上做了很多探索，在官方放出的benchmark上表现还是不错的。如果使用非GPU进行模型的部署，TVM值得一试。不过在GPU上，得益于Nvidia的CUDA生态，目前TensorRT仍然用起来更方便，综合性能更好。如果你和我一样，主要仍然在GPU上搞事情，可以密切关注TVM的发展，并尝试使用在自己的项目中，不过我觉得还是优先考虑TensorRT。<del>另一方面，TVM的代码实在是看不太懂啊。。。</del></p>
<h2 id="想要更多"><a href="#想要更多" class="headerlink" title="想要更多"></a>想要更多</h2><ul>
<li>TVM paper：<a href="https://arxiv.org/abs/1802.04799" target="_blank" rel="noopener">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li>
<li>TVM 项目主页：<a href="https://tvm.ai/" target="_blank" rel="noopener">TVM</a></li>
</ul>
<p>后续TVM的介绍，不知道啥时候有时间再写。。。随缘吧。。。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>tvm</tag>
      </tags>
  </entry>
  <entry>
    <title>重读 C++ Primer</title>
    <url>/2019/05/01/cpp-primer-review/</url>
    <content><![CDATA[<p>重读C++ Primer第五版，整理一些糊涂的语法知识点。</p>
<div align="center">    
 <img src="/img/cpp_is_terrible.jpg" width="200" height="300" alt="入门到放弃" align="center">
</div>

<a id="more"></a>
<h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><p>总结一些比较容易搞乱的基础语法。</p>
<h3 id="const-限定说明符"><a href="#const-限定说明符" class="headerlink" title="const 限定说明符"></a>const 限定说明符</h3><ul>
<li><code>const</code>对象一般只在当前文件可见，如果希望在其他文件访问，在声明和定义时，均需加上<code>extern</code>关键字。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">100</span>;</span><br></pre></td></tr></table></figure>
<ul>
<li>顶层<code>const</code>和底层<code>const</code></li>
</ul>
<p>指针本身也是对象，所以有所谓的“常量指针”（指针本身不能赋值）和“指向常量的指针”（指针指向的那个对象不能赋值）。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="comment">// 指针指向的对象不能经由指针赋值</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* p1 = &amp;a;</span><br><span class="line">*p = <span class="number">0</span>;  <span class="comment">// 错误</span></span><br><span class="line"><span class="comment">// 指针本身不能再赋值</span></span><br><span class="line"><span class="keyword">int</span>* <span class="keyword">const</span> p2 = &amp;a;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">p2 = &amp;p;   <span class="comment">// 错误</span></span><br></pre></td></tr></table></figure>
<p>如何记住这条规则？c++中类型说明从右向左读即可。例如<code>p1</code>，其左侧首先遇到<code>int*</code>，故其是个“普通”指针（没有被<code>const</code>修饰），再往左才读到<code>const</code>，故这个指针指向的内容是常量，不能修改。<code>p2</code>同理。</p>
<p>把“指针本身是常量”的行为称为“顶层const”（top-level），把“指针指向内容是常量”的行为称为“底层const”（low-level）。</p>
<h3 id="auto-和-decltype"><a href="#auto-和-decltype" class="headerlink" title="auto 和 decltype"></a>auto 和 decltype</h3><ul>
<li><code>auto</code>类型推断的规则</li>
</ul>
<p>编译器推断<code>auto</code>声明变量的类型时，可能和初始值类型不一样。当初始值类型为引用时，编译器以被引用对象的类型作为<code>auto</code>的类型，除非显式指明。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span>&amp; ri = i;</span><br><span class="line"><span class="comment">// type of j: int</span></span><br><span class="line"><span class="keyword">auto</span> j = ri;</span><br><span class="line"><span class="comment">// type of rj: int&amp;</span></span><br><span class="line"><span class="keyword">auto</span>&amp; rj = ri;</span><br></pre></td></tr></table></figure>
<p>另外，<code>auto</code>只会保留底层<code>const</code>，忽略顶层<code>const</code>，除非显式指定。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="keyword">const</span> p = &amp;a;</span><br><span class="line"><span class="comment">// type of b: int</span></span><br><span class="line"><span class="keyword">auto</span> b = a;</span><br><span class="line"><span class="comment">// type of p1: const int*</span></span><br><span class="line"><span class="keyword">auto</span> p1 = p;</span><br><span class="line"><span class="comment">// type of p2: const int* const</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">auto</span> p2 = p;</span><br><span class="line">p1 = &amp;b;   <span class="comment">// ok, p1 本身已经不是const的了</span></span><br><span class="line">p2 = &amp;b;   <span class="comment">// wrong! 显式指定了 p2 本身是 const</span></span><br><span class="line">*p1 = <span class="number">10</span>;  <span class="comment">// wrong! p1 保留了底层const，指向的内容仍然不可改变</span></span><br></pre></td></tr></table></figure>
<ul>
<li><code>decltype</code> 类型推断规则</li>
</ul>
<p>和<code>auto</code>不同，<code>decltype</code>保留表达式的顶层<code>const</code>和引用。</p>
<ol>
<li>如果表达式是变量，那么返回该变量的类型；</li>
<li>如果表达式不是纯变量，返回表达式结果的类型；</li>
<li>如果表达式是解引用，返回引用类型。</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">42</span>, *p = &amp;i, &amp;r = i;</span><br><span class="line"><span class="keyword">decltype</span>(i) j;    <span class="comment">// ok, j is a int</span></span><br><span class="line"><span class="keyword">decltype</span>(r) y;    <span class="comment">// wrong! y是引用类型，必须初始化</span></span><br><span class="line"><span class="keyword">decltype</span>(r + <span class="number">0</span>) z;  <span class="comment">// ok, r+0 返回值是int</span></span><br><span class="line"><span class="keyword">decltype</span>(*p) c;   <span class="comment">// wrong! 解引用的结果是引用，必须初始化</span></span><br></pre></td></tr></table></figure>
<p>有一种情况特殊，如果是春变量，但是变量名加上括号，结果将是引用。原因：变量加上括号，将会被当做表达式。而变量又可以被赋值，所以得到了引用。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">dectype((i)) d;  <span class="comment">// wrong! d是引用</span></span><br></pre></td></tr></table></figure>
<h2 id="泛型算法"><a href="#泛型算法" class="headerlink" title="泛型算法"></a>泛型算法</h2><p>C++的标准库中实现了很多泛型算法，如<code>find</code>, <code>sort</code>等。它们大多定义在<code>&lt;algorithm&gt;</code>头文件中，一些数值相关的定义在<code>&lt;numeric&gt;</code>中。通过“迭代器”这一层抽象，泛型算法可以不关心所操作数据实际储存的容器，不过仍然受制于实际数据类型。例如<code>find</code>中，为了比较当前元素是否为所求值，要求元素类型实现<code>==</code>运算。好在这些算法大多支持自定义操作。</p>
<h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>在标准库的<code>&lt;iterator&gt;</code>中，定义了如下几种通用迭代器。</p>
<ul>
<li>插入迭代器</li>
</ul>
<p>插入器是一个迭代器的适配器，接受一个容器，生成一个用于该容器的迭代器，能够实现向该容器插入元素。插入迭代器有三种，区别在于插入元素的位置：</p>
<ol>
<li><code>back_inserter</code>，创建一个使用<code>push_back</code>插入的迭代器</li>
<li><code>front_inserter</code>，创建一个使用<code>push_front</code>插入的迭代器</li>
<li><code>inserter</code>，创建一个使用<code>insert</code>的迭代器，在给定的迭代器前面插入元素</li>
</ol>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用back_inserter插入数据</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b;</span><br><span class="line">    <span class="comment">// copy a -&gt; b, 动态改变b的大小</span></span><br><span class="line">    copy(begin(a), end(a), back_inserter(b));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v: b) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// b: 1, 2, 3, 4, 5</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用inserter，将数据插入指定位置</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b &#123;<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>&#125;;</span><br><span class="line">    <span class="comment">// find iter of value 8</span></span><br><span class="line">    <span class="keyword">auto</span> iter = find(b.begin(), b.end(), <span class="number">8</span>);</span><br><span class="line">    <span class="comment">// copy a -&gt; b before value 8</span></span><br><span class="line">    copy(begin(a), end(a), inserter(b, iter));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v : b) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// b: 6, 7, 1, 2, 3, 4, 5, 8</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里要注意的是，当使用<code>front_inserter</code>时，由于插入总是在容器头部发生，所以最后的插入结果是原始数据序列的逆序。</p>
<ul>
<li>流迭代器</li>
</ul>
<p>虽然输入输出流不是容器，不过也有用于这些IO对象的迭代器：<code>istream_iterator</code>和<code>ostream_iterator</code>。这样，我们可以通过它们向对应的输入输出流读写数据，</p>
<p>创建输入流迭代器时，必须指定其要操作的数据类型，并将其绑定到某个流（标准输入输出流或文件流），或使用默认初始化，得到当做尾后值使用的迭代器。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function">istream_iterator&lt;<span class="keyword">int</span>&gt; <span class="title">in_iter</span><span class="params">(<span class="built_in">cin</span>)</span></span>;</span><br><span class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; in_eof;</span><br><span class="line"><span class="comment">// 使用迭代器构建vector</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">values</span><span class="params">(in_iter, in_eof)</span></span>;</span><br></pre></td></tr></table></figure>
<p>创建输出流迭代器时，必须指定其要操作的数据类型，并向其绑定到某个流，还可以传入第二个参数，类型是C风格的字符串（字符串字面常量或指向<code>\0</code>结尾的字符数组指针），表示在输出数据之后，还会输出此字符串。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="comment">// 输出：1       2       3       4       5 </span></span><br><span class="line">copy(v.begin(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">"\t"</span>));</span><br></pre></td></tr></table></figure>
<ul>
<li>反向迭代器</li>
</ul>
<p>顾名思义，反向迭代器的迭代顺序和正常的迭代器是相反的。使用<code>rbegin</code>和<code>rend</code>可以获得绑定在该容器的反向迭代器。不过<code>forward_list</code>和流对象，由于没有同时实现<code>++</code>和<code>--</code>，所以没有反向迭代器。</p>
<p>反向迭代器常常用来在容器中查找最后一个满足条件的元素。这时候要注意，如果继续使用该迭代器，顺序仍然是反向的。如果需要正向迭代器，可以使用<code>.base()</code>方法得到对应的正向迭代器。不过要注意，正向迭代器和反向迭代器的位置会不一样哦~</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 找到数组中最后一个5,并将其后数字打印出来</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v &#123;<span class="number">10</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">auto</span> iter = find(v.rbegin(), v.rend(), <span class="number">5</span>);</span><br><span class="line"><span class="comment">// 输出：5,4,5,10,</span></span><br><span class="line">copy(iter, v.rend(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line"><span class="comment">// 输出：1,2, 注意并没有输出5</span></span><br><span class="line">copy(iter.base(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</span><br></pre></td></tr></table></figure>
<h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2><p>拖延症发作。。。</p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLO Caffe模型转换BN的坑</title>
    <url>/2019/03/09/darknet-caffe-converter/</url>
    <content><![CDATA[<p>YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。</p>
<a id="more"></a>
<h1 id="Darknet中BN的计算"><a href="#Darknet中BN的计算" class="headerlink" title="Darknet中BN的计算"></a>Darknet中BN的计算</h1><p>以CPU代码为例，在Darknet中，BN做normalization的操作如下，<a href="https://github.com/pjreddie/darknet/blob/master/src/blas.c#L147" target="_blank" rel="noopener">normalize_cpu</a></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> b, f, i;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; spatial; ++i)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = b*filters*spatial + f*spatial + i;</span><br><span class="line">                x[index] = (x[index] - mean[f])/(<span class="built_in">sqrt</span>(variance[f]) + <span class="number">.000001</span>f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，Darknet中的BN计算如下：</p>
<script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2} + \epsilon}</script><p>而且，$\epsilon$参数是固定的，为$1\times 10^{-6}$。</p>
<h1 id="问题和解决"><a href="#问题和解决" class="headerlink" title="问题和解决"></a>问题和解决</h1><p>然而，在Caffe（以及大部分其他框架）中，$\epsilon$的位置是在根号里面的，也就是：</p>
<script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><p>另外，查看<code>caffe.proto</code>可以知道，Caffe默认的$\epsilon$值为$1\times 10^{-5}$。</p>
<p>所以，在转换为caffe prototxt时，需要设置<code>batch_norm_param</code>如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">batch_norm_param &#123;</span><br><span class="line">  use_global_stats: true</span><br><span class="line">  eps: 1e-06</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外，需要重新求解$\sigma^2$，按照layer输出要相等的等量关系，可以求得：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_running_var</span><span class="params">(var, eps=DARKNET_EPS)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.square(np.sqrt(var) + eps) - eps</span><br></pre></td></tr></table></figure>
<p>这里调整之后，转换后的Caffe模型和原始Darknet模型的输出误差已经是$1\times 10^{-7}$量级，可以认为转换成功。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>MacOS Mojave更新之后一定要做这几件事！</title>
    <url>/2018/10/27/mac-update-mojave/</url>
    <content><![CDATA[<p>很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。<br><a id="more"></a></p>
<h2 id="Git等工具"><a href="#Git等工具" class="headerlink" title="Git等工具"></a>Git等工具</h2><p>试图使用<code>git</code>是出现了如下错误：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone xx.git</span><br><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span><br></pre></td></tr></table></figure>
<p>解决办法参考<a href="https://apple.stackexchange.com/questions/254380/macos-mojave-invalid-active-developer-path" target="_blank" rel="noopener">macOS Mojave: invalid active developer path</a>中的最高赞回答：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure></p>
<h2 id="osxfuse"><a href="#osxfuse" class="headerlink" title="osxfuse"></a>osxfuse</h2><p>参考Github讨论帖<a href="https://github.com/osxfuse/osxfuse/issues/542" target="_blank" rel="noopener">osxfuse not compatible with MacOS Mojave</a>，从官网下载最新的3.8.2版本安装即可。</p>
<h2 id="VSCode等编辑器字体变“瘦”"><a href="#VSCode等编辑器字体变“瘦”" class="headerlink" title="VSCode等编辑器字体变“瘦”"></a>VSCode等编辑器字体变“瘦”</h2><p>更新之后，发现VSCode编辑器中的字体变得“很瘦”，不美观。执行下面的命令，并重启机器，应该可以恢复。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defaults write -g CGFontRenderingFontSmoothingDisabled -bool NO</span><br></pre></td></tr></table></figure></p>
<h2 id="Mos-Caffine-IINA-等APP"><a href="#Mos-Caffine-IINA-等APP" class="headerlink" title="Mos Caffine IINA 等APP"></a>Mos Caffine IINA 等APP</h2><p>Mos可以平滑Mac上外接鼠标的滚动，并调整鼠标滚动方向和Windows相同。更新后发现Mos失灵。这应该是和新版本中更强的权限管理有关，解决办法是在”安全隐私设置” -&gt; “辅助功能”中，先把Mos的勾勾去掉，然后重新勾选。Caffine同样的操作。</p>
<p>IINA是一款Mac上的播放器软件，是我在Mac上的默认播放器。更新后点击媒体文件，发现只是弹出IINA软件的界面，却没有自动播放。解决办法是在媒体文件上右键，在打开方式中重新选择IINA，并勾选默认打开方式选项。</p>
<p>更新新系统后，遇到的坑暂时就这么多。希望能够帮助到需要的人。</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Rethinking The Value of Network Pruning</title>
    <url>/2018/10/22/paper-rethinking-the-value-of-network-pruning/</url>
    <content><![CDATA[<p><a href="https://openreview.net/forum?id=rJlnB3C5Ym" target="_blank" rel="noopener">这篇文章</a>是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/">论文总结 - 模型剪枝 Model Pruning</a>。</p>
<p>这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。</p>
<p>然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：<a href="https://github.com/Eric-mingjie/rethinking-network-pruning" target="_blank" rel="noopener">Eric-mingjie/rethinking-network-pruning</a>。</p>
<a id="more"></a>
<h2 id="FLOP的计算"><a href="#FLOP的计算" class="headerlink" title="FLOP的计算"></a>FLOP的计算</h2><p>代码中有关于PyTorch模型的FLOPs的计算，见<a href="https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py" target="_blank" rel="noopener">compute_flops.py</a>。可以很方便地应用到自己的代码中。</p>
<h2 id="ThiNet的实现"><a href="#ThiNet的实现" class="headerlink" title="ThiNet的实现"></a>ThiNet的实现</h2><h2 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>几个仍然有疑问的地方：</p>
<ol>
<li><p>作者已经证明在ImageNet/CIFAR等样本分布均衡的数据集上的结论，如果样本分布不均衡呢？有三种思路有待验证：</p>
<ul>
<li>prune模型需要从大模型处继承权重，然后直接在不均衡数据集上训练即可；</li>
<li>prune模型不需要从大模型处继承权重， 但是需要先在ImageNet数据集上训练，然后再在不均衡数据集上训练；</li>
<li>prune模型直接在不均衡数据集上训练（以我的经验，这种思路应该是不work的）</li>
</ul>
</li>
<li><p>prune前的大模型权重不重要，结构重要，这是本文的结论之一。自动搜索树的prune算法可以看做是模型结构搜索，但是大模型给出了搜索空间的一个很好的初始点。这个初始点是否是任务无关的？也就是说，对A任务有效的小模型，是否在B任务上也是很work的？</p>
</li>
<li><p>现在的网络搜索中应用了强化学习/遗传算法等方法，这些方法怎么能够和prune结合？ECCV 2018中HanSong和He Yihui发表了AMC方法。</p>
</li>
</ol>
<p>总之，作者用自己辛勤的实验，给我们指出了一个”可能的”（毕竟文章还没被接收）误区，但是仍然有很多乌云漂浮在上面，需要更多的实验。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>论文总结 - 模型剪枝 Model Pruning</title>
    <url>/2018/10/03/paper-summary-model-pruning/</url>
    <content><![CDATA[<p>模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。</p>
<p><img src="/img/paper-summary-model-pruning-joke.jpg" alt="剪枝的学问"></p>
<a id="more"></a>
<h2 id="Deep-Compression-Han-Song"><a href="#Deep-Compression-Han-Song" class="headerlink" title="Deep Compression, Han Song"></a>Deep Compression, Han Song</h2><p>抛去LeCun等人在90年代初的几篇论文，HanSong是这个领域的先行者。发表了一系列关于模型压缩的论文。其中NIPS 2015上的这篇<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">Learning both weights and connections for efficient neural network</a>着重讨论了对模型进行剪枝的方法。这篇论文之前我已经写过了<a href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/">阅读总结</a>，比较详细。</p>
<p>概括来说，作者提出的主要观点包括，L1 norm作为neuron是否重要的metric，train -&gt; pruning -&gt; retrain三阶段方法以及iteratively pruning。需要注意的是，作者的方法只能得到非结构化的稀疏，对于作者的专用硬件EIE可能会很有帮助。但是如果想要在通用GPU或CPU上用这种方法做加速，是不太现实的。</p>
<h2 id="SSL，WenWei"><a href="#SSL，WenWei" class="headerlink" title="SSL，WenWei"></a>SSL，WenWei</h2><p>既然非结构化稀疏对现有的通用GPU/CPU不友好，那么可以考虑构造结构化的稀疏。将Conv中的某个filter或filter的某个方形区域甚至是某个layer直接去掉，应该是可以获得加速效果的。WenWei<a href="https://arxiv.org/abs/1608.03665" target="_blank" rel="noopener">论文Learning Structured Sparsity in Deep Neural Networks</a>发表在NIPS 2016上，介绍了如何使用LASSO，给损失函数加入相应的惩罚，进行结构化稀疏。这篇论文之前也已经写过博客，可以参考<a href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/">博客文章</a>。</p>
<p>概括来说，作者引入LASSO正则惩罚项，通过不同的具体形式，构造了对不同结构化稀疏的损失函数。</p>
<h2 id="L1-norm-Filter-Pruning，Li-Hao"><a href="#L1-norm-Filter-Pruning，Li-Hao" class="headerlink" title="L1-norm Filter Pruning，Li Hao"></a>L1-norm Filter Pruning，Li Hao</h2><p>在通用GPU/CPU上，加速效果最好的还是整个Filter直接去掉。作者发表在ICLR 2017上的<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="noopener">论文Pruning Filters for Efficient ConvNets</a>提出了一种简单的对卷积层的filter进行剪枝的方法。</p>
<p>这篇论文真的很简单。。。主要观点就是通过Filter的L1 norm来判断这个filter是否重要。人为设定剪枝比例后，将该层不重要的那些filter直接去掉，并进行fine tune。在确定剪枝比例的时候，假定每个layer都是互相独立的，分别对其在不同剪枝比例下进行剪枝，并评估模型在验证集上的表现，做sensitivity分析，然后确定合理的剪枝比例。在实现的时候要注意，第$i$个layer中的第$j$个filter被去除，会导致其输出的feature map中的第$j$个channel缺失，所以要相应调整后续的BN层和Conv层的对应channel上的参数。</p>
<p>另外，实现起来还有一些细节，这些可以参见原始论文。提一点，在对ResNet这种有旁路结构的网络进行剪枝时，每个block中的最后一个conv不太好处理。因为它的输出要与旁路做加和运算。如果channel数量不匹配，是没法做的。作者在这里的处理方法是，听identity那一路的。如果那一路确定了剪枝后剩余的index是多少，那么$\mathcal{F}(x)$那一路的最后那个conv也这样剪枝。</p>
<p>这里给出一张在ImageNet上做sensitivity analysis的图表。需要对每个待剪枝的layer进行类似的分析。</p>
<p><img src="/img/paper-model-pruning-filter-pruning-sensitivity-results.png" alt="sensitivity分析"></p>
<h2 id="Automated-Gradual-Pruning-Gupta"><a href="#Automated-Gradual-Pruning-Gupta" class="headerlink" title="Automated Gradual Pruning, Gupta"></a>Automated Gradual Pruning, Gupta</h2><p>这篇文章发表在NIPS 2017的一个关于移动设备的workshop上，名字很有意思（这些人起名字为什么都这么熟练啊）：<a href="https://arxiv.org/abs/1710.01878" target="_blank" rel="noopener">To prune, or not to prune: exploring the efficacy of pruning for model compression</a>。TensorFlow的repo中已经有了对应的实现（亲儿子。。）：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning" target="_blank" rel="noopener">Model pruning: Training tensorflow models to have masked connections</a>。哈姆雷特不能回答的问题，作者的答案则是Yes。</p>
<p><img src="/img/paper-model-pruning-why-so-baixue.jpg" alt="为什么你们起名字这么熟练啊"></p>
<p>这篇文章主要有两个贡献。一是比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但是compact-small模型的性能，得出结论：对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，都是前者更好。具体的数据参考论文。</p>
<p>二是提出了一个渐进的自动调节的pruning策略。首先，作者也着眼于非结构化稀疏。同时和上面几篇文章一样，作者也使用绝对值大小作为衡量importance的标准，作者提出，sparsity可以按照下式自动调节：</p>
<script type="math/tex; mode=display">s_t = s_f + (s_i-s_f)(1-\frac{t-t_0}{n\Delta t})^3 \quad \text{for}\quad t \in \{t_0, t_0+\Delta t,\dots,t_0+n\Delta t\}</script><p>其中，$s_i$是初始剪枝比例，一般为$0$。$s_f$为最终的剪枝比例，开始剪枝的迭代次数为$t_0$，剪枝间隔为$\Delta t$，共进行$n$次。</p>
<h2 id="Net-Sliming-Liu-Zhuang-amp-Huang-Gao"><a href="#Net-Sliming-Liu-Zhuang-amp-Huang-Gao" class="headerlink" title="Net Sliming, Liu Zhuang &amp; Huang Gao"></a>Net Sliming, Liu Zhuang &amp; Huang Gao</h2><p>这篇文章<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="noopener">Learning Efficient Convolutional Networks through Network Slimming</a>发表在ICCV 2017，利用CNN网络中的必备组件——BN层中的gamma参数，实现端到端地学习剪枝参数，决定某个layer中该去除掉哪些channel。作者中有DenseNet的作者——姚班学生刘壮和康奈尔大学博士后黄高。代码已经开源：<a href="https://github.com/liuzhuang13/slimming" target="_blank" rel="noopener">liuzhuang13/slimming</a>。</p>
<p>作者的主要贡献是提出可以使用BN层的gamma参数，标志其前面的conv输出的feature map的某个channel是否重要，相应地，也是conv参数中的那个filter是否重要。</p>
<p>首先，需要给BN的gamma参数加上L1 正则惩罚训练模型，新的损失函数变为$L= \sum_{(x,y)}l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma}g(\gamma)$。</p>
<p>接着将该网络中的所有gamma进行排序，根据人为给出的剪枝比例，去掉那些gamma很小的channel，也就是对应的filter。最后进行finetune。这个过程可以反复多次，得到更好的效果。如下所示：<br><img src="/img/paper-model-pruning-net-sliming-procedure.png" alt="Net Sliming的大致流程"></p>
<p>还是上面遇到过的问题，如果处理ResNet或者DenseNet Feature map会多路输出的问题。这里作者提出使用一个”channel selection layer”，统一对该feature map的输出进行处理，只选择没有被mask掉的那些channel输出。具体实现可以参见开源代码<a href="https://github.com/Eric-mingjie/network-slimming/blob/master/models/channel_selection.py#L6" target="_blank" rel="noopener">channel selection layer</a>：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">channel_selection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.</span></span><br><span class="line"><span class="string">    The output shape of this layer is determined by the number of 1 in `self.indexes`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize the `indexes` with all one vector with the length same as the number of channels.</span></span><br><span class="line"><span class="string">        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(channel_selection, self).__init__()</span><br><span class="line">        self.indexes = nn.Parameter(torch.ones(num_channels))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_tensor)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Parameter</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))</span><br><span class="line">        <span class="keyword">if</span> selected_index.size == <span class="number">1</span>:</span><br><span class="line">            selected_index = np.resize(selected_index, (<span class="number">1</span>,))</span><br><span class="line">        output = input_tensor[:, selected_index, :, :]</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>
<p>略微解释一下：在开始加入L1正则，惩罚gamma的时候，相当于identity变换；当确定剪枝参数后，相应index会被置为$0$，被mask掉，这样输出就没有这个channel了。后面的几路都可以用这个共同的输出。</p>
<h2 id="AutoPruner-Wu-Jianxin"><a href="#AutoPruner-Wu-Jianxin" class="headerlink" title="AutoPruner, Wu Jianxin"></a>AutoPruner, Wu Jianxin</h2><p>这篇文章<a href="https://arxiv.org/abs/1805.08941" target="_blank" rel="noopener">AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference</a>是南大Wu Jianxin组新进发的文章，还没有投稿到任何学术会议或期刊，只是挂在了Arvix上，应该是还不够完善。他们还有一篇文章ThiNet：<a href="https://arxiv.org/abs/1707.06342" target="_blank" rel="noopener">ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression</a>发表在ICCV 2017上。</p>
<p>这篇文章的主要贡献是提出了一种端到端的模型剪枝方法，如下图所示。为第$i$个Conv输出加上一个旁路，输入为其输出的Feature map，依次经过Batch-wise Pooling -&gt; FC -&gt; scaled sigmoid的变换，按channel输出取值在$[0,1]$范围的向量作为mask，与Feature map做积，mask掉相应的channel。通过学习FC的参数，就可以得到适当的mask，判断该剪掉第$i$个Conv的哪个filter。其中，scaled sigmoid变换是指$y = \sigma(\alpha x)$。通过训练过程中不断调大$\alpha$，就可以控制sigmoid的“硬度”，最终实现$0-1$门的效果。<br><img src="/img/paper-summary-autopruner-arch.png" alt="AutoPruner框图"></p>
<p>构造损失函数$\mathcal{L} = \mathcal{L}_{\text{cross-entropy}} + \lambda \Vert \frac{\Vert v \Vert_1}{C} - r \Vert_2^2$。其中，$v$是sigmoid输出的mask，$C$为输出的channel数量，$r$为目标稀疏度。</p>
<p>不过在具体的细节上，作者表示要注意的东西很多。主要是FC层的初始化和几个超参数的处理。作者在论文中提出了相应想法：</p>
<ul>
<li>FC层初始化权重为$0$均值，方差为$10\sqrt{\frac{2}{n}}$的高斯分布，其中$n = C\times H \times W$。</li>
<li>上述$\alpha$的控制，如何增长$\alpha$。作者设计了一套if-else的规则。</li>
<li>上述损失函数中的比例$\lambda$，作者使用了$\lambda = 100 \vert r_b - r\vert$的自适应调节方法。</li>
</ul>
<p><img src="/img/paper-summary-model-compression-autopruner-alg.png" alt="AutoPruner Alg"></p>
<h2 id="Rethinking-Net-Pruning-匿名"><a href="#Rethinking-Net-Pruning-匿名" class="headerlink" title="Rethinking Net Pruning, 匿名"></a>Rethinking Net Pruning, 匿名</h2><p>这篇文章<a href="https://openreview.net/pdf?id=rJlnB3C5Ym" target="_blank" rel="noopener">Rethinking the Value of Network Pruning</a>有意思了。严格说来，它还在ICLR 2019的匿名评审阶段，并没有被接收。不过这篇文章的炮口已经瞄准了之前提出的好几个model pruning方法，对它们的结果提出了质疑。上面的链接中，也有被diss的方法之一的作者He Yihui和本文作者的交流。</p>
<p>之前的剪枝算法大多考虑两个问题：</p>
<ol>
<li>怎么求得一个高效的剪枝模型结构，如何确定剪枝方式和剪枝比例：在哪里剪，剪多少</li>
<li>剪枝模型的参数求取：如何保留原始模型中重要的weight，对进行补偿，使得accuracy等性能指标回复到原始模型</li>
</ol>
<p>而本文的作者check了六种SOA的工作，发现：在剪枝算法得到的模型上进行finetune，只比相同结构，但是使用random初始化权重的网络performance好了一点点，甚至有的时候还不如。作者的结论是：</p>
<ol>
<li>训练一个over parameter的model对最终得到一个efficient的小模型不是必要的</li>
<li>为了得到剪枝后的小模型，求取大模型中的important参数其实并不打紧</li>
<li>剪枝得到的结构，相比求得的weight，更重要。所以不如将剪枝算法看做是网络结构搜索的一种特例。</li>
</ol>
<p>作者立了两个论点来打：</p>
<ol>
<li>要先训练一个over-parameter的大模型，然后在其基础上剪枝。因为大模型有更强大的表达能力。</li>
<li>剪枝之后的网络结构和权重都很重要，是剪枝模型finetune的基础。</li>
</ol>
<p>作者试图通过实验证明，很多剪枝方法并没有他们声称的那么有效，很多时候，无需剪枝之后的权重，而是直接随机初始化并训练，就能达到这些论文中的剪枝方法的效果。当然，这些论文并不是一无是处。作者提出，是剪枝之后的结构更重要。这些剪枝方法可以看做是网络结构的搜索。</p>
<p>论文的其他部分就是对几种现有方法的实验和diss。我还没有细看，如果后续这篇论文得到了接收，再做总结吧~夹带一些私货，基于几篇论文的实现经验和在真实数据集上的测试，这篇文章的看法我是同意的。</p>
<p>更新：这篇文章的作者原来正是Net Sliming的作者Liu Zhuang和Huang Gao，那实验和结论应该是很有保障的。最近这篇文章确实也引起了大家的注意，值得好好看一看。</p>
<h2 id="其他论文等资源"><a href="#其他论文等资源" class="headerlink" title="其他论文等资源"></a>其他论文等资源</h2><ul>
<li><a href="https://nervanasystems.github.io/distiller/index.html" target="_blank" rel="noopener">Distiller</a>：一个使用PyTorch实现的剪枝工具包</li>
</ul>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>VIM安装YouCompleteMe和Jedi进行自动补全</title>
    <url>/2018/10/02/vim-you-complete-me/</url>
    <content><![CDATA[<p>这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~</p>
<p><img src="/img/vim-config-demo.png" alt="我的VIM"></p>
<a id="more"></a>
<h2 id="使用Jedi和YouCompleteMe配置Vim"><a href="#使用Jedi和YouCompleteMe配置Vim" class="headerlink" title="使用Jedi和YouCompleteMe配置Vim"></a>使用Jedi和YouCompleteMe配置Vim</h2><p>在远程开发机上调试代码时，我的习惯是大型项目使用sshfs将其镜像到本地，然后使用VSCode打开编辑。VSCode中有终端可以方便的ssh到远端开发机，我将”CTRL+`”配置成了编辑器和终端之间的切换快捷键。加上vim插件，就可以实现不用鼠标，不离开当前编辑环境进行代码编写和调试了。</p>
<p>然而，如果是想在开发机上写一段小的代码，上述方法就显得太麻烦了。</p>
<h2 id="编译Vim"><a href="#编译Vim" class="headerlink" title="编译Vim"></a>编译Vim</h2><p>编译Vim，注意我们要设定其安装目录为anaconda下的bin目录：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">./configure --with-features=huge --<span class="built_in">enable</span>-multibyte --<span class="built_in">enable</span>-pythoninterp=yes --with-python-config-dir=/path/to/anaconda/bin/python-config --<span class="built_in">enable</span>-gui=gtk2 --prefix=/path/to/anaconda</span><br></pre></td></tr></table></figure>
<p>编译并安装：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">make -j4 VIMRUNTIMEDIR=/path/to/anaconda/share/vim/vim81</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></p>
<p>安装后，可以查看vim的version进行确认。安装没有问题，会提示刚才编译的版本信息。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">vim --version</span><br></pre></td></tr></table></figure></p>
<p>使用Vundle管理插件，这个没有什么问题，直接按照README提示即可，见：<a href="https://github.com/VundleVim/Vundle.vim" target="_blank" rel="noopener">Vundle@Github</a>。</p>
<p>使用Vundle进行插件管理，只需要以下面的形式指明插件目录或Github仓库名称，进入vim后，在Normal状态，输入<code>:PluginInstall</code>即可。</p>
<h2 id="Jedi"><a href="#Jedi" class="headerlink" title="Jedi"></a>Jedi</h2><p>首先需要安装jedi的python包：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">pip install jedi</span><br></pre></td></tr></table></figure></p>
<p>使用Vbudle安装<a href="https://github.com/davidhalter/jedi-vim" target="_blank" rel="noopener">jedi-vim</a>，并在<code>.vimrc</code>中添加以下内容。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">let g:jedi#force_py_version=2.7</span><br></pre></td></tr></table></figure></p>
<h2 id="YouCompleteMe"><a href="#YouCompleteMe" class="headerlink" title="YouCompleteMe"></a>YouCompleteMe</h2><p>使用Vundle安装<a href="https://github.com/Valloric/YouCompleteMe#ubuntu-linux-x64" target="_blank" rel="noopener">YouCompleteMe</a>。</p>
<p>之后，进入目录<code>.vim/bundle/YouCompleteMe</code>，执行<code>./install.py</code>。如果需要C++支持，执行<code>./install.py --clang-completer</code>。</p>
<p>但是，其中遇到了问题，找不到Python.h文件。使用<code>locate Python.h</code>，明确该文件确实存在，且其位于<code>/path/to/anaconda/include/python2.7</code>后，手动修改CMakeLists.txt，指定该文件目录位置即可。</p>
<p>修改这个：<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/CMakeLists.txt</code><br>和<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/CMakeLists.txt</code>，向其中添加：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">set</span>( CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -I/path/to/anaconda/include/python2.7"</span> )</span><br></pre></td></tr></table></figure>
<p>强行指定头文件包含目录。</p>
<h2 id="括号自动补全"><a href="#括号自动补全" class="headerlink" title="括号自动补全"></a>括号自动补全</h2><p>虽然SO上有人指出可以直接通过设置<code>.vimrc</code>的方法实现，不过还是直接用现成的插件吧。推荐使用<a href="https://github.com/jiangmiao/auto-pairs" target="_blank" rel="noopener">jiangmiao/auto-pairs</a>。可以按照README的说明进行安装。</p>
]]></content>
      <tags>
        <tag>vim</tag>
        <tag>tools</tag>
      </tags>
  </entry>
  <entry>
    <title>MXNet fit介绍</title>
    <url>/2018/10/02/mxnet-fit-usage/</url>
    <content><![CDATA[<p>在MXNet中，<code>Module</code>提供了训练模型的方便接口。使用<code>symbol</code>将计算图建好之后，用<code>Module</code>包装一下，就可以通过<code>fit()</code>方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用<code>fit()</code>接口就不太合适。这里把<code>fit()</code>代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。</p>
<p>其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个<a href="https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks" target="_blank" rel="noopener">Slide分享</a>，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。</p>
<p><img src="/img/differences-of-deep-learning-frameworks-22-638.jpg" alt="Differences of Deep Learning Frameworks"></p>
<a id="more"></a>
<h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先，需要将数据绑定到计算图上，并初始化模型的参数，并初始化求解器。这些是求解模型必不可少的。</p>
<p>其次，还会建立训练的metric，方便我们掌握训练进程和当前模型在训练任务的表现。</p>
<p>这些是在为后续迭代进行梯度下降更新做准备。</p>
<h2 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a>迭代更新</h2><p>使用SGD进行训练的时候，我们需要不停地从数据迭代器中获取包含data和label的batch，并将其feed到网络模型中。进行forward computing后进行bp，获得梯度，并根据具体的优化方法（SGD, SGD with momentum, RMSprop等）进行参数更新。</p>
<p>这部分可以抽成：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># in an epoch</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> end_epoch:</span><br><span class="line">    batch = next(train_iter)</span><br><span class="line">    m.forward_backward(batch)</span><br><span class="line">    m.update()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        next_batch = next(data_iter)</span><br><span class="line">        m.prepare(next_batch)</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        end_epoch = <span class="literal">True</span></span><br></pre></td></tr></table></figure></p>
<h2 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h2><p>在训练的时候，观察输出的各种metric是必不可少的。我们对训练过程的把握就是通过metric给出的信息。通常在分类任务中常用到的metric有Accuracy，TopK-Accuracy以及交叉熵损失等，这些已经在MXNet中有了现成的实现。而在<code>fit</code>中，调用了<code>m.update_metric(eval_metric, data_batch.label)</code>实现。这里的<code>eval_metric</code>就是我们指定的metric，而<code>label</code>是batch提供的label。注意，在MXNet中，label一般都是以<code>list</code>的形式给出（对应于多任务学习），也就是说这里的label是<code>list of NDArray</code>。当自己魔改的时候要注意。</p>
<h2 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h2><p>计算了eval_metric等信息，我们需要将其在屏幕上打印出来。MXNet中可以通过callback实现。另外，保存模型checkpoint这样的功能也是通过callback实现的。一种常用的场景是每过若干个batch，做一次logging，打印当前的metric信息，如交叉熵损失降到多少了，准确率提高到多少了等。MXNet会将以下信息打包成<code>BatchEndParam</code>类型（其实是一个自定义的<code>namedtuple</code>）的变量，包括当前epoch，当前迭代次数，评估的metric。如果你需要更多的信息或者更自由的logging监控，也可以参考代码自己实现。</p>
<p>我们以常用的<code>Speedometer</code>看一下如何使用这些信息，其功能如下，将训练的速度和metric打印出来。</p>
<blockquote>
<p>Logs training speed and evaluation metrics periodically</p>
</blockquote>
<p>PS:这里有个隐藏的坑。MXNet中的<code>Speedometer</code>每回调一次，会把<code>metric</code>的内容清除。这在训练的时候当然没问题。但是如果是在validation上跑，就会有问题了。这样最终得到的只是最后一个回调周期那些batch的metric，而不是整个验证集上的。如果在<code>fit</code>方法中传入了<code>eval_batch_end_callback</code>参数就要注意这个问题了。解决办法一是在<code>Speedometer</code>实例初始化时传入<code>auto_reset=False</code>，另一种干脆就不要加这个参数，默认为<code>None</code>好了。同样的问题也发生在调用<code>Module.score()</code>方法来获取模型在验证集上metric的时候。</p>
<p>可以在<code>Speedometer</code>代码中寻找下面这几行，会更清楚：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> param.eval_metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    name_value = param.eval_metric.get_name_value()</span><br><span class="line">    <span class="keyword">if</span> self.auto_reset:</span><br><span class="line">        param.eval_metric.reset()</span><br></pre></td></tr></table></figure>
<h2 id="在验证集上测试"><a href="#在验证集上测试" class="headerlink" title="在验证集上测试"></a>在验证集上测试</h2><p>当在训练集上跑过一个epoch后，如果提供了验证集的迭代器，会在验证集上对模型进行测试。这里，MXNet直接封装了<code>score()</code>方法。在<code>score</code>中，基本流程和<code>fit()</code>相同，只是我们只需要forward computing即可。</p>
<h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><p>用了一段时间的MXNet，给我的最大的感觉是MXNet就像一个写计算图的前端，提供了很方便的python接口生成静态图，以及很多“可插拔”的插件（虽然可能不是很全，更像是一份guide而不是拿来即用的tool），如上文中的metric等，使其更适合做成流程化的基础DL平台，供给更上层方便地配置使用。缺点就是隐藏了比较多的实现细节（当然，你完全可以从代码中自己学习，比如从<code>fit()</code>代码了解神经网络的大致训练流程）。至于MXNet宣扬的诸如速度快，图优化，省计算资源等优点，因为我没有过数据对比，就不说了。</p>
<p>缺点就是写图的时候有时不太灵活（可能也是我写的看的还比较少），即使是和TensorFlow这种同为静态图的DL框架比。另外，貌似MXNet中很多东西都没有跟上最新的论文等，比如Cosine的learning rate decay就没有。Model Zoo也比较少(gluon可能会好一点，Gluon-CV和Gluon-NLP貌似是在搞一些论文复现的工作)。对开发来讲，很多东西都需要阅读代码才能知道是怎么回事，只是读文档的话容易踩坑。</p>
<p>说到这里，感觉MXNet的python训练接口（包括module，optimizer，metric等）更像是一份example代码，是在教你怎么去用MXNet，而不像一个灵活地强大的工具箱。当然，很多东西不能得兼，希望MXNet越来越好。</p>
]]></content>
      <tags>
        <tag>mxnet</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Like What You Like - Knowledge Distill via Neuron Selectivity Transfer</title>
    <url>/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/</url>
    <content><![CDATA[<p>好长时间没有写博客了，国庆假期把最近看的东西整理一下。<a href="https://arxiv.org/abs/1707.01219" target="_blank" rel="noopener">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a>这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。</p>
<p><img src="/img/paper-nst-kt-like-what-you-like.gif" alt="jump if you jump"></p>
<p>另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：<a href="https://arxiv.org/abs/1707.01213" target="_blank" rel="noopener">Data-driven sparse structure selection for deep neural networks</a>。</p>
<p>另另外，其实这两篇文章挂在Arxiv的时间很接近，<a href="https://www.zhihu.com/question/62068158" target="_blank" rel="noopener">知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？</a>有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。</p>
<a id="more"></a>
<h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>KD的一个痛点在于其只适用于softmax分类问题。这样，对于Detection。Segmentation等一系列问题，没有办法应用。另一个问题在于当分类类别数较少时，KD效果不理想。这个问题比较好理解，假设我们面对一个二分类问题，那么我们并不关心类间的similarity，而是尽可能把两类分开即可。同时，这篇文章的实验部分也验证了这个猜想：当分类问题分类类别数较多时，使用KD能够取得best的结果。</p>
<p>作者联想到，我们是否可以把CNN中某个中间层输出的feature map利用起来呢？Student输出的feature map要和Teacher的相似，相当于是Student学习到了Teacher提取特征的能力。在CNN中，每个filter都是在和一个feature map上的patch做卷积得到输出，很多个filter都做卷积运算，就得到了feature（map）。另外，当filter和patch有相似的结构时，得到的激活比较大。举个例子，如果filter是Sobel算子，那么当输入image是边缘的时候，得到的响应是最大的。filter学习出来的是输入中的某些模式。当模式匹配上时，激活。这里也可以参考一些对CNN中filter做可视化的研究。</p>
<p>顺着上面的思路，有人提出了Attention Transfer的方法，可以参见这篇文章：<a href="https://arxiv.org/abs/1612.03928" target="_blank" rel="noopener">Improving the Performance of Convolutional Neural Networks via Attention Transfer</a>。而在NST这篇文章中，作者引入了新的损失函数，用于衡量Student和Teacher对相同输入的激活Feature map的不同，可以说除了下面要介绍的数学概念以外，没有什么难理解的地方。整个训练的网络结构如下所示：<br><img src="/img/paper-nst-student-and-teacher.png" alt="NST知识蒸馏的整体框图结构"></p>
<h2 id="Maximum-Mean-Discrepancy"><a href="#Maximum-Mean-Discrepancy" class="headerlink" title="Maximum Mean Discrepancy"></a>Maximum Mean Discrepancy</h2><p>MMD 是用来衡量sampled data之间分布差异的距离量度。如果有两个不同的分布$p$和$q$，以及从两个分布中采样得到的Data set$\mathcal{X}$和$\mathcal{Y}$。那么MMD距离如下：</p>
<script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2</script><p>其中，$\phi$表示某个mapping function。变形之后（内积打开括号），可以得到：</p>
<script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)</script><p>其中，$k$是某个kernel function，$k(x, y) = \phi(x)^{T}\phi(y)$。</p>
<p>我们可以使用MMD来衡量Student模型和Teacher模型中间输出的激活feature map的相似程度。通过优化这个损失函数，使得S的输出分布接近T。通过引入MMD，将NST loss定义如下，下标$S$表示Student的输出，$T$表示Teacher的输出。第一项$\mathcal{H}$是指由样本类别标签计算的CrossEntropy Loss。第二项即为上述的MMD Loss。</p>
<script type="math/tex; mode=display">\mathcal{L} = \mathcal{H}(y, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)</script><p>注意，为了确保后一项有意义，需要保证$F_T$和$F_S$有相同长度。具体来说，对于网络中间输出的feature map，我们将每个channel上的$HW$维的feature vector作为分布$\mathcal{X}$的一个采样。按照作者的设定，我们需要保证S和T对应的feature map在spatial dimension上必须一样大。如果不一样，可以使用插值方法进行扩展。</p>
<p>为了不受相对幅值大小的影响，需要对feature vector做normalization。</p>
<p>对于kernal的选择，作者提出了三种可行方案：线性，多项式和高斯核。在后续通过实验对比了它们的性能。</p>
<h2 id="和其他方法的关联"><a href="#和其他方法的关联" class="headerlink" title="和其他方法的关联"></a>和其他方法的关联</h2><p>如果使用线性核函数，也就是$\phi$是一个identity mapping，那么MMD就成了直接比较两个样本分布质心的距离。这时候，和上文提到的AT方法的一种形式是类似的。（这个我觉得有点强行扯关系。。。）</p>
<p>如果使用二次多项式核函数，可以得到，$\mathcal{L}_{MMD}(F_T, F_S) = \Vert G_T - G_S\Vert_F^2$。其中，$G \in \mathbb{R}^{HW\times HW}$为Gram矩阵，其中的元素$g_{ij} = (f^i)^Tf^j$。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在CIFAR10，ImageNet等数据集上进行了实验。Student均使用Inception-BN网络，Teacher分别使用了ResNet-1001和ResNet-101。一些具体的参数设置参考论文即可。</p>
<p>下面是CIFAR10上的结果。可以看到，单一方法下，CIFAR10分类NST效果最好，CIFAR100分类KD最好。组合方法中，KD+NST最好。<br><img src="/img/paper-nst-cifar10-results.png" alt="CIFAR10 结果"></p>
<p>下面是ImageNet上的结果。KD+NST的组合仍然是效果最好的。<br><img src="/img/paper-nst-imagenet-results.png" alt="ImageNet结果"></p>
<p>作者还对NST前后，Student和Teacher的输出Feature map做了聚类，发现NST确实能够使得S的输出去接近T的输出分布。如下图所示：<br><img src="/img/paper-nst-visulization-teacher-student-feature-map.png" alt="NST减小了T和S的激活feature map的distance"></p>
<p>此外，作者还实验了在Detection任务上的表现。在PASCAL VOC2007数据集上基于Faster RCNN方法进行了实验。backbone网络仍然是Inception BN，从<code>4b</code>layer获取feature map，此时stide为16。</p>
<p><img src="/img/paper-nst-pascal-voc-results.png" alt="PASCAL VOC结果"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Distilling the Knowledge in a Neural Network</title>
    <url>/2018/06/07/knowledge-distilling/</url>
    <content><![CDATA[<p>知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。</p>
<a id="more"></a>
<h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这里我将Wang Naiyang在知乎相关问题的<a href="https://www.zhihu.com/question/50519680/answer/136363665" target="_blank" rel="noopener">回答</a>粘贴如下，将KD方法的motivation讲的很清楚。图森也发了论文对KD进行了改进，下篇笔记总结。</p>
<blockquote>
<p>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</p>
</blockquote>
<h2 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h2><p>这篇论文很好阅读。论文中实现蒸馏是靠soften softmax prob实现的。在分类任务中，常常使用交叉熵作为损失函数，使用one-hot编码的标注好的类别标签${1,2,\dots,K}$作为target，如下所示：</p>
<script type="math/tex; mode=display">\mathcal{L} = -\sum_{i=1}^{K}t_i\log p_i</script><p>作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，$2$和$3$就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字$2$时，很有可能它给出的概率是：数字$2$为$0.99$，数字$3$为$10^{-2}$，数字$7$为$10^{-4}$。如何能够利用训练好的Teacher模型给出的这种信息呢？</p>
<p>可以使用带温度的softmax函数。对于softmax的输入（下文统一称为logit），我们按照下式给出输出：</p>
<script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>其中，当$T = 1$时，就是普通的softmax变换。这里令$T &gt; 1$，就得到了软化的softmax。（这个很好理解，除以一个比$1$大的数，相当于被squash了，线性的sqush被指数放大，差距就不会这么大了）。OK，有了这个东西，我们将Teacher网络和Student的最后充当分类器的那个全连接层的输出都做这个处理。</p>
<p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。<br>可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失：</p>
<script type="math/tex; mode=display">\mathcal{L}_{soft}=-\sum_{i=1}^{K}p_i\log q_i</script><p>其中，$p_i$为Teacher模型给出的soft target，$q_i$为Student模型给出的soft output。作者发现，最好的方式是做一个multi task learning，将上面这个损失函数和真正的交叉熵损失加权相加。相应地，我们将其称为hard target。</p>
<script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}</script><p>其中，$\mathcal{L}_{hard}$是分类问题中经典的交叉熵损失。由于做softened softmax计算时，需要除以$T$，导致soft target关联的梯度幅值被缩小了$T^2$倍，所以有必要在$\lambda$中预先考虑到$T^2$这个因子。</p>
<p>PS:这里有一篇地平线烫叔关于多任务中loss函数设计的回答：<a href="https://www.zhihu.com/question/268105631/answer/335246543" target="_blank" rel="noopener">神经网络中，设计loss function有哪些技巧? - Alan Huang的回答 - 知乎</a>。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>这里给出一个开源的MXNet的实现:<a href="https://github.com/TuSimple/neuron-selectivity-transfer/blob/master/symbol/transfer.py#L4" target="_blank" rel="noopener">kd loss by mxnet</a>。MXNet中的<code>SoftmaxOutput</code>不仅能直接支持one-hot编码类型的array作为label输入，甚至label的<code>dtype</code>也可以不是整型！</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd</span><span class="params">(student_hard_logits, teacher_hard_logits, temperature, weight_lambda, prefix)</span>:</span></span><br><span class="line">    student_soft_logits = student_hard_logits / temperature</span><br><span class="line">    teacher_soft_logits = teacher_hard_logits / temperature</span><br><span class="line">    teacher_soft_labels = mx.symbol.SoftmaxActivation(teacher_soft_logits,</span><br><span class="line">        name=<span class="string">"teacher%s_soft_labels"</span> % prefix)</span><br><span class="line">    kd_loss = mx.symbol.SoftmaxOutput(data=student_soft_logits, label=teacher_soft_labels,</span><br><span class="line">                                      grad_scale=weight_lambda, name=<span class="string">"%skd_loss"</span> % prefix)</span><br><span class="line">    <span class="keyword">return</span> kd_loss</span><br></pre></td></tr></table></figure>
<h2 id="matching-logit是特例"><a href="#matching-logit是特例" class="headerlink" title="matching logit是特例"></a>matching logit是特例</h2><p>（这部分没什么用，练习推导了一下交叉熵损失的梯度计算）</p>
<p>在Hinton之前，有学者提出可以匹配Teacher和Student输出的logit，Hinton指出这是本文方法在一定假设下的近似。为了和论文中的符号相同，下面我们使用$C$表示soft target带来的loss，Teacher和Student第$i$个神经元输出的logit分别为$v_i$和$z_i$，输出的softened softmax分别为$p_i$和$q_i$。那么我们有：</p>
<script type="math/tex; mode=display">C = -\sum_{j=1}^{C}p_j \log q_j</script><p>而且，</p>
<script type="math/tex; mode=display">p_i = \frac{\exp(v_i/T)}{\sum_j \exp(v_j/T)}</script><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>让我们暂时忽略$T$（最后我们乘上$\frac{1}{T}$即可），我们有：</p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} = -\sum_{j=1}^{K}p_j\frac{1}{q_j}\frac{\partial q_j}{\partial z_i}</script><p>分情况讨论，当$i = j$时，有：</p>
<script type="math/tex; mode=display">\frac{\partial q_j}{\partial z_i} = q_i (1-q_i)</script><p>当$i \neq j$时，有：</p>
<script type="math/tex; mode=display">\begin{aligned}
\frac{\partial q_j}{\partial z_i} &= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2}  \\
&=-q_iq_j
\end{aligned}</script><p>这样，我们有：</p>
<script type="math/tex; mode=display">
\begin{aligned} 
\frac{\partial C}{\partial z_i} &= - p_i\frac{1}{q_i}q_i(1-q_i) + \sum_{j=1, j\neq i}^{K}p_j\frac{1}{q_j}q_iq_j  \\
&= -p_i + p_iq_i + \sum_{j=1, j\neq i}^K p_jq_i \\
&= q_i -p_i
\end{aligned}</script><p>当然，其实上面的推导过程只不过是重复了一遍one-hot编码的交叉熵损失的计算。</p>
<p>这样，如果我们假设logit是零均值的，也就是说$\sum_j z_j = \sum_j v_j = 0$，那么有：</p>
<script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} \sim \frac{1}{NT^2}(z_i - v_i)</script><p>所以说，MSE下进行logit的匹配，是本文方法的一个特例。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者使用了MNIST进行图片分类的实验，一个有趣的地方在于（和论文前半部分举的$2$和$3$识别的例子呼应），作者在数据集中有意地去除了标签为$3$的样本。没有KD的student网络不能识别测试时候提供的$3$，有KD的student网络能够识别一些$3$（虽然它从来没有在训练样本中出现过！）。后面，作者在语音识别和一个Google内部的很大的图像分类数据集（JFT dataset）上做了实验，</p>
<h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul>
<li>知乎上关于soft target的讨论，有Wang Naiyan和Zhou Bolei的分析：<a href="https://www.zhihu.com/question/50519680" target="_blank" rel="noopener">如何理解soft target这一做法？
</a></li>
</ul>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>（译）PyTorch 0.4.0 Migration Guide</title>
    <url>/2018/04/27/pytorch-040-migration-guide/</url>
    <content><![CDATA[<p>PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个<a href="http://pytorch.org/2018/04/22/0_4_0-migration-guide.html" target="_blank" rel="noopener">转换指导</a>，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有：</p>
<ul>
<li><code>Tensor</code>和<code>Variable</code>合并，<code>autograd</code>的机制有所不同，变得更简单，使用<code>requires_grad</code>和上下文相关环境管理。</li>
<li>Numpy风格的<code>Tensor</code>构建。</li>
<li>提出了<code>device</code>，更简单地在cpu和gpu中移动数据。</li>
</ul>
<a id="more"></a>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在0.4.0版本中，PyTorch引入了许多令人兴奋的新特性和bug fixes。为了方便以前版本的使用者转换到新的版本，我们编写了此指导，主要包括以下几个重要的方面：</p>
<ul>
<li><code>Tensors</code> 和 <code>Variables</code> 已经merge到一起了</li>
<li>支持0维的Tensor（即标量scalar）</li>
<li>弃用了 <code>volatile</code> 标志</li>
<li><code>dtypes</code>, <code>devices</code>, 和 Numpy 风格的 Tensor构造函数</li>
<li>（更好地编写）设备无关代码</li>
</ul>
<p>下面分条介绍。</p>
<h2 id="Tensor-和-Variable-合并"><a href="#Tensor-和-Variable-合并" class="headerlink" title="Tensor 和 Variable 合并"></a><code>Tensor</code> 和 <code>Variable</code> 合并</h2><p>在PyTorch以前的版本中，<code>Tensor</code>类似于<code>numpy</code>中的<code>ndarray</code>，只是对多维数组的抽象。为了能够使用自动求导机制，必须使用<code>Variable</code>对其进行包装。而现在，这两个东西已经完全合并成一个了，以前<code>Variable</code>的使用情境都可以使用<code>Tensor</code>。所以以前训练的时候总要额外写的warpping语句用不到了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> data_loader:</span><br><span class="line">    <span class="comment">## 用不到了</span></span><br><span class="line">    data, target = Variable(data), Variable(target)</span><br><span class="line">    loss = criterion(model(data), target)</span><br></pre></td></tr></table></figure>
<h3 id="Tensor的类型type"><a href="#Tensor的类型type" class="headerlink" title="Tensor的类型type()"></a><code>Tensor</code>的类型<code>type()</code></h3><p>以前我们可以使用<code>type()</code>获取<code>Tensor</code>的data type（FloatTensor，LongTensor等）。现在需要使用<code>x.type()</code>获取类型或<code>isinstance()</code>判别类型。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.DoubleTensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(x))  <span class="comment"># 曾经会给出 torch.DoubleTensor</span></span><br><span class="line"><span class="string">"&lt;class 'torch.Tensor'&gt;"</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(x.type())  <span class="comment"># OK: 'torch.DoubleTensor'</span></span><br><span class="line"><span class="string">'torch.DoubleTensor'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>print(isinstance(x, torch.DoubleTensor))  <span class="comment"># OK: True</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="autograd现在如何追踪计算图的历史"><a href="#autograd现在如何追踪计算图的历史" class="headerlink" title="autograd现在如何追踪计算图的历史"></a><code>autograd</code>现在如何追踪计算图的历史</h3><p><code>Tensor</code>和<code>Variable</code>的合并，简化了计算图的构建，具体规则见本条和以下几条说明。</p>
<p><code>requires_grad</code>, 这个<code>autograd</code>中的核心标志量,现在成了<code>Tensor</code>的属性。之前的<code>Variable</code>使用规则可以同样应用于<code>Tensor</code>，<code>autograd</code>自动跟踪那些至少有一个input的<code>requires_grad==True</code>的计算节点构成的图。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.ones(<span class="number">1</span>)  <span class="comment">## 默认requires_grad = False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">1</span>)  <span class="comment">## 同样，y的requires_grad标志也是False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z = x + y</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所有的输入节点都不要求梯度，所以z的requires_grad也是False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所以如果试图对z做梯度反传，会抛出Error</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward()</span><br><span class="line">RuntimeError: element <span class="number">0</span> of tensors does <span class="keyword">not</span> require grad <span class="keyword">and</span> does <span class="keyword">not</span> have a grad_fn</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 通过手动指定的方式创建 requires_grad=True 的Tensor</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w = torch.ones(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 把它和之前requires_grad=False的节点相加，得到输出</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>total = w + z</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 由于w需要梯度，所以total也需要</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>total.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 可以做bp</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>total.backward()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>w.grad</span><br><span class="line">tensor([ <span class="number">1.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 不用有时间浪费在求取 x y z的梯度上，因为它们没有 require grad，它们的grad == None</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>z.grad == x.grad == y.grad == <span class="literal">None</span></span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="操作-requires-grad-标志"><a href="#操作-requires-grad-标志" class="headerlink" title="操作 requires_grad 标志"></a>操作 <code>requires_grad</code> 标志</h3><p>除了直接设置这个属性，你可以使用<code>my_tensor.requires_grad_()</code>就地修改这个标志（还记得吗，以<code>_</code>结尾的方法名表示in-place的操作）。或者就在构造的时候传入此参数。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad_()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor = torch.zeros(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="data怎么办？What-about-data"><a href="#data怎么办？What-about-data" class="headerlink" title=".data怎么办？What about .data?"></a><code>.data</code>怎么办？What about .data?</h3><p>原来版本中，对于某个<code>Variable</code>，我们可以通过<code>x.data</code>的方式获取其包装的<code>Tensor</code>。现在两者已经merge到了一起，如果你调用<code>y = x.data</code>仍然和以前相似，<code>y</code>现在会共享<code>x</code>的data，并与<code>x</code>的计算历史无关，且其<code>requires_grad</code>标志为<code>False</code>。</p>
<p>然而，<code>.data</code>有的时候可能会成为代码中不安全的一个点。对<code>x.data</code>的任何带动都不会被<code>aotograd</code>跟踪。所以，当做反传的时候，计算的梯度可能会不对，一种更安全的替代方法是调用<code>x.detach()</code>，仍然会返回一个共享<code>x</code>data的Tensor，且<code>requires_grad=False</code>，但是当<code>x</code>需要bp的时候，会报告那些in-place的操作。</p>
<blockquote>
<p>However, .data can be unsafe in some cases. Any changes on x.data wouldn’t be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward.</p>
</blockquote>
<p>这里有些绕，可以看下下面的示例代码：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 一个简单的计算图：y = sum(x**2)</span></span><br><span class="line">x = torch.ones((<span class="number">1</span> ,<span class="number">2</span>))</span><br><span class="line">x.requires_grad_()</span><br><span class="line">y = torch.sum(x**<span class="number">2</span>)</span><br><span class="line">y.backward()</span><br><span class="line">x.grad   <span class="comment"># grad: [2, 2, 2]</span></span><br><span class="line"><span class="comment"># 使用.data，在计算完y之后，又改动了x，会造成梯度计算错误</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = torch.sum(x**<span class="number">2</span>)</span><br><span class="line">data = x.data</span><br><span class="line">data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line">x.grad   <span class="comment"># grad: [4, 2, 2] 错了哦~</span></span><br><span class="line"><span class="comment"># 使用detach，同样的操作，会抛出异常</span></span><br><span class="line">x.grad.zero_()</span><br><span class="line">y = torch.sum(x**<span class="number">2</span>)</span><br><span class="line">data = x.detach()</span><br><span class="line">data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">2</span></span><br><span class="line">y.backward()</span><br><span class="line"><span class="comment"># 抛出如下异常</span></span><br><span class="line"><span class="comment"># RuntimeError: one of the variables needed for gradient </span></span><br><span class="line"><span class="comment"># computation has been modified by an inplace operation</span></span><br></pre></td></tr></table></figure>
<h2 id="支持0维-scalar-的Tensor"><a href="#支持0维-scalar-的Tensor" class="headerlink" title="支持0维(scalar)的Tensor"></a>支持0维(scalar)的Tensor</h2><p>原来的版本中，对Tensor vector（1D Tensor）做索引得到的结果是一个python number，但是对一个Variable vector来说，得到的就是一个<code>size(1,)</code>的vector!对于reduction function（如<code>torch.sum</code>，<code>torch.max</code>）也有这样的问题。</p>
<p>所以我们引入了scalar（0D Tensor）。它可以使用<code>torch.tensor()</code> 函数来创建，现在你可以这样做：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>)         <span class="comment"># 直接创建scalar</span></span><br><span class="line">tensor(<span class="number">3.1416</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>).size()  <span class="comment"># scalar 是 0D</span></span><br><span class="line">torch.Size([])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">3</span>]).size()     <span class="comment"># 和1D对比</span></span><br><span class="line">torch.Size([<span class="number">1</span>])</span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vector = torch.arange(<span class="number">2</span>, <span class="number">6</span>)  <span class="comment"># 1D的vector</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vector</span><br><span class="line">tensor([ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vector.size()</span><br><span class="line">torch.Size([<span class="number">4</span>])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>]                    <span class="comment"># 对1D的vector做indexing，得到的是scalar</span></span><br><span class="line">tensor(<span class="number">5.</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>].item()             <span class="comment"># 使用.item()获取python number</span></span><br><span class="line"><span class="number">5.0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mysum = torch.tensor([<span class="number">2</span>, <span class="number">3</span>]).sum()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mysum</span><br><span class="line">tensor(<span class="number">5</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>mysum.size()</span><br><span class="line">torch.Size([])</span><br></pre></td></tr></table></figure>
<h3 id="累积losses"><a href="#累积losses" class="headerlink" title="累积losses"></a>累积losses</h3><p>我们在训练的时候，经常有这样的用法：<code>total_loss += loss.data[0]</code>。<code>loss</code>通常都是由损失函数计算出来的一个标量，也就是包装了<code>(1,)</code>大小<code>Tensor</code>的<code>Variable</code>。在新的版本中，<code>loss</code>则变成了0D的scalar。对一个scalar做indexing是没有意义的，应该使用<code>loss.item()</code>获取python number。</p>
<p>注意，如果你在做累加的时候没有转换为python number，你的程序可能会出现不必要的内存占用。因为<code>autograd</code>会记录调用过程，以便做反向传播。所以，你现在应该写成 <code>total_loss += loss.item()</code>。</p>
<h2 id="弃用volatile标志"><a href="#弃用volatile标志" class="headerlink" title="弃用volatile标志"></a>弃用<code>volatile</code>标志</h2><p><code>volatile</code> 标志被弃用了，现在没有任何效果。以前的版本中，一个设置<code>volatile=True</code>的<code>Variable</code> 表明其不会被<code>autograd</code>追踪。现在，被替换成了一个更灵活的上下文管理器，如<code>torch.no_grad()</code>，<code>torch.set_grad_enable(grad_mode)</code>等。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">1</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():    <span class="comment"># 使用 torch,no_grad()构建不需要track的上下文环境</span></span><br><span class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line">&gt;&gt;&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>is_train = <span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.set_grad_enabled(is_train):   <span class="comment"># 在inference的时候，设置不要track</span></span><br><span class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="literal">True</span>)  <span class="comment"># 当然也可以不用with构建上下文环境，而单独这样用</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</span><br><span class="line"><span class="literal">False</span></span><br></pre></td></tr></table></figure>
<h2 id="dtypes-devices-和NumPy风格的构建函数"><a href="#dtypes-devices-和NumPy风格的构建函数" class="headerlink" title="dtypes, devices 和NumPy风格的构建函数"></a><code>dtypes</code>, <code>devices</code> 和NumPy风格的构建函数</h2><p>以前的版本中，我们需要以”tensor type”的形式给出对data type（如<code>float</code>或<code>double</code>），device type（如cpu或gpu）以及layout（dense或sparse）的限定。例如，<code>torch.cuda.sparse.DoubleTensor</code>用来构造一个data type是<code>double</code>，在GPU上以及sparse的tensor。</p>
<p>现在我们引入了<code>torch.dtype</code>，<code>torch.device</code>和<code>torch.layout</code>来更好地使用Numpy风格的构建函数。</p>
<h3 id="torch-dtype"><a href="#torch-dtype" class="headerlink" title="torch.dtype"></a><code>torch.dtype</code></h3><p>下面是可用的 <code>torch.dtypes</code> (data types) 和它们对应的tensor types。可以使用<code>x.dtype</code>获取。</p>
<table>
   <tr>
      <td>data type</td>
      <td>torch.dtype</td>
      <td>Tensor types</td>
   </tr>
   <tr>
      <td>32-bit floating point</td>
      <td>torch.float32 or torch.float</td>
      <td>torch.*.FloatTensor</td>
   </tr>
   <tr>
      <td>64-bit floating point</td>
      <td>torch.float64 or torch.double</td>
      <td>torch.*.DoubleTensor</td>
   </tr>
   <tr>
      <td>16-bit floating point</td>
      <td>torch.float16 or torch.half</td>
      <td>torch.*.HalfTensor</td>
   </tr>
   <tr>
      <td>8-bit integer (unsigned)</td>
      <td>torch.uint8</td>
      <td>torch.*.ByteTensor</td>
   </tr>
   <tr>
      <td>8-bit integer (signed)</td>
      <td>torch.int8</td>
      <td>torch.*.CharTensor</td>
   </tr>
   <tr>
      <td>16-bit integer (signed)</td>
      <td>torch.int16 or torch.short</td>
      <td>torch.*.ShortTensor</td>
   </tr>
   <tr>
      <td>32-bit integer (signed)</td>
      <td>torch.int32 or torch.int</td>
      <td>torch.*.IntTensor</td>
   </tr>
   <tr>
      <td>64-bit integer (signed)</td>
      <td>torch.int64 or torch.long</td>
      <td>torch.*.LongTensor</td>
   </tr>
</table>

<h3 id="torch-device"><a href="#torch-device" class="headerlink" title="torch.device"></a><code>torch.device</code></h3><p><code>torch.device</code>包含了device type（如cpu或cuda）和可能的设备id。使用<code>torch.device(&#39;{device_type}&#39;)</code>或<code>torch.device(&#39;{device_type}:{device_ordinal}&#39;)</code>的方式来初始化。 </p>
<p>如果没有指定<code>device ordinal</code>，那么默认是当前的device。例如，<code>torch.device(&#39;cuda&#39;)</code>相当于<code>torch.device(&#39;cuda:X&#39;)</code>，其中，<code>X</code>是<code>torch.cuda.current_device()</code>的返回结果。</p>
<p>使用<code>x.device</code>来获取。</p>
<h3 id="torch-layout"><a href="#torch-layout" class="headerlink" title="torch.layout"></a><code>torch.layout</code></h3><p><code>torch.layout</code>代表了<code>Tensor</code>的data layout。 目前支持的是<code>torch.strided</code> (dense，也是默认的) 和 <code>torch.sparse_coo</code> (COOG格式的稀疏tensor)。</p>
<p>使用<code>x.layout</code>来获取。</p>
<h3 id="创建Tensor（Numpy风格）"><a href="#创建Tensor（Numpy风格）" class="headerlink" title="创建Tensor（Numpy风格）"></a>创建<code>Tensor</code>（Numpy风格）</h3><p>你可以使用<code>dtype</code>，<code>device</code>，<code>layout</code>和<code>requires_grad</code>更好地控制<code>Tensor</code>的创建。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>device = torch.device(<span class="string">"cuda:1"</span>) </span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">3</span>, dtype=torch.float64, device=device)</span><br><span class="line">tensor([[<span class="number">-0.6344</span>,  <span class="number">0.8562</span>, <span class="number">-1.2758</span>],</span><br><span class="line">        [ <span class="number">0.8414</span>,  <span class="number">1.7962</span>,  <span class="number">1.0589</span>],</span><br><span class="line">        [<span class="number">-0.1369</span>, <span class="number">-1.0462</span>, <span class="number">-0.4373</span>]], dtype=torch.float64, device=<span class="string">'cuda:1'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad  <span class="comment"># default is False</span></span><br><span class="line"><span class="literal">False</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">3</span>, requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</span><br><span class="line"><span class="literal">True</span></span><br></pre></td></tr></table></figure>
<h3 id="torch-tensor-data"><a href="#torch-tensor-data" class="headerlink" title="torch.tensor(data, ...)"></a><code>torch.tensor(data, ...)</code></h3><p><code>torch.tensor</code>是新加入的<code>Tesnor</code>构建函数。它接受一个”array-like”的参数，并将其value copy到一个新的<code>Tensor</code>中。可以将它看做<code>numpy.array</code>的等价物。不同于<code>torch.*Tensor</code>方法，你可以创建0D的Tensor（也就是scalar）。此外，如果<code>dtype</code>参数没有给出，它会自动推断。推荐使用这个函数从已有的data，如Python List创建<code>Tensor</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>cuda = torch.device(<span class="string">"cuda"</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.half, device=cuda)</span><br><span class="line">tensor([[ <span class="number">1</span>],</span><br><span class="line">        [ <span class="number">2</span>],</span><br><span class="line">        [ <span class="number">3</span>]], device=<span class="string">'cuda:0'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">1</span>)               <span class="comment"># scalar</span></span><br><span class="line">tensor(<span class="number">1</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2.3</span>]).dtype  <span class="comment"># type inferece</span></span><br><span class="line">torch.float32</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).dtype    <span class="comment"># type inferece</span></span><br><span class="line">torch.int64</span><br></pre></td></tr></table></figure>
<p>我们还加了更多的<code>Tensor</code>创建方法。其中有一些<code>torch.*_like</code>，<code>tensor.new_*</code>这样的形式。</p>
<ul>
<li><p><code>torch.*_like</code>的参数是一个input tensor， 它返回一个相同属性的tensor，除非有特殊指定。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x)</span><br><span class="line">tensor([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x, dtype=torch.int)</span><br><span class="line">tensor([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>], dtype=torch.int32)</span><br></pre></td></tr></table></figure>
</li>
<li><p><code>tensor.new_*</code>类似，不过它通常需要接受一个指定shape的参数。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">2</span>)</span><br><span class="line">tensor([ <span class="number">1.</span>,  <span class="number">1.</span>], dtype=torch.float64)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">4</span>, dtype=torch.int)</span><br><span class="line">tensor([ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>], dtype=torch.int32)</span><br></pre></td></tr></table></figure>
</li>
</ul>
<p>为了指定shape参数，你可以使用<code>tuple</code>，如<code>torch.zeros((2, 3))</code>（Numpy风格）或者可变数量参数<code>torch.zeros(2, 3)</code>（以前的版本只支持这种）。</p>
<table>
   <tr>
      <td>Name</td>
      <td>Returned Tensor</td>
      <td>torch.*_likevariant</td>
      <td>tensor.new_*variant</td>
   </tr>
   <tr>
      <td>torch.empty</td>
      <td>unintialized memory</td>
      <td>✔</td>
      <td>✔</td>
   </tr>
   <tr>
      <td>torch.zeros</td>
      <td>all zeros</td>
      <td>✔</td>
      <td>✔</td>
   </tr>
   <tr>
      <td>torch.ones</td>
      <td>all ones</td>
      <td>✔</td>
      <td>✔</td>
   </tr>
   <tr>
      <td>torch.full</td>
      <td>filled with a given value</td>
      <td>✔</td>
      <td>✔</td>
   </tr>
   <tr>
      <td>torch.rand</td>
      <td>i.i.d. continuous Uniform[0, 1)</td>
      <td>✔</td>
      <td></td>
   </tr>
   <tr>
      <td>torch.randn</td>
      <td>i.i.d. Normal(0, 1)</td>
      <td>✔</td>
      <td></td>
   </tr>
   <tr>
      <td>torch.randint</td>
      <td>i.i.d. discrete Uniform in given range</td>
      <td>✔</td>
      <td></td>
   </tr>
   <tr>
      <td>torch.randperm</td>
      <td>random permutation of {0, 1, ..., n - 1}</td>
      <td></td>
      <td></td>
   </tr>
   <tr>
      <td>torch.tensor</td>
      <td>copied from existing data (list, NumPy ndarray, etc.)</td>
      <td></td>
      <td>✔</td>
   </tr>
   <tr>
      <td>torch.from_numpy*</td>
      <td>from NumPy ndarray (sharing storage without copying)</td>
      <td></td>
      <td></td>
   </tr>
   <tr>
      <td>torch.arange, torch.range and torch.linspace</td>
      <td>uniformly spaced values in a given range</td>
      <td></td>
      <td></td>
   </tr>
   <tr>
      <td>torch.logspace</td>
      <td>logarithmically spaced values in a given range</td>
      <td></td>
      <td></td>
   </tr>
   <tr>
      <td>torch.eye</td>
      <td>identity matrix</td>
      <td></td>
      <td></td>
   </tr>
</table>

<p>注：<code>torch.from_numpy</code>只接受NumPy <code>ndarray</code>作为输入参数。</p>
<h2 id="书写设备无关代码（device-agnostic-code）"><a href="#书写设备无关代码（device-agnostic-code）" class="headerlink" title="书写设备无关代码（device-agnostic code）"></a>书写设备无关代码（device-agnostic code）</h2><p>以前版本很难写设备无关代码。我们使用两种方法使其变得简单：</p>
<ul>
<li><code>Tensor</code>的<code>device</code>属性可以给出其<code>torch.device</code>（<code>get_device</code>只能获取CUDA tensor）</li>
<li>使用<code>x.to()</code>方法，可以很容易将<code>Tensor</code>或者<code>Module</code>在devices间移动（而不用调用<code>x.cpu()</code>或者<code>x.cuda()</code>。</li>
</ul>
<p>推荐使用下面的模式：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 在脚本开始的地方，指定device</span></span><br><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 一些代码</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 当你想创建新的Tensor或者Module时候，使用下面的方法</span></span><br><span class="line"><span class="comment"># 如果已经在相应的device上了，将不会发生copy</span></span><br><span class="line">input = data.to(device)</span><br><span class="line">model = MyModule(...).to(device)</span><br></pre></td></tr></table></figure>
<h2 id="在nn-Module中对于submodule，parameter和buffer名字新的约束"><a href="#在nn-Module中对于submodule，parameter和buffer名字新的约束" class="headerlink" title="在nn.Module中对于submodule，parameter和buffer名字新的约束"></a>在<code>nn.Module</code>中对于submodule，parameter和buffer名字新的约束</h2><p>当使用<code>module.add_module(name, value)</code>, <code>module.add_parameter(name, value)</code> 或者 <code>module.add_buffer(name, value)</code>时候不要使用空字符串或者包含<code>.</code>的字符串，可能会导致<code>state_dict</code>中的数据丢失。如果你在load这样的<code>state_dict</code>，注意打补丁，并且应该更新代码，规避这个问题。</p>
<h2 id="一个具体的例子"><a href="#一个具体的例子" class="headerlink" title="一个具体的例子"></a>一个具体的例子</h2><p>下面是一个code snippet，展示了从0.3.1跨越到0.4.0的不同。</p>
<h3 id="0-3-1-version"><a href="#0-3-1-version" class="headerlink" title="0.3.1 version"></a>0.3.1 version</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">model = MyRNN()</span><br><span class="line"><span class="keyword">if</span> use_cuda:</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</span><br><span class="line">    input, target = Variable(input), Variable(target)</span><br><span class="line">    hidden = Variable(torch.zeros(*h_shape))  <span class="comment"># init hidden</span></span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        input, target, hidden = input.cuda(), target.cuda(), hidden.cuda()</span><br><span class="line">    ...  <span class="comment"># get loss and optimize</span></span><br><span class="line">    total_loss += loss.data[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</span><br><span class="line">    input = Variable(input, volatile=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">if</span> use_cuda:</span><br><span class="line">        ...</span><br><span class="line">    ...</span><br></pre></td></tr></table></figure>
<h3 id="0-4-0-version"><a href="#0-4-0-version" class="headerlink" title="0.4.0 version"></a>0.4.0 version</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># torch.device object used throughout this script</span></span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line"></span><br><span class="line">model = MyRNN().to(device)</span><br><span class="line"></span><br><span class="line"><span class="comment"># train</span></span><br><span class="line">total_loss = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</span><br><span class="line">    input, target = input.to(device), target.to(device)</span><br><span class="line">    hidden = input.new_zeros(*h_shape)  <span class="comment"># has the same device &amp; dtype as `input`</span></span><br><span class="line">    ...  <span class="comment"># get loss and optimize</span></span><br><span class="line">    total_loss += loss.item()           <span class="comment"># get Python number from 1-element Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># evaluate</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():                   <span class="comment"># operations inside don't track history</span></span><br><span class="line">    <span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</span><br><span class="line">        ...</span><br></pre></td></tr></table></figure>
<h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul>
<li><a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0" target="_blank" rel="noopener">Release Note</a></li>
<li><a href="http://pytorch.org/docs/stable/index.html" target="_blank" rel="noopener">Documentation</a></li>
</ul>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>JupyterNotebook设置Python环境</title>
    <url>/2018/04/09/set-env-in-jupyternotebook/</url>
    <content><![CDATA[<p>使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。<br><a id="more"></a></p>
<h2 id="conda的虚拟环境"><a href="#conda的虚拟环境" class="headerlink" title="conda的虚拟环境"></a>conda的虚拟环境</h2><p>在Anaconda中，我们可以使用<code>conda create -n your_env_name python=your_python_version</code>的方法创建虚拟环境，并使用<code>source activate your_env_name</code>方式激活该虚拟环境，并在其中安装与默认（主）python环境不同的软件包等。</p>
<p>当激活该虚拟环境时，ipython下是可以正常加载的。但是打开Jupyter Notebook，会发现其加载的仍然是默认的Python kernel，而我们需要在notebook中也能使用新添加的虚拟环境。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>解决方法见这个帖子：<a href="https://stackoverflow.com/questions/39604271/conda-environments-not-showing-up-in-jupyter-notebook" target="_blank" rel="noopener">Conda environments not showing up in Jupyter Notebook</a>.</p>
<p>首先，安装<code>nb_conda_kernels</code>包：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">conda install nb_conda_kernels</span><br></pre></td></tr></table></figure></p>
<p>然后，打开Notebook，点击<code>New</code>，会出现当前所有安装的虚拟环境以供选择，如下所示。<br><img src="/img/set-env-in-notebook-choose-kernel.png" alt="选择特定的kernel加载"></p>
<p>如果是已经编辑过的notebook，只需要打开该笔记本，在菜单栏中选择<code>Kernel -&gt; choose kernel -&gt; your env kernel</code>即可。<br><img src="/img/set-env-in-notebook-change-kernel.png" alt="改变当前notebook的kernel"></p>
<p>关于<code>nb_conda_kernels</code>的详细信息，可以参考其GitHub页面：<a href="https://github.com/Anaconda-Platform/nb_conda_kernels" target="_blank" rel="noopener">nb_conda_kernels</a>。</p>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>数值优化之牛顿方法</title>
    <url>/2018/04/03/newton-method/</url>
    <content><![CDATA[<p>简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于<a href="https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95" target="_blank" rel="noopener">Wiki页面</a>。<br><img src="/img/newton-method-demo.gif" width="400" height="300" alt="牛顿法动图" align="center"><br><a id="more"></a></p>
<h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>牛顿方法，是一种用来求解方程$f(x) = 0$的根的方法。从题图可以看出它是如何使用的。</p>
<p>首先，需要给定根的初始值$x_0$。接下来，在函数曲线上找到其所对应的点$(x_0, f(x_0))$，并过该点做切线交$x$轴于一点$x_1$。从$x_1$出发，重复上述操作，直至收敛。</p>
<p>根据图上的几何关系和导数的几何意义，有：</p>
<script type="math/tex; mode=display">x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}</script><h2 id="优化上的应用"><a href="#优化上的应用" class="headerlink" title="优化上的应用"></a>优化上的应用</h2><p>做优化的时候，我们常常需要的是求解某个损失函数$L$的极值。在极值点处，函数的导数为$0$。所以这个问题被转换为了求解$L$的导数的零点。我们有</p>
<script type="math/tex; mode=display">\theta_{n+1} = \theta_n - \frac{L^\prime(\theta_n)}{L^{\prime\prime}(\theta_n)}</script><h2 id="推广到向量形式"><a href="#推广到向量形式" class="headerlink" title="推广到向量形式"></a>推广到向量形式</h2><p>机器学习中的优化问题常常是在高维空间进行，可以将其推广到向量形式：</p>
<script type="math/tex; mode=display">\theta_{n+1} = \theta_n - H^{-1}\nabla_\theta L(\theta_n)</script><p>其中，$H$表示海森矩阵，是一个$n\times n$的矩阵，其中元素为：</p>
<script type="math/tex; mode=display">H_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}</script><p>特别地，当海森矩阵为正定时，此时的极值为极小值（可以使用二阶的泰勒展开式证明）。</p>
<p>PS:忘了什么是正定矩阵了吗？想想二次型的概念，对于$\forall x$不为$0$向量，都有$x^THx &gt; 0$。</p>
<h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>牛顿方法的收敛速度较SGD为快（二阶收敛），但是会涉及到求解一个$n\times n$的海森矩阵的逆，所以虽然需要的迭代次数更少，但反而可能比较耗时（$n$的大小）。</p>
<h2 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h2><p>由于牛顿方法中需要计算海森矩阵的逆，所以很多时候并不实用。大家就想出了一些近似计算$H^{-1}$的方法，如L-BFGS等。</p>
<p><em>推导过程待续。。。</em></p>
<p>L-BFGS的资料网上还是比较多的，这里有一个PyTorch中L-BFGS方法的实现：<a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/lbfgs.py" target="_blank" rel="noopener">optim.lbfgs</a>。</p>
<p>这里有一篇不错的文章<a href="http://www.hankcs.com/ml/l-bfgs.html" target="_blank" rel="noopener">数值优化：理解L-BFGS算法</a>，本博客写作过程参考很多。</p>
]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Feature Pyramid Networks for Object Detection (FPN)</title>
    <url>/2018/04/02/paper-fpn/</url>
    <content><![CDATA[<p>图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取<a href="https://xmfbit.github.io/2017/01/30/cs131-sift/">SIFT特征</a>就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。<br><a id="more"></a></p>
<h2 id="Pyramid-or-not-It’s-a-question"><a href="#Pyramid-or-not-It’s-a-question" class="headerlink" title="Pyramid or not? It’s a question."></a>Pyramid or not? It’s a question.</h2><p>在DL席卷CV之前，特征大多需要研究人员手工设计，如SIFT/Harr/HoG等。人们在使用这些特征的时候发现，往往需要使用图像金字塔，在multi scale下进行检测，才能得到不错的结果。然而，使用CNN时，由于其本身具有的一定的尺度不变性，大家常常是只在单一scale下（也就是原始图像作为输入），就可以达到不错的结果。不过很多时候参加COCO等竞赛的队伍还是会在TEST的时候使用这项技术，能够取得更好的成绩。但是这样会造成计算时间的巨大开销，TRAIN和TEST的不一致。TRAIN中引入金字塔，内存就会吃紧。所以主流的Fast/Faster RCNN并没有使用金字塔。</p>
<p>换个角度，我们知道在CNN中，输入会逐层处理，经过Conv/Pooling的操作后，不同深度的layer产生的feature map的spatial dimension是不一样的，这就是作者在摘要中提到的“inherent multi-scale pyramidal hierarchy of deep CNN”。不过，还有一个问题，就是深层和浅层的feature map虽然构成了一个feature pyramid，但是它们的语义并不对等：深层layer的feature map有更抽象的语义信息，而浅层feature map有较高的resolution，但是语义信息还是too yong too simple。</p>
<p>SSD做过这方面的探索。但是它采用的方法是从浅层layer引出，又加了一些layer，导致无法reuse high resolution的feature map。我们发现，浅层的high resolution feature map对检测小目标很有用处。</p>
<p>那我们想要怎样呢？</p>
<ul>
<li>高层的low resolution，strong semantic info特征如何和浅层的high resolution，weak semantic info自然地结合？</li>
<li>不引入过多的额外计算，最好也只需要用single scale的原始输入。</li>
</ul>
<p>用一张图总结一下。下图中蓝色的轮廓线框起来的就是不同layer输出的feature map。蓝色线越粗，代表其语义信息越强。在（a）中，是将图像做成金字塔，分别跑一个NN来做，这样计算量极大。（b）中是目前Faster RCNN等采用的方法，只在single scale上做。（c）中是直接将各个layer输出的层级feature map自然地看做feature pyramid来做。（d）是本文的方法，不同层级的feature map做了merge，能够使得每个level的语义信息都比较强（注意看蓝色线的粗细）。<br><img src="/img/paper-fpn-different-pyramids.png" alt="不同金字塔方法"></p>
<p>我们使用这种名为FPN的技术，不用什么工程上的小花招，就打败了目前COCO上的最好结果。不止detection，FPN也能用在图像分割上（当然，现在我们知道，MaskRCNN中的关键技术之一就是FPN）。</p>
<h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>有人可能会想，其实前面的网络有人也做过不同深度layer的merge啊，通过skip connection就可以了。作者指出，那种方法仍然是只能在最终的single scale的output feature map上做，而我们的方法是在all level上完成，如下图所示。<br><img src="/img/paper-fpn-different-with-related-work.png" alt="我们才是真正的金字塔"></p>
<h3 id="Bottom-up-pathway"><a href="#Bottom-up-pathway" class="headerlink" title="Bottom-up pathway"></a>Bottom-up pathway</h3><p> Bottom-up pathway指的是网络的前向计算部分，会产生一系列scale相差2x的feature map。当然，在这些downsample中间，还会有layer的输出spatial dimension是一致的。那些连续的有着相同spatial dimension输出的layer是一个stage。这样，我们就完成了传统金字塔方法和CNN网络的名词的对应。</p>
<p> 以ResNet为例，我们用每个stage中最后一个residual block的输出作为构建金字塔的feature map，也就是<code>C2~C5</code>。它们的步长分别是$4, 8, 16, 32$。我们没用<code>conv1</code>。</p>
<h3 id="Top-down-pathway和lateral-connection"><a href="#Top-down-pathway和lateral-connection" class="headerlink" title="Top-down pathway和lateral connection"></a>Top-down pathway和lateral connection</h3><p>Top-down pathway是指将深层的有更强语义信息的feature经过upsampling变成higher resolution的过程。然后再与bottom-up得到的feature经过lateral connection（侧边连接）进行增强。</p>
<p>下面这张图展示了做lateral connection的过程。注意在图中，越深的layer位于图的上部。我们以框出来放大的那部分举例子。从更深的层输出的feature经过<code>2x up</code>处理（spatial dimension一致了），从左面来的浅层的feature经过<code>1x1 conv</code>处理（channel dimension一致了），再进行element-wise的相加，得到了该stage最后用于prediction的feature（其实还要经过一个<code>3x3 conv</code>的处理，见下引文）。<br><img src="/img/paper-fpn-lateral-connection.png" alt="lateral connection"></p>
<p>一些细节，直接引用：</p>
<blockquote>
<p>To start the iteration, we simply attach a 1x1 convolutional layer on C5 to produce the coarsest resolution map. Finally, we append a 3x3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling.</p>
</blockquote>
<p>此外，由于金字塔上的所有feature共享classifier和regressor，要求它们的channel dimension必须一致。本文固定使用$256$。而且这些外的conv layer没有使用非线性激活。</p>
<p>这里给出一个基于PyTorch的FPN的第三方实现<a href="https://github.com/kuangliu/pytorch-fpn/blob/master/fpn.py" target="_blank" rel="noopener">kuangliu/pytorch-fpn</a>，可以对照论文捋一遍。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## ResNet的block</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    expansion = <span class="number">4</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, planes, stride=<span class="number">1</span>)</span>:</span></span><br><span class="line">        super(Bottleneck, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=<span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(self.expansion*planes)</span><br><span class="line"></span><br><span class="line">        self.shortcut = nn.Sequential()</span><br><span class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_planes != self.expansion*planes:</span><br><span class="line">            self.shortcut = nn.Sequential(</span><br><span class="line">                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                nn.BatchNorm2d(self.expansion*planes)</span><br><span class="line">            )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        out = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        out = F.relu(self.bn2(self.conv2(out)))</span><br><span class="line">        out = self.bn3(self.conv3(out))</span><br><span class="line">        out += self.shortcut(x)</span><br><span class="line">        out = F.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FPN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, num_blocks)</span>:</span></span><br><span class="line">        super(FPN, self).__init__()</span><br><span class="line">        self.in_planes = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Bottom-up layers, backbone of the network</span></span><br><span class="line">        self.layer1 = self._make_layer(block,  <span class="number">64</span>, num_blocks[<span class="number">0</span>], stride=<span class="number">1</span>)</span><br><span class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, num_blocks[<span class="number">1</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, num_blocks[<span class="number">2</span>], stride=<span class="number">2</span>)</span><br><span class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, num_blocks[<span class="number">3</span>], stride=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Top layer</span></span><br><span class="line">        <span class="comment"># 我们需要在C5后面接一个1x1, 256 conv，得到金字塔最顶端的feature</span></span><br><span class="line">        self.toplayer = nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)  <span class="comment"># Reduce channels</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Smooth layers</span></span><br><span class="line">        <span class="comment"># 这个是上面引文中提到的抗aliasing的3x3卷积</span></span><br><span class="line">        self.smooth1 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.smooth2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.smooth3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Lateral layers</span></span><br><span class="line">        <span class="comment"># 为了匹配channel dimension引入的1x1卷积</span></span><br><span class="line">        <span class="comment"># 注意这些backbone之外的extra conv，输出都是256 channel</span></span><br><span class="line">        self.latlayer1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.latlayer2 = nn.Conv2d( <span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line">        self.latlayer3 = nn.Conv2d( <span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, planes, num_blocks, stride)</span>:</span></span><br><span class="line">        strides = [stride] + [<span class="number">1</span>]*(num_blocks<span class="number">-1</span>)</span><br><span class="line">        layers = []</span><br><span class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</span><br><span class="line">            layers.append(block(self.in_planes, planes, stride))</span><br><span class="line">            self.in_planes = planes * block.expansion</span><br><span class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    <span class="comment">## FPN的lateral connection部分: upsample以后，element-wise相加</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_upsample_add</span><span class="params">(self, x, y)</span>:</span></span><br><span class="line">        <span class="string">'''Upsample and add two feature maps.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">          x: (Variable) top feature map to be upsampled.</span></span><br><span class="line"><span class="string">          y: (Variable) lateral feature map.</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">          (Variable) added feature map.</span></span><br><span class="line"><span class="string">        Note in PyTorch, when input size is odd, the upsampled feature map</span></span><br><span class="line"><span class="string">        with `F.upsample(..., scale_factor=2, mode='nearest')`</span></span><br><span class="line"><span class="string">        maybe not equal to the lateral feature map size.</span></span><br><span class="line"><span class="string">        e.g.</span></span><br><span class="line"><span class="string">        original input size: [N,_,15,15] -&gt;</span></span><br><span class="line"><span class="string">        conv2d feature map size: [N,_,8,8] -&gt;</span></span><br><span class="line"><span class="string">        upsampled feature map size: [N,_,16,16]</span></span><br><span class="line"><span class="string">        So we choose bilinear upsample which supports arbitrary output sizes.</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        _,_,H,W = y.size()</span><br><span class="line">        <span class="keyword">return</span> F.upsample(x, size=(H,W), mode=<span class="string">'bilinear'</span>) + y</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># Bottom-up</span></span><br><span class="line">        c1 = F.relu(self.bn1(self.conv1(x)))</span><br><span class="line">        c1 = F.max_pool2d(c1, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</span><br><span class="line">        c2 = self.layer1(c1)</span><br><span class="line">        c3 = self.layer2(c2)</span><br><span class="line">        c4 = self.layer3(c3)</span><br><span class="line">        c5 = self.layer4(c4)</span><br><span class="line">        <span class="comment"># Top-down</span></span><br><span class="line">        <span class="comment"># P5: 金字塔最顶上的feature</span></span><br><span class="line">        p5 = self.toplayer(c5)</span><br><span class="line">        <span class="comment"># P4: 上一层 p5 + 侧边来的 c4</span></span><br><span class="line">        <span class="comment"># 其余同理</span></span><br><span class="line">        p4 = self._upsample_add(p5, self.latlayer1(c4))</span><br><span class="line">        p3 = self._upsample_add(p4, self.latlayer2(c3))</span><br><span class="line">        p2 = self._upsample_add(p3, self.latlayer3(c2))</span><br><span class="line">        <span class="comment"># Smooth</span></span><br><span class="line">        <span class="comment"># 输出做一下smooth</span></span><br><span class="line">        p4 = self.smooth1(p4)</span><br><span class="line">        p3 = self.smooth2(p3)</span><br><span class="line">        p2 = self.smooth3(p2)</span><br><span class="line">        <span class="keyword">return</span> p2, p3, p4, p5</span><br></pre></td></tr></table></figure></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>下面作者会把FPN应用到FasterRCNN的两个重要步骤：RPN和Fast RCNN。</p>
<h3 id="FPN加持的RPN"><a href="#FPN加持的RPN" class="headerlink" title="FPN加持的RPN"></a>FPN加持的RPN</h3><p>在Faster RCNN中，RPN用来提供ROI的proposal。backbone网络输出的single feature map上接了$3\times 3$大小的卷积核来实现sliding window的功能，后面接两个$1\times 1$的卷积分别用来做objectness的分类和bounding box基于anchor box的回归。我们把最后的classifier和regressor部分叫做head。</p>
<p>使用FPN时，我们在金字塔每层的输出feature map上都接上这样的head结构（$3\times 3$的卷积 + two sibling $1\times 1$的卷积）。同时，我们不再使用多尺度的anchor box，而是在每个level上分别使用不同大小的anchor box。具体说，对应于特征金字塔的$5$个level的特征，<code>P2 - P6</code>，anchor box的大小分别是$32^2, 64^2, 128^2, 256^2, 512^2$。不过每层的anchor box仍然要照顾到不同的长宽比例，我们使用了$3$个不同的比例：$1:2, 1:1, 2:1$（和原来一样）。这样，我们一共有$5\times 3 = 15$个anchor box。</p>
<p>训练过程中，我们需要给anchor boxes赋上对应的正负标签。对于那些与ground truth有最大IoU或者与任意一个ground truth的IoU超过$0.7$的anchor boxes，是positive label；那些与所有ground truth的IoU都小于$0.3$的是negtive label。</p>
<p>有一个疑问是head的参数是否要在不同的level上共享。我们试验了共享与不共享两个方法，accuracy是相近的。这也说明不同level之间语义信息是相似的，只是resolution不同。</p>
<h3 id="FPN加持的Fast-RCNN"><a href="#FPN加持的Fast-RCNN" class="headerlink" title="FPN加持的Fast RCNN"></a>FPN加持的Fast RCNN</h3><p>Fast RCNN的原始方法是只在single scale的feature map上做的，要想使用FPN，首先应该解决的问题是前端提供的ROI proposal应该对应到pyramid的哪一个label。由于我们的网络基本都是在ImageNet训练的网络上做transfer learning得到的，我们就以base model在ImageNet上训练时候的输入$224\times 224$作为参考，依据当前ROI和它的大小比例，确定该把这个ROI对应到哪个level。如下所示：</p>
<script type="math/tex; mode=display">k = \lfloor k_0 + \log_2(\sqrt{wh}/224)\rfloor</script><p>后面接的predictor head我们这里直接连了两个$1024d$的fc layer，再接final classification和regression的部分。同样的，这些参数对于不同level来说是共享的。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>detection</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - YOLO v3</title>
    <url>/2018/04/01/paper-yolov3/</url>
    <content><![CDATA[<p>YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">YOLOv3: An Incremental Improvement</a>。下面这张图是YOLO V3与RetinaNet的比较。<br><img src="/img/paper-yolov3-comparison-retinanet.png" alt="YOLO v3和RetinaNet的比较"></p>
<p>可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。<br><a id="more"></a></p>
<h2 id="YOLO-v3比你们不知道高到哪里去了"><a href="#YOLO-v3比你们不知道高到哪里去了" class="headerlink" title="YOLO v3比你们不知道高到哪里去了"></a>YOLO v3比你们不知道高到哪里去了</h2><p>YOLO v3在保持其一贯的检测速度快的特点前提下，性能又有了提升：输入图像为$320\times 320$大小的图像，可以在$22$ms跑完，mAP达到了$28.2$，这个数据和SSD相同，但是快了$3$倍。在TitanX上，YOLO v3可以在$51$ms内完成，$AP_{50}$的值为$57.9$。而RetinaNet需要$198$ms，$AP_{50}$近似却略低，为$57.5$。</p>
<h3 id="ps：啥是AP"><a href="#ps：啥是AP" class="headerlink" title="ps：啥是AP"></a>ps：啥是AP</h3><p>AP就是average precision啦。在detection中，我们认为当预测的bounding box和ground truth的IoU大于某个阈值（如取为$0.5$）时，认为是一个True Positive。如果小于这个阈值，就是一个False Positive。</p>
<p>所谓precision，就是指检测出的框框中有多少是True Positive。另外，还有一个指标叫做recall，是指所有的ground truth里面，有多少被检测出来了。这两个概念都是来自于classification问题，通过设定上面IoU的阈值，就可以迁移到detection中了。</p>
<p>我们可以取不同的阈值，这样就可以绘出一条precisio vs recall的曲线，计算曲线下的面积，就是AP值。COCO中使用了<code>0.5:0.05:0.95</code>十个离散点近似计算（参考<a href="http://cocodataset.org/#detections-eval" target="_blank" rel="noopener">COCO的说明文档网页</a>）。detection中常常需要同时检测图像中多个类别的物体，我们将不同类别的AP求平均，就是mAP。</p>
<p>如果我们只看某个固定的阈值，如$0.5$，计算所有类别的平均AP，那么就用$AP_{50}$来表示。所以YOLO v3单拿出来$AP_{50}$说事，是为了证明虽然我的bounding box不如你RetinaNet那么精准（IoU相对较小），但是如果你对框框的位置不是那么敏感（$0.5$的阈值很多时候够用了），那么我是可以做到比你更好更快的。</p>
<h2 id="Bounding-Box位置的回归"><a href="#Bounding-Box位置的回归" class="headerlink" title="Bounding Box位置的回归"></a>Bounding Box位置的回归</h2><p>这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的$p_w$和$p_h$）。网络预测四个值：$t_x$，$t_y$，$t_w$，$t_h$。我们知道，YOLO网络最后输出是一个$M\times M$的feature map，对应于$M \times M$个cell。如果某个cell距离image的top left corner距离为$(c_x, c_y)$（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为：</p>
<script type="math/tex; mode=display">\begin{aligned}b_x &= \sigma(t_x) + c_x\\ b_y &= \sigma(t_y) + c_y\\ b_w &= p_w e^{t_w}\\ b_h &= p_h e^{t_h}\end{aligned}</script><p>PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$b_w = p_w t_w^\prime$），而是要做一个对数变换，实际预测的是$\log(\cdot)$。这里小小解释一下。</p>
<p>这是因为如果不做变换，直接预测相对形变$t_w^\prime$，那么要求$t_w^\prime &gt; 0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。</p>
<p><img src="/img/paper=yolov3-bbox-regression.png" alt="bounding box的回归"></p>
<p>在训练的时候，使用平方误差损失。</p>
<p>另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给$1$；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取$0.5$），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有倍assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。</p>
<h2 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h2><p>我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如<code>Woman</code>和<code>Person</code>这样的重叠标签。</p>
<h2 id="FPN加持的多尺度预测"><a href="#FPN加持的多尺度预测" class="headerlink" title="FPN加持的多尺度预测"></a>FPN加持的多尺度预测</h2><p>之前YOLO的一个弱点就是缺少多尺度变换，使用<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">FPN</a>中的思路，v3在$3$个不同的尺度上做预测。在COCO上，我们每个尺度都预测$3$个框框，所以一共是$9$个。所以输出的feature map的大小是$N\times N\times [3\times (4+1+80)]$。</p>
<p>然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了$2$倍。</p>
<p>照上一段所说方法，再一次在final scale尺度下给出预测。</p>
<h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在v3中，作者新建了一个名为<code>yolo</code>的layer，其参数如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[yolo]</span><br><span class="line">mask = 0,1,2</span><br><span class="line">## 9组anchor对应9个框框</span><br><span class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</span><br><span class="line">classes=20   ## VOC20类</span><br><span class="line">num=9</span><br><span class="line">jitter=.3</span><br><span class="line">ignore_thresh = .5</span><br><span class="line">truth_thresh = 1</span><br><span class="line">random=1</span><br></pre></td></tr></table></figure></p>
<p>打开<code>yolo_layer.c</code>文件，找到<code>forward</code><a href>部分代码</a>。可以看到，首先，对输入进行activation。注意，如论文所说，对类别进行预测的时候，没有使用v2中的softmax或softmax tree，而是直接使用了logistic变换。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</span><br><span class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.n; ++n)&#123;</span><br><span class="line">        <span class="keyword">int</span> index = entry_index(l, b, n*l.w*l.h, <span class="number">0</span>);</span><br><span class="line">        <span class="comment">// 对 tx, ty进行logistic变换</span></span><br><span class="line">        activate_array(l.output + index, <span class="number">2</span>*l.w*l.h, LOGISTIC);</span><br><span class="line">        index = entry_index(l, b, n*l.w*l.h, <span class="number">4</span>);</span><br><span class="line">        <span class="comment">// 对confidence和C类进行logistic变换</span></span><br><span class="line">        activate_array(l.output + index, (<span class="number">1</span>+l.classes)*l.w*l.h, LOGISTIC);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们看一下如何计算梯度。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; l.h; ++j) &#123;</span><br><span class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; l.w; ++i) &#123;</span><br><span class="line">        <span class="keyword">for</span> (n = <span class="number">0</span>; n &lt; l.n; ++n) &#123;</span><br><span class="line">            <span class="comment">// 对每个预测的bounding box</span></span><br><span class="line">            <span class="comment">// 找到与其IoU最大的ground truth</span></span><br><span class="line">            <span class="keyword">int</span> box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</span><br><span class="line">            box pred = get_yolo_box(l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.w*l.h);</span><br><span class="line">            <span class="keyword">float</span> best_iou = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">int</span> <span class="keyword">best_t</span> = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</span><br><span class="line">                box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</span><br><span class="line">                <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</span><br><span class="line">                <span class="keyword">float</span> iou = box_iou(pred, truth);</span><br><span class="line">                <span class="keyword">if</span> (iou &gt; best_iou) &#123;</span><br><span class="line">                    best_iou = iou;</span><br><span class="line">                    <span class="keyword">best_t</span> = t;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">int</span> obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</span><br><span class="line">            avg_anyobj += l.output[obj_index];</span><br><span class="line">            <span class="comment">// 计算梯度</span></span><br><span class="line">            <span class="comment">// 如果大于ignore_thresh, 那么忽略</span></span><br><span class="line">            <span class="comment">// 如果小于ignore_thresh，target = 0</span></span><br><span class="line">            <span class="comment">// diff = -gradient = target - output</span></span><br><span class="line">            <span class="comment">// 为什么是上式，见下面的数学分析</span></span><br><span class="line">            l.delta[obj_index] = <span class="number">0</span> - l.output[obj_index];</span><br><span class="line">            <span class="keyword">if</span> (best_iou &gt; l.ignore_thresh) &#123;</span><br><span class="line">                l.delta[obj_index] = <span class="number">0</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 这里仍然有疑问，为何使用truth_thresh?这个值是1</span></span><br><span class="line">            <span class="comment">// 按道理，iou无论如何不可能大于1啊。。。</span></span><br><span class="line">            <span class="keyword">if</span> (best_iou &gt; l.truth_thresh) &#123;</span><br><span class="line">                <span class="comment">// confidence target = 1</span></span><br><span class="line">                l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</span><br><span class="line">                <span class="keyword">int</span> <span class="class"><span class="keyword">class</span> = <span class="title">net</span>.<span class="title">truth</span>[<span class="title">best_t</span>*(4 + 1) + <span class="title">b</span>*<span class="title">l</span>.<span class="title">truths</span> + 4];</span></span><br><span class="line">                <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="class"><span class="keyword">class</span> = <span class="title">l</span>.<span class="title">map</span>[<span class="title">class</span>];</span></span><br><span class="line">                <span class="keyword">int</span> class_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</span><br><span class="line">                <span class="comment">// 对class进行求导</span></span><br><span class="line">                delta_yolo_class(l.output, l.delta, class_index, class, l.classes, l.w*l.h, <span class="number">0</span>);</span><br><span class="line">                box truth = float_to_box(net.truth + <span class="keyword">best_t</span>*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</span><br><span class="line">                <span class="comment">// 对box位置参数进行求导</span></span><br><span class="line">                delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.delta, (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>我们首先来说一下为何confidence（包括后面的classification）的<code>diff</code>计算为何是<code>target - output</code>的形式。对于logistic regression，假设logistic函数的输入是$o = f(x;\theta)$。其中，$\theta$是网络的参数。那么输出$y = h(o)$，其中$h$指logistic激活函数（或sigmoid函数）。那么，我们有：</p>
<script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h(o)\\ P(y=0|x) &= 1-h(o)\end{aligned}</script><p>写出对数极大似然函数，我们有：</p>
<script type="math/tex; mode=display">\log L = \sum y\log h+(1-y)\log(1-h)</script><p>为了使用SGD，上式两边取相反数，我们有损失函数：</p>
<script type="math/tex; mode=display">J = -\log L = \sum -y\log h-(1-y)\log(1-h)</script><p>对第$i$个输入$o_i$求导，我们有：</p>
<script type="math/tex; mode=display">\begin{aligned}\frac{\partial J}{\partial o_i} &= \frac{\partial J}{\partial h_i}\frac{\partial h_i}{\partial o_i}\\
&= [-y_i/h_i-(y_i-1)/(1-h_i)] \frac{\partial h_i}{\partial o_i} \\
&= \frac{h_i-y_i}{h_i(1-h_i)} \frac{\partial h_i}{\partial o_i}\end{aligned}</script><p>根据logistic函数的求导性质，有：</p>
<script type="math/tex; mode=display">\frac{\partial h_i}{\partial o_i} = h_i(1-h_i)</script><p>所以，有</p>
<script type="math/tex; mode=display">\frac{\partial J}{\partial o_i} = h_i-y_i</script><p>其中，$h_i$即为logistic激活后的输出，$y_i$为target。由于YOLO代码中均使用<code>diff</code>，也就是<code>-gradient</code>，所以有<code>delta = target - output</code>。</p>
<p>关于logistic回归，还可以参考我的博客：<a href="https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/">CS229 简单的监督学习方法</a>。</p>
<p>下面，我们看下两个关键的子函数，<code>delta_yolo_class</code>和<code>delta_yolo_box</code>的实现。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// class是类别的ground truth</span></span><br><span class="line"><span class="comment">// classes是类别总数</span></span><br><span class="line"><span class="comment">// index是feature map一维数组里面class prediction的起始索引</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">delta_yolo_class</span><span class="params">(<span class="keyword">float</span> *output, <span class="keyword">float</span> *delta, <span class="keyword">int</span> index, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> class, <span class="keyword">int</span> classes, <span class="keyword">int</span> stride, <span class="keyword">float</span> *avg_cat)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> n;</span><br><span class="line">    <span class="comment">// 这里暂时不懂</span></span><br><span class="line">    <span class="keyword">if</span> (delta[index])&#123;</span><br><span class="line">        delta[index + stride*<span class="class"><span class="keyword">class</span>] = 1 - <span class="title">output</span>[<span class="title">index</span> + <span class="title">stride</span>*<span class="title">class</span>];</span></span><br><span class="line">        <span class="keyword">if</span>(avg_cat) *avg_cat += output[index + stride*class];</span><br><span class="line">        <span class="keyword">return</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; classes; ++n)&#123;</span><br><span class="line">        <span class="comment">// 见上，diff = target - prediction</span></span><br><span class="line">        delta[index + stride*n] = ((n == class)?<span class="number">1</span> : <span class="number">0</span>) - output[index + stride*n];</span><br><span class="line">        <span class="keyword">if</span>(n == class &amp;&amp; avg_cat) *avg_cat += output[index + stride*n];</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="comment">// box delta这里没什么可说的，就是square error的求导</span></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">delta_yolo_box</span><span class="params">(box truth, <span class="keyword">float</span> *x, <span class="keyword">float</span> *biases, <span class="keyword">int</span> n, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> lw, <span class="keyword">int</span> lh, <span class="keyword">int</span> w, <span class="keyword">int</span> h, </span></span></span><br><span class="line"><span class="function"><span class="params">  <span class="keyword">float</span> *delta, <span class="keyword">float</span> scale, <span class="keyword">int</span> stride)</span> </span>&#123;</span><br><span class="line">    box pred = get_yolo_box(x, biases, n, index, i, j, lw, lh, w, h, stride);</span><br><span class="line">    <span class="keyword">float</span> iou = box_iou(pred, truth);</span><br><span class="line">    <span class="keyword">float</span> tx = (truth.x*lw - i);</span><br><span class="line">    <span class="keyword">float</span> ty = (truth.y*lh - j);</span><br><span class="line">    <span class="keyword">float</span> tw = <span class="built_in">log</span>(truth.w*w / biases[<span class="number">2</span>*n]);</span><br><span class="line">    <span class="keyword">float</span> th = <span class="built_in">log</span>(truth.h*h / biases[<span class="number">2</span>*n + <span class="number">1</span>]);</span><br><span class="line"></span><br><span class="line">    delta[index + <span class="number">0</span>*stride] = scale * (tx - x[index + <span class="number">0</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">1</span>*stride] = scale * (ty - x[index + <span class="number">1</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">2</span>*stride] = scale * (tw - x[index + <span class="number">2</span>*stride]);</span><br><span class="line">    delta[index + <span class="number">3</span>*stride] = scale * (th - x[index + <span class="number">3</span>*stride]);</span><br><span class="line">    <span class="keyword">return</span> iou;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面，我们遍历了每一个prediction的bounding box，下面我们还要遍历每个ground truth，根据IoU，为其分配一个最佳的匹配。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 遍历ground truth</span></span><br><span class="line"><span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</span><br><span class="line">    box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</span><br><span class="line">    <span class="comment">// 找到iou最大的那个bounding box</span></span><br><span class="line">    <span class="keyword">float</span> best_iou = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> best_n = <span class="number">0</span>;</span><br><span class="line">    i = (truth.x * l.w);</span><br><span class="line">    j = (truth.y * l.h);</span><br><span class="line">    box truth_shift = truth;</span><br><span class="line">    truth_shift.x = truth_shift.y = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.total; ++n)&#123;</span><br><span class="line">        box pred = &#123;<span class="number">0</span>&#125;;</span><br><span class="line">        pred.w = l.biases[<span class="number">2</span>*n]/net.w;</span><br><span class="line">        pred.h = l.biases[<span class="number">2</span>*n+<span class="number">1</span>]/net.h;</span><br><span class="line">        <span class="keyword">float</span> iou = box_iou(pred, truth_shift);</span><br><span class="line">        <span class="keyword">if</span> (iou &gt; best_iou)&#123;</span><br><span class="line">            best_iou = iou;</span><br><span class="line">            best_n = n;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">int</span> mask_n = int_index(l.mask, best_n, l.n);</span><br><span class="line">    <span class="keyword">if</span>(mask_n &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        <span class="keyword">int</span> box_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</span><br><span class="line">        <span class="keyword">float</span> iou = delta_yolo_box(truth, l.output, l.biases, best_n, </span><br><span class="line">          box_index, i, j, l.w, l.h, net.w, net.h, l.delta, </span><br><span class="line">          (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</span><br><span class="line">        <span class="keyword">int</span> obj_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</span><br><span class="line">        avg_obj += l.output[obj_index];</span><br><span class="line">        <span class="comment">// 对应objectness target = 1</span></span><br><span class="line">        l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</span><br><span class="line">        <span class="keyword">int</span> <span class="class"><span class="keyword">class</span> = <span class="title">net</span>.<span class="title">truth</span>[<span class="title">t</span>*(4 + 1) + <span class="title">b</span>*<span class="title">l</span>.<span class="title">truths</span> + 4];</span></span><br><span class="line">        <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="class"><span class="keyword">class</span> = <span class="title">l</span>.<span class="title">map</span>[<span class="title">class</span>];</span></span><br><span class="line">        <span class="keyword">int</span> class_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</span><br><span class="line">        delta_yolo_class(l.output, l.delta, class_index, class, l.classes, l.w*l.h, &amp;avg_cat);</span><br><span class="line">        ++count;</span><br><span class="line">        ++class_count;</span><br><span class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.5</span>) recall += <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.75</span>) recall75 += <span class="number">1</span>;</span><br><span class="line">        avg_iou += iou;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="Darknet网络架构"><a href="#Darknet网络架构" class="headerlink" title="Darknet网络架构"></a>Darknet网络架构</h2><p>引入了ResidualNet的思路（$3\times 3$和$1\times 1$的卷积核，shortcut连接），构建了Darknet-53网络。<br><img src="/img/paper-yolov3-darknet53.png" alt="darknet-63"></p>
<h2 id="YOLO的优势和劣势"><a href="#YOLO的优势和劣势" class="headerlink" title="YOLO的优势和劣势"></a>YOLO的优势和劣势</h2><p>把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚：</p>
<blockquote>
<p>Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! “Training humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.” [16] If humans have a hard time telling the difference, how much does it matter?</p>
</blockquote>
<p>使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。</p>
<p>下面是具体的数据比较。<br><img src="/img/paper-yolov3-comparisons.png" alt="具体数据比较"></p>
<h2 id="我们是身经百战，见得多了"><a href="#我们是身经百战，见得多了" class="headerlink" title="我们是身经百战，见得多了"></a>我们是身经百战，见得多了</h2><p>作者还贴心地给出了什么方法没有奏效。</p>
<ul>
<li>anchor box坐标$(x, y)$的预测。预测anchor box的offset，no stable，不好。</li>
<li>线性offset预测，而不是logistic。精度下降。</li>
<li>focal loss。精度下降。</li>
<li>双IoU阈值，像Faster RCNN那样。效果不好。</li>
</ul>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>下面是一些可供利用的参考资料：</p>
<ul>
<li>YOLO的项目主页<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">Darknet YOLO</a></li>
<li>作者主页上的<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="noopener">paper链接</a></li>
<li>知乎专栏上的<a href="https://zhuanlan.zhihu.com/p/34945787" target="_blank" rel="noopener">全文翻译</a></li>
<li>FPN论文<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="noopener">Feature pyramid networks for object detection</a></li>
<li>知乎上的解答：<a href="https://www.zhihu.com/question/41540197" target="_blank" rel="noopener">AP是什么，怎么计算</a></li>
</ul>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>yolo</tag>
        <tag>detection</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
    <url>/2018/03/24/paper-squeezenet/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="noopener">SqueezeNet</a>由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。</p>
<a id="more"></a>
<h2 id="模型压缩相关工作"><a href="#模型压缩相关工作" class="headerlink" title="模型压缩相关工作"></a>模型压缩相关工作</h2><p>模型压缩的好处主要有以下几点：</p>
<ul>
<li>更好的分布式训练。server之间的通信往往限制了分布式训练的提速比例，较少的网络参数能够降低对server间通信需求。</li>
<li>云端向终端的部署，需要更低的带宽，例如手机app更新或无人车的软件包更新。</li>
<li>更易于在FPGA等硬件上部署，因为它们往往都有着非常受限的片上RAM。</li>
</ul>
<p>相关工作主要有两个方向，即模型压缩和模型结构自身探索。</p>
<p>模型压缩方面的工作主要有，使用SVD分解，Deep Compression等。模型结构方面比较有意义的工作是GoogLeNet的Inception module（可在博客内搜索<em>Xception</em>查看Xception的作者是如何受此启发发明Xception结构的）。</p>
<p>本文的作者从网络设计角度出发，提出了名为SqueezeNet的网络结构，使用比AlexNet少$50$倍的参数，在ImageNet上取得了comparable的结果。此外，还探究了CNN的arch是如何影响model size和最终的accuracy的。主要从两个方面进行了探索，分别是<em>CNN microarch</em>和<em>CNN macroarch</em>。前者意为在更小的粒度上，如每一层的layer怎么设计，来考察；后者是在更为宏观的角度，如一个CNN中的不同layer该如何组织来考察。</p>
<p><em>PS: 吐槽：看完之后觉得基本没探索出什么太有用的可以迁移到其他地方的规律。。。只是比较了自己的SqueezeNet在不同参数下的性能，有些标题党之嫌，题目很大，但是里面的内容并不完全是这样。CNN的设计还是实验实验再实验。</em></p>
<h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p>为了简单，下文简称<em>SNet</em>。SNet的基本组成是叫做<em>Fire</em>的module。我们知道，对于一个CONV layer，它的参数数量计算应该是：$K \times K \times M \times N$。其中，$K$是filter的spatial size，$M$和$N$分别是输入feature map和输出activation的channel size。由此，设计SNet时，作者的依据主要是以下几点：</p>
<ul>
<li>把$3\times 3$的卷积替换成$1\times 1$，相当于减小上式中的$K$。</li>
<li>减少$3\times 3$filter对应的输入feature map的channel，相当于减少上式的$M$。</li>
<li>delayed downsample。使得activation的feature map能够足够大，这样对提高accuracy有益。CNN中的downsample主要是通过CONV layer或pooling layer中stride设置大于$1$得到的，作者指出，应将这种操作尽量后移。</li>
</ul>
<blockquote>
<p>Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.</p>
</blockquote>
<h3 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h3><p>Fire Module是SNet的基本组成单元，如下图所示。可以分为两个部分，一个是上面的<em>squeeze</em>部分，是一组$1\times 1$的卷积，用来将输入的channel squeeze到一个较小的值。后面是<em>expand</em>部分，由$1\times 1$和$3\times 3$卷积mix起来。使用$s_{1 x 1}$，$e_{1x1}$和$e_{3x3}$表示squeeze和expand中两种不同卷积的channel数量，令$s_{1x1} &lt; e_{1x1} + e_{3x3}$，用来实现上述策略2.<br><img src="/img/paper-squeezenet-fire-module.png" alt="Fire Module示意"></p>
<p>下面，对照PyTorch实现的SNet代码看下Fire的实现，注意上面说的CONV后面都接了ReLU。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fire</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, squeeze_planes,</span></span></span><br><span class="line"><span class="function"><span class="params">                 expand1x1_planes, expand3x3_planes)</span>:</span></span><br><span class="line">        super(Fire, self).__init__()</span><br><span class="line">        self.inplanes = inplanes</span><br><span class="line">        <span class="comment">## squeeze 部分</span></span><br><span class="line">        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.squeeze_activation = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">## expand 1x1 部分</span></span><br><span class="line">        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,</span><br><span class="line">                                   kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.expand1x1_activation = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment">## expand 3x3部分</span></span><br><span class="line">        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,</span><br><span class="line">                                   kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">        self.expand3x3_activation = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.squeeze_activation(self.squeeze(x))</span><br><span class="line">        <span class="comment">## 将expand 部分1x1和3x3的cat到一起</span></span><br><span class="line">        <span class="keyword">return</span> torch.cat([</span><br><span class="line">            self.expand1x1_activation(self.expand1x1(x)),</span><br><span class="line">            self.expand3x3_activation(self.expand3x3(x))], <span class="number">1</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="SNet"><a href="#SNet" class="headerlink" title="SNet"></a>SNet</h3><p>有了Fire Module这个基础材料，我们就可以搭建SNet了。一个单独的<code>conv1</code> layer，后面接了$8$个连续的Fire Module，最后再接一个<code>conv10</code> layer。此外，在<code>conv1</code>，<code>fire4</code>, <code>fire8</code>和<code>conv10</code>后面各有一个<code>stride=2</code>的MAX Pooling layer。这些pooling的位置相对靠后，是对上述策略$3$的实践。我们还可以在不同的Fire Module中加入ResNet中的bypass结构。这样，形成了下图三种不同的SNet结构。<br><img src="/img/paper-squeezenet-macroarch.png" alt="SNet的三种形式"></p>
<p>一些细节：</p>
<ul>
<li>为了使得$1\times 1$和$3\times 3$的卷积核能够有相同spatial size的输出，$3\times 3$的卷积输入加了<code>padding=1</code>。</li>
<li>在squeeze layer和expand layer中加入了ReLU。</li>
<li>在<code>fire 9</code>后加入了drop ratio为$0.5$的Dropout layer。</li>
<li>受NIN启发，SNet中没有fc层。</li>
<li>更多的细节和训练参数的设置可以参考GitHub上的<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="noopener">官方repo</a>。</li>
</ul>
<p>同样的，我们可以参考PyTorch中的实现。注意下面实现了v1.0和v1.1版本，两者略有不同。v1.1版本参数更少，也能够达到v1.0的精度。</p>
<blockquote>
<p>SqueezeNet v1.1 (in this repo), which requires 2.4x less computation than SqueezeNet v1.0 without diminshing accuracy.</p>
</blockquote>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqueezeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, version=<span class="number">1.0</span>, num_classes=<span class="number">1000</span>)</span>:</span></span><br><span class="line">        super(SqueezeNet, self).__init__()</span><br><span class="line">        <span class="keyword">if</span> version <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1.0</span>, <span class="number">1.1</span>]:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported SqueezeNet version &#123;version&#125;:"</span></span><br><span class="line">                             <span class="string">"1.0 or 1.1 expected"</span>.format(version=version))</span><br><span class="line">        self.num_classes = num_classes</span><br><span class="line">        <span class="keyword">if</span> version == <span class="number">1.0</span>:</span><br><span class="line">            self.features = nn.Sequential(</span><br><span class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">96</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</span><br><span class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</span><br><span class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.features = nn.Sequential(</span><br><span class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</span><br><span class="line">                nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">64</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</span><br><span class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>),</span><br><span class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</span><br><span class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</span><br><span class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># Final convolution is initialized differently form the rest</span></span><br><span class="line">        final_conv = nn.Conv2d(<span class="number">512</span>, self.num_classes, kernel_size=<span class="number">1</span>)</span><br><span class="line">        self.classifier = nn.Sequential(</span><br><span class="line">            nn.Dropout(p=<span class="number">0.5</span>),</span><br><span class="line">            final_conv,</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.AvgPool2d(<span class="number">13</span>, stride=<span class="number">1</span>)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>把SNet和AlexNet分别经过Deep Compression，在ImageNet上测试结果如下。可以看到，未被压缩时，SNet比AlexNet少了$50$倍，accuracy是差不多的。经过压缩，SNet更是可以进一步瘦身成不到$0.5$M，比原始的AlexNet瘦身了$500+$倍。<br><img src="/img/paper-squeezenet-benchmark.png" alt="性能比较"></p>
<p>注意上述结果是使用HanSong的Deep Compression技术（聚类+codebook）得到的。这种方法得到的模型在通用计算平台（CPU/GPU）上的优势并不明显，需要在作者提出的EIE硬件上才能充分发挥其性能。对于线性的量化（直接用量化后的$8$位定点存储模型），<a href="http://lepsucd.com/?page_id=630" target="_blank" rel="noopener">Ristretto</a>实现了SNet的量化，但是有一个点的损失。</p>
<h2 id="Micro-Arch探索"><a href="#Micro-Arch探索" class="headerlink" title="Micro Arch探索"></a>Micro Arch探索</h2><p>所谓CNN的Micro Arch，是指如何确定各层的参数，如filter的个数，kernel size的大小等。在SNet中，主要是filter的个数，即上文提到的$s_{1x1}$，$e_{1x1}$和$e_{3x3}$。这样，$8$个Fire Module就有$24$个超参数，数量太多，我们需要加一些约束，暴露主要矛盾，把问题变简单一点。</p>
<p>我们设定$base_e$是第一个Fire Module的expand layer的filter个数，每隔$freq$个Fire Module，会加上$incr_e$这么多。那么任意一个Fire Module的expand layer filter的个数为$e_i = base_e + (incr_e \times \lfloor \frac{i}{freq}\rfloor)$。</p>
<p>在expand layer，我们有$e_i = e_{i,1x1} + e_{i,3x3}$，设定$pct_{3x3} = e_{i,3x3}/e_i$为$3\times 3$的conv占的比例。</p>
<p>设定$SR = s_{i,1x1} / e_i$，为squeeze和expand filter个数比例。</p>
<h3 id="SR的影响"><a href="#SR的影响" class="headerlink" title="SR的影响"></a>SR的影响</h3><p>$SR$于区间$[0.125, 1]$之间取，accuracy基本随着$SR$增大而提升，同时模型的size也在变大。但$SR$从$0.75$提升到$1.0$，accuracy无提升。publish的SNet使用了$SR=0.125$。<br><img src="/img/paper-squeeze-sr-impact.png" alt="SR"></p>
<h3 id="1X1和3x3的比例pct的影响"><a href="#1X1和3x3的比例pct的影响" class="headerlink" title="1X1和3x3的比例pct的影响"></a>1X1和3x3的比例pct的影响</h3><p>为了减少参数，我们把部分$3\times 3$的卷积换成了$1\times 1$的，构成了expand layer。那么两者的比例对模型的影响？$pct$在$[0.01, 0.99]$之间变化。同样，accuracy和model size基本都随着$pct$增大而提升。当大于$0.5$时，模型的accuracy基本无提升。<br><img src="/img/paper-squeezenet-pct-impact.png" alt="pct"></p>
<h2 id="Macro-Arch探索"><a href="#Macro-Arch探索" class="headerlink" title="Macro Arch探索"></a>Macro Arch探索</h2><p>这里主要讨论了是否使用ResNet中的bypass结构。<br><img src="/img/paper-squeezenet-bypass.png" alt="bypass比较"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - MobileNets, Efficient Convolutional Neural Networks for Mobile Vision Applications</title>
    <url>/2018/03/23/paper-mobilenet/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/abs/1704.04861" target="_blank" rel="noopener">MobileNet</a>是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的<a href="https://arxiv.org/abs/1801.04381" target="_blank" rel="noopener">MobileNet v2</a>以后讨论。<br><a id="more"></a></p>
<h2 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h2><p>Depthwise Separable Conv把卷积操作拆成两个部分。第一部分，depthwise conv时，每个filter只在一个channel上进行操作。第二部分，pointwise conv是使用$1\times 1$的卷积核做channel上的combination。在Caffe等DL框架中，一般是设定卷积层的<code>group</code>参数，使其等于input的channel数来实现depthwise conv的。而pointwise conv和使用标准卷积并无不同，只是需要设置<code>kernel size = 1</code>。如下，是使用PyTorch的一个<a href="https://github.com/marvis/pytorch-mobilenet/blob/master/main.py#L67" target="_blank" rel="noopener">例子</a>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv_dw</span><span class="params">(inp, oup, stride)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> nn.Sequential(</span><br><span class="line">        <span class="comment">## 通过设置group=input channels来实现depthwise conv</span></span><br><span class="line">        nn.Conv2d(inp, inp, <span class="number">3</span>, stride, <span class="number">1</span>, groups=inp, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(inp),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    </span><br><span class="line">        nn.Conv2d(inp, oup, <span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">        nn.BatchNorm2d(oup),</span><br><span class="line">        nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">    )</span><br></pre></td></tr></table></figure>
<p>这样做的好处就是能够大大减少计算量。假设原始conv的filter个数为$N$，kernel size大小为$D_k$，输入的维度为$D_F\times D_F\times M$，那么总的计算量是$D_K\times D_K\times M\times N\times D_F\times D_F$（设定<code>stride=1</code>，即输入输出的feature map在spatial两个维度上相同）。</p>
<p>改成上述Depthwise Separable Conv后，计算量变为两个独立操作之和，即$D_K\times D_K\times M\times D_F \times D_F + M\times N\times D_F\times D_F$，计算量是原来的$\frac{1}{N} + \frac{1}{D_K^2} &lt; 1$。<br><img src="/img/paper-mobilenet-depthwise-separable-conv.png" alt="Depthwise Separable Conv示意图"></p>
<p>在实际使用时，我们在两个卷积操作之间加上BN和非线性变换层，如下图所示：<br><img src="/img/paper-mobilenet-conv-unit.png" alt="Conv-BN-ReLU"></p>
<h2 id="MobileNet"><a href="#MobileNet" class="headerlink" title="MobileNet"></a>MobileNet</h2><p>下图展示了如何使用Depthwise Separable Conv构建MobileNet。表中的<code>dw</code>表示depthwise conv，后面接的<code>stride=1</code>的conv即为pointwise conv。可以看到，网络就是这样的单元堆叠而成的。最后使用了一个全局的均值pooling，后面接上fc-1000来做分类。<br><img src="/img/paper-mobilenet-net-arch.png" alt="body arch"></p>
<p>此外，作者指出目前的深度学习框架大多使用GEMM实现卷积层的计算（如Caffe等先使用im2col，再使用GEMM）。但是pointwis= conv其实不需要reordering，说明目前的框架这里还有提升的空间。（不清楚目前PyTorch，TensorFlow等对pointwise conv和depthwise conv的支持如何）</p>
<p>在训练的时候，一个注意的地方是，对depthwise conv layer，weight decay的参数要小，因为这层本来就没多少个参数。</p>
<p>这里，给出PyTorch的一个<a href="https://github.com/marvis/pytorch-mobilenet/blob/master/main.py#L78" target="_blank" rel="noopener">第三方实现</a>。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">self.model = nn.Sequential(</span><br><span class="line">    conv_bn(  <span class="number">3</span>,  <span class="number">32</span>, <span class="number">2</span>), </span><br><span class="line">    conv_dw( <span class="number">32</span>,  <span class="number">64</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw( <span class="number">64</span>, <span class="number">128</span>, <span class="number">2</span>),</span><br><span class="line">    conv_dw(<span class="number">128</span>, <span class="number">128</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">128</span>, <span class="number">256</span>, <span class="number">2</span>),</span><br><span class="line">    conv_dw(<span class="number">256</span>, <span class="number">256</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">256</span>, <span class="number">512</span>, <span class="number">2</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">512</span>, <span class="number">1</span>),</span><br><span class="line">    conv_dw(<span class="number">512</span>, <span class="number">1024</span>, <span class="number">2</span>),</span><br><span class="line">    conv_dw(<span class="number">1024</span>, <span class="number">1024</span>, <span class="number">1</span>),</span><br><span class="line">    nn.AvgPool2d(<span class="number">7</span>),</span><br><span class="line">)</span><br><span class="line">self.fc = nn.Linear(<span class="number">1024</span>, <span class="number">1000</span>)</span><br></pre></td></tr></table></figure></p>
<h3 id="网络设计超参数的影响"><a href="#网络设计超参数的影响" class="headerlink" title="网络设计超参数的影响"></a>网络设计超参数的影响</h3><h4 id="wider？or-thinner？"><a href="#wider？or-thinner？" class="headerlink" title="wider？or thinner？"></a>wider？or thinner？</h4><p>描述网络，除了常见的深度，还有一个指标就是宽度。网络的宽度受filter个数的影响。更多的filter，说明网络更胖，提取feature的能力”看起来“就会越强。MobileNet使用一个超参数$\alpha$来实验。某个层的filter个数越多，带来的结果就是下一层filter的input channel会变多，$\alpha$就是前后input channel的数量比例。可以得到，计算量会大致变为原来的$\alpha^2$倍。</p>
<h4 id="resolution"><a href="#resolution" class="headerlink" title="resolution"></a>resolution</h4><p>如果输入的spatial dimension变成原来的$\rho$倍，也就是$D_F$变了，那么会对计算量带来影响。利用上面总结的计算公式不难发现，和$\alpha$一样，计算量会变成原来的$\rho^2$倍。</p>
<p>实际中，我们令$\alpha$和$\rho$都小于$1$，构建了更少参数的mobilenet。下面是一个具体参数设置下，网络计算量和参数数目的变化情况。<br><img src="/img/paper-mobilenet-alpha-rho-effect.png" alt="具体参数设置下的reduce情况"></p>
<h3 id="Depthwise-Separable-Conv真的可以？"><a href="#Depthwise-Separable-Conv真的可以？" class="headerlink" title="Depthwise Separable Conv真的可以？"></a>Depthwise Separable Conv真的可以？</h3><p>同样的网络结构，区别在于使用/不使用Depthwise Separable Conv技术，在ImageNet上的精度相差很少（使用这一技术，下降了$1$个点），但是参数和计算量却节省了很多。<br><img src="/img/paper-mobilenet-depthwise-vs-full-conv.png" alt="Depthwise Separable vs Full Convolution MobileNet"></p>
<h3 id="更浅的网络还是更瘦的网络"><a href="#更浅的网络还是更瘦的网络" class="headerlink" title="更浅的网络还是更瘦的网络"></a>更浅的网络还是更瘦的网络</h3><p>如果我们要缩减网络的参数，是更浅的网络更好，还是更瘦的网络更好呢？作者设计了参数和计算量相近的两个网络进行了比较，结论是相对而言，缩减网络深度不是个好主意。<br><img src="/img/paper-mobilenet-narrow-vs-shallow-net.png" alt="Narrow vs Shallow MobileNet"></p>
<h3 id="alpha和rho的定量影响"><a href="#alpha和rho的定量影响" class="headerlink" title="alpha和rho的定量影响"></a>alpha和rho的定量影响</h3><p>定量地比较了不同$\alpha$和$\rho$的设置下，网络的性能。$\alpha$越小，网络精度越低，而且下降速度是加快的。<br><img src="/img/paper-mobilenet-alpha-compact.png" alt="MobileNet Width Multiplier"></p>
<p>输入图像的resolution越小，网络精度也越低。<br><img src="/img/paper-mobilenet-rho-compact.png" alt="MobileNet Resolution"></p>
<h3 id="和其他网络的对比"><a href="#和其他网络的对比" class="headerlink" title="和其他网络的对比"></a>和其他网络的对比</h3><p>这里只贴出结果。一个值得注意的地方是，SqueezeNet虽然参数很少，但是计算量却很大。而MobileNet可以达到参数也很少。这是通过depthwise separable conv带来的好处。<br><img src="/img/paper-mobilenet-comparision-with-other-model.png" alt="与其他主流模型的比较"></p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>接下来，论文讨论了MobileNet在多种不同任务上的表现，证明了它的泛化能力和良好表现。可以在网上找到很多基于MobileNet的detection，classification等的开源项目代码，这里就不再多说了。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
        <tag>model arch</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Xception, Deep Learning with Depthwise separable Convolution</title>
    <url>/2018/03/22/paper-xception/</url>
    <content><![CDATA[<p>在MobileNet, ShuffleMet等轻量级网络中，<strong>depthwise separable conv</strong>是一个很流行的设计。借助<a href="https://arxiv.org/abs/1610.02357" target="_blank" rel="noopener">Xception: Deep Learning with Depthwise separable Convolution</a>，对这种分解卷积的思路做一个总结。<br><a id="more"></a></p>
<h2 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h2><p>自从AlexNet以来，DNN的网络设计经过了ZFNet-&gt;VGGNet-&gt;GoogLeNet-&gt;ResNet等几个发展阶段。本文作者的思路正是受GoogLeNet中Inception结构启发。Inception结构是最早有别于VGG等“直筒型”结构的网络module。以Inception V3为例，一个典型的Inception模块长下面这个样子：<br><img src="/img/paper-xception-inception-module.png" alt="一个典型的Inception结构"></p>
<p>对于一个CONV层来说，它要学习的是一个$3D$的filter，包括两个空间维度（spatial dimension），即width和height；以及一个channel dimension。这个filter和输入在$3$个维度上进行卷积操作，得到最终的输出。可以用伪代码表示如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 对于第i个filter</span><br><span class="line">// 计算输入中心点(x, y)对应的卷积结果</span><br><span class="line">sum = 0</span><br><span class="line">for c in 1:C</span><br><span class="line">  for h in 1:K</span><br><span class="line">    for w in 1:K</span><br><span class="line">      sum += in[c, y-K/2+h, x-K/2+w] * filter_i[c, h, w]</span><br><span class="line">out[i, y, x] = sum</span><br></pre></td></tr></table></figure></p>
<p>可以看到，在$3D$卷积中，channel这个维度和spatial的两个维度并无不同。</p>
<p>在Inception中，卷积操作更加轻量级。输入首先被$1\times 1$的卷积核处理，得到了跨channel的组合(cross-channel correlation)，同时将输入的channel dimension减少了$3\sim 4$倍（一会$4$个支路要做<code>concat</code>操作）。这个结果被后续的$3\times 3$卷积和$5\times 5$卷积核处理，处理方法和普通的卷积一样，见上。</p>
<p>由此作者想到，Inception能够work证明后面的一条假设就是：卷积的channel相关性和spatial相关性是可以解耦的，我们没必要要把它们一起完成。</p>
<h2 id="简化Inception，提取主要矛盾"><a href="#简化Inception，提取主要矛盾" class="headerlink" title="简化Inception，提取主要矛盾"></a>简化Inception，提取主要矛盾</h2><p>接着，为了更好地分析问题，作者将Inception结构做了简化，保留了主要结构，去掉了AVE Pooling操作，如下所示。<br><img src="/img/paper-xception-simplified-inception-module.png" alt="简化后的Inception"></p>
<p>好的，我们现在将底层的$3$个$1\times 1$的卷积核组合起来，其实上面的图和下图是等价的。一个“大的”$1\times 1$的卷积核（channels数目变多），它的输出结果在channel上被分为若干组（group），每组分别和不同的$3\times 3$卷积核做卷积，再将这$3$份输出拼接起来，得到最后的输出。<br><img src="/img/paper-xception-equivalent-inception-module.png" alt="另一种形式"></p>
<p>那么，如果我们把分组数目继续调大呢？极限情况，我们可以使得group number = channel number，如下所示：<br><img src="/img/paper-xception-extreme-version.png" alt="极限模式"></p>
<h2 id="Depthwise-Separable-Conv"><a href="#Depthwise-Separable-Conv" class="headerlink" title="Depthwise Separable Conv"></a>Depthwise Separable Conv</h2><p>这种结构和一种名为<strong>depthwise separable conv</strong>的技术很相似，即首先使用group conv在spatial dimension上卷积，然后使用$1\times 1$的卷积核做cross channel的卷积（又叫做<em>pointwise conv</em>）。主要有两点不同：</p>
<ul>
<li>操作的顺序。在TensorFlow等框架中，depthwise separable conv的实现是先使用channelwise的filter只在spatial dimension上做卷积，再使用$1\times 1$的卷积核做跨channel的融合。而Inception中先使用$1\times 1$的卷积核。</li>
<li>非线性变换的缺席。在Inception中，每个conv操作后面都有ReLU的非线性变换，而depthwise separable conv没有。</li>
</ul>
<p>第一点不同不太重要，尤其是在深层网络中，这些block都是堆叠在一起的。第二点论文后面通过实验进行了比较。可以看出，去掉中间的非线性激活，能够取得更好的结果。<br><img src="/img/paper-xception-experiment-intermediate-activation.png" alt="非线性激活的影响"></p>
<h2 id="Xception网络架构"><a href="#Xception网络架构" class="headerlink" title="Xception网络架构"></a>Xception网络架构</h2><p>基于上面的分析，作者认为这样的假设是合理的：cross channel的相关和spatial的相关可以<strong>完全</strong>解耦。</p>
<blockquote>
<p>we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations in the feature maps of convolutional neural networks can be <em>entirely</em> decoupled. </p>
</blockquote>
<p>Xception的结构基于ResNet，但是将其中的卷积层换成了depthwise separable conv。如下图所示。整个网络被分为了三个部分：Entry，Middle和Exit。</p>
<blockquote>
<p>The Xception architecture: the data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not included in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).</p>
</blockquote>
<p><img src="/img/paper-xception-arch.png" alt="Xception的网络结构"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>CS229 简单的监督学习方法</title>
    <url>/2018/03/21/cs229-supervised-learning/</url>
    <content><![CDATA[<p>回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。<br><a id="more"></a></p>
<h2 id="最小二乘法"><a href="#最小二乘法" class="headerlink" title="最小二乘法"></a>最小二乘法</h2><p>最小二乘法是一个线性模型，即：</p>
<script type="math/tex; mode=display">\hat{y} = h_\theta(x) = \sum_{i=1}^{m}\theta_i x_i = \theta^T x</script><p>定义损失函数为Mean Square Error(MSE)，如下所示。其中，不戴帽子的$y$表示给定的ground truth。</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2}(\hat{y}-y)^2</script><p>那么，最小二乘就是要找到这样的参数$\theta^*$，使得：</p>
<script type="math/tex; mode=display">\theta^* = \arg\min J(\theta)</script><h3 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h3><p>使用梯度下降方法求解上述优化问题，我们有：</p>
<script type="math/tex; mode=display">\theta_{i+1} = \theta_{i} - \alpha \nabla_\theta J(\theta)</script><p>求导，有：</p>
<script type="math/tex; mode=display">\begin{aligned}\nabla_\theta J(\theta) &= \frac{1}{2}\nabla_\theta (\theta^T x - y)^2 \\
&= (\theta^T x - y) x\end{aligned}</script><p>由于这里的损失函数是一个凸函数，所以梯度下降方法能够保证到达全局的极值点。</p>
<p>上面的梯度下降只是对单个样本来做的。实际上，我们可以取整个训练集或者训练集的一部分，计算平均损失函数$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}J_i(\theta)$，做梯度下降，道理是一样的，只不过相差了常数因子$\frac{1}{N}$。</p>
<h3 id="正则方程"><a href="#正则方程" class="headerlink" title="正则方程"></a>正则方程</h3><p>除了梯度下降方法之外，上述问题还存在着解析解。我们将所有的样本输入$x^{(i)}$作为行向量，构成矩阵$X \in \mathbb{R}^{N\times d}$。其中，$N$为样本总数，$d$为单个样本的特征个数。那么，对于参数$\theta\in\mathbb{R}^{d\times 1}$来说，$X\theta$的第$i$行就可以给出模型对第$i$个样本的预测结果。我们将ground truth排成一个$N\times 1$的矩阵，那么，损失函数可以写作：</p>
<script type="math/tex; mode=display">J(\theta) = \frac{1}{2N} \Vert X\theta-y \Vert_2^2</script><p>将$\Vert x\Vert_2^2$写作$x^T x$，同时略去常数项，我们有：</p>
<script type="math/tex; mode=display">\begin{aligned}J &= (X\theta - y)^T (X\theta - y) \\
&= \theta^T X^T X\theta - 2\theta^T x^T y +y^T y\end{aligned}</script><p>对其求导，有：</p>
<script type="math/tex; mode=display">\nabla_\theta J = X^T X\theta - X^T y</script><p><img src="/img/cs229-supervised-learning-least-square-normal-equation.png" alt="具体计算过程贴图"></p>
<p>这其中，主要用到的矩阵求导性质如下：<br><img src="/img/cs229-supervised-learning-some-useful-matrix-derivatives.png" alt="一些典型求导结果"><br>令导数为$0$，求得极值点处：</p>
<script type="math/tex; mode=display">\theta^* = (X^TX)^{-1}X^T y</script><h3 id="概率解释"><a href="#概率解释" class="headerlink" title="概率解释"></a>概率解释</h3><p>这里对上述做法给出一个概率论上的解释。首先我们要引入似然函数（likelihood function）的概念。</p>
<p>似然函数是一个关于模型参数$\theta$的函数，它描述了某个参数$\theta$下，给出输入$x$，得到输出$y$的概率。用具体的公式表示如下：</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}P(y^{(i)}|x^{(i)};\theta)</script><p>假设线性模型的预测结果和ground truth之间的误差服从Gaussian分布，也就是说，</p>
<script type="math/tex; mode=display">y - \theta^T x  =  \epsilon \sim \mathcal{N}(0, \sigma^2)</script><p>那么上面的似然函数可以写作：</p>
<script type="math/tex; mode=display">L(\theta) = \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})</script><p>如何估计参数$\theta$呢？我们可以认为，参数$\theta$使得出现样本点$(x^{(i)}, y^{(i)})$的概率变大，所以才能被我们观测到。自然，我们需要使得似然函数$L(\theta)$取得极大值，也就是说：</p>
<script type="math/tex; mode=display">\theta^* = \arg\max L(\theta)</script><p>通过引入$\log(\cdot)$，可以将连乘变成连加，同时不改变函数的单调性。这样，实际上我们操作的是对数似然函数$\log L(\theta)$。有：</p>
<script type="math/tex; mode=display">\begin{aligned} \mathcal{l} &= \log L(\theta) \\
&= \sum_{i=1}^{N}\log \frac{1}{\sqrt{2\pi\sigma^2}} \exp (\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\
&= N\log\frac{1}{\sqrt{2\pi\sigma^2}} -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{N}(y^{(i)}-\theta^T x^{(i)})^2 \end{aligned}</script><p>略去前面的常数项不管，后面一项正好是最小二乘法的损失函数。要想最大化对数似然函数，也就是要最小化上面的损失函数。</p>
<p>所以，最小二乘法的损失函数可以由数据集的噪声服从Gaussian分布自然地导出。</p>
<h3 id="加权最小二乘法"><a href="#加权最小二乘法" class="headerlink" title="加权最小二乘法"></a>加权最小二乘法</h3><p>加权最小二乘法是指对数据集中的数据赋予不同的权重，一个重要的用途是使用权重$w^{(i)} = \exp (-\frac{(x^{(i)}-x)^2}{2\tau^2})$做局部最小二乘。不再多说。</p>
<h2 id="logistic回归"><a href="#logistic回归" class="headerlink" title="logistic回归"></a>logistic回归</h2><p>虽然叫回归，但是logistic回归解决的问题是分类问题。</p>
<h3 id="logistic函数"><a href="#logistic函数" class="headerlink" title="logistic函数"></a>logistic函数</h3><p>logistic函数$\sigma(x) = \frac{1}{1+e^{-x}}$，又叫sigmoid函数，将输入$(-\infty, +\infty)$压缩到$(0, 1)$之间。它的形状如下：<br><img src="/img/cs229-supervised-learning-sigmoid.png" alt="sigmoid函数"></p>
<p>对其求导，发现导数值可以完全不依赖于输入$x$：</p>
<script type="math/tex; mode=display">\frac{d\sigma(x)} {dx} = \sigma(x)(1-\sigma(x))</script><p>我们将logistic函数的输入取做$x$的feature的线性组合，就得到了假设函数$h_\theta(x) = \sigma(\theta^T x)$。</p>
<h3 id="logistic回归-1"><a href="#logistic回归-1" class="headerlink" title="logistic回归"></a>logistic回归</h3><p>logistic函数的输出既然是在$(0,1)$上，我们可以将其作为概率。也就是说，我们认为它的输出是样本点属于类别$1$的概率：</p>
<script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h_\theta(x) \\
P(y=0|x) &= 1-h_\theta(x) \end{aligned}</script><p>或者我们写的更紧凑些：</p>
<script type="math/tex; mode=display">P(y|x) = (h_\theta(x))^y (1-h_\theta(x))^{(1-y)}</script><p>我们仍然使用上述极大似然的估计方法，求取参数$\theta$，为求简练，隐去了上标$(i)$。</p>
<script type="math/tex; mode=display">\begin{aligned}L(\theta) &= \prod_{i=1}^{N}P(y|x;\theta) \\
&=\prod (h_\theta(x))^y (1-h_\theta(x))^{(1-y)} \end{aligned}</script><p>取对数：</p>
<script type="math/tex; mode=display">\log L(\theta) = \sum_{i=1}^{N}y\log(h(x)) + (1-y)\log(1-h(x))</script><p>所以，我们的损失函数为$J(\theta) = - [y\log(h(x)) + (1-y)\log(1-h(x))]$。把$h(x)$换成$P$，岂不就是深度学习中常用的交叉损失熵在二分类下的特殊情况？</p>
<p>回到logistic回归，使用梯度下降，我们可以得到更新参数的策略：</p>
<script type="math/tex; mode=display">\theta_{i+1} = \theta_i - \alpha (h_\theta(x) - y)x</script><p>啊哈！形式和最小二乘法完全一样。只不过要注意，现在的$h_\theta(x)$已经变成了一个非线性函数。</p>
<h3 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h3><p>在上述logistic回归基础上，我们强制将其输出映射到$\lbrace 1, -1\rbrace$。即将$\sigma(x)$换成$g(x)$：</p>
<script type="math/tex; mode=display">g(x) = \begin{cases} 1, \quad\text{if}\quad x \ge 0\\ 0, \quad\text{if}\quad x < 0\end{cases}</script><p>使用同样的更新方法，我们就得到了感知机模型（perceptron machine）。</p>
]]></content>
      <tags>
        <tag>公开课</tag>
        <tag>cs229</tag>
      </tags>
  </entry>
  <entry>
    <title>Hack PyCaffe</title>
    <url>/2018/03/16/caffe-hack-python-interface/</url>
    <content><![CDATA[<p>这篇文章主要是<a href="https://github.com/nitnelave/pycaffe_tutorial/blob/master/04%20Hacking%20the%20Python%20API.ipynb" target="_blank" rel="noopener">Github: PyCaffe Tutorial</a>中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。<br><img src="/img/caffe-hack-pycaffe-python-cpp-binding.jpg" alt="Python&amp;&amp;CPP binding"><br><a id="more"></a></p>
<h2 id="PyCaffe的代码组织结构"><a href="#PyCaffe的代码组织结构" class="headerlink" title="PyCaffe的代码组织结构"></a>PyCaffe的代码组织结构</h2><p>见Caffe的<code>python</code>目录。下面这张图是与PyCaffe相关的代码的分布。其中<code>src</code>和<code>include</code>是Caffe框架的后端C++实现，<code>python</code>目录中是与PyCaffe关系更密切的代码。可以看到，除了<code>_caffe.cpp</code>以外，其他都是纯python代码。<code>_caffe.cpp</code>使用boost提供了C++与python的绑定，而其他python脚本在此层的抽象隔离之上，继续完善了相关功能，提供了更加丰富的API、<br><img src="/img/hack-pycaffe-code-organization.png" alt="代码组织结构"></p>
<h2 id="添加纯Python功能"><a href="#添加纯Python功能" class="headerlink" title="添加纯Python功能"></a>添加纯Python功能</h2><p>首先，我们介绍如何在C++构建的PyCaffe隔离之上，用纯python实现想要的功能。</p>
<h3 id="添加的功能和PyCaffe基本平行，不需要改变已有代码"><a href="#添加的功能和PyCaffe基本平行，不需要改变已有代码" class="headerlink" title="添加的功能和PyCaffe基本平行，不需要改变已有代码"></a>添加的功能和PyCaffe基本平行，不需要改变已有代码</h3><p>有的时候想加入的功能和PyCaffe的关系基本是平行的，比如想仿照<code>PyTorch</code>等框架，加入对数据进行预处理的<code>Transformer</code>功能（这个API其实已经在PyCaffe中实现了，这里只是举个例子）。为了实现这个功能，我们可能需要使用<code>numpy</code>和<code>opencv</code>等包装图像的预处理操作，但是和Caffe本身基本没什么关系。在这样的情况下，我们直接编写即可。要注意在<code>python/caffe/__init__.py</code>中import相关的子模块或函数。这个例子可以参考<code>caffe.io</code>的实现（见<code>python/caffe/io.py</code>文件）。</p>
<h3 id="添加的功能需要Caffe的支持，向已有的类中添加函数"><a href="#添加的功能需要Caffe的支持，向已有的类中添加函数" class="headerlink" title="添加的功能需要Caffe的支持，向已有的类中添加函数"></a>添加的功能需要Caffe的支持，向已有的类中添加函数</h3><p>如果添加的功能需要Caffe的支持，可以在<code>pycaffe.py</code>内添加，详见<code>Net</code>的例子。由于python的灵活性，我们可以参考<code>Net</code>的实现方式，待函数实现完成后，使用<code>&lt;class&gt;.&lt;function&gt; = my_function</code>动态地添加。如下所示，注意<code>_Net_forward</code>函数的第一个参数必须是<code>self</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_forward</span><span class="params">(self, blobs=None, start=None, end=None, **kwargs)</span>:</span></span><br><span class="line">    <span class="comment"># do something</span></span><br><span class="line">Net.forward = _Net_forward</span><br></pre></td></tr></table></figure>
<p>与之相似，我们还可以为已经存在的类添加字段。注意，函数用<code>@property</code>装饰，且参数有且只有一个<code>self</code>，</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># This function will be called when accessing net.blobs</span></span><br><span class="line"><span class="meta">@property</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_Net_blobs</span><span class="params">(self)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    An OrderedDict (bottom to top, i.e., input to output) of network</span></span><br><span class="line"><span class="string">    blobs indexed by name</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> hasattr(self, <span class="string">'_blobs_dict'</span>):</span><br><span class="line">        self._blobs_dict = OrderedDict(zip(self._blob_names, self._blobs))</span><br><span class="line">    <span class="keyword">return</span> self._blobs_dict </span><br><span class="line"></span><br><span class="line"><span class="comment"># Set the field `blobs` to call _Net_blobs</span></span><br><span class="line">Net.blobs = _Net_blobs</span><br></pre></td></tr></table></figure>
<p>PyCaffe中已经实现的类主要有：<code>Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver</code>。</p>
<h2 id="使用C-添加功能"><a href="#使用C-添加功能" class="headerlink" title="使用C++添加功能"></a>使用C++添加功能</h2><p>当遇到如下情况时，可能需要修改C++代码：</p>
<ul>
<li>为了获取更底层的权限控制，如一些私有字段。</li>
<li>性能考虑。</li>
</ul>
<p>这时，你应该去修改<code>python/caffe/_caffe.cpp</code>文件。这个文件使用了boost实现了python与C++的绑定。</p>
<p>为了添加一个字段，可以在<code>Blob</code>部分添加如下的代码。这样，就会将python中<code>Blob</code>类的<code>num</code>字段绑定到C++的<code>Blob&lt;Dtype&gt;::num()</code>方法上。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">.add_property(<span class="string">"num"</span>, &amp;Blob&lt;Dtype&gt;::num)</span><br></pre></td></tr></table></figure></p>
<p>使用<code>.def</code>可以为python相应的类绑定方法。在下面的代码中，首先实现了<code>Net_Save</code>方法，然后将其绑定到了python中<code>Net</code>类的<code>save</code>方法上。这样，通过python调用<code>net.save(filename)</code>即可。</p>
<p>注意，当你修改了<code>_caffe,cpp</code>后，记得使用<code>make pycaffe</code>重新生成动态链接库。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"># Declare the function</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Net_Save</span><span class="params">(<span class="keyword">const</span> Net&lt;Dtype&gt;&amp; net, <span class="built_in">string</span> filename)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"></span><br><span class="line">bp::class_&lt;Net&lt;Dtype&gt;&gt;(<span class="string">"Net"</span>, bp::no_init)</span><br><span class="line"># Now we can call net.save(file)</span><br><span class="line">.def(<span class="string">"save"</span>, &amp;Net_Save)</span><br></pre></td></tr></table></figure>
<p>当然，上面介绍的这些还很基础，关于boost的python绑定，可以参考官方的文档：<a href="http://www.boost.org/doc/libs/1_58_0/libs/python/doc/tutorial/doc/html/index.html" target="_blank" rel="noopener">boost: python binding</a></p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Learning both Weights and Connections for Efficient Neural Networks</title>
    <url>/2018/03/14/paper-network-prune-hansong/</url>
    <content><![CDATA[<p>Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">Learning both Weights and Connections for Efficient Neural Networks</a>。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。<br><img src="/img/paper-pruning-network-demo.png" alt="Pruning的主要过程"><br><a id="more"></a></p>
<h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>DNN虽然能够解决很多以前很难解决的问题，但是一个应用方面的问题就是这些模型通常都太大了。尤其是当运行在手机等移动设备上时，对电源和网络带宽都是负担。对于电源来说，由于模型巨大，所以只能在外部内存DRAM中加载，造成能耗上升。具体数值见下表。所以模型压缩很有必要。本文就是使用剪枝的方法，将模型中不重要的权重设置为$0$，将原来的dense model转变为sparse model，达到压缩的目的。<br><img src="/img/paper-pruning-network-energy-for-different-memory-hieracy.png" alt="操作数地址的不同造成的功耗对比"></p>
<h3 id="解决什么问题？"><a href="#解决什么问题？" class="headerlink" title="解决什么问题？"></a>解决什么问题？</h3><p>如何在不损失精度的前提下，对DNN进行剪枝（或者说稀疏化），从而压缩模型。</p>
<h3 id="为什么剪枝是work的？"><a href="#为什么剪枝是work的？" class="headerlink" title="为什么剪枝是work的？"></a>为什么剪枝是work的？</h3><p>为什么能够通过剪枝的方法来压缩模型呢？难道剪掉的那些连接真的不重要到可以去掉吗？论文中，作者指出，DNN模型广泛存在着参数过多的问题，具有很大的冗余（见参考文献NIPS 2013的一篇文章<a href="https://arxiv.org/abs/1306.0543" target="_blank" rel="noopener">Predicting parameters in deep learning</a>）。</p>
<blockquote>
<p>Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models </p>
</blockquote>
<p>另外，作者也为自己的剪枝方法找到了生理学上的依据，生理学上发现，对于哺乳动物来说，婴儿期会产生许多的突触连接，在后续的成长过程中，不怎么用的那些突出会退化消失。</p>
<h3 id="怎么做"><a href="#怎么做" class="headerlink" title="怎么做"></a>怎么做</h3><p>作者的方法分为三个步骤：</p>
<ul>
<li>Train Connectivity: 按照正常方法训练初始模型。作者认为该模型中权重的大小表征了其重要程度</li>
<li>Prune Connection: 将初始模型中那些低于某个阈值的的权重参数置成$0$（即所谓剪枝）</li>
<li>Re-Train: 重新训练，以期其他未被剪枝的权重能够补偿pruning带来的精度下降</li>
</ul>
<p>为了达到一个满意的压缩比例和精度要求，$2$和$3$要重复多次。</p>
<h2 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h2><p>为了减少网络的冗余，减小模型的size，有以下相关工作：</p>
<ul>
<li>定点化。将weight使用8bit定点表示，32bit浮点表示activation。</li>
<li>低秩近似。使用矩阵分解等方法。</li>
<li>网络设计上，NIN等使用Global Average Pooling取代FC层，可以大大减少参数量，这种结构已经得到了广泛使用。而FC也并非无用。在Pooling后面再接一个fc层，便于后续做迁移学习transfer learning。</li>
<li>从优化上下手，使用损失函数的Hessian矩阵，比直接用weight decay更好。</li>
<li>HashedNet等工作，这里不再详述。</li>
</ul>
<h2 id="如何Prune"><a href="#如何Prune" class="headerlink" title="如何Prune"></a>如何Prune</h2><p>主要分为三步，上面 概述 中 怎么做 部分已经简单列出。下面的算法流程摘自作者的博士论文，可能更加详细清楚。<br><img src="/img/paper-pruning-network-algrithem.png" alt="剪枝算法"></p>
<h3 id="正则项的选择"><a href="#正则项的选择" class="headerlink" title="正则项的选择"></a>正则项的选择</h3><p>L1和L2都可以用来做正则，惩罚模型的复杂度。使用不同的正则方法会对pruning和retraining产生影响。实验发现，采用L2做正则项较好。见下图，可以看到详细的比较结果，分别是with/without retrain下L1和L2正则对精度的影响。还可以看到一个共性的地方，就是当pruning的比例大于某个阈值后，模型的精度会快速下降。</p>
<p><img src="/img/paper-pruning-network-regularization.png" alt="L1/L2 Regularization"></p>
<h3 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h3><p>Dropout是一项防止过拟合的技术。要注意的是，在retraining的时候，我们需要对Dropout ratio做出调整。因为网络中的很多连接都被剪枝剪下来了，所以dropout的比例要变小。下面给出定量的估计。</p>
<p>对于FC层来说，如果第$i$层的神经元个数是$N_i$，那么该层的连接数$C_i$用乘法原理可以很容易得到：$C_i = N_{i-1}N_i$。也就是说，连接数$C\sim N^2$。而dropout是作用于神经元的（dropout是将$N_i$个神经元输出按照概率dropout掉）。所以，比例$D^2 \sim C$，最后得到：</p>
<script type="math/tex; mode=display">D_r = D_o \sqrt{\frac{C_{ir}}{C_{io}}}</script><p>其中，下标$r$表示retraining，$o$表示初始模型(original)。</p>
<h2 id="Local-Pruning"><a href="#Local-Pruning" class="headerlink" title="Local Pruning"></a>Local Pruning</h2><p>在retraining部分，在初始模型基础上继续fine tune较好。为了能够更有效地训练，在训练FC层的时候，可以将CONV的参数固定住。反之亦然。</p>
<p>另外，不同深度和类型的layer对剪枝的敏感度是不一样的。作者指出，CONV比FC更敏感，第$1$个CONV比后面的要敏感。下图是AlexNet中各个layer剪枝比例和模型精度下降之间的关系。可以印证上面的结论。</p>
<p><img src="/img/paper-pruning-network-layer-sensitivity.png" alt="CONV和FC的prune和精度下降的关系"></p>
<h2 id="多次迭代剪枝"><a href="#多次迭代剪枝" class="headerlink" title="多次迭代剪枝"></a>多次迭代剪枝</h2><p>应该迭代地进行多次剪枝 + 重新训练这套组合拳。作者还尝试过根据参数的绝对值依概率进行剪枝，效果不好。<br><img src="/img/paper-pruning-network-iterative-pruning.png" alt="迭代剪枝"></p>
<h2 id="对神经元进行剪枝"><a href="#对神经元进行剪枝" class="headerlink" title="对神经元进行剪枝"></a>对神经元进行剪枝</h2><p>将神经元之间的connection剪枝后（或者说将权重稀疏化了），那些$0$输入$0$输出的神经元也应该被剪枝了。然后，我们又可以继续以这个神经元出发，剪掉与它相关的connection。这个步骤可以在训练的时候自动发生。因为如果某个神经元已经是dead状态，那么它的梯度也会是$0$。那么只有正则项推着它向$0$的方向。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>使用Caffe实现，需要加入一个<code>mask</code>来表示剪枝。剪枝的阈值，是该layer的权重标准差乘上某个超参数。这里：<a href="https://github.com/BVLC/caffe/pull/4294/files" target="_blank" rel="noopener">Add pruning possibilities at inner_product_layer #4294 </a>，有人基于Caffe官方的repo给FC层加上了剪枝。这里：<a href="https://github.com/may0324/DeepCompression-caffe" target="_blank" rel="noopener">Github: DeepCompression</a>,，有人实现了Deep Compression，可以参考他们的实现思路。</p>
<p>对于实验结果，论文中比对了LeNet和AlexNet。此外，作者的博士论文中给出了更加详细的实验结果，在更多的流行的模型上取得了不错的压缩比例。直接引用如下，做一个mark：</p>
<blockquote>
<p>On the ImageNet dataset, the pruning method reduced the number of parameters of AlexNet by a factor of 9× (61 to 6.7 million), without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13× (138 to 10.3 million), again with no loss of accuracy. We also experimented with the more efficient fully-convolutional neural networks: GoogleNet (Inception-V1), SqueezeNet, and ResNet-50, which have zero or very thin fully connected layers. From these experiments we find that they share very similar pruning ratios before the accuracy drops: 70% of the parameters in those fully-convolutional neural networks can be pruned. GoogleNet is pruned from 7 million to 2 million parameters, SqueezeNet from 1.2 million to 0.38 million, and ResNet-50 from 25.5 million to 7.47 million, all with no loss of Top-1 and Top-5 accuracy on Imagenet.</p>
</blockquote>
<p><img src="/img/paper-pruning-network-results.png" alt="Results"></p>
<p>下面 参考资料 部分也给出了作者在GitHub上放出的Deep Compression的结果，可以前去参考。</p>
<h3 id="学习率的设置"><a href="#学习率的设置" class="headerlink" title="学习率的设置"></a>学习率的设置</h3><p>跑模型跑实验，一个重要的超参数就是学习率$LR$。这里作者也给了一个经验规律。一般在训练初始模型的时候，学习率都是逐渐下降的。刚开始是一个较大的值$LR_1$，最后是一个较小的值$LR_2$。它们之间可能有数量级的差别。作者指出，retraining的学习率应该介于两者之间。可以取做比$LR_1$小$1 \sim 2$个数量级。</p>
<h3 id="RNN和LSTM"><a href="#RNN和LSTM" class="headerlink" title="RNN和LSTM"></a>RNN和LSTM</h3><p>在博士论文中，作者还是用这一技术对RNN/LSTM在Neural Talk任务上做了剪枝，取得了不错的结果。<br><img src="/img/paper-pruning-network-lstm.png" alt="LSTM"></p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul>
<li>HanSong的个人主页：<a href="http://stanford.edu/~songhan/" target="_blank" rel="noopener">Homepage</a></li>
<li>HanSong的博士论文：<a href="https://purl.stanford.edu/qf934gh3708" target="_blank" rel="noopener">Efficient Methods and Hardware for Deep Learning</a></li>
<li>后续的Deep Compression论文：<a href="https://arxiv.org/abs/1510.00149" target="_blank" rel="noopener">DEEP COMPRESSION- COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING</a></li>
<li>Deep Compression AlexNet: <a href="https://github.com/songhan/Deep-Compression-AlexNet" target="_blank" rel="noopener">Github: Deep-Compression-AlexNet</a></li>
</ul>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe中的Net实现</title>
    <url>/2018/02/28/caffe-net/</url>
    <content><![CDATA[<p>Caffe中使用<code>Net</code>实现神经网络，这篇文章对应Caffe代码总结<code>Net</code>的实现。<br><img src="/img/caffe-net-demo.jpg" width="300" height="200" alt="Net示意" align="center"><br><a id="more"></a></p>
<h2 id="proto中定义的参数"><a href="#proto中定义的参数" class="headerlink" title="proto中定义的参数"></a>proto中定义的参数</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">message NetParameter &#123;</span><br><span class="line">  // net的名字</span><br><span class="line">  optional string name = 1; // consider giving the network a name</span><br><span class="line">  // 以下几个都是弃用的参数，为了定义输入的blob（大小）</span><br><span class="line">  // 下面有使用推荐`InputParameter`进行输入设置的方法</span><br><span class="line">  // DEPRECATED. See InputParameter. The input blobs to the network.</span><br><span class="line">  repeated string input = 3;</span><br><span class="line">  // DEPRECATED. See InputParameter. The shape of the input blobs.</span><br><span class="line">  repeated BlobShape input_shape = 8;</span><br><span class="line"></span><br><span class="line">  // 4D input dimensions -- deprecated.  Use &quot;input_shape&quot; instead.</span><br><span class="line">  // If specified, for each input blob there should be four</span><br><span class="line">  // values specifying the num, channels, height and width of the input blob.</span><br><span class="line">  // Thus, there should be a total of (4 * #input) numbers.</span><br><span class="line">  repeated int32 input_dim = 4;</span><br><span class="line">  </span><br><span class="line">  // Whether the network will force every layer to carry out backward operation.</span><br><span class="line">  // If set False, then whether to carry out backward is determined</span><br><span class="line">  // automatically according to the net structure and learning rates.</span><br><span class="line">  optional bool force_backward = 5 [default = false];</span><br><span class="line">  // The current &quot;state&quot; of the network, including the phase, level, and stage.</span><br><span class="line">  // Some layers may be included/excluded depending on this state and the states</span><br><span class="line">  // specified in the layers&apos; include and exclude fields.</span><br><span class="line">  optional NetState state = 6;</span><br><span class="line"></span><br><span class="line">  // Print debugging information about results while running Net::Forward,</span><br><span class="line">  // Net::Backward, and Net::Update.</span><br><span class="line">  optional bool debug_info = 7 [default = false];</span><br><span class="line"></span><br><span class="line">  // The layers that make up the net.  Each of their configurations, including</span><br><span class="line">  // connectivity and behavior, is specified as a LayerParameter.</span><br><span class="line">  repeated LayerParameter layer = 100;  // ID 100 so layers are printed last.</span><br><span class="line"></span><br><span class="line">  // DEPRECATED: use &apos;layer&apos; instead.</span><br><span class="line">  repeated V1LayerParameter layers = 2;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="Input的定义"><a href="#Input的定义" class="headerlink" title="Input的定义"></a>Input的定义</h3><p>在<code>train</code>和<code>deploy</code>的时候，输入的定义常常是不同的。在<code>train</code>时，我们需要提供数据$x$和真实值$y$，这样网络的输出$\hat{y} = \mathcal{F}_\theta (x)$与真实值$y$计算损失，bp，更新网络参数$\theta$。</p>
<p>在<code>deploy</code>时，推荐使用<code>InputLayer</code>定义网络的输入，下面是<code>$CAFFE/models/bvlc_alexnet/deploy.prototxt</code>中的输入定义：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">  name: &quot;data&quot;</span><br><span class="line">  type: &quot;Input&quot;</span><br><span class="line">  // 该层layer的输出blob名称为data，供后续layer使用</span><br><span class="line">  top: &quot;data&quot;</span><br><span class="line">  // 定义输入blob的大小：10 x 3 x 227 x 227</span><br><span class="line">  // 说明batch size = 10</span><br><span class="line">  // 输入彩色图像，channel = 3, RGB</span><br><span class="line">  // 输入image的大小：227 x 227</span><br><span class="line">  input_param &#123; shape: &#123; dim: 10 dim: 3 dim: 227 dim: 227 &#125; &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="头文件"><a href="#头文件" class="headerlink" title="头文件"></a>头文件</h2><p><code>Net</code>的描述头文件位于<code>$CAFFE/include/caffe/net.hpp</code>中。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>捉bug记 - JupyterNotebook中使用pycaffe加载多个模型一直等待的现象</title>
    <url>/2018/02/27/bug-pycaffe-jupyternotebook-awaiting-for-data/</url>
    <content><![CDATA[<p>JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。</p>
<h2 id="bug重现"><a href="#bug重现" class="headerlink" title="bug重现"></a>bug重现</h2><p>我想在notebook中比较两个使用不同方法训练出来的模型，它们使用了同样的LMDB文件进行训练。加载第一个模型没有问题，但当加载第二个模型时，却一直等待。在StackOverflow上我发现了类似的问题，可以见：<a href="https://stackoverflow.com/questions/37260158/cant-load-2-models-in-pycaffe" target="_blank" rel="noopener">Can’t load 2 models in pycaffe</a>。</p>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>这是由于pycaffe（是否要加上jupyter-notebook？因为不用notebook，以前没有出现过类似问题）不能并发读取同样的LMDB所导致的。但是很遗憾，没有发现太好的解决办法。最后只能是将LMDB重新copy了一份，并修改prototxt文件，使得两个模型分别读取不同的LMDB。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe中卷积的大致实现思路</title>
    <url>/2018/02/26/conv-in-caffe/</url>
    <content><![CDATA[<p>参考资料：知乎：<a href="https://www.zhihu.com/question/28385679" target="_blank" rel="noopener">在Caffe中如何计算卷积</a>。<br><img src="/img/conv-in-caffe-naive-loop.png" alt="Naive Loop"><br><a id="more"></a></p>
<p>使用<code>im2col</code>将输入的图像或特征图转换为矩阵，后续就可以使用现成的线代运算优化库，如BLAS中的GEMM，来快速计算。<br><img src="/img/conv-in-caffe-im2col-followed-gemm.png" alt="im2col-&gt;gemm"></p>
<p>im2col的工作原理如下：每个要和卷积核做卷积的patch被抻成了一个feature vector。不同位置的patch，顺序堆叠起来，<br><img src="/img/conv-in-caffe-im2col-1.png" alt="patches堆起来"></p>
<p>最后就变成了这样：<br><img src="/img/conv-in-caffe-im2col-2.png" alt="最后的样子"></p>
<p>同样的，对卷积核也做类似的变换。将单一的卷积核抻成一个行向量，然后把<code>c_out</code>个卷积核顺序排列起来。<br><img src="/img/conv-in-caffe-im2col-3.png" alt="卷积核 to col"></p>
<p>我们记图像那个矩阵是<code>A</code>，记卷积那个矩阵是<code>F</code>。那么，对于第<code>i</code>个卷积核来说，它现在实际上是<code>F</code>里面的第<code>i</code>个行向量。为了计算它在原来图像上的各个位置的卷积，现在我们需要它和矩阵<code>A</code>中的每行做点积。也就是 <code>F_i * [A_1^T, A_2^T, … A_i^T]</code> （也就是<code>A</code>的转置）。推广到其他的卷积核，就是说，最后的结果是<code>F*A^T</code>.</p>
<p>我们可以用矩阵维度验证。<code>F</code>的维度是<code>Cout x (C x K x K)</code>. 输入的Feature map matrix的维度是<code>(H x W) x (C x K x K)</code>。那么上述矩阵乘法的结果就是 <code>Cout x (H x W)</code>。正好可以看做输出的三维blob的大小：<code>Cout x H x W</code>。</p>
<p>这里<a href="https://github.com/Yangqing/caffe/wiki/Convolution-in-Caffe:-a-memo" target="_blank" rel="noopener">Convolution in Caffe: a memo</a>还有贾扬清对于自己当时在caffe中实现conv的”心路历程“，题图出自此处。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Learning Structured Sparsity in Deep Neural Networks</title>
    <url>/2018/02/24/paper-ssl-dnn/</url>
    <content><![CDATA[<p>DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在<a href="https://arxiv.org/pdf/1608.03665.pdf" target="_blank" rel="noopener">这篇文章</a>中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者<a href="http://www.pittnuts.com/" target="_blank" rel="noopener">温伟</a>，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：<a href="https://github.com/wenwei202/caffe/tree/scnn" target="_blank" rel="noopener">GitHub</a><br><img src="/img/paper-ssldnn.png" alt="SSL的原理示意图"><br><a id="more"></a></p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>为了满足DNN的计算速度要求，我们提出了Structure Sparisity Learning (SSL)技术来正则化DNN的“结构”（例如CNN filter的参数，filter的形状，包括channel和网络层深）。它可以带来：</p>
<ul>
<li>大的DNN —&gt; 紧凑的模型 —&gt; 计算开销节省</li>
<li>硬件友好的结构化稀疏 —&gt; 便于在专用硬件上加速</li>
<li>提供了正则化，提高网络泛化能力 —&gt; 提高了精度</li>
</ul>
<p>实验结果显示，这种方法可以在CPU/GPU上对AlexNet分别达到平均$5.1$和$3,1$倍的加速。在CIFAR10上训练ResNet，从$20$层减少到$18$层，并提高了精度。</p>
<h2 id="LASSO"><a href="#LASSO" class="headerlink" title="LASSO"></a>LASSO</h2><p>SSL是基于Group LASSO的，所以正式介绍文章之前，首先简单介绍LASSO和Group LASSO，<br><a href="https://en.wikipedia.org/wiki/Lasso_(statistics" target="_blank" rel="noopener">LASSO</a>)(least absolute shrinkage and selection operator)是指统计学习中的特征选择方法。以最小二乘法求解线性模型为例，可以加上L1 norm作为正则化约束，见下式，其中$\beta$是模型的参数。具体推导过程可以参见wiki页面。</p>
<script type="math/tex; mode=display">\min_{\beta \in R^p}\frac{1}{N} \Vert(y-X\beta)\Vert_2^2 + \lambda \Vert \beta \Vert_1</script><p>而Group LASSO就是将参数分组，进行LASSO操作。</p>
<p>这里只是简单介绍一下LASSO。SSL下面还会详细介绍，不必过多执着于LASSO。</p>
<h2 id="结构化稀疏"><a href="#结构化稀疏" class="headerlink" title="结构化稀疏"></a>结构化稀疏</h2><p>DNN通常参数很多，计算量很大。为了减少计算开销，目前的研究包括：稀疏化，connection pruning, low rank approximation等。然而前两种方法只能得到随机的稀疏，无规律的内存读取仍然制约了速度。下面这种图是一个例子。我们使用了L1正则进行稀疏。和原模型相比，精度损失了$2$个点。虽然稀疏度比较高，但是实际加速效果却很差。可以看到，在<code>conv3</code>，<code>conv4</code>和<code>conv5</code>中，有的情况下反而加速比是大于$1$的。<br><img src="/img/paper-ssldnn-random-sparity-is-bad.png" alt="随机稀疏的实际加速效果"></p>
<p>low rank approx利用矩阵分解，将预训练好的模型的参数分解为小矩阵的乘积。这种方法需要较多的迭代次数，同时，网络结构是不能更改的。</p>
<p>基于下面实验中观察的事实，我们提出了SSL来直接学习结构化稀疏。</p>
<ul>
<li>网络中的filter和channel存在冗余</li>
<li>filter稀疏化为别的形状能够去除不必要的计算</li>
<li>网络的深度虽然重要，但是并不意味着深层的layer对网络性能一定是好的</li>
</ul>
<p>假设第$l$个卷积层的参数是一个$4D$的Tensor，$W^{(l)}\in R^{N_l \times C_l \times M_l \times N_l}$，那么SSL方法可以表示为优化下面这个损失函数：</p>
<script type="math/tex; mode=display">E(W)=E_D{W} + \lambda R(W) + \lambda_g \sum_{l=1}^{L}R_g(W^{(l)})</script><p>这里，$W$代表DNN中所有权重的集合。$E_D(W)$代表在训练集上的loss。$R$是非结构化的正则项，例如L2 norm。$R_g$是指结构化稀疏的正则项，注意是逐层计算的。对于每一层来说（也就是上述最后一项求和的每一项），group LASSO可以表示为：</p>
<script type="math/tex; mode=display">R_g(w) = \sum_{g=1}^{G}\Vert w^{(g)} \Vert_g</script><p>其中，$w^{(g)}$是该层权重$W^{(l)}$的一部分，不同的分组可以重叠。$G$是分组的组数。$\Vert \cdot \Vert_g$指的是group LASSO，这里使用的是$\Vert w^{(g)}\Vert_g = \sqrt{\sum_{i=1}^{|w^{(g)}|}(w_i^{(g)})^2}$，也就是$2$范数。</p>
<h2 id="SSL"><a href="#SSL" class="headerlink" title="SSL"></a>SSL</h2><p>有了上面的损失函数，SSL就取决于如何对weight进行分组。对不同的分组情况分类讨论如下。图示见博客开头的题图。</p>
<h3 id="惩罚不重要的filter和channel"><a href="#惩罚不重要的filter和channel" class="headerlink" title="惩罚不重要的filter和channel"></a>惩罚不重要的filter和channel</h3><p>假设$W^{(l)}_{n_l,:,:,:}$是第$n$个filter，$W^{(l)}_{:, c_l, :,:}$是所有weight的第$c$个channel。可以通过下面的约束来去除相对不重要的filter和channel。注意，如果第$l$层的weight中某个filter变成了$0$，那么输出的feature map中就有一个全$0$，所以filter和channel的结构化稀疏要放到一起。下面是这种形式下的损失函数。为了简单，后面的讨论中都略去了正常的正则化项$R(W)$。</p>
<script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_n \sum_{l=1}^{L}(\sum_{n_l=1}^{N_l}\Vert W^{(l)}_{n_l,:,:,:}\Vert_g) + \lambda_c\sum_{l=1}^{L}(\sum_{cl=1}^{C_l}\Vert W^{(l)}_{:,c_l,:,:}\Vert_g)</script><h3 id="任意形状的filter"><a href="#任意形状的filter" class="headerlink" title="任意形状的filter"></a>任意形状的filter</h3><p>所谓任意形状的filter，就是将filter中的一些权重置为$0$。可以使用下面的分组方法：</p>
<script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_s \sum_{l=1}^{L}(\sum_{c_l=1}^{C_l}\sum_{m_l=1}^{M_l}\sum_{k_l=1}^{K_l})\Vert W^{(l)}_{:,c_l,m_l,k_l} \Vert_g</script><h3 id="网络深度"><a href="#网络深度" class="headerlink" title="网络深度"></a>网络深度</h3><p>损失函数如下：</p>
<script type="math/tex; mode=display">E(W) = E_D(W) + \lambda_d \sum_{l=1}^{L}\Vert W^{(l)}\Vert_g</script><p>不过要注意的是，某个layer被稀疏掉了，会切断信息的流通。所以受ResNet启发，加上了short-cut结构。即使SSL移去了该layer所有的filter，上层的feature map仍然可以传导到后面。</p>
<h3 id="两类特殊的稀疏规则"><a href="#两类特殊的稀疏规则" class="headerlink" title="两类特殊的稀疏规则"></a>两类特殊的稀疏规则</h3><p>特意提出下面两种稀疏规则，下面的实验即是基于这两种特殊的稀疏结构。</p>
<h4 id="2D-filter-sparsity"><a href="#2D-filter-sparsity" class="headerlink" title="2D filter sparsity"></a>2D filter sparsity</h4><p>卷积层中的3D卷积可以看做是2D卷积的组合（做卷积的时候spatial和channel是不相交的）。这种结构化稀疏是将该卷积层中的每个2D的filter，$W^{(l)}_{n_l,c_l,:,:}$，看做一个group，做group LASSO。这相当于是上述filter-wise和channel-wise的组合。</p>
<h4 id="filter-wise和shape-wise的组合加速GEMM"><a href="#filter-wise和shape-wise的组合加速GEMM" class="headerlink" title="filter-wise和shape-wise的组合加速GEMM"></a>filter-wise和shape-wise的组合加速GEMM</h4><p>在Caffe中，3D的权重tensor是reshape成了一个行向量，然后$N<em>l$个filter的行向量堆叠在一起，就成了一个2D的矩阵。这个矩阵的每一列对应的是$W^{(l)}</em>{:,c_l,m_l,k_l}$，称为shape sparsity。两者组合，矩阵的零行和零列可以被抽去，相当于GEMM的矩阵行列数少了，起到了加速的效果。</p>
<h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>分别在MNIST，CIFAR10和ImageNet上做了实验，使用公开的模型做baseline，并以此为基础使用SSL训练。</p>
<h3 id="LeNet-amp-MLP-MNIST"><a href="#LeNet-amp-MLP-MNIST" class="headerlink" title="LeNet&amp;MLP@MNIST"></a>LeNet&amp;MLP@MNIST</h3><p>分别使用Caffe中实现的LeNet和MLP做实验。</p>
<h4 id="LeNet"><a href="#LeNet" class="headerlink" title="LeNet"></a>LeNet</h4><p>限制SSL为filter-wise和channel-wise稀疏化，来惩罚不重要的filter。下表中，LeNet-1是baseline，2和3是使用不同强度得到的稀疏化结果。可以看到，精度基本没有损失($0.1%$)，但是filter和channel数量都有了较大减少，FLOP大大减少，加速效果比较明显。<br><img src="/img/paper-ssldnn-lenet-penalizing-unimportant-filter-channel.png" alt="实验结果1"></p>
<p>将网络<code>conv1</code>的filter可视化如下。可以看到，对于LeNet2来说，大多数filter都被稀疏掉了。<br><img src="/img/paper-ssldnn-experiment-on-lenet.png" alt="LeNet的实验结果"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>model compression</tag>
      </tags>
  </entry>
  <entry>
    <title>捉bug记 - Cannot create Cublas handle. Cublas won&#39;t be available.</title>
    <url>/2018/02/08/bug-pycaffe-using-cublas/</url>
    <content><![CDATA[<p>这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">common.cpp:114] Cannot create Cublas handle. Cublas won&apos;t be available.</span><br><span class="line">common.cpp:121] Cannot create Curand generator. Curand won&apos;t be available.</span><br></pre></td></tr></table></figure></p>
<a id="more"></a>
<p>令人疑惑的是，这个python脚本我前段时间已经用过几次了，却没有这样的问题。</p>
<p>如果在Google上搜索这个问题，很多讨论都是把锅推给了驱动，不过我使用的这台服务器并没有更新过驱动或系统。本来想要试试重启大法，但是上面还有其他人在跑的任务，所以重启不太现实。</p>
<p>最后找到了这个issue: <a href="https://github.com/BVLC/caffe/issues/440" target="_blank" rel="noopener">Cannot use Caffe on another GPU when GPU 0 has full memory</a>。联想到我目前使用的服务器上GPU０也正是在跑着一项很吃显存的任务（如下所示），所以赶紧试了一下里面@longjon的方法。<br><img src="/img/bug_pycaffe_nvidia_smi_result.png" alt="nvidia-smi给出的显卡使用信息"></p>
<p>使用<code>CUDA_VISIBLE_DEVICES</code>变量，指定Caffe能看到的显卡设备。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">CUDA_VISIBLE_DEVICES=2 python my_script.py --gpu_id=0</span><br></pre></td></tr></table></figure></p>
<p>果然就可以了！</p>
<p>这个问题应该出在pycaffe的初始化上。这里不再深究。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>python</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>论文 - Visualizing and Understanding ConvNet</title>
    <url>/2018/02/08/paper-visualize-convnet/</url>
    <content><![CDATA[<p><a href="https://arxiv.org/pdf/1311.2901.pdf" target="_blank" rel="noopener">Visualizing &amp; Understanding ConvNet</a>这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。</p>
<p><img src="/img/paper_visconvnet_demo.png" alt="可视化结果"><br><a id="more"></a></p>
<h2 id="引言"><a href="#引言" class="headerlink" title="引言"></a>引言</h2><p>继AlexNet之后，CNN在ImageNet竞赛中得到了广泛应用。AlextNet成功的原因包括以下三点：</p>
<ul>
<li>Large data。</li>
<li>硬件GPU性能。</li>
<li>一些技巧提升了模型的泛化能力，如Dropout技术。</li>
</ul>
<p>不过CNN仍然像一只黑盒子，缺少可解释性。这使得对CNN的调试变得比较困难。我们提出了一种思路，可以找出究竟input中的什么东西对应了激活后的Feature map。</p>
<p>(对于神经网络的可解释性，可以从基础理论入手，也可以从实践中的经验入手。本文作者实际上就是在探索如何能够更好得使用经验对CNN进行调试。这种方法仍然没有触及到CNN本质的可解释性的东西，不过仍然在工程实践中有很大的意义，相当于将黑盒子变成了灰盒子。从人工取火到炼金术到现代化学，也不是这么一个过程吗？)</p>
<p>在AlexNet中，每个卷积单元常常由以下几个部分组成：</p>
<ul>
<li>卷积层，使用一组学习到的$3D$滤波器与输入（上一层的输出或网络输入的数据）做卷积操作。</li>
<li>非线性激活层，通常使用<code>ReLU(x) = max(0, x)</code>。</li>
<li>可选的，池化层，缩小Feature map的尺寸。</li>
<li>可选的，LRN层（现在已经基本不使用）。</li>
</ul>
<h2 id="DeconvNet"><a href="#DeconvNet" class="headerlink" title="DeconvNet"></a>DeconvNet</h2><p>我们使用DeconvNet这项技术，寻找与输出的激活对应的输入模式。这样，我们可以看到，输入中的哪个部分被神经元捕获，产生了较强的激活。</p>
<p>如图所示，展示了DeconvNet是如何构造的。<br><img src="/img/paer_visconvnet_deconvnet_structure.png" alt="DeconvNet的构造"></p>
<p>首先，图像被送入卷积网络中，得到输出的feature map。对于输出的某个激活，我们可以将其他激活值置成全$0$，然后顺着deconvNet计算，得到与之对应的输入。具体来说，我们需要对三种不同的layer进行反向操作。</p>
<h3 id="Uppooling"><a href="#Uppooling" class="headerlink" title="Uppooling"></a>Uppooling</h3><p>在CNN中，max pooling操作是不可逆的（信息丢掉了）。我们可以使用近似操作：记录最大值的位置；在deconvNet中，保留该标记位置处的激活值。如下图所示。右侧为CNN中的max pooling操作。中间switches显示的是最大值的位置（用灰色标出）。在左侧的deconvNet中，激活值对应给到相应的灰色位置。这个操作被称为Uppooing。<br><img src="/img/paper_visconvnet_uppooling.png" alt="Uppooling示意图"></p>
<h3 id="Rectification"><a href="#Rectification" class="headerlink" title="Rectification"></a>Rectification</h3><p>在CNN中，一般使用relu作为非线性激活。deconvNet中也做同样的处理。</p>
<h3 id="Filtering"><a href="#Filtering" class="headerlink" title="Filtering"></a>Filtering</h3><p>在CNN中，一组待学习的filter用来与输入的feature map做卷积。得到输出。在deconvNet中，使用deconv操作，输入是整流之后的feature map。</p>
<p>对于最终输出的activation中的每个值，经过deconv的作用，最终会对应到输入pixel space上的一小块区域，显示了它们对最终输出的贡献。</p>
<h2 id="CNN的可视化"><a href="#CNN的可视化" class="headerlink" title="CNN的可视化"></a>CNN的可视化</h2><p>要想可视化，先要有训练好的CNN模型。这里用作可视化的模型基于AlexNet，但是去掉了group。另外，为了可视化效果，将layer $1$的filter size从$11\times 11$变成$7\times 7$，步长变成$2$。具体训练过程不再详述。</p>
<p>训练完之后，我们将ImageNet的validation数据集送入到网络中进行前向计算，</p>
<p>如下所示，是layer $1$的可视化结果。可以看到，右下方的可视化结果被分成了$9\times 9$的方格，每个方格内又细分成了$9\times 9$的小格子。其中，大格子对应的是$9$个filter，小格子对应的是top 9的激活利用deconvNet反算回去对应的image patch、因为layer 1的filter个数正好也是$9$，所以可能稍显迷惑。<br><img src="/img/paper_visconvnet_layer1_demo.png" alt="layer 1的可视化"></p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>这里是关于CNN可视化的一些额外资料：</p>
<ul>
<li>Zeiler关于本文的talk：<a href="https://www.youtube.com/watch?v=ghEmQSxT6tw" target="_blank" rel="noopener">Visualizing and Understanding Deep Neural Networks by Matt Zeiler</a></li>
<li>斯坦福CS231课程的讲义：<a href="http://cs231n.github.io/understanding-cnn/" target="_blank" rel="noopener">Visualizing what ConvNets learn</a></li>
<li>ICML 2015上的另一篇CNN可视化的paper：<a href="https://arxiv.org/pdf/1506.06579.pdf" target="_blank" rel="noopener">Understanding Neural Networks Through Deep Visualization</a>以及他们的开源工具：<a href="https://github.com/yosinski/deep-visualization-toolbox" target="_blank" rel="noopener">deep-visualization-toolbox
</a></li>
<li>一篇知乎专栏的文章：<a href="https://zhuanlan.zhihu.com/p/24833574" target="_blank" rel="noopener">Deep Visualization:可视化并理解CNN</a></li>
</ul>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>visulization</tag>
      </tags>
  </entry>
  <entry>
    <title>Incremental Network Quantization 论文阅读</title>
    <url>/2018/01/25/inq-paper/</url>
    <content><![CDATA[<p>卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。<a href="https://arxiv.org/abs/1702.03044" target="_blank" rel="noopener">Incremental Network Quantization</a>这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization" target="_blank" rel="noopener">Incremental-Network-Quantization</a>。<br><img src="/img/paper-inq-result.png" alt="实验结果"><br><a id="more"></a></p>
<h2 id="量化方法"><a href="#量化方法" class="headerlink" title="量化方法"></a>量化方法</h2><p>INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）：<br><img src="/img/paper-inq-quantize-set.png" alt="权重集合"></p>
<p>假设用$b$bit表示权重，我们分出$1$位单独表示$0$。</p>
<p>PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发<a href="https://github.com/Zhouaojun/Incremental-Network-Quantization/issues/12" target="_blank" rel="noopener">issue</a>问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。</p>
<p>这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为：</p>
<script type="math/tex; mode=display">(n_1-n_2 + 1) \times 2 = 2^{b-1}</script><p>其中，乘以$2$是考虑到正负对称的表示范围。</p>
<p>如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有：</p>
<script type="math/tex; mode=display">n_1 = \lfloor \log_2(4s/3) \rfloor</script><p>其中，$s$是权重当中绝对值最大的那个，即$s = \max \vert W_l\vert$。</p>
<p>之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。</p>
<h2 id="训练方法"><a href="#训练方法" class="headerlink" title="训练方法"></a>训练方法</h2><p>量化完成后，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。</p>
<p>re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。</p>
<blockquote>
<p>Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained.</p>
</blockquote>
<p>训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。<br><img src="/img/paper-inq-algorithm-demo.png" alt="训练图解"></p>
<h3 id="pruning-inspired-strategy"><a href="#pruning-inspired-strategy" class="headerlink" title="pruning-inspired strategy"></a>pruning-inspired strategy</h3><p>在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。<br><img src="/img/paper-inq-different-quantize.png" alt="两种量化方法的比较"></p>
<p>在代码部分，INQ基于Caffe框架，主要修改的地方集中于<code>blob.cpp</code>和<code>sgd_solver.cpp</code>中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，<code>data_copy</code>使用完之后没有释放。关于代码中<code>mask</code>的作用，下文介绍。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// blob.cpp</span></span><br><span class="line"><span class="comment">// INQ  </span></span><br><span class="line"><span class="keyword">if</span>(is_quantization)</span><br><span class="line">&#123;</span><br><span class="line">  Dtype* data_copy=(Dtype*) <span class="built_in">malloc</span>(count_*<span class="keyword">sizeof</span>(Dtype));</span><br><span class="line">  caffe_copy(count_,data_vec,data_copy);</span><br><span class="line">  caffe_abs(count_,data_copy,data_copy);</span><br><span class="line">  <span class="built_in">std</span>::sort(data_copy,data_copy+count_); <span class="comment">//data_copy order from small to large</span></span><br><span class="line">  </span><br><span class="line">  <span class="comment">//caculate the n1</span></span><br><span class="line">  Dtype max_data=data_copy[count_<span class="number">-1</span>];</span><br><span class="line">  <span class="keyword">int</span> n1=(<span class="keyword">int</span>)<span class="built_in">floor</span>(log2(max_data*<span class="number">4.0</span>/<span class="number">3.0</span>));</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//quantizate the top 30% of each layer, change the "partition" until partition=0</span></span><br><span class="line">  <span class="keyword">int</span> partition=<span class="keyword">int</span>(count_*<span class="number">0.7</span>)<span class="number">-1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; (count_); ++i) &#123;</span><br><span class="line">  </span><br><span class="line">    <span class="keyword">if</span>(<span class="built_in">std</span>::<span class="built_in">abs</span>(data_vec[i])&gt;=data_copy[partition])</span><br><span class="line">      &#123;</span><br><span class="line">        data_vec[i] = weightCluster_zero(data_vec[i],n1);</span><br><span class="line"> </span><br><span class="line">        mask_vec[i]=<span class="number">0</span>;</span><br><span class="line">      &#125;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<h3 id="参数更新"><a href="#参数更新" class="headerlink" title="参数更新"></a>参数更新</h3><p>在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，<code>mask</code>中的值都是$1$，这样和<code>diff</code>相乘仍然不变；不更新的参数，<code>mask</code>中的值都是$0$，和<code>diff</code>乘起来，相当于强制把梯度变成了$0$。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// sgd_solver.cpp</span><br><span class="line">caffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff());</span><br></pre></td></tr></table></figure>
<h2 id="结语"><a href="#结语" class="headerlink" title="结语"></a>结语</h2><p>论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的<a href="https://github.com/Zhouaojun/Efficient-Deep-Learning" target="_blank" rel="noopener">repo</a>，也可以作为参考。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>quantization</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe 中的 SyncedMem介绍</title>
    <url>/2018/01/12/caffe-syncedmem/</url>
    <content><![CDATA[<p><code>Blob</code>是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为<code>Blob</code>，开始在各个<code>Layer</code>之间传递，最终得到输出。下面这张图展示了<code>Blob</code>和<code>Layer</code>之间的关系：<br> <img src="/img/caffe_syncedmem_blob_flow.jpg" width="300" height="200" alt="blob的流动" align="center"></p>
<p>Caffe中的<code>Blob</code>在实现的时候，使用了<code>SyncedMem</code>管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中<code>SyncedMem</code>的实现做一总结。<br><a id="more"></a></p>
<h2 id="SyncedMem的作用"><a href="#SyncedMem的作用" class="headerlink" title="SyncedMem的作用"></a>SyncedMem的作用</h2><p><code>Blob</code>是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。</p>
<h2 id="同步的实现思路"><a href="#同步的实现思路" class="headerlink" title="同步的实现思路"></a>同步的实现思路</h2><p>在<code>SyncedMem</code>的实现代码中，作者使用一个枚举量<code>head_</code>来标记当前的状态。如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// in SyncedMem</span></span><br><span class="line"><span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</span><br><span class="line"><span class="comment">// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD</span></span><br><span class="line"><span class="comment">// 这里也是一样，标志着最新的数据位于哪里</span></span><br><span class="line">SyncedHead head_;</span><br></pre></td></tr></table></figure>
<p>这样，利用<code>head_</code>变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。<br><img src="/img/caffe_syncedmem_transfer.png" alt="状态转换图"></p>
<h2 id="具体实现"><a href="#具体实现" class="headerlink" title="具体实现"></a>具体实现</h2><p><code>SyncedMem</code>的类声明如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @brief Manages memory allocation and synchronization between the host (CPU)</span></span><br><span class="line"><span class="comment"> *        and device (GPU).</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> * TODO(dox): more thorough description.</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SyncedMemory</span> &#123;</span></span><br><span class="line"> <span class="keyword">public</span>:</span><br><span class="line">  SyncedMemory();</span><br><span class="line">  <span class="function"><span class="keyword">explicit</span> <span class="title">SyncedMemory</span><span class="params">(<span class="keyword">size_t</span> size)</span></span>;</span><br><span class="line">  ~SyncedMemory();</span><br><span class="line">  <span class="comment">// 获取CPU data指针</span></span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// 设置CPU data指针</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_cpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">  <span class="comment">// 获取GPU data指针</span></span><br><span class="line">  <span class="function"><span class="keyword">const</span> <span class="keyword">void</span>* <span class="title">gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// 设置GPU data指针</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">set_gpu_data</span><span class="params">(<span class="keyword">void</span>* data)</span></span>;</span><br><span class="line">  <span class="comment">// 获取CPU data指针，并在后续将改变指针所指向内存的值</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_cpu_data</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// 获取GPU data指针，并在后续将改变指针所指向内存的值</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span>* <span class="title">mutable_gpu_data</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="comment">// CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步</span></span><br><span class="line">  <span class="keyword">enum</span> SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;</span><br><span class="line">  <span class="function">SyncedHead <span class="title">head</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> head_; &#125;</span><br><span class="line">  <span class="comment">// 内存大小</span></span><br><span class="line">  <span class="function"><span class="keyword">size_t</span> <span class="title">size</span><span class="params">()</span> </span>&#123; <span class="keyword">return</span> size_; &#125;</span><br><span class="line"></span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">async_gpu_push</span><span class="params">(<span class="keyword">const</span> cudaStream_t&amp; stream)</span></span>;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line"></span><br><span class="line"> <span class="keyword">private</span>:</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">check_device</span><span class="params">()</span></span>;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_cpu</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="function"><span class="keyword">void</span> <span class="title">to_gpu</span><span class="params">()</span></span>;</span><br><span class="line">  <span class="keyword">void</span>* cpu_ptr_;</span><br><span class="line">  <span class="keyword">void</span>* gpu_ptr_;</span><br><span class="line">  <span class="keyword">size_t</span> size_;</span><br><span class="line">  SyncedHead head_;</span><br><span class="line">  <span class="keyword">bool</span> own_cpu_data_;</span><br><span class="line">  <span class="keyword">bool</span> cpu_malloc_use_cuda_;</span><br><span class="line">  <span class="keyword">bool</span> own_gpu_data_;</span><br><span class="line">  <span class="comment">// GPU设备编号</span></span><br><span class="line">  <span class="keyword">int</span> device_;</span><br><span class="line"></span><br><span class="line">  DISABLE_COPY_AND_ASSIGN(SyncedMemory);</span><br><span class="line">&#125;;  <span class="comment">// class SyncedMemory</span></span><br></pre></td></tr></table></figure>
<p>我们以<code>to_cpu()</code>为例，看一下如何在不同状态之间切换。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">inline</span> <span class="keyword">void</span> <span class="title">SyncedMemory::to_gpu</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  <span class="comment">// 检查设备状态（使用条件编译，只在DEBUG中使能）</span></span><br><span class="line">  check_device();</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">  <span class="keyword">switch</span> (head_) &#123;</span><br><span class="line">  <span class="keyword">case</span> UNINITIALIZED:</span><br><span class="line">    <span class="comment">// 还没有初始化呢~所以内存啥的还没开</span></span><br><span class="line">    <span class="comment">// 先在GPU上开块显存吧~</span></span><br><span class="line">    CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</span><br><span class="line">    caffe_gpu_memset(size_, <span class="number">0</span>, gpu_ptr_);</span><br><span class="line">    <span class="comment">// 接着，改变状态标志</span></span><br><span class="line">    head_ = HEAD_AT_GPU;</span><br><span class="line">    own_gpu_data_ = <span class="literal">true</span>;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="keyword">case</span> HEAD_AT_CPU:</span><br><span class="line">    <span class="comment">// 数据在CPU上~如果需要，先在显存上开内存</span></span><br><span class="line">    <span class="keyword">if</span> (gpu_ptr_ == <span class="literal">NULL</span>) &#123;</span><br><span class="line">      CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_));</span><br><span class="line">      own_gpu_data_ = <span class="literal">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// 数据拷贝</span></span><br><span class="line">    caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_);</span><br><span class="line">    <span class="comment">// 改变状态变量</span></span><br><span class="line">    head_ = SYNCED;</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  <span class="comment">// 已经在GPU或者已经同步了，什么都不做</span></span><br><span class="line">  <span class="keyword">case</span> HEAD_AT_GPU:</span><br><span class="line">  <span class="keyword">case</span> SYNCED:</span><br><span class="line">    <span class="keyword">break</span>;</span><br><span class="line">  &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">  <span class="comment">// NO_GPU 是一个宏，打印FATAL ERROR日志信息</span></span><br><span class="line">  <span class="comment">// 编译选项没有开GPU支持，只能说 无可奉告</span></span><br><span class="line">  NO_GPU;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>注意到，除了<code>head_</code>以外，<code>SyncedMemory</code>中还有<code>own_gpu_data_</code>（同样，也有<code>own_cpu_data_</code>）的成员。这个变量是用来标志当前CPU或GPU上有没有分配内存，从而当我们使用<code>set_c/gpu_data</code>或析构函数被调用的时候，能够正确释放内存/显存的。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe中的BatchNorm实现</title>
    <url>/2018/01/08/caffe-batch-norm/</url>
    <content><![CDATA[<p>这篇博客总结了Caffe中BN的实现。<br><a id="more"></a></p>
<h2 id="BN简介"><a href="#BN简介" class="headerlink" title="BN简介"></a>BN简介</h2><p>由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。</p>
<p>BN是Batch Normalization的简称，来源于Google研究人员的论文：<a href="https://arxiv.org/pdf/1502.03167.pdf" target="_blank" rel="noopener">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</a>。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。</p>
<p>在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\epsilon$是为了防止除数是$0$，$i$是channel的index。</p>
<script type="math/tex; mode=display">\hat{x_i} = \frac{x_i-\mu_i}{\sqrt{Var(x_i)+\epsilon}}</script><p>不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\gamma$和$\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。</p>
<script type="math/tex; mode=display">y_i = \gamma \hat{x_i} + \beta</script><p>如下图所示，展示了BN变换的过程。<br><img src="/img/caffe_bn_what_is_bn.jpg" alt="BN变换"></p>
<p>上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。</p>
<h2 id="BN-in-Caffe"><a href="#BN-in-Caffe" class="headerlink" title="BN in Caffe"></a>BN in Caffe</h2><p>在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。</p>
<p>下面的这段代码取自He Kaiming的Residual Net50的<a href="https://github.com/KaimingHe/deep-residual-networks/blob/master/prototxt/ResNet-50-deploy.prototxt#L21" target="_blank" rel="noopener">模型定义文件</a>。在这里，设置<code>batch_norm_param</code>中<code>use_global_stats</code>为<code>true</code>，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的<code>bias_term: true</code>是不可省略的。这个选项将其配置为线性变换层。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">	bottom: &quot;conv1&quot;</span><br><span class="line">	top: &quot;conv1&quot;</span><br><span class="line">	name: &quot;bn_conv1&quot;</span><br><span class="line">	type: &quot;BatchNorm&quot;</span><br><span class="line">	batch_norm_param &#123;</span><br><span class="line">		use_global_stats: true</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">layer &#123;</span><br><span class="line">	bottom: &quot;conv1&quot;</span><br><span class="line">	top: &quot;conv1&quot;</span><br><span class="line">	name: &quot;scale_conv1&quot;</span><br><span class="line">	type: &quot;Scale&quot;</span><br><span class="line">	scale_param &#123;</span><br><span class="line">		bias_term: true</span><br><span class="line">	&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。</p>
<h2 id="BatchNorm-层的实现"><a href="#BatchNorm-层的实现" class="headerlink" title="BatchNorm 层的实现"></a>BatchNorm 层的实现</h2><p>上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。</p>
<h3 id="proto定义的相关参数"><a href="#proto定义的相关参数" class="headerlink" title="proto定义的相关参数"></a>proto定义的相关参数</h3><p>我们首先看一下<code>caffe.proto</code>中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">message BatchNormParameter &#123;</span><br><span class="line">  // If false, normalization is performed over the current mini-batch</span><br><span class="line">  // and global statistics are accumulated (but not yet used) by a moving</span><br><span class="line">  // average.</span><br><span class="line">  // If true, those accumulated mean and variance values are used for the</span><br><span class="line">  // normalization.</span><br><span class="line">  // By default, it is set to false when the network is in the training</span><br><span class="line">  // phase and true when the network is in the testing phase.</span><br><span class="line">  // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是</span><br><span class="line">  // 当前batch的均值和方差。</span><br><span class="line">  // 设置为True，使用全局统计量做规范化。</span><br><span class="line">  // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。</span><br><span class="line">  // 当train时为false，当test时为true。</span><br><span class="line">  optional bool use_global_stats = 1;</span><br><span class="line">  </span><br><span class="line">  // What fraction of the moving average remains each iteration?</span><br><span class="line">  // Smaller values make the moving average decay faster, giving more</span><br><span class="line">  // weight to the recent values.</span><br><span class="line">  // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the</span><br><span class="line">  // current mean @f$ Y_t @f$ by</span><br><span class="line">  // @f$ S_t = (1-\beta)Y_t + \beta \cdot S_&#123;t-1&#125; @f$, where @f$ \beta @f$</span><br><span class="line">  // is the moving_average_fraction parameter.</span><br><span class="line">  // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是</span><br><span class="line">  // St = (1-beta)*Yt + beta*S_&#123;t-1&#125;</span><br><span class="line">  // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差</span><br><span class="line">  // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。</span><br><span class="line">  optional float moving_average_fraction = 2 [default = .999];</span><br><span class="line">  </span><br><span class="line">  // Small value to add to the variance estimate so that we don&apos;t divide by</span><br><span class="line">  // zero.</span><br><span class="line">  // 防止除数为0加上去的eps</span><br><span class="line">  optional float eps = 3 [default = 1e-5];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解<code>use_global_stats</code>变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。</p>
<h3 id="SetUp"><a href="#SetUp" class="headerlink" title="SetUp"></a>SetUp</h3><p>BN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了<code>use_global_stats</code>标志，则会使用用户给定的配置。所以一般在使用BN时，无需对<code>use_global_stats</code>进行配置。</p>
<p>这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的<a href="https://wiki2.org/en/Moving_average" target="_blank" rel="noopener">moving average条目</a>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::LayerSetUp(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">      <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  BatchNormParameter param = <span class="keyword">this</span>-&gt;layer_param_.batch_norm_param();</span><br><span class="line">  moving_average_fraction_ = param.moving_average_fraction();</span><br><span class="line">  <span class="comment">// 默认根据当前是否处在TEST模式而决定是否使用全局mean和var</span></span><br><span class="line">  use_global_stats_ = <span class="keyword">this</span>-&gt;phase_ == TEST;</span><br><span class="line">  <span class="keyword">if</span> (param.has_use_global_stats())</span><br><span class="line">    use_global_stats_ = param.use_global_stats();</span><br><span class="line">  <span class="comment">// 得到channels数量</span></span><br><span class="line">  <span class="comment">// 为了防止越界，首先检查输入是否为1D</span></span><br><span class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>]-&gt;num_axes() == <span class="number">1</span>)</span><br><span class="line">    channels_ = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">else</span></span><br><span class="line">    channels_ = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">1</span>);</span><br><span class="line">  eps_ = param.eps();</span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;blobs_.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    LOG(INFO) &lt;&lt; <span class="string">"Skipping parameter initialization"</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 参数共3个</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_.resize(<span class="number">3</span>);</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; sz;</span><br><span class="line">    sz.push_back(channels_);</span><br><span class="line">    <span class="comment">// mean 和var都是1D的长度为channels的向量</span></span><br><span class="line">    <span class="comment">// 因为在规范化过程中，要逐channel进行，即：</span></span><br><span class="line">    <span class="comment">// for c in range(channels):</span></span><br><span class="line">    <span class="comment">//     x_hat[c] = (x[c] - mean[c]) / std[c]</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</span><br><span class="line">    <span class="comment">// 这里的解释见下</span></span><br><span class="line">    sz[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>].reset(<span class="keyword">new</span> Blob&lt;Dtype&gt;(sz));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">3</span>; ++i) &#123;</span><br><span class="line">      caffe_set(<span class="keyword">this</span>-&gt;blobs_[i]-&gt;count(), Dtype(<span class="number">0</span>),</span><br><span class="line">                <span class="keyword">this</span>-&gt;blobs_[i]-&gt;mutable_cpu_data());</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// Mask statistics from optimization by setting local learning rates</span></span><br><span class="line">  <span class="comment">// for mean, variance, and the bias correction to zero.</span></span><br><span class="line">  <span class="comment">// mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate</span></span><br><span class="line">  <span class="comment">// 设置为0</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="keyword">this</span>-&gt;blobs_.size(); ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (<span class="keyword">this</span>-&gt;layer_param_.param_size() == i) &#123;</span><br><span class="line">      ParamSpec* fixed_param_spec = <span class="keyword">this</span>-&gt;layer_param_.add_param();</span><br><span class="line">      fixed_param_spec-&gt;set_lr_mult(<span class="number">0.f</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      CHECK_EQ(<span class="keyword">this</span>-&gt;layer_param_.param(i).lr_mult(), <span class="number">0.f</span>)</span><br><span class="line">          &lt;&lt; <span class="string">"Cannot configure batch normalization statistics as layer "</span></span><br><span class="line">          &lt;&lt; <span class="string">"parameters."</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\alpha$来做平滑滤波，如下所示：</p>
<script type="math/tex; mode=display">S_t = \alpha Y_t + (1-\alpha) S_{t-1}</script><p>上面的式子等价于：</p>
<script type="math/tex; mode=display">S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n}</script><p>其中，<script type="math/tex">\text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1}</script></p>
<script type="math/tex; mode=display">\text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1}</script><p>而Caffe中BN的实现中，<code>blobs_[0]</code>和<code>blobs_[1]</code>中存储的实际是$\text{WeightedSum}_n$，而<code>blos_[2]</code>中存储的是$\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mu = blobs_[0] / blobs_[2]</span><br><span class="line">var = blobs_[1] / blobs_[2]</span><br></pre></td></tr></table></figure></p>
<h3 id="Forward"><a href="#Forward" class="headerlink" title="Forward"></a>Forward</h3><p>下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="keyword">void</span> BatchNormLayer&lt;Dtype&gt;::Forward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123;</span><br><span class="line">  <span class="keyword">const</span> Dtype* bottom_data = bottom[<span class="number">0</span>]-&gt;cpu_data();</span><br><span class="line">  Dtype* top_data = top[<span class="number">0</span>]-&gt;mutable_cpu_data();</span><br><span class="line">  <span class="keyword">int</span> num = bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">int</span> spatial_dim = bottom[<span class="number">0</span>]-&gt;count()/(bottom[<span class="number">0</span>]-&gt;shape(<span class="number">0</span>)*channels_);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果不是就地操作，首先将bottom的数据复制到top</span></span><br><span class="line">  <span class="keyword">if</span> (bottom[<span class="number">0</span>] != top[<span class="number">0</span>]) &#123;</span><br><span class="line">    caffe_copy(bottom[<span class="number">0</span>]-&gt;count(), bottom_data, top_data);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 如果使用全局统计量，我们需要先计算出真正的mean和var</span></span><br><span class="line">  <span class="keyword">if</span> (use_global_stats_) &#123;</span><br><span class="line">    <span class="comment">// use the stored mean/variance estimates.</span></span><br><span class="line">    <span class="keyword">const</span> Dtype scale_factor = <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>] == <span class="number">0</span> ?</span><br><span class="line">        <span class="number">0</span> : <span class="number">1</span> / <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;cpu_data()[<span class="number">0</span>];</span><br><span class="line">    <span class="comment">// mean = blobs[0] / blobs[2]</span></span><br><span class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</span><br><span class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;cpu_data(), mean_.mutable_cpu_data());</span><br><span class="line">    <span class="comment">// var = blobs[1] / blobs[2]</span></span><br><span class="line">    caffe_cpu_scale(variance_.count(), scale_factor,</span><br><span class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;cpu_data(), variance_.mutable_cpu_data());</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="comment">// 不使用全局统计量时，我们要根据当前batch的mean和var做规范化</span></span><br><span class="line">    <span class="comment">// compute mean</span></span><br><span class="line">    <span class="comment">// spatial_sum_multiplier_是全1向量</span></span><br><span class="line">    <span class="comment">// batch_sum_multiplier_也是全1向量</span></span><br><span class="line">    <span class="comment">// gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。</span></span><br><span class="line">    <span class="comment">// 下面式子是将bottom_data这个矩阵与一个全1向量相乘，</span></span><br><span class="line">    <span class="comment">// 相当于是在统计行和。</span></span><br><span class="line">    <span class="comment">// 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数</span></span><br><span class="line">    <span class="comment">// 所以这是在计算每个channel的feature map的和</span></span><br><span class="line">    <span class="comment">// 结果out[n][c]是指输入第n个sample的第c个channel的和</span></span><br><span class="line">    <span class="comment">// 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面</span></span><br><span class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</span><br><span class="line">        <span class="number">1.</span> / (num * spatial_dim), bottom_data,</span><br><span class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">        num_by_chans_.mutable_cpu_data());</span><br><span class="line">    <span class="comment">// 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和</span></span><br><span class="line">    <span class="comment">// 这样，就求出了各个channel的和。</span></span><br><span class="line">    <span class="comment">// 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量</span></span><br><span class="line">    <span class="comment">// 到此，我们就完成了对当前batch的平均值的求解</span></span><br><span class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</span><br><span class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">        mean_.mutable_cpu_data());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// subtract mean</span></span><br><span class="line">  <span class="comment">// gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C</span></span><br><span class="line">  <span class="comment">// 下面这个是在做broadcasting subtraction</span></span><br><span class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">      batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">      num_by_chans_.mutable_cpu_data());</span><br><span class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</span><br><span class="line">      spatial_dim, <span class="number">1</span>, <span class="number">-1</span>, num_by_chans_.cpu_data(),</span><br><span class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">1.</span>, top_data);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 计算当前的var</span></span><br><span class="line">  <span class="keyword">if</span> (!use_global_stats_) &#123;</span><br><span class="line">    <span class="comment">// compute variance using var(X) = E((X-EX)^2)</span></span><br><span class="line">    caffe_sqr&lt;Dtype&gt;(top[<span class="number">0</span>]-&gt;count(), top_data,</span><br><span class="line">                     temp_.mutable_cpu_data());  <span class="comment">// (X-EX)^2</span></span><br><span class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim,</span><br><span class="line">        <span class="number">1.</span> / (num * spatial_dim), temp_.cpu_data(),</span><br><span class="line">        spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">        num_by_chans_.mutable_cpu_data());</span><br><span class="line">    caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, <span class="number">1.</span>,</span><br><span class="line">        num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">        variance_.mutable_cpu_data());  <span class="comment">// E((X_EX)^2)</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">// compute and save moving average</span></span><br><span class="line">    <span class="comment">// 做滑动平均，更新全局统计量，这里可以参见上面的式子</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] *= moving_average_fraction_;</span><br><span class="line">    <span class="keyword">this</span>-&gt;blobs_[<span class="number">2</span>]-&gt;mutable_cpu_data()[<span class="number">0</span>] += <span class="number">1</span>;</span><br><span class="line">    caffe_cpu_axpby(mean_.count(), Dtype(<span class="number">1</span>), mean_.cpu_data(),</span><br><span class="line">        moving_average_fraction_, <span class="keyword">this</span>-&gt;blobs_[<span class="number">0</span>]-&gt;mutable_cpu_data());</span><br><span class="line">    <span class="keyword">int</span> m = bottom[<span class="number">0</span>]-&gt;count()/channels_;</span><br><span class="line">    Dtype bias_correction_factor = m &gt; <span class="number">1</span> ? Dtype(m)/(m<span class="number">-1</span>) : <span class="number">1</span>;</span><br><span class="line">    caffe_cpu_axpby(variance_.count(), bias_correction_factor,</span><br><span class="line">        variance_.cpu_data(), moving_average_fraction_,</span><br><span class="line">        <span class="keyword">this</span>-&gt;blobs_[<span class="number">1</span>]-&gt;mutable_cpu_data());</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// normalize variance</span></span><br><span class="line">  caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data());</span><br><span class="line">  caffe_sqrt(variance_.count(), variance_.cpu_data(),</span><br><span class="line">             variance_.mutable_cpu_data());</span><br><span class="line"></span><br><span class="line">  <span class="comment">// replicate variance to input size</span></span><br><span class="line">  <span class="comment">// 同样是在做broadcasting</span></span><br><span class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, <span class="number">1</span>, <span class="number">1</span>,</span><br><span class="line">      batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), <span class="number">0.</span>,</span><br><span class="line">      num_by_chans_.mutable_cpu_data());</span><br><span class="line">  caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num,</span><br><span class="line">      spatial_dim, <span class="number">1</span>, <span class="number">1.</span>, num_by_chans_.cpu_data(),</span><br><span class="line">      spatial_sum_multiplier_.cpu_data(), <span class="number">0.</span>, temp_.mutable_cpu_data());</span><br><span class="line">  caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data);</span><br><span class="line">  <span class="comment">// TODO(cdoersch): The caching is only needed because later in-place layers</span></span><br><span class="line">  <span class="comment">//                 might clobber the data.  Can we skip this if they won't?</span></span><br><span class="line">  caffe_copy(x_norm_.count(), top_data,</span><br><span class="line">      x_norm_.mutable_cpu_data());</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>由上面的计算过程不难得出，当经过很多轮迭代之后，<code>blobs_[2]</code>的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的<code>blobs_[2]</code>的值，也就是$\text{WeightedCount}_n$，使用$\alpha$表示<code>moving_average_fraction_</code>，那么我们有：</p>
<script type="math/tex; mode=display">m_t = 1 + \alpha m_{t-1}</script><p>可以求取$m_t$的通项后令$t=\infty$，可以得到，$m_{\infty}=\frac{1}{1-\alpha}$。</p>
<h3 id="Backward"><a href="#Backward" class="headerlink" title="Backward"></a>Backward</h3><p>在做BP的时候，我们需要分情况讨论。</p>
<ul>
<li>当<code>use_global_stats == true</code>的时候，BN所做的操作是一个线性变换<script type="math/tex; mode=display">BN(x) = \frac{x-\mu}{\sqrt{Var}}</script>所以<script type="math/tex; mode=display">\frac{\partial L}{\partial x} = \frac{1}{\sqrt{Var}}\frac{\partial L}{\partial y}</script></li>
</ul>
<p>对应的代码如下。其中，<code>temp_</code>是broadcasting之后的输入<code>x</code>的标准差（见上面<code>Forward</code>部分的代码最后），做逐元素的除法即可。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (use_global_stats_) &#123;</span><br><span class="line">  caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff);</span><br><span class="line">  <span class="keyword">return</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>当<code>use_global_stats == false</code>的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入<code>x</code>的函数。所以就麻烦了不少。这里我并没有推导，而是看了<a href="https://kevinzakka.github.io/2016/09/14/batch_normalization/" target="_blank" rel="noopener">这篇博客</a>，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。<br><img src="/img/caffe_bn_bp_of_bn.jpg" alt="BP的推导结果"></li>
</ul>
<p>我们使用$y$来代替上面的$\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子：</p>
<script type="math/tex; mode=display">\frac{\partial f}{\partial x_i} = \frac{\frac{\partial f}{\partial y}-E[\frac{\partial f}{\partial y}]-yE[\frac{\partial f}{\partial y}y]}{\sqrt{\sigma^2+\epsilon}}</script><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// dE(Y)/dX =</span></span><br><span class="line"><span class="comment">//   (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)</span></span><br><span class="line"><span class="comment">//     ./ sqrt(var(X) + eps)</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">// where \cdot and ./ are hadamard product and elementwise division,</span></span><br><span class="line"><span class="comment">// respectively, dE/dY is the top diff, and mean/var/sum are all computed</span></span><br><span class="line"><span class="comment">// along all dimensions except the channels dimension.  In the above</span></span><br><span class="line"><span class="comment">// equation, the operations allow for expansion (i.e. broadcast) along all</span></span><br><span class="line"><span class="comment">// dimensions except the channels dimension where required.</span></span><br></pre></td></tr></table></figure>
<p>下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。</p>
<h2 id="Scale层的实现"><a href="#Scale层的实现" class="headerlink" title="Scale层的实现"></a>Scale层的实现</h2><p>Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式：</p>
<ul>
<li>当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。</li>
<li>当输入blob为一个时，计算输入blob与一个可学习参数<code>gamma</code>的按元素相乘结果。</li>
<li>当设置<code>bias_term: true</code>时，添加一个偏置项。</li>
</ul>
<p>用于BN的线性变换的计算方法很直接，这里不再多说了。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>shell编程</title>
    <url>/2017/11/10/shell-programming/</url>
    <content><![CDATA[<p>介绍基本的shell编程方法，参考的教程是<a href="http://www.freeos.com/guides/lsst/" target="_blank" rel="noopener">Linux Shell Scripting Tutorial, A Beginner’s handbook</a>。<br><img src="/img/shell-programming-bash-logo.png" alt="Bash Logo"><br><a id="more"></a></p>
<h2 id="变量"><a href="#变量" class="headerlink" title="变量"></a>变量</h2><p>变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。</p>
<p>shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。</p>
<p>在代码中使用变量值的时候，需要在前面加上<code>$</code>。<code>echo</code>命令可以在控制台打印相应输出。所以使用<code>echo $var</code>就可以输出变量<code>var</code>的值。</p>
<h3 id="系统变量"><a href="#系统变量" class="headerlink" title="系统变量"></a>系统变量</h3><p>系统变量是指Linux中自带的一些变量。例如<code>HOME</code>,<code>PATH</code>等。其中<code>PATH</code>又叫环境变量。更多的系统变量见下表：<br><img src="/img/shell-programming-system-variables.jpg" alt="系统变量列表"></p>
<h3 id="用户定义的变量"><a href="#用户定义的变量" class="headerlink" title="用户定义的变量"></a>用户定义的变量</h3><p>用户自定义变量是用户命名并赋值的变量。使用下面的方法定义：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意不要在等号两边插入空格</span></span><br><span class="line">name=value</span><br><span class="line"><span class="comment"># 如 n=10</span></span><br></pre></td></tr></table></figure>
<h3 id="局部变量和全局变量"><a href="#局部变量和全局变量" class="headerlink" title="局部变量和全局变量"></a>局部变量和全局变量</h3><p>局部变量是指在当前代码块内可见的变量，使用<code>local</code>声明。例如下面的代码，将依次输出：111, 222, 111.<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/sh</span></span><br><span class="line">num=111 <span class="comment"># 全局变量</span></span><br><span class="line">func1()</span><br><span class="line">&#123;</span><br><span class="line">  <span class="built_in">local</span> num=222 <span class="comment"># 局部变量</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="variable">$num</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"before---<span class="variable">$num</span>"</span></span><br><span class="line">func1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"after---<span class="variable">$num</span>"</span></span><br></pre></td></tr></table></figure></p>
<h3 id="变量之间的运算"><a href="#变量之间的运算" class="headerlink" title="变量之间的运算"></a>变量之间的运算</h3><p>使用<code>expr</code>可以进行变量之间的运算，如下所示：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 注意要在操作符两边空余空格</span></span><br><span class="line">expr 1 + 3</span><br><span class="line"><span class="comment"># 由于*是特殊字符，所以乘法要使用转义</span></span><br><span class="line">expr 10 \* 2</span><br></pre></td></tr></table></figure>
<h3 id="和””"><a href="#和””" class="headerlink" title="``和””"></a>``和””</h3><p>使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a=`expr 10 \* 3`</span><br><span class="line"><span class="comment"># output: 3</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$a</span></span><br><span class="line"><span class="comment"># output: a</span></span><br><span class="line"><span class="built_in">echo</span> a</span><br><span class="line"><span class="comment"># output: expr 10 \* 3</span></span><br><span class="line">a=<span class="string">"expr 10 \* 3"</span></span><br><span class="line"><span class="built_in">echo</span> <span class="variable">$a</span></span><br></pre></td></tr></table></figure>
<p>另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">a=1</span><br><span class="line"><span class="built_in">echo</span> <span class="string">"<span class="variable">$a</span>"</span>  <span class="comment"># 输出 1</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'$a'</span>  <span class="comment"># 输出 $a</span></span><br></pre></td></tr></table></figure>
<h3 id="读取输入"><a href="#读取输入" class="headerlink" title="读取输入"></a>读取输入</h3><p>使用<code>read var1, var2, ...</code>的方式从键盘的输入读取变量的值。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># input a=1</span></span><br><span class="line"><span class="built_in">read</span> a</span><br><span class="line"><span class="comment"># ouptut: 2</span></span><br><span class="line"><span class="built_in">echo</span> `expr <span class="variable">$a</span> + 1`</span><br></pre></td></tr></table></figure>
<h2 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h2><h3 id="命令的返回值"><a href="#命令的返回值" class="headerlink" title="命令的返回值"></a>命令的返回值</h3><p>当bash命令成功执行后，返回给系统的返回值为<code>0</code>；否则为非零。可以据此判断上步操作的状态。使用<code>$?</code>可以取出上一步执行的返回值。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 将echo 错输为ecoh</span></span><br><span class="line">ecoh <span class="string">"hello"</span></span><br><span class="line"><span class="comment"># output: 非零(127)</span></span><br><span class="line"><span class="built_in">echo</span> $?</span><br><span class="line"><span class="comment"># output: 0</span></span><br><span class="line"><span class="built_in">echo</span> $?</span><br></pre></td></tr></table></figure>
<h3 id="通配符"><a href="#通配符" class="headerlink" title="通配符"></a>通配符</h3><p>通配符是指<code>*</code>,<code>?</code>和<code>[...]</code>这三类。</p>
<p><code>*</code>可以匹配任意多的字符，<code>?</code>用来匹配一个字符。<code>[...]</code>用来匹配括号内的字符。见下表。<br><img src="/img/shell-programming-wild-cards.jpg" alt="通配符"></p>
<p><code>[...]</code>表示法还有如下变形：</p>
<ul>
<li>使用<code>-</code>用来指示范围。如<code>[a-z]</code>，表示<code>a</code>到<code>z</code>间任意一个字符。</li>
<li>使用<code>^</code>或<code>!</code>表示取反。如<code>[!a-p]</code>表示除了<code>a</code>到<code>p</code>间字符的其他字符。</li>
</ul>
<h3 id="输入输出重定向"><a href="#输入输出重定向" class="headerlink" title="输入输出重定向"></a>输入输出重定向</h3><p>重定向是指改变命令的输出位置。使用<code>&gt;</code>进行输出重定向。使用<code>&lt;</code>进行输入重定向。例如，<code>ls -l &gt; a.txt</code>，将本目录下的文件信息输出到文本文件<code>a.txt</code>中，而不再输出到终端。</p>
<p>此外，<code>&gt;&gt;</code>同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。</p>
<p>搭配使用<code>&lt;</code>和<code>&gt;</code>可以做文件处理。例如，<code>tr group1 group2</code>命令可以将<code>group1</code>中的字符变换为<code>group2</code>中对应位置的字符。使用如下命令：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span> &lt; ori.txt &gt; out.txt</span><br></pre></td></tr></table></figure>
<p>可以将<code>ori.txt</code>中的小写字母转换为大写字母输出到<code>out.txt</code>中。</p>
<h3 id="管道（pipeline）"><a href="#管道（pipeline）" class="headerlink" title="管道（pipeline）"></a>管道（pipeline）</h3><p>管道<code>|</code>可以将第一个程序的输出作为第二个程序的输入。例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">cat ori.txt | tr <span class="string">"[a-z]"</span> <span class="string">"A-Z"</span></span><br></pre></td></tr></table></figure>
<p>会将<code>ori.txt</code>中的小写字母转换为大写，并在终端输出。</p>
<h3 id="过滤器（Filter）"><a href="#过滤器（Filter）" class="headerlink" title="过滤器（Filter）"></a>过滤器（Filter）</h3><p>Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">sort &lt; names.txt | uniq &gt; u_names.txt</span><br></pre></td></tr></table></figure>
<p><code>uniq</code>命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件<code>names.txt</code>中的行文本去重后输出到<code>u_names.txt</code>中去。</p>
<h2 id="控制流"><a href="#控制流" class="headerlink" title="控制流"></a>控制流</h2><h3 id="if-条件控制"><a href="#if-条件控制" class="headerlink" title="if 条件控制"></a>if 条件控制</h3><p>在bash中使用<code>if</code>条件控制的语法和MATLAB等很像，要在末尾加上类似<code>end</code>的指示符，如下：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> condition</span><br><span class="line"><span class="keyword">then</span> </span><br><span class="line">XXX</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure>
<p>或者加上<code>else</code>，使用如下的形式：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> condition</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="keyword">do</span> something</span><br><span class="line"><span class="keyword">elif</span> condition</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="keyword">do</span> something</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="keyword">do</span> something</span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
<p>那么，如何做逻辑运算呢？需要借助<code>test</code>关键字。</p>
<p>对于整数来说，我们可以使用<code>if test op1 oprator op2</code>的方式，判断操作数<code>op1</code>和<code>op2</code>的大小关系。其中，<code>operator</code>可以是<code>-gt</code>，<code>-eq</code>等。</p>
<p>或者另一种写法：<code>if [ op1 operator op2 ]</code>，但是注意后者<code>[]</code>与操作数之间有空格。<br>如下表所示（点击可放大）：</p>
<p><img src="/img/shell-programming-if-operators.jpg" alt="比较整数的逻辑运算"></p>
<p>对于字符串，支持的逻辑判断如下：<br><img src="/img/bash-programming-comparing-string.jpg" alt="比较字符串的逻辑运算"></p>
<p>举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意<code>[]</code>的两边一定要加空格。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line">a=1</span><br><span class="line"><span class="keyword">if</span> [ <span class="variable">$1</span>=<span class="variable">$a</span> ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"you input 1"</span></span><br><span class="line"><span class="keyword">elif</span> [ <span class="variable">$1</span>=2 ]</span><br><span class="line"><span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"you input 2"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"you input <span class="variable">$1</span>"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>过秦论</title>
    <url>/2017/10/27/fuck-gfw/</url>
    <content><![CDATA[<p>秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。<br><img src="/img/just-a-joke.png" alt="just a joke"></p>
<a id="more"></a>
<p>　　秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 </p>
<p>　　孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 </p>
<p>　　延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。</p>
<p>　　始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 </p>
<p>　　且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。</p>
]]></content>
      <tags>
        <tag>扯淡</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu/Mac 工具软件列表</title>
    <url>/2017/10/22/useful-tools-list/</url>
    <content><![CDATA[<p>工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。<br><a id="more"></a></p>
<h2 id="zsh和Oh-my-zsh"><a href="#zsh和Oh-my-zsh" class="headerlink" title="zsh和Oh-my-zsh"></a>zsh和Oh-my-zsh</h2><p>如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用<a href="https://github.com/robbyrussell/oh-my-zsh" target="_blank" rel="noopener">oh-my-zsh</a>武装zsh。</p>
<p>关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了<code>colored-man-pages</code>。顾名思义，它可以将使用<code>man</code>查询时的页面彩色输出。如下所示。<br><img src="/img/useful_tools_colored_man_pages.jpg" alt="彩色的cp man页面"></p>
<h2 id="autojump"><a href="#autojump" class="headerlink" title="autojump"></a>autojump</h2><p>使用<a href="https://github.com/wting/autojump" target="_blank" rel="noopener">autojump</a>，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。</p>
<p>除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 跳转到path并使用文件资源管理器打开</span><br><span class="line">jo path</span><br></pre></td></tr></table></figure></p>
<h2 id="tldr"><a href="#tldr" class="headerlink" title="tldr"></a>tldr</h2><p><a href="https://github.com/tldr-pages/tldr" target="_blank" rel="noopener">tldr</a> (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。</p>
<p>例如，使用其查询<code>tar</code>的常用方法：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">tldr tar</span><br><span class="line"><span class="comment"># output</span></span><br><span class="line">tar</span><br><span class="line"></span><br><span class="line">Archiving utility.</span><br><span class="line">Often combined with a compression method, such as gzip or bzip.</span><br><span class="line"></span><br><span class="line">- Create an archive from files:</span><br><span class="line">    tar cf target.tar file1 file2 file3</span><br><span class="line"></span><br><span class="line">- Create a gzipped archive:</span><br><span class="line">    tar czf target.tar.gz file1 file2 file3</span><br><span class="line"></span><br><span class="line">- Extract an archive <span class="keyword">in</span> a target folder:</span><br><span class="line">    tar xf source.tar -C folder</span><br><span class="line"></span><br><span class="line">- Extract a gzipped archive <span class="keyword">in</span> the current directory:</span><br><span class="line">    tar xzf source.tar.gz</span><br><span class="line"></span><br><span class="line">- Extract a bzipped archive <span class="keyword">in</span> the current directory:</span><br><span class="line">    tar xjf source.tar.bz2</span><br><span class="line"></span><br><span class="line">- Create a compressed archive, using archive suffix to determine the compression program:</span><br><span class="line">    tar caf target.tar.xz file1 file2 file3</span><br><span class="line"></span><br><span class="line">- List the contents of a tar file:</span><br><span class="line">    tar tvf source.tar</span><br></pre></td></tr></table></figure>
<p>tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是<a href="https://github.com/lord63/tldr.py" target="_blank" rel="noopener">tldr-py</a>。</p>
<h2 id="tmux"><a href="#tmux" class="headerlink" title="tmux"></a>tmux</h2><p>用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。</p>
<p>虽然Ubuntu14.04可以通过<code>apt-get</code>的方式安装tmux，不过为了能够使用一款好用的配置<a href="https://github.com/gpakosz/.tmux" target="_blank" rel="noopener">oh-my-tmux</a>（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装<a href="https://github.com/tmux/tmux" target="_blank" rel="noopener">tmux</a>。如果使用Ubuntu16.04及以上版本，那么官方源中的tmux已经足够新，无需自己编译安装。</p>
<h2 id="guake"><a href="#guake" class="headerlink" title="guake"></a>guake</h2><p><a href="https://github.com/Guake/guake" target="_blank" rel="noopener">guake</a>是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。</p>
<h2 id="Dash-Zeal"><a href="#Dash-Zeal" class="headerlink" title="Dash/Zeal"></a>Dash/Zeal</h2><p>Dash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇<a href="https://xmfbit.github.io/2017/08/26/doc2dash-usage/">博客</a>，介绍如何自己制作文档导入Zeal中。</p>
<h2 id="sshfs"><a href="#sshfs" class="headerlink" title="sshfs"></a>sshfs</h2><p>使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用<code>rm</code>）。</p>
<h2 id="Alfred-Mutate"><a href="#Alfred-Mutate" class="headerlink" title="Alfred/Mutate"></a>Alfred/Mutate</h2><p>Alfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生<br>功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。</p>
<p><a href="https://github.com/qdore/Mutate" target="_blank" rel="noopener">Mutate</a>是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。</p>
<h2 id="Mos"><a href="#Mos" class="headerlink" title="Mos"></a>Mos</h2><p>Mac笔记本的触摸板默认的竖直滚动方向是和触摸屏一样的，也就是手指向上移动，页面向下滚动。习惯了这种方式的话，如果插上外接鼠标就会显得很不方便，因为系统默认鼠标也是按照这种逻辑。但是实际上，很多人更习惯于向下滚动滚轮，页面向下滚动。另外，鼠标在Mac上的滚动不是很自然。<a href="https://github.com/Caldis/Mos" target="_blank" rel="noopener">Mos</a>是一款国内开发者开源的调节鼠标滚动方向的工具软件。</p>
<blockquote>
<p>一个用于在 MacOS 上平滑你的鼠标滚动效果或单独设置滚动方向的小工具, 让你的滚轮爽如触控板 | A lightweight tool used to smooth scrolling and set scroll direction independently for your mouse on MacOS <a href="http://mos.caldis.me" target="_blank" rel="noopener">http://mos.caldis.me</a></p>
</blockquote>
<p>可以使用下面的命令进行安装，或自行前往<a href="https://github.com/Caldis/Mos/releases/" target="_blank" rel="noopener">Release页面</a>下载安装。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">brew cask install mos</span><br></pre></td></tr></table></figure></p>
]]></content>
  </entry>
  <entry>
    <title>学一点PyQT</title>
    <url>/2017/08/29/learn-pyqt/</url>
    <content><![CDATA[<p>Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读<a href="http://zetcode.com/gui/pyqt5/introduction/" target="_blank" rel="noopener">PyQT5的一篇教程</a>时候的记录。<br><a id="more"></a></p>
<h2 id="主要模块"><a href="#主要模块" class="headerlink" title="主要模块"></a>主要模块</h2><p>PyQt5中的主要三个模块如下：</p>
<ul>
<li><code>QtCore</code>: 和GUI无关的核心功能：文件，时间，多线程等</li>
<li><code>QtGui</code>：和GUI相关的的东西：事件处理，2D图形，字体和文本等</li>
<li><code>QtWidget</code>：GUI中的相关组件，例如按钮，窗口等。</li>
</ul>
<p>其他模块还有<code>QtBluetooth</code>，<code>QtNetwork</code>等，都是比较专用的模块，用到再说。</p>
<h2 id="HelloWorld"><a href="#HelloWorld" class="headerlink" title="HelloWorld"></a>HelloWorld</h2><p>这里首先给出一段简单的程序，可以在桌面上显示一个窗口。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    w = QWidget()</span><br><span class="line">    w.resize(<span class="number">250</span>, <span class="number">150</span>)</span><br><span class="line">    w.move(<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">    w.setWindowTitle(<span class="string">'Simple'</span>)</span><br><span class="line">    w.show()</span><br><span class="line"></span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure></p>
<p>下面介绍上面代码的含义：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">app = QApplication(sys.argv)</span><br></pre></td></tr></table></figure></p>
<p>每个Qt5应用必须首先创建一个application，后面会用到。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">w = QWidget()</span><br><span class="line">w.resize(<span class="number">250</span>, <span class="number">150</span>)</span><br><span class="line">w.move(<span class="number">300</span>, <span class="number">300</span>)</span><br><span class="line">w.setWindowTitle(<span class="string">'Simple'</span>)</span><br><span class="line">w.show()</span><br></pre></td></tr></table></figure></p>
<p><code>QtWidget</code>是所有组件的父类，我们创建了一个<code>Widget</code>。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用<code>show()</code>将其显示出来。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>
<p>进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。</p>
<h2 id="添加一个按钮"><a href="#添加一个按钮" class="headerlink" title="添加一个按钮"></a>添加一个按钮</h2><p>下面，我们为窗口添加按钮，并为其添加事件响应动作。</p>
<p>参考文档可知，按钮<code>QPushButton</code>存在这样的构造函数：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">__init__ (self, QWidget parent = <span class="literal">None</span>)</span><br></pre></td></tr></table></figure></p>
<p>下面的代码在初始化<code>QPushButton</code>实例<code>btn</code>时，将<code>self</code>作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了<code>sizeHint()</code>方法自适应调节其大小。</p>
<p>同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号<code>clicked</code>会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了<code>__call__()</code>方法的对象）。</p>
<p>我们调用了现成的处理函数，来达到关闭窗口的目的。使用<code>instance()</code>可以得到当前application实例，调用其<code>quit()</code>方法即是退出当前应用，自然窗口就被关闭了。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QApplication, QWidget, QPushButton, QToolTip</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtCore <span class="keyword">import</span> QCoreApplication</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyWindow, self).__init__()</span><br><span class="line">        self._init_ui()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></span><br><span class="line">        btn = QPushButton(<span class="string">'quit'</span>, self)</span><br><span class="line">        btn.clicked.connect(QCoreApplication.instance().quit)</span><br><span class="line">        btn.setToolTip(<span class="string">'This is a &lt;b&gt;QPushButton&lt;/b&gt; widget'</span>)</span><br><span class="line">        btn.move(<span class="number">50</span>, <span class="number">50</span>)</span><br><span class="line">        btn.resize(btn.sizeHint())</span><br><span class="line"></span><br><span class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</span><br><span class="line">        self.setWindowTitle(<span class="string">'Window with Button'</span>)</span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    window = MyWindow()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure></p>
<h2 id="使用Event处理事件"><a href="#使用Event处理事件" class="headerlink" title="使用Event处理事件"></a>使用Event处理事件</h2><p>除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用<code>event.accept()</code>或<code>ignore()</code>完成对事件的处理。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> QWidget, QMessageBox, QApplication</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyWindow, self).__init__()</span><br><span class="line">        self._init_ui()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">200</span>)</span><br><span class="line">        self.show()</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">closeEvent</span><span class="params">(self, ev)</span>:</span></span><br><span class="line">        reply = QMessageBox.question(self, <span class="string">'Message'</span>, <span class="string">'Are you sure?'</span>,</span><br><span class="line">                    QMessageBox.Yes | QMessageBox.No, QMessageBox.No)</span><br><span class="line">        <span class="keyword">if</span> reply == QMessageBox.Yes:</span><br><span class="line">            ev.accept()</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ev.ignore()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    win = MyWindow()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>
<h2 id="使用Layout组织Widget"><a href="#使用Layout组织Widget" class="headerlink" title="使用Layout组织Widget"></a>使用Layout组织Widget</h2><p>组织Widget的方式可以通过绝对位置调整，但是更推荐使用<code>Layout</code>组织。</p>
<p>绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点：</p>
<ul>
<li>不同平台可能显示效果不统一；</li>
<li>当parent resize的时候，widget大小和位置并不会自动调整</li>
<li>编码太麻烦，牵一发而动全身</li>
</ul>
<p>下面介绍几种常见的<code>Layout</code>类。</p>
<h3 id="Box-Layout"><a href="#Box-Layout" class="headerlink" title="Box Layout"></a>Box Layout</h3><p>有<code>QVBoxLayout</code>和<code>QHBoxLayout</code>，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用<code>addSkretch()</code>方法将一个<code>QSpacerItem</code>实例对象插入到了layout中，占据了相应位置。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QPushButton,</span><br><span class="line">    QHBoxLayout, QVBoxLayout, QApplication)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyWindow, self).__init__()</span><br><span class="line">        self._init_ui()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></span><br><span class="line">        okButton = QPushButton(<span class="string">"OK"</span>)</span><br><span class="line">        cancelButton = QPushButton(<span class="string">"Cancel"</span>)</span><br><span class="line"></span><br><span class="line">        hbox = QHBoxLayout()</span><br><span class="line">        hbox.addStretch(<span class="number">1</span>)</span><br><span class="line">        hbox.addWidget(okButton)</span><br><span class="line">        hbox.addWidget(cancelButton)</span><br><span class="line"></span><br><span class="line">        vbox = QVBoxLayout()</span><br><span class="line">        vbox.addStretch(<span class="number">1</span>)</span><br><span class="line">        vbox.addLayout(hbox)</span><br><span class="line"></span><br><span class="line">        self.setLayout(vbox)    </span><br><span class="line"></span><br><span class="line">        self.setGeometry(<span class="number">300</span>, <span class="number">300</span>, <span class="number">300</span>, <span class="number">150</span>)</span><br><span class="line">        self.setWindowTitle(<span class="string">'Buttons'</span>)    </span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    win = MyWindow()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure>
<h3 id="Grid-Layout"><a href="#Grid-Layout" class="headerlink" title="Grid Layout"></a>Grid Layout</h3><p><code>QGridLayout</code>将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">from</span> PyQt5.QtWidgets <span class="keyword">import</span> (QWidget, QGridLayout,</span><br><span class="line">    QPushButton, QApplication)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MyWindow</span><span class="params">(QWidget)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MyWindow, self).__init__()</span><br><span class="line">        self._init_ui()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_init_ui</span><span class="params">(self)</span>:</span></span><br><span class="line">        grid = QGridLayout()</span><br><span class="line">        self.setLayout(grid)</span><br><span class="line">        names = [<span class="string">'Cls'</span>, <span class="string">'Bck'</span>, <span class="string">''</span>, <span class="string">'Close'</span>,</span><br><span class="line">                 <span class="string">'7'</span>, <span class="string">'8'</span>, <span class="string">'9'</span>, <span class="string">'/'</span>,</span><br><span class="line">                <span class="string">'4'</span>, <span class="string">'5'</span>, <span class="string">'6'</span>, <span class="string">'*'</span>,</span><br><span class="line">                 <span class="string">'1'</span>, <span class="string">'2'</span>, <span class="string">'3'</span>, <span class="string">'-'</span>,</span><br><span class="line">                <span class="string">'0'</span>, <span class="string">'.'</span>, <span class="string">'='</span>, <span class="string">'+'</span>]</span><br><span class="line">        positions = [(i,j) <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>) <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)]</span><br><span class="line">        <span class="keyword">for</span> position, name <span class="keyword">in</span> zip(positions, names):</span><br><span class="line">            <span class="keyword">if</span> name == <span class="string">''</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            button = QPushButton(name)</span><br><span class="line">            grid.addWidget(button, *position)</span><br><span class="line"></span><br><span class="line">        self.move(<span class="number">300</span>, <span class="number">150</span>)</span><br><span class="line">        self.setWindowTitle(<span class="string">'Calculator'</span>)</span><br><span class="line">        self.show()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line"></span><br><span class="line">    app = QApplication(sys.argv)</span><br><span class="line">    win = MyWindow()</span><br><span class="line">    sys.exit(app.exec_())</span><br></pre></td></tr></table></figure></p>
<p>另外，我们还可以通过<code>setSpacing()</code>方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在<code>addWidget()</code>方法中指定要扩展的行列数。</p>
<h2 id="事件驱动"><a href="#事件驱动" class="headerlink" title="事件驱动"></a>事件驱动</h2><p>PyQt提供了两种事件驱动的处理方式：</p>
<ul>
<li>使用<code>event</code>句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。</li>
<li>信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。</li>
</ul>
<p>信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的<code>connect()</code>函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。</p>
<p>而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。</p>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>qt</tag>
      </tags>
  </entry>
  <entry>
    <title>doc2dash——制作自己的dash文档</title>
    <url>/2017/08/26/doc2dash-usage/</url>
    <content><![CDATA[<p>Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如<a href="https://zealdocs.org" target="_blank" rel="noopener">zeal</a>就是一款Windows/Linux平台通用的Dash替代软件。</p>
<p>这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？<a href="https://doc2dash.readthedocs.io/en/stable/" target="_blank" rel="noopener">doc2dash</a>就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？</p>
<p>本文章是基于doc2dash的官方介绍，对其使用进行的总结。<br><img src="/img/doc2dash_pytorch_example.jpg" alt="Demo"></p>
<a id="more"></a>
<h2 id="安装doc2dash"><a href="#安装doc2dash" class="headerlink" title="安装doc2dash"></a>安装doc2dash</h2><p>doc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用<code>conda create</code>命令新建用于doc2dash的虚拟环境。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">conda create -n doc2dash</span><br></pre></td></tr></table></figure></p>
<p>接下来，激活虚拟环境，并使用<code>pip install</code>命令安装。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">source</span> activate doc2dash</span><br><span class="line">pip install doc2dash</span><br></pre></td></tr></table></figure></p>
<p>doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。</p>
<h2 id="生成PyTorch文档"><a href="#生成PyTorch文档" class="headerlink" title="生成PyTorch文档"></a>生成PyTorch文档</h2><p>doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install sphinx_rtd_theme</span><br></pre></td></tr></table></figure>
<p>进入PyTorch的文档目录<code>docs/</code>，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择<code>make html</code>命令生成相应的HTML文档，生成的位置为<code>build/html</code>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># in directory $PYTORCH/docs, run</span><br><span class="line">make html</span><br></pre></td></tr></table></figure>
<p>接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用<code>-n</code>指定生成的文件名称，后面跟source文件夹路径即可。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># $PYTORCH/docs/build/html即为生成的HTML目录</span></span><br><span class="line">doc2dash -n pytorch <span class="variable">$PYTORCH</span>/docs/build/html</span><br></pre></td></tr></table></figure>
<p>之后，把生成的<code>pytorch.docset</code>导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。<br><img src="/img/doc2dash_how_to_add_docset.jpg" alt="添加docset"></p>
<h2 id="在Ubuntu上安装zeal"><a href="#在Ubuntu上安装zeal" class="headerlink" title="在Ubuntu上安装zeal"></a>在Ubuntu上安装zeal</h2><p>zeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见<a href="https://zealdocs.org/download.html#linux" target="_blank" rel="noopener">官方网站介绍</a>）。</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo add-apt-repository ppa:zeal-developers/ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install zeal</span><br></pre></td></tr></table></figure>
<p>安装后，可以使用<code>Tool/Docsets</code>下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在<code>$HOME/.local/share/Zeal/Zeal/docsets</code>。</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>使用IPDB调试Python代码</title>
    <url>/2017/08/21/debugging-with-ipdb/</url>
    <content><![CDATA[<p>IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。<br><a id="more"></a></p>
<h2 id="安装与使用"><a href="#安装与使用" class="headerlink" title="安装与使用"></a>安装与使用</h2><p>IPDB以Python第三方库的形式给出，使用<code>pip install ipdb</code>即可轻松安装。</p>
<p>在使用时，有两种常见方式。</p>
<h3 id="集成到源代码中"><a href="#集成到源代码中" class="headerlink" title="集成到源代码中"></a>集成到源代码中</h3><p>通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> ipdb</span><br><span class="line"><span class="comment"># some code</span></span><br><span class="line">x = <span class="number">10</span></span><br><span class="line">ipdb.set_trace()</span><br><span class="line">y = <span class="number">20</span></span><br><span class="line"><span class="comment"># other code</span></span><br></pre></td></tr></table></figure></p>
<p>则程序会在执行完<code>x = 10</code>这条语句之后停止，展开Ipython环境，就可以自由地调试了。</p>
<h3 id="命令式"><a href="#命令式" class="headerlink" title="命令式"></a>命令式</h3><p>上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">python -m ipdb your_code.py</span><br></pre></td></tr></table></figure></p>
<h2 id="常用命令"><a href="#常用命令" class="headerlink" title="常用命令"></a>常用命令</h2><p>IPDB调试环境提供的常见命令有：</p>
<h3 id="帮助"><a href="#帮助" class="headerlink" title="帮助"></a>帮助</h3><p>帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？</p>
<p>使用<code>h</code>即可调出IPDB的帮助。可以使用<code>help command</code>的方法查询特定命令的具体用法。</p>
<h3 id="下一条语句"><a href="#下一条语句" class="headerlink" title="下一条语句"></a>下一条语句</h3><p>使用<code>n</code>(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？</p>
<h3 id="进入函数内部"><a href="#进入函数内部" class="headerlink" title="进入函数内部"></a>进入函数内部</h3><p>使用<code>s</code>(step into)进入函数调用的内部。</p>
<h3 id="打断点"><a href="#打断点" class="headerlink" title="打断点"></a>打断点</h3><p>使用<code>b line_number</code>(break)的方式给指定的行号位置加上断点。使用<code>b file_name:line_number</code>的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。</p>
<p>另外，打断点还支持指定条件下进入，可以查询帮助文档。</p>
<h3 id="一直执行直到遇到下一个断点"><a href="#一直执行直到遇到下一个断点" class="headerlink" title="一直执行直到遇到下一个断点"></a>一直执行直到遇到下一个断点</h3><p>使用<code>c</code>(continue)执行代码直到遇到某个断点或程序执行完毕。</p>
<h3 id="一直执行直到返回"><a href="#一直执行直到返回" class="headerlink" title="一直执行直到返回"></a>一直执行直到返回</h3><p>使用<code>r</code>(return)执行代码直到当前所在的这个函数返回。</p>
<h3 id="跳过某段代码"><a href="#跳过某段代码" class="headerlink" title="跳过某段代码"></a>跳过某段代码</h3><p>使用<code>j line_number</code>(jump)可以跳过某段代码，直接执行指定行号所在的代码。</p>
<h3 id="更多上下文"><a href="#更多上下文" class="headerlink" title="更多上下文"></a>更多上下文</h3><p>在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用<code>l first[, second]</code>(list)命令。</p>
<p>其中<code>first</code>指示向上最多显示的行号，<code>second</code>指示向下最多显示的行号（可以省略）。当<code>second</code>小于<code>first</code>时，<code>second</code>指的是从<code>first</code>开始的向下的行数（相对值vs绝对值）。</p>
<p>根据<a href="https://stackoverflow.com/questions/6240887/how-can-i-make-ipdb-show-more-lines-of-context-while-debugging" target="_blank" rel="noopener">SO上的这个问题</a>，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。</p>
<h3 id="我在哪里"><a href="#我在哪里" class="headerlink" title="我在哪里"></a>我在哪里</h3><p>调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用<code>w</code>或者<code>where</code>可以打印出目前所在的行号位置以及上下文信息。</p>
<h3 id="这是啥"><a href="#这是啥" class="headerlink" title="这是啥"></a>这是啥</h3><p>我们可以使用<code>whatis variable_name</code>的方法，查看变量的类别（感觉有点鸡肋，用<code>type</code>也可以办到）。</p>
<h3 id="列出当前函数的全部参数"><a href="#列出当前函数的全部参数" class="headerlink" title="列出当前函数的全部参数"></a>列出当前函数的全部参数</h3><p>当你身处一个函数内部的时候，可以使用<code>a</code>(argument)打印出传入函数的所有参数的值。</p>
<h3 id="打印"><a href="#打印" class="headerlink" title="打印"></a>打印</h3><p>使用<code>p</code>(print)和<code>pp</code>(pretty print)可以打印表达式的值。</p>
<h3 id="清除断点"><a href="#清除断点" class="headerlink" title="清除断点"></a>清除断点</h3><p>使用<code>cl</code>或者<code>clear file:line_number</code>清除断点。如果没有参数，则清除所有断点。</p>
<h3 id="再来一次"><a href="#再来一次" class="headerlink" title="再来一次"></a>再来一次</h3><p>使用<code>restart</code>重新启动调试器，断点等信息都会保留。<code>restart</code>实际是<code>run</code>的别名，使用<code>run args</code>的方式传入参数。</p>
<h3 id="退出"><a href="#退出" class="headerlink" title="退出"></a>退出</h3><p>使用<code>q</code>退出调试，并清除所有信息。</p>
<p>当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！</p>
]]></content>
      <tags>
        <tag>python</tag>
        <tag>debug</tag>
      </tags>
  </entry>
  <entry>
    <title>Focal Loss论文阅读 - Focal Loss for Dense Object Detection</title>
    <url>/2017/08/14/focal-loss-paper/</url>
    <content><![CDATA[<p>Focal Loss这篇文章是He Kaiming和RBG发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的<a href="https://www.zhihu.com/question/63581984" target="_blank" rel="noopener">讨论</a>。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。</p>
<p>下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。</p>
<p><img src="/img/focal_loss_different_model_comparison.jpg" alt="不同模型关于精度和速度的比较"></p>
<p>Update@2018.03.26 YOLO更新了v3版本，见<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">项目主页</a>，并“点名”与有Focal Loss加持的Retina Net相比较，见下图。<br><img src="/img/yolov3-comparision-with-retina.png" alt="YOLO v3"><br><a id="more"></a></p>
<h2 id="为什么要有Focal-Loss？"><a href="#为什么要有Focal-Loss？" class="headerlink" title="为什么要有Focal Loss？"></a>为什么要有Focal Loss？</h2><p>目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。</p>
<p>论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \times 13 \times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。</p>
<p>基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。</p>
<script type="math/tex; mode=display">\text{FL}(p_t) = -(1-p_t)^\gamma \log(p_t)</script><h2 id="物体检测的两种主流方法"><a href="#物体检测的两种主流方法" class="headerlink" title="物体检测的两种主流方法"></a>物体检测的两种主流方法</h2><p>在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。</p>
<p>R-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。</p>
<p>在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题：</p>
<ul>
<li>过于简单，有效信息过少，使得训练效率低；</li>
<li>简单的负样本在训练过程中压倒性优势，使得模型发生退化。</li>
</ul>
<p>在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。</p>
<h2 id="Focal-Loss"><a href="#Focal-Loss" class="headerlink" title="Focal Loss"></a>Focal Loss</h2><p>Focal Loss从交叉熵损失而来。二分类的交叉熵损失如下：</p>
<script type="math/tex; mode=display">\text{CE}(p, y) = \begin{cases}-\log(p) \quad &\text{if}\quad y = 1\\ -\log(1-p) &\text{otherwise}\end{cases}</script><p>对应的，多分类的交叉熵损失是这样的：</p>
<script type="math/tex; mode=display">\text{CE}(p, y) = -\log(p_y)</script><p>如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。<br><img src="/img/focal_loss_vs_ce_loss.jpg" alt="FL vs CELoss"></p>
<p>为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。</p>
<script type="math/tex; mode=display">\text{CE}(p) = -\alpha_t\log(p_t)</script><p>作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\gamma$取不同值的时候的函数值变化。作者发现，$\gamma=2$时能够获得最佳的效果提升。</p>
<script type="math/tex; mode=display">\text{FL}(p_t) = -(1-p_t)^\gamma\log(p_t)</script><p>在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。</p>
<h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>这里给出PyTorch中第三方给出的<a href="https://github.com/DingKe/pytorch_workplace/blob/master/focalloss/loss.py" target="_blank" rel="noopener">Focal Loss的实现</a>。在下面的代码中，首先实现了<code>one-hot</code>编码，给定类别总数<code>classes</code>和当前类别<code>index</code>，生成one-hot向量。那么，Focal Loss可以用下面的式子计算（可以对照交叉损失熵使用onehot编码的计算）。其中，$\odot$表示element-wise乘法。</p>
<script type="math/tex; mode=display">L = -\sum_{i}^{C}\text{onehot}\odot (1-P_i)^\gamma \log P_i</script><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">one_hot</span><span class="params">(index, classes)</span>:</span></span><br><span class="line">    size = index.size() + (classes,)</span><br><span class="line">    view = index.size() + (<span class="number">1</span>,)</span><br><span class="line"></span><br><span class="line">    mask = torch.Tensor(*size).fill_(<span class="number">0</span>)</span><br><span class="line">    index = index.view(*view)</span><br><span class="line">    ones = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> isinstance(index, Variable):</span><br><span class="line">        ones = Variable(torch.Tensor(index.size()).fill_(<span class="number">1</span>))</span><br><span class="line">        mask = Variable(mask, volatile=index.volatile)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> mask.scatter_(<span class="number">1</span>, index, ones)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FocalLoss</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, gamma=<span class="number">0</span>, eps=<span class="number">1e-7</span>)</span>:</span></span><br><span class="line">        super(FocalLoss, self).__init__()</span><br><span class="line">        self.gamma = gamma</span><br><span class="line">        self.eps = eps</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target)</span>:</span></span><br><span class="line">        y = one_hot(target, input.size(<span class="number">-1</span>))</span><br><span class="line">        logit = F.softmax(input)</span><br><span class="line">        logit = logit.clamp(self.eps, <span class="number">1.</span> - self.eps)</span><br><span class="line"></span><br><span class="line">        loss = <span class="number">-1</span> * y * torch.log(logit) <span class="comment"># cross entropy</span></span><br><span class="line">        loss = loss * (<span class="number">1</span> - logit) ** self.gamma <span class="comment"># focal loss</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss.sum()</span><br></pre></td></tr></table></figure>
<h2 id="模型初始化"><a href="#模型初始化" class="headerlink" title="模型初始化"></a>模型初始化</h2><p>对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。</p>
<p>在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\log((1-\pi)/\pi)$。这里的$\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。</p>
<p>这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。</p>
<h2 id="RetinaNet"><a href="#RetinaNet" class="headerlink" title="RetinaNet"></a>RetinaNet</h2><p>作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。（待续）</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>DELL 游匣7559安装Ubuntu和CUDA记录</title>
    <url>/2017/08/10/install-ubuntu-in-dell/</url>
    <content><![CDATA[<p>虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考<a href="https://hemenkapadia.github.io/blog/2016/11/11/Ubuntu-with-Nvidia-CUDA-Bumblebee.html" target="_blank" rel="noopener">这篇文章</a>成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。<br><img src="/img/install_ubuntu_in_dell_kaiyuandafahao.jpg" alt="开源大法好"><br><a id="more"></a></p>
<h2 id="蛋疼的过往"><a href="#蛋疼的过往" class="headerlink" title="蛋疼的过往"></a>蛋疼的过往</h2><p>之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。</p>
<h2 id="安装Ubuntu"><a href="#安装Ubuntu" class="headerlink" title="安装Ubuntu"></a>安装Ubuntu</h2><p>我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。</p>
<p>选择“Install Ubuntu”选项，按<code>e</code>，找到包含有<code>quiet splash</code>的那行脚本，将<code>quiet splash</code>替换为以下内容：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">nomodeset i915.modeset=1 quiet splash</span><br></pre></td></tr></table></figure>
<p>之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按<code>e</code>，按照上述方法替换<code>quiet splash</code>。确定可以进入Ubuntu系统并登陆。</p>
<h2 id="GRUB设置"><a href="#GRUB设置" class="headerlink" title="GRUB设置"></a>GRUB设置</h2><p>下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：<code>sudo vi /etc/default/grub</code>，找到包含<code>GRUB_CMDLINE_LINUX_DEFAULT</code>的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）：</p>
<figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">GRUB_CMDLINE_LINUX_DEFAULT=<span class="string">"nomodeset i915.modeset=1 quiet splash"</span></span><br></pre></td></tr></table></figure>
<h2 id="更新系统软件"><a href="#更新系统软件" class="headerlink" title="更新系统软件"></a>更新系统软件</h2><p>配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新，</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get upgrade</span><br><span class="line">sudo apt-get dist-upgrade</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure>
<p>参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？</p>
<p>由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt-get install vim</span><br></pre></td></tr></table></figure></p>
<p>更新完之后，重启，确认可以正常登陆系统。</p>
<h2 id="移除原有的Nvidia和Nouveau驱动"><a href="#移除原有的Nvidia和Nouveau驱动" class="headerlink" title="移除原有的Nvidia和Nouveau驱动"></a>移除原有的Nvidia和Nouveau驱动</h2><p>按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br></pre></td></tr></table></figure></p>
<p>之后，执行卸载操作：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo apt-get remove --purge nvidia*</span><br><span class="line">sudo apt-get remove --purge bumblebee*</span><br><span class="line">sudo apt-get --purge remove xserver-xorg-video-nouveau*</span><br></pre></td></tr></table></figure></p>
<p>编辑配置文件，<code>/etc/modprobe.d/blacklist.conf</code>，将Nouveau加入到黑名单中：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">blacklist nouveau</span><br><span class="line">blacklist lbm-nouveau</span><br><span class="line">alias nouveau off</span><br><span class="line">alias lbm-nouveau off</span><br><span class="line">options nouveau modeset=0</span><br></pre></td></tr></table></figure></p>
<p>编辑<code>/etc/init/gpu-manager.conf</code>文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Comment these start on settings ; GPU Manager ruins our work</span></span><br><span class="line"><span class="comment">#start on (starting lightdm</span></span><br><span class="line"><span class="comment">#          or starting kdm</span></span><br><span class="line"><span class="comment">#          or starting xdm</span></span><br><span class="line"><span class="comment">#          or starting lxdm)</span></span><br><span class="line">task</span><br><span class="line"><span class="built_in">exec</span> gpu-manager --<span class="built_in">log</span> /var/<span class="built_in">log</span>/gpu-manager.log</span><br></pre></td></tr></table></figure></p>
<p>之后，更新initramfs并重启。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo update-initramfs -u -k all</span><br></pre></td></tr></table></figure></p>
<p>重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 正常情况下，下面的命令应该不产生任何输出</span></span><br><span class="line">lsmod | grep nouveau</span><br></pre></td></tr></table></figure></p>
<p>并确定关闭了gpu-manager服务：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo service gpu-manager stop</span><br></pre></td></tr></table></figure></p>
<p>至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。<br><img src="/img/install_ubuntu_in_dell_weixiaodaizhepibei.jpg" alt="微笑中带着疲惫"></p>
<h2 id="安装CUDA"><a href="#安装CUDA" class="headerlink" title="安装CUDA"></a>安装CUDA</h2><p>鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。</p>
<p>按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo service lightdm stop</span><br><span class="line">sudo apt-get install linux-headers-$(uname -r)</span><br><span class="line">sudo apt-get install mesa-utils</span><br></pre></td></tr></table></figure></p>
<p>安装CUDA包：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line">sudo dpkg -i YOUR_CUDA_DEB_PATH</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install cuda-8-0</span><br><span class="line">sudo apt-get autoremove</span><br></pre></td></tr></table></figure></p>
<p>安装完毕之后使用<code>sudo reboot</code>重启，确定能够正常登陆系统。</p>
<p>在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！</p>
<h2 id="测试CUDA"><a href="#测试CUDA" class="headerlink" title="测试CUDA"></a>测试CUDA</h2><p>我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到<code>~/.bashrc</code>或者<code>~/.zshrc</code>（如果使用zsh）中去。<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> PATH=<span class="string">"<span class="variable">$PATH</span>:/usr/local/cuda-8.0/bin"</span></span><br><span class="line"><span class="built_in">export</span> LD_LIBRARY_PATH=<span class="string">"<span class="variable">$LD_LIBRARY_PATH</span>:/usr/local/cuda-8.0/lib"</span></span><br></pre></td></tr></table></figure></p>
<p>接下来，我们将使用CUDA自带的example进行测试：<br><figure class="highlight sh"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 导入我们刚加入的环境变量</span></span><br><span class="line"><span class="built_in">source</span> ~/.bashrc</span><br><span class="line"><span class="built_in">cd</span> /usr/<span class="built_in">local</span>/cuda-8.0/bin</span><br><span class="line"><span class="comment"># 将CUDA example拷贝到$HOME下</span></span><br><span class="line">./cuda-install-samples-8.0.sh ~</span><br><span class="line"><span class="comment"># 进入拷贝到的那个目录 build</span></span><br><span class="line"><span class="built_in">cd</span> ~/NVIDIA_CUDA-8.0_Samples</span><br><span class="line">make -j12</span><br><span class="line"><span class="comment"># 自己挑选几个目录进去运行编译生成的可执行文件测试吧~</span></span><br></pre></td></tr></table></figure></p>
<h2 id="Last-But-Not-Least"><a href="#Last-But-Not-Least" class="headerlink" title="Last But Not Least"></a>Last But Not Least</h2><p>安装完CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。<br><img src="/img/install_ubuntu_in_dell_weixiaojiuhao.jpg" alt="微笑就好"></p>
]]></content>
      <tags>
        <tag>ubuntu</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 8 定制new和delete</title>
    <url>/2017/07/03/effective-cpp-08/</url>
    <content><![CDATA[<p>手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是<code>operator new</code>和<code>operator delete</code>，配角是new_handler，它在当<code>operator new</code>无法满足客户内存需求时候被调用。</p>
<p>另外，<code>operator new</code>和<code>operator delete</code>只用于分配单一对象内存。对于数组，应使用<code>operator new[]</code>，并通过<code>operator delete[]</code>归还。除非特别指定，本章中的各项既适用于单一<code>operator new</code>，也适用于<code>operator new[]</code>。</p>
<p>最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。<br><img src="/img/effectivecpp_08_memory_leak_everywherre.jpg" alt="memory_leak_everywhere"><br><a id="more"></a></p>
<h2 id="49-了解new-handler的行为"><a href="#49-了解new-handler的行为" class="headerlink" title="49 了解new-handler的行为"></a>49 了解new-handler的行为</h2><p>什么是new-handler？当<code>operator new</code>无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。</p>
<p>为了指定new-handler，必须调用位于标准库<code>&lt;new&gt;</code>的函数<code>set_new_handler</code>。其声明如下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">void</span> <span class="params">(*new_handler)</span> <span class="params">()</span></span>;</span><br><span class="line">    <span class="function">new_handler <span class="title">set_new_handler</span><span class="params">(new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中，传入参数<code>p</code>是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。<code>throw()</code>表示该函数不抛出任何异常。</p>
<p>当<code>operator new</code>无法满足内存需求时，会不断调用<code>set_new_handler()</code>，直到找到足够的内存。更加具体的介绍见条款51.</p>
<p>一个设计良好的new_handler函数可以是以下的设计策略：</p>
<ul>
<li>设法找到更多的内存可供使用，以便使得下一次的<code>operator new</code>成功。</li>
<li>安装另一个new_handler函数。即在其中再次调用<code>set_new_handler</code>，找到其他的擦屁股函数接盘。</li>
<li>卸载new_handler函数。即将<code>NULL</code>指针传进<code>set_new_handler()</code>中去。这样，<code>operator new</code>会抛出异常。</li>
<li>抛出<code>bad_alloc</code>（或其派生类）异常。</li>
<li>不返回（放弃治疗），直接告诉程序exit或abort。</li>
</ul>
<p>有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的<code>set_new_handler()</code>函数和<code>operator new</code>。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::new_handler <span class="title">set_new_handler</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::new_handler current_handler;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 实现文件</span></span><br><span class="line"><span class="function"><span class="built_in">std</span>::new_handler <span class="title">A::set_new_handler</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::new_hanlder old = current_handler;</span><br><span class="line">    current_handler = p;</span><br><span class="line">    <span class="keyword">return</span> old;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>静态成员变量必须在类外进行定义（除非是<code>const</code>且为整数型），所以需要在类外定义：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 实现文件</span></span><br><span class="line"><span class="built_in">std</span>::new_handler A::current_handler = <span class="number">0</span>;</span><br></pre></td></tr></table></figure></p>
<p>在实现自定义的<code>operator new</code>的时候，首先调用<code>set_new_handler()</code>将自己的擦屁股函数安装为默认，然后调用global的<code>operator new</code>进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 这个类实现了自动安装与恢复new_handler</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Helper</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Helper</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span>: <span class="title">handler</span><span class="params">(p)</span> </span>&#123;&#125;</span><br><span class="line">    ~Helper() &#123;<span class="built_in">std</span>::set_new_handler(handler); &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">std</span>::new_handler handler;</span><br><span class="line">    <span class="comment">// 禁止拷贝构造与赋值</span></span><br><span class="line">    Helper(<span class="keyword">const</span> Helper&amp;);</span><br><span class="line">    Helper&amp; <span class="keyword">operator</span>= (<span class="keyword">const</span> Helper&amp;);</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 实现类A自定义的operator new</span></span><br><span class="line"><span class="function"><span class="keyword">void</span>* A::<span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 存储了函数返回值，也就是原始的 new_handler</span></span><br><span class="line">    <span class="function">Helper <span class="title">h</span><span class="params">(<span class="built_in">std</span>::set_new_handler(current_handler))</span></span>;</span><br><span class="line">    <span class="keyword">return</span> ::<span class="keyword">operator</span> <span class="keyword">new</span>(size);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的<code>current_handler</code>变量，我们把这个基类做成模板。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HandlerHelper</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="built_in">std</span>::new_handler <span class="title">set_new_handler</span><span class="params">(<span class="built_in">std</span>::new_handler p)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span></span>;</span><br><span class="line">    ... <span class="comment">// 其他的new版本，见条款52</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">static</span> <span class="built_in">std</span>::new_handler current_handler;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样</span></span><br></pre></td></tr></table></figure>
<p>这样，我们只要让类<code>A</code>继承自<code>HandlerHelper&lt;A&gt;</code>即可（看上去很怪异。。。）：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span>:</span> <span class="keyword">public</span> HandlerHelper&lt;A&gt; &#123;</span><br><span class="line">    ...</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2 id="50-了解替换new和delete的合适时机"><a href="#50-了解替换new和delete的合适时机" class="headerlink" title="50 了解替换new和delete的合适时机"></a>50 了解替换<code>new</code>和<code>delete</code>的合适时机</h2><p>最常见的理由（替换之后你能得到什么好处）：</p>
<ul>
<li>检测运用上的错误。比如缓冲区越界，我们可以在<code>delete</code>的时候进行检查。</li>
<li>强化效能。编译器实现的<code>operator new</code>是为了普适性的功能，改成自定义版本可能提升效能。</li>
<li>收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的<code>operator new</code>和<code>delete</code>能够收集到这些信息。</li>
</ul>
<p>但是，写出能正常工作的<code>new</code>却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的<code>Pool</code>对分配大量小型对象很有帮助。</p>
<h2 id="51-编写new和delete时候需要遵守常规"><a href="#51-编写new和delete时候需要遵守常规" class="headerlink" title="51 编写new和delete时候需要遵守常规"></a>51 编写<code>new</code>和<code>delete</code>时候需要遵守常规</h2><p>自定义的<code>operator new</code>需要满足以下几点：</p>
<ul>
<li>如果有足够的内存，则返回其指针；否则，遵循条款49的约定。</li>
<li>具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为<code>NULL</code>时，才抛出异常<code>bad_alloc</code>。</li>
<li>C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。</li>
<li>还要避免掩盖正常的<code>operator new</code>。</li>
</ul>
<p>下面就是一个自定义<code>operator new</code>的例子：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size)</span> <span class="title">throw</span><span class="params">(bad_alloc)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 你的operator new也可能接受额外参数</span></span><br><span class="line">    <span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line">    <span class="keyword">if</span>(size == <span class="number">0</span>) &#123;</span><br><span class="line">        size = <span class="number">1</span>; <span class="comment">// 处理0byte申请</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span>(<span class="literal">true</span>) &#123;</span><br><span class="line">        <span class="comment">// ... try to alloc memory</span></span><br><span class="line">        <span class="keyword">if</span>(success) &#123;</span><br><span class="line">            <span class="keyword">return</span> the pointer;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 处理分配失败，找出当前的handler</span></span><br><span class="line">        <span class="comment">// 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄</span></span><br><span class="line">        <span class="comment">// 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数</span></span><br><span class="line">        new_handler globalHandler = set_new_handler(<span class="number">0</span>);</span><br><span class="line">        set_new_handler(globalHandler);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span>(globalHandler) &#123;</span><br><span class="line">            (*globalHandler)();</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">throw</span> bad_alloc();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>在自定义<code>operator delete</code>时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(memory == <span class="number">0</span>) <span class="keyword">return</span>;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="52-写了placement-new也要写placement-delete"><a href="#52-写了placement-new也要写placement-delete" class="headerlink" title="52 写了placement new也要写placement delete"></a>52 写了placement new也要写placement delete</h2><p>如果<code>operator new</code>接受的参数除了一定会有的那个<code>size_t</code>之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size, <span class="keyword">void</span>* memory)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br></pre></td></tr></table></figure></p>
<p>上述placement new已经被纳入C++规范（可以在头文件<code>&lt;new&gt;</code>中找到它。）这个函数常用来在<code>vector</code>的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。</p>
<p>本条款主要探讨与placement new使用不当相关的内存泄漏问题。<br>当你写一个<code>new</code>表达式时，共有两个函数被调用：</p>
<ul>
<li>分配内存的<code>operator new</code></li>
<li>该类的构造函数</li>
</ul>
<p>假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的<code>operator new</code>对应的delete版本。</p>
<p>如果目前面对的是正常签名的<code>operator new delete</code>，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 某个类Wedget内部有自定义的placement new如下</span></span><br><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="keyword">size_t</span> size, ostream&amp; logStream)</span> <span class="title">throw</span> <span class="params">(bad_alloc)</span></span>;</span><br><span class="line"></span><br><span class="line">Widget* pw = <span class="keyword">new</span> (<span class="built_in">std</span>::<span class="built_in">cerr</span>) Widget;</span><br></pre></td></tr></table></figure></p>
<p>如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, ostream&amp; logStream)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br><span class="line"><span class="comment">// 这样，即使下式抛出异常，也能正确处理</span></span><br><span class="line">Widget* pw = <span class="keyword">new</span> (<span class="built_in">std</span>::<span class="built_in">cerr</span>) Widget;</span><br></pre></td></tr></table></figure></p>
<p>然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">delete</span> pw;</span><br></pre></td></tr></table></figure></p>
<p>那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。</p>
<p>一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">StdNewDeleteForms</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 正常的new和delete</span></span><br><span class="line">    static void* operator new(std::size_t size) throw std::bad_alloc) &#123;</span><br><span class="line">        <span class="keyword">return</span> ::<span class="keyword">operator</span> <span class="keyword">new</span>(size);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ::<span class="function"><span class="keyword">operator</span> <span class="title">delete</span><span class="params">(memory)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// placement new 和 delete</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size, <span class="keyword">void</span>* p)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ::<span class="function"><span class="keyword">operator</span> <span class="title">new</span><span class="params">(size, p)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, <span class="keyword">void</span>* p)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ::<span class="function"><span class="keyword">operator</span> <span class="title">delete</span><span class="params">(memory, p)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// nothrow new 和 delete</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="keyword">nothrow_t</span>&amp; nt)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> ::<span class="keyword">operator</span> <span class="keyword">new</span>(size, nt);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, <span class="keyword">const</span> <span class="built_in">std</span>::<span class="keyword">nothrow_t</span>&amp;)</span> <span class="title">throw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        ::<span class="function"><span class="keyword">operator</span> <span class="title">delete</span><span class="params">(mempry)</span></span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和<code>using</code>声明（见条款39），取得标准形式。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Widget</span>:</span> <span class="keyword">public</span> StdNewDeleteForms &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 使用标准new 和 delete</span></span><br><span class="line">    <span class="keyword">using</span> StdNewDeleteForms::<span class="keyword">operator</span> <span class="keyword">new</span>;</span><br><span class="line">    <span class="keyword">using</span> StdNetDeleteForms::<span class="keyword">operator</span> <span class="keyword">delete</span>;</span><br><span class="line">    <span class="comment">// 添加自定义的placement new 和 delete</span></span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span>* <span class="keyword">operator</span> <span class="title">new</span><span class="params">(<span class="built_in">std</span>::<span class="keyword">size_t</span> size,</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="built_in">std</span>::ostream&amp; logStream)</span> <span class="title">throw</span><span class="params">(<span class="built_in">std</span>::bad_alloc)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="keyword">void</span> <span class="keyword">operator</span> <span class="title">delete</span><span class="params">(<span class="keyword">void</span>* memory, <span class="built_in">std</span>::ostream&amp; logStream)</span> <span class="title">throw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法</title>
    <url>/2017/06/25/hinton-nnml-06/</url>
    <content><![CDATA[<p>第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">博客</a>和对应的<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">论文</a>以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。</p>
<p>下图即来自上面的这篇博客。</p>
<p><img src="/img/contours_evaluation_optimizers.gif" alt="几种优化方法的可视化"><br><a id="more"></a></p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。</p>
<p>具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\epsilon$为学习率，$\alpha$为momentum系数。</p>
<script type="math/tex; mode=display">\Delta w_t = v_t = \alpha v_{t-1} - \epsilon g_t = \Delta w_t - \epsilon g_t</script><p>为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：</p>
<script type="math/tex; mode=display">v_t = \alpha(v_{t-1} + \frac{\epsilon g}{1-\alpha}) - \frac{\epsilon g}{1-\alpha}</script><p>由于$\alpha &lt; 0$，所以当$t = \infty$时，只剩下了后面的常数项，即：</p>
<script type="math/tex; mode=display">v_\infty = -\frac{\epsilon}{1-\alpha}g</script><p>也就是说，权重更新的幅度变成了原来的$\frac{1}{1-\alpha}$倍。若取$\alpha=0.99$，则加速$100$倍。</p>
<p>Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。</p>
<p>一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。</p>
<p><img src="/img/hinton_06_nesterov_momentum.png" alt="Nesterov方法"></p>
<h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><p>这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。</p>
<p>例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。<br><img src="/img/hinton_06_learningrate.png" alt="Different learning rate gain"></p>
<p>下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。</p>
<p><img src="/img/hinton_06_tricks_for_adaptive_lr.png" alt="Tricks for adaptive lr"></p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>rprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。</p>
<p>我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。</p>
<script type="math/tex; mode=display">\text{MeanSquare}(w, t) = 0.9 \text{MeanSquare}(w, t-1) + 0.1g_t^2</script><p>然后，将梯度除以上面的得到的Mean Square值。</p>
<p>RMSProp还有一些变种，列举如下：<br><img src="/img/hinton_06_rmsprop_improvement.png" alt="Otehr RMSProp"></p>
<h2 id="课程总结"><a href="#课程总结" class="headerlink" title="课程总结"></a>课程总结</h2><ul>
<li>对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。</li>
<li>对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。</li>
</ul>
<p>如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。<br><img src="/img/hinton_06_summary.png" alt="总结"></p>
<h2 id="“Modern”-SGD"><a href="#“Modern”-SGD" class="headerlink" title="“Modern” SGD"></a>“Modern” SGD</h2><p>从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。</p>
<ul>
<li>学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。</li>
<li>学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。</li>
<li>学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。</li>
<li>高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）</li>
</ul>
<p><img src="/img/hinton_06_maanmian.jpg" alt="马鞍面"></p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="noopener">Adagrad</a>对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：</p>
<script type="math/tex; mode=display">g_{i} = \bigtriangledown_{\theta_i} J(\theta)</script><p>Adagrad使用一个系数来为不同的参数修正学习率，如下：</p>
<script type="math/tex; mode=display">\hat{g_i} = \frac{1}{\sqrt{G_i+\epsilon}}g_i</script><p>其中，$G_i$是截止到当前时间步$t$时，参数$\theta_i$对应梯度$g_i$的平方和。</p>
<p>我们可以把上面的式子写成矩阵形式。其中，$\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \odot g_t$。</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t</script><p>我们再来看PyTorch中的相关实现：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># for each gradient of parameters:</span></span><br><span class="line"><span class="comment"># addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t</span></span><br><span class="line"><span class="comment"># let epsilon = 1E-10</span></span><br><span class="line">state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)   <span class="comment"># 计算 G</span></span><br><span class="line">std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)  <span class="comment"># 计算 \sqrt(G)</span></span><br><span class="line">p.data.addcdiv_(-clr, grad, std)       <span class="comment"># 更新</span></span><br></pre></td></tr></table></figure>
<p>由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。</p>
<p>Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。</p>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p><a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">Adadelta</a>给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。</p>
<script type="math/tex; mode=display">E[g_t^2] = \gamma E[g_{t-1}^2] + (1-\gamma)g_t^2</script><p>由于$\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。</p>
<script type="math/tex; mode=display">\hat{g}_t = \frac{1}{\text{RMS}[g]}g_t</script><p>作者还观察到，这样更新的话，其实$\theta$和$\Delta \theta$的单位是不一样的（此时$\Delta \theta$是无量纲数）。所以，作者提出再乘上一个$\text{RMS}[\Delta \theta]$来平衡（同时去掉了学习率$\eta$），所以，最终的参数更新如下：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\text{RMS}[\Delta \theta]}{\text{RMS}[g]}g_t</script><p>这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率<code>lr</code>这一参数设定，默认值为$1.0$。代码注释中，我使用<code>MS</code>来指代$E[x^2]$。即，$\text{RMS}[x] = \sqrt{\text{MS}[x]+\epsilon}$。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># update: MS[g] = MS[g]*\rho + g*g*(1-\rho)</span></span><br><span class="line">square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</span><br><span class="line"><span class="comment"># current RMS[g] = sqrt(MS[g] + \epsilon)</span></span><br><span class="line">std = square_avg.add(eps).sqrt_()</span><br><span class="line"><span class="comment"># \Delta \theta = RMS[\Delta \theta] / RMS[g]) * g</span></span><br><span class="line">delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</span><br><span class="line"><span class="comment"># update parameter: \theta -= lr * \Delta \theta</span></span><br><span class="line">p.data.add_(-group[<span class="string">'lr'</span>], delta)</span><br><span class="line"><span class="comment"># update MS[\Delta \theta] = MS[\Delta \theta] * \rho + \Delta \theta^2 * (1-\rho)</span></span><br><span class="line">acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</span><br></pre></td></tr></table></figure></p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adaptive momen Estimation（Adam，自适应矩估计）</a>，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。</p>
<script type="math/tex; mode=display">\begin{aligned}m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t\\v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2\end{aligned}</script><p>作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：</p>
<script type="math/tex; mode=display">\begin{aligned}\hat{m} &= \frac{m}{1-\beta_1}\\ \hat{v}&=\frac{v}{1-\beta_2}\end{aligned}</script><p>参数的更新如下：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t} + \epsilon}}\hat{m_t}</script><p>作者给出$\beta_1 = 0.9$，$\beta_2=0.999$，$\epsilon=10^{-8}$。</p>
<p>为了更好地理解PyTorch中的实现方式，需要对上式进行变形：</p>
<script type="math/tex; mode=display">\Delta \theta = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta \frac{m_t}{\sqrt{v_t}}</script><p>代码中令$\text{step_size} =  \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta$。同时，$\beta$也要以指数规律衰减，即：$\beta_t = \beta_0^t$。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># exp_avg is `m`: expected average of g</span></span><br><span class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line"><span class="comment"># exp_avg_sq is `v`: expected average of g's square</span></span><br><span class="line">exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># \sqrt&#123;v_t + \epsilon&#125;</span></span><br><span class="line">denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 - \beta_1^t</span></span><br><span class="line">bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line"><span class="comment"># 1 - \beta_2^t</span></span><br><span class="line">bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</span><br><span class="line"><span class="comment"># get step_size</span></span><br><span class="line">step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</span><br><span class="line"><span class="comment"># delta = -step_size * m / sqrt(v)</span></span><br><span class="line">p.data.addcdiv_(-step_size, exp_avg, denom)</span><br></pre></td></tr></table></figure>
<h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p>上面Adam中，实际上我们是用梯度$g$的$2$范数（$\sqrt{\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。</p>
<p>由于无穷范数就是求绝对值最大的分量，所以这种方法叫做<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">AdaMax</a>。其对应的$\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：</p>
<script type="math/tex; mode=display">u_t = \beta_2^\infty u_{t-1} + (1-\beta_2^\infty) g_t^\infty</script><p>我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。</p>
<p><img src="/img/hinton_06_adamax.png" alt="Adamax中ut的推导"></p>
<p>相应的更新权重操作为：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t -\frac{\eta}{u_t}\hat{m}_t</script><p>在PyTorch中的实现如下：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Update biased first moment estimate, which is \hat&#123;m&#125;_t</span></span><br><span class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line"><span class="comment"># 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个</span></span><br><span class="line"><span class="comment"># Update the exponentially weighted infinity norm.</span></span><br><span class="line">norm_buf = torch.cat([</span><br><span class="line">    exp_inf.mul_(beta2).unsqueeze(<span class="number">0</span>),</span><br><span class="line">    grad.abs().add_(eps).unsqueeze_(<span class="number">0</span>)</span><br><span class="line">], <span class="number">0</span>)</span><br><span class="line"><span class="comment">## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）</span></span><br><span class="line">torch.max(norm_buf, <span class="number">0</span>, keepdim=<span class="literal">False</span>, out=(exp_inf, exp_inf.new().long()))</span><br><span class="line"></span><br><span class="line"><span class="comment">## beta1 correction</span></span><br><span class="line">bias_correction = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line">clr = group[<span class="string">'lr'</span>] / bias_correction</span><br><span class="line"></span><br><span class="line">p.data.addcdiv_(-clr, exp_avg, exp_inf)</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>公开课</tag>
        <tag>deep learning</tag>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 7 模板与泛型编程</title>
    <url>/2017/06/23/effective-cpp-07/</url>
    <content><![CDATA[<p>模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。</p>
<p><img src="/img/effective_cpp_07_joke.jpg" alt="来自大师的嘲讽~"><br><a id="more"></a></p>
<h2 id="41-了解隐式接口和编译器多态"><a href="#41-了解隐式接口和编译器多态" class="headerlink" title="41 了解隐式接口和编译器多态"></a>41 了解隐式接口和编译器多态</h2><p>OOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。</p>
<p>而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型<code>T</code>支持操作符<code>&lt;</code>。隐式接口不基于函数的声明，而是由有效表达式组成。</p>
<p>同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">bool</span> <span class="title">fun</span><span class="params">(<span class="keyword">const</span> T&amp; a, <span class="keyword">const</span> T&amp; b)</span> </span>&#123;<span class="keyword">return</span> a &lt; b;&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="42-了解typename的双重意义"><a href="#42-了解typename的双重意义" class="headerlink" title="42 了解typename的双重意义"></a>42 了解<code>typename</code>的双重意义</h2><p><code>typename</code>和<code>class</code>一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span>/<span class="class"><span class="keyword">class</span> <span class="title">T</span>&gt;</span></span><br><span class="line"><span class="class"><span class="title">void</span> <span class="title">fun</span>(<span class="title">T</span>&amp; <span class="title">a</span>) &#123;</span>...&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是有一个地方只能用<code>typename</code>，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 打印容器内的第二个元素</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> C&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_2nd_element</span><span class="params">(<span class="keyword">const</span> C&amp; container)</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 嵌套类型</span></span><br><span class="line">    <span class="function"><span class="keyword">typename</span> C::const_iterator <span class="title">it</span><span class="params">(container.begin())</span></span>;</span><br><span class="line">    <span class="built_in">cout</span> &lt;&lt; *++it;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base&lt;T&gt;::Nested &#123; <span class="comment">//即使Nest是嵌套类型，这里也不用typename</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Derived</span><span class="params">(<span class="keyword">int</span> x)</span></span></span><br><span class="line"><span class="function">     :Base&lt;T&gt;::<span class="title">Nested</span><span class="params">(x)</span> </span>&#123; <span class="comment">// 成员初始化列表也不用</span></span><br><span class="line">        <span class="keyword">typename</span> Base&lt;T&gt;::Nested tmp; <span class="comment">//这时候要使用</span></span><br><span class="line">     &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="43-学习处理模板化基类内的名称"><a href="#43-学习处理模板化基类内的名称" class="headerlink" title="43 学习处理模板化基类内的名称"></a>43 学习处理模板化基类内的名称</h2><p>这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TypeA</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TypeB</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Type&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">do_something</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        Type x;</span><br><span class="line">        x.fun();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Type&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base&lt;Type&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">do_something_too</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        do_something();  <span class="comment">// 调用基类的函数，这里无法编译</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这是因为存在如下可能，<code>Base&lt;Tyep&gt;</code>对某种<code>Type</code>进行了特化。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span>&lt;TypeC&gt; &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 这里没有实现 dom_somthing 函数</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>所以存在这样的可能：<code>class Derived&lt;TypeC&gt;: public Base&lt;TypeC&gt;</code>，而这里面是没有<code>do_something</code>函数的。</p>
<p>为了解决这个问题，有三种办法：</p>
<ul>
<li><p>在基类调用方法前面加上<code>this-&gt;</code>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_something_too</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;do_something();  <span class="comment">// 调用基类的函数，这里无法编译</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
</li>
<li><p>使用<code>using</code>声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。</p>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">using</span> Base&lt;Type&gt;::do_something;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_something_too</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">this</span>-&gt;do_something();  <span class="comment">// 调用基类的函数，这里无法编译</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li>明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">do_something_too</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    Base&lt;Type&gt;::do_something();  <span class="comment">// 调用基类的函数，这里无法编译</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="44-将与参数无关的代码抽离模板"><a href="#44-将与参数无关的代码抽离模板" class="headerlink" title="44 将与参数无关的代码抽离模板"></a>44 将与参数无关的代码抽离模板</h2><p>由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。</p>
<p>如下所示是一个$N$阶方阵，其中<code>n</code>是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">size_t</span> n&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Matrix</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invert</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MatrixBase</span> &#123;</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    MatrixBase(<span class="keyword">size_t</span> n, T* pMem) <span class="comment">// 存储矩阵大小和指针</span></span><br><span class="line">     :size(n), pData(pMem) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">setDataPtr</span><span class="params">(T* ptr)</span> </span>&#123;pData = ptr;&#125; <span class="comment">// 设置指针</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">invert</span><span class="params">()</span></span>;   <span class="comment">// 实现求逆</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">size_t</span> size;</span><br><span class="line">    T* pData;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用<code>private</code>继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T, <span class="keyword">size_t</span> n&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Matrix</span>:</span> <span class="keyword">private</span> MatrixBase&lt;T&gt; &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Matrix(): MatrixBase&lt;T&gt;(n, <span class="number">0</span>), pData(<span class="keyword">new</span> T[n*n]) &#123;</span><br><span class="line">        <span class="keyword">this</span>-&gt;setDataPtr(pData.get()); <span class="comment">// 将指针副本传给基类</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    boost::scoped_array&lt;T&gt; pData;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量<code>n</code>是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。</p>
<p>上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上<code>int</code>和<code>long</code>有相同的二进制表述。那么<code>vector&lt;int&gt;</code>和<code>vector&lt;long&gt;</code>的成员函数可能完全相同，也会造成代码膨胀。</p>
<p>在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如<code>vector&lt;int*&gt;, list&lt;const int*&gt;</code>等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即<code>T*</code>）时，你应该让它调用另一个无类型指针<code>void*</code>的函数，由后者完成实际工作。</p>
<h2 id="45-运用成员函数模板接受所有兼容类型"><a href="#45-运用成员函数模板接受所有兼容类型" class="headerlink" title="45 运用成员函数模板接受所有兼容类型"></a>45 运用成员函数模板接受所有兼容类型</h2><p>使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Base* p = <span class="keyword">new</span> Base;</span><br><span class="line">Base* p = <span class="keyword">new</span> Derived;</span><br></pre></td></tr></table></figure></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 一个通用的智能指针模板</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SmartPointer</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 为了兼容类型，需要再引入一个模板参数 U</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> U&gt;</span><br><span class="line">    SmartPointer(<span class="keyword">const</span> SmartPointer&lt;U&gt;&amp; other)</span><br><span class="line">        <span class="comment">// 这里可能会发生指针之间的隐式类型转换</span></span><br><span class="line">       :ptr(other.get())  &#123;...&#125;</span><br><span class="line">    <span class="function">T* <span class="title">get</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> ptr; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    T* ptr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>成员函数模板还可以用来作为赋值操作。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">shared_ptr</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    ...</span><br><span class="line">    <span class="comment">// 接受任意兼容的shared_ptr赋值</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Y&gt;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&amp; <span class="keyword">operator</span> = (<span class="built_in">shared_ptr</span>&lt;Y&gt; <span class="keyword">const</span>&amp; r);</span><br><span class="line">    <span class="comment">// 接受任意兼容的auto_ptr赋值</span></span><br><span class="line">    <span class="keyword">template</span> &lt;<span class="keyword">typename</span> Y&gt;</span><br><span class="line">    <span class="built_in">shared_ptr</span>&amp; <span class="keyword">operator</span> = (<span class="built_in">auto_ptr</span>&lt;Y&gt; <span class="keyword">const</span>&amp; r);</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="built_in">shared_ptr</span>&amp; <span class="keyword">operator</span> = (<span class="built_in">shared_ptr</span> <span class="keyword">const</span>&amp; r);</span><br></pre></td></tr></table></figure></p>
<h2 id="46-需要类型转换时请为模板定义（friend）非成员函数"><a href="#46-需要类型转换时请为模板定义（friend）非成员函数" class="headerlink" title="46 需要类型转换时请为模板定义（friend）非成员函数"></a>46 需要类型转换时请为模板定义（friend）非成员函数</h2><p>回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类<code>Rational</code>声明为模板。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rational</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Rational(<span class="keyword">const</span> T&amp; numerator=<span class="number">0</span>, <span class="keyword">const</span> T&amp; denominator=<span class="number">1</span>);</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="keyword">const</span> Rational&lt;T&gt; <span class="keyword">operator</span>*(<span class="keyword">const</span> Rational&lt;T&gt;&amp; lhs,</span><br><span class="line">                            <span class="keyword">const</span> Rational&lt;T&gt;&amp; rhs)</span><br><span class="line">&#123;...&#125;</span><br><span class="line"></span><br><span class="line"><span class="function">Rational&lt;<span class="keyword">int</span>&gt; <span class="title">onehalf</span><span class="params">(<span class="number">1</span>, <span class="number">2</span>)</span></span>;</span><br><span class="line">Rational&lt;<span class="keyword">int</span>&gt; res = onehalf * <span class="number">2</span>;  <span class="comment">// 改成模板后便会编译错误！</span></span><br></pre></td></tr></table></figure>
<p>这是因为在进行模板类型推导时，并未将<code>2</code>进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。</p>
<p>解决方法是将这个运算符重载函数声明为<code>Rational&lt;T&gt;</code>的友元函数。这样，在<code>onehalf</code>被声明时，<code>Rational&lt;int&gt;</code>类被具现化，则该友元函数也被声明出来了。</p>
<p>然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt; <span class="class"><span class="keyword">class</span> <span class="title">Rational</span>;</span>   <span class="comment">// 前向声明</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">const</span> Rational&lt;T&gt; <span class="title">doMultiply</span><span class="params">(<span class="keyword">const</span> Rational&lt;T&gt;&amp; lhs,</span></span></span><br><span class="line"><span class="function"><span class="params">                             <span class="keyword">const</span> Rational&lt;T&gt;&amp; rhs)</span> </span>&#123;&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rational</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Rational(<span class="keyword">const</span> T&amp; numerator=<span class="number">0</span>, <span class="keyword">const</span> T&amp; denominator=<span class="number">1</span>) &#123;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">    <span class="keyword">friend</span> <span class="keyword">const</span> Rational&lt;T&gt; <span class="keyword">operator</span>*(<span class="keyword">const</span> Rational&amp; lhs,</span><br><span class="line">                                       <span class="keyword">const</span> Rational&amp; rhs)</span><br><span class="line">    &#123;<span class="keyword">return</span> doMultiply(lhs, rhs); &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<h2 id="47-使用trait表现类型信息"><a href="#47-使用trait表现类型信息" class="headerlink" title="47 使用trait表现类型信息"></a>47 使用<code>trait</code>表现类型信息</h2><p>STL中的<code>advance</code>函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。</p>
<ul>
<li>输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如<code>istream_iterator</code>。</li>
<li>输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如<code>ostream_iterator</code>。</li>
<li>前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。</li>
<li>双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。</li>
<li>随机迭代器。可以随意跳转任意距离，例如<code>vector</code>或原始指针。</li>
</ul>
<p>为了对它们进行分类，C++有对应的tag标签。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">input_iterator_tag</span> &#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">output_iterator_tag</span> &#123;</span>&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">forward_iterator_tag</span>:</span> <span class="keyword">public</span> input_iterator_tag &#123;&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">bidirectional_iterator_tag</span>:</span> <span class="keyword">public</span> forward_iterator_tag &#123;&#125;;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">random_access_iterator_tag</span>:</span> <span class="keyword">public</span> bidirectional_iterator_tag &#123;&#125;;</span><br></pre></td></tr></table></figure></p>
<p>所以我们可以在<code>advance</code>的代码中，对迭代器的类型进行判断，从而采取不同的操作。<code>trait</code>就是能够让你在编译器获得类型信息。</p>
<p>我们希望<code>trait</code>也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的<code>trait</code>（而且C++11加入了更多），其中针对迭代器的是<code>iterator_traits</code>。</p>
<p>为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过<code>typedef</code>来实现）。如队列的迭代器支持随机访问，则：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">deque</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="class"><span class="keyword">class</span> <span class="title">iterator</span> &#123;</span></span><br><span class="line">    <span class="keyword">public</span>:</span><br><span class="line">        <span class="keyword">typedef</span> random_access_iterator_tag iterator_category;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>这样，我们就能在<code>iterator_traits</code>内部通过访问迭代器的<code>iterator_category</code>来获得其类型信息啦~如下所示，<code>iterator_traits</code>只是鹦鹉学舌般地表现<code>IterT</code>说自己是什么。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> IterT&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">iterator_traits</span> &#123;</span></span><br><span class="line">    <span class="keyword">typedef</span> <span class="keyword">typename</span> IterT::iterator_category iterator_category;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>如何支持原始指针呢？用模板特化就好了~</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">iterator_traits</span>&lt;T*&gt; &#123;</span></span><br><span class="line">    <span class="keyword">typedef</span> random_access_iterator_tag iterator_category;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>总结起来，如何设计并实现一个<code>traits</code>呢？</p>
<ul>
<li>确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。</li>
<li>为该信息取一个名称，如<code>iterator_category</code></li>
<li>提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。</li>
</ul>
<p>好了，下面我们可以实现<code>advance</code>了。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> IterT, <span class="keyword">typename</span> DistT&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">advance</span><span class="params">(IterT&amp; iter, DistT d)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(<span class="keyword">typeid</span>(<span class="keyword">typename</span> <span class="built_in">std</span>::iterator_traits&lt;IterT&gt;::iterator_category</span><br><span class="line">        == <span class="keyword">typeid</span>(<span class="built_in">std</span>::random_access_iterator_tag) &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的<code>if-else</code>功能。</p>
<p>我们为不同类型的迭代器实现不同的移动方法。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> IterT, <span class="keyword">typename</span> DistT&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">doAdvance</span><span class="params">(IterT&amp; iter, Dist d, <span class="built_in">std</span>::random_access_iterator_tag)</span> </span>&#123;</span><br><span class="line">    iter += d;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// ...其他类型的迭代器对应的 doadvance</span></span><br><span class="line"></span><br><span class="line"><span class="comment">// 用advance函数包装这些重载函数</span></span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Iter, <span class="keyword">typename</span> DistT&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">advance</span><span class="params">(IterT&amp; iter, Dist d)</span> </span>&#123;</span><br><span class="line">    doAdvance(iter, d, <span class="keyword">typename</span> <span class="built_in">std</span>::iterator_traits&lt;IterT&gt;::iterator_category());</span><br><span class="line">    <span class="comment">// 注意 typename</span></span><br><span class="line">    <span class="comment">// 注意传入的是对象实例，所以要 iterator_category()</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>也就是说</p>
<ul>
<li>首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在<code>trait</code>参数。</li>
<li>建立包装函数（包工头），调用上述劳工函数并传递<code>trait</code>信息。</li>
</ul>
<h2 id="48-认识模板元编程"><a href="#48-认识模板元编程" class="headerlink" title="48 认识模板元编程"></a>48 认识模板元编程</h2><p>模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。</p>
<p>条款47介绍了选择分支结构如何借由<code>trait</code>实现。这里介绍循环由递归模板具现化实现的方法。</p>
<p>为了生成斐波那契数列，我们首先定义一个模板参数为<code>n</code>的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">unsigned</span> n&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">F</span> &#123;</span></span><br><span class="line">    <span class="keyword">enum</span> &#123;value = n * F&lt;n<span class="number">-1</span>&gt;::value &#125;;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">F</span>&lt;0&gt; &#123;</span></span><br><span class="line">    <span class="keyword">enum</span> &#123;value = <span class="number">1</span> &#125;;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>TMP博大精深，想要深入学习，还是要参考相关书籍。</p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 6 继承与面向对象设计</title>
    <url>/2017/06/17/effective-cpp-06/</url>
    <content><![CDATA[<p>C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？</p>
<p>真•面向对象编程！<br><img src="/img/effectivecpp_06_joke.jpg" alt="面向对象编程"><br><a id="more"></a></p>
<h2 id="32-确定public继承塑模出Is-a的关系"><a href="#32-确定public继承塑模出Is-a的关系" class="headerlink" title="32 确定public继承塑模出Is-a的关系"></a>32 确定<code>public</code>继承塑模出Is-a的关系</h2><p>请把这条规则记在心中：<code>public</code>继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。</p>
<p>不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了<code>fly()</code>这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。</p>
<h2 id="33-避免遮掩继承而来的名称"><a href="#33-避免遮掩继承而来的名称" class="headerlink" title="33 避免遮掩继承而来的名称"></a>33 避免遮掩继承而来的名称</h2><p>这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。</p>
<p>一种解决办法是使用<code>using</code>声明。如下所示：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">f1</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">f3</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">f3</span><span class="params">(<span class="keyword">double</span>)</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">using</span> Base::f3;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">f1</span><span class="params">()</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">f3</span><span class="params">()</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">Derived d;</span><br><span class="line">d.f1();  <span class="comment">// 没问题，调用了Derived中的f1</span></span><br><span class="line">d.f3();  <span class="comment">// 没问题，调用了Derived中的f3</span></span><br><span class="line"><span class="keyword">double</span> x;</span><br><span class="line">d.f3(x);  <span class="comment">// 没问题，调用了Base中的f3。</span></span><br><span class="line"><span class="comment">// 但是如果没有using声明的话，Base::f3会被冲掉。</span></span><br></pre></td></tr></table></figure></p>
<h2 id="34-区分接口继承和实现继承"><a href="#34-区分接口继承和实现继承" class="headerlink" title="34 区分接口继承和实现继承"></a>34 区分接口继承和实现继承</h2><p>表面上直截了当的<code>public</code>继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">error</span><span class="params">(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; msg)</span></span>;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">getID</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rect</span>:</span> <span class="keyword">public</span> Shape &#123;...&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Circle</span>:</span> <span class="keyword">public</span> Shape &#123;...&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li><p>纯虚函数<br>声明纯虚函数（如<code>draw()</code>函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。<br>不过，你仍然可以给纯虚函数提供函数定义。</p>
</li>
<li><p>虚函数<br>非纯虚函数（如<code>error()</code>函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。<br>然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。</p>
</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>= <span class="number">0</span>;   <span class="comment">// 注意，我们改写成了纯虚函数</span></span><br><span class="line"><span class="keyword">protected</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">default_fun</span><span class="params">()</span> </span>&#123;...&#125;;   <span class="comment">// 缺省实现</span></span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 此时，若想使用缺省实现，就必须显式地调用</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        default_fun();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>不过这样导致一个多余的<code>default_fun()</code>函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Base</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="comment">// 为纯虚函数提供定义</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Base::fun</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="comment">// 缺省行为</span></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 显式调用基类的纯虚函数，实现缺省行为</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>&#123;Base::fun(); &#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived2</span>:</span> <span class="keyword">public</span> Base &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="comment">// 实现自定义的行为</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">fun</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<ul>
<li>非虚函数<br>这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。</li>
</ul>
<h2 id="35-考虑virtual函数之外的其他选择"><a href="#35-考虑virtual函数之外的其他选择" class="headerlink" title="35 考虑virtual函数之外的其他选择"></a>35 考虑<code>virtual</code>函数之外的其他选择</h2><p>虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。</p>
<p>在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">healthValue</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<ul>
<li>使用non-virtual interface实现template method模式<br>这种流派主张<code>virtual</code>函数应该几乎总是私有的。较好的设计时将<code>healthValue()</code>函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">healthValue</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="comment">// ... 前期准备</span></span><br><span class="line">        <span class="keyword">int</span> ret_val = doHealthValue();</span><br><span class="line">        <span class="comment">// ... 后续清理</span></span><br><span class="line">        <span class="keyword">return</span> ret_val</span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">virtual</span> <span class="keyword">int</span> doHealthValue() <span class="keyword">const</span> &#123;...&#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写<code>doHealthValue()</code>函数，针对自身的特点计算血量。</p>
<ul>
<li>使用函数指针实现策略模式<br>上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span>;</span>   <span class="comment">// 前置声明</span></span><br><span class="line"><span class="comment">// 计算血量的缺省方法</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">defaultHealthValue</span><span class="params">(<span class="keyword">const</span> GameCharacter&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">typedef</span> <span class="title">int</span> <span class="params">(*HealthCalcFun)</span> <span class="params">(<span class="keyword">const</span> GameCharacter&amp;)</span></span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">GameCharacter</span><span class="params">(HealthCalcFun f=defaultHealthValue</span></span></span><br><span class="line"><span class="function"><span class="params">        :healthFunc(f)&#123;</span></span></span><br><span class="line"><span class="function"><span class="params">        <span class="comment">// ...</span></span></span></span><br><span class="line"><span class="function"><span class="params">    &#125;</span></span></span><br><span class="line"><span class="function"><span class="params"><span class="keyword">private</span>:</span></span></span><br><span class="line"><span class="function"><span class="params">    HealthCalcFun healthFunc;</span></span></span><br><span class="line"><span class="function"><span class="params">&#125;;</span></span></span><br></pre></td></tr></table></figure>
<p>这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。<br>或者我们可以在运行时候，通过设定<code>healthFunc</code>，来实现动态血量计算方法的变化。</p>
<ul>
<li>借由<code>std::function</code>实现策略模式<br>作为上面的改进，我们可以使用<code>std::function</code>（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于<code>std::function</code>的大致介绍，可以看<a href="http://en.cppreference.com/w/cpp/utility/functional/function" target="_blank" rel="noopener">这里</a>）。</li>
</ul>
<p>我们只需将上面的<code>typedef</code>改掉即可。不再使用函数指针，而是更加高级更加通用的<code>std::function</code>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="built_in">std</span>::function&lt;<span class="keyword">int</span>(<span class="keyword">const</span> GameCharacter&amp;)&gt; HealthCalcFun;</span><br></pre></td></tr></table></figure>
<ul>
<li>使用古典的策略模式<br>如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。<code>GameCharacter</code>类中则含有一个指向<code>HealthCalcFun</code>类实例的指针。</li>
</ul>
<p><img src="/img/effectivecpp_strategy_pattern.png" alt="使用UML表示的策略模式"></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">//我们首先定义HealthCalcFunc基类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span>;</span>    <span class="comment">// 前向声明</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">HealthCalcFunc</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">int</span> <span class="title">calc</span><span class="params">(<span class="keyword">const</span> GameCharacter&amp; gc)</span> <span class="keyword">const</span> </span>&#123;...&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">HealthCalcFunc defaultCalcFunc;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">GameCharacter</span> &#123;</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    HealthCalcFunc* pfun;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">GameCharacter</span><span class="params">(HealthCalcFunc* p=&amp;defaultCalcFunc)</span>:</span></span><br><span class="line"><span class="function">        <span class="title">pfun</span><span class="params">(p)</span> </span>&#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">int</span> <span class="title">healthValue</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> pfun-&gt;calc(*<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>该条款给出了虚函数的若干替代方案。</p>
<h2 id="36-绝不重新定义继承而来的非虚函数"><a href="#36-绝不重新定义继承而来的非虚函数" class="headerlink" title="36 绝不重新定义继承而来的非虚函数"></a>36 绝不重新定义继承而来的非虚函数</h2><p>在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。</p>
<p>如下所示。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">B</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">mf</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line">&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">D</span>:</span> <span class="keyword">public</span> B &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">mf</span><span class="params">()</span> </span>&#123;...&#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">D d;</span><br><span class="line">B* pb = &amp;d;</span><br><span class="line">D* pd = &amp;d;</span><br><span class="line"></span><br><span class="line">pb-&gt;mf();   <span class="comment">// 调用的是B::mf()</span></span><br><span class="line">pd-&gt;mf();   <span class="comment">// 调用的是D::mf()</span></span><br></pre></td></tr></table></figure></p>
<p>这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于<code>pb</code>被声明为一个指向<code>B</code>的指针，所以其调用的是<code>B</code>的成员函数<code>mf()</code>。</p>
<p>为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。</p>
<h2 id="37-绝不重新定义继承而来的缺省参数值"><a href="#37-绝不重新定义继承而来的缺省参数值" class="headerlink" title="37 绝不重新定义继承而来的缺省参数值"></a>37 绝不重新定义继承而来的缺省参数值</h2><p>由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。</p>
<p>静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Circle是Shape的派生类</span></span><br><span class="line">Shape* ps;</span><br><span class="line">Shape* pc = <span class="keyword">new</span> Circle;   <span class="comment">// 静态类型都是Shape</span></span><br></pre></td></tr></table></figure></p>
<p>动态类型是指当前所指对象的类型。就上例来说，<code>pc</code>的动态类型是<code>Circle*</code>，而<code>ps</code>没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">ps = <span class="keyword">new</span> Circle;   <span class="comment">// 现在ps的动态类型是Circle*</span></span><br></pre></td></tr></table></figure></p>
<p>虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。</p>
<p>不过遵守此项条款，有时又会造成不便。看下例：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">enum</span> ShapeColor &#123;RED, GREEN&#125;;</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">(ShapeColor c=RED)</span> <span class="keyword">const</span></span>=<span class="number">0</span>;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Circle</span>:</span> <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">(ShapeColor c=RED)</span> <span class="keyword">const</span></span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成<code>GREEN</code>为默认参数，需要在基类和派生类中同时修改。</p>
<p>一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Shape</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">(ShapeColor c=RED)</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">        doDraw(c);  <span class="comment">// 调用私有的虚函数</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="comment">//真正的工作在此完成</span></span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">doDraw</span><span class="params">(ShapeColor c)</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;  </span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Circle</span>:</span> <span class="keyword">public</span> Shape &#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="keyword">void</span> <span class="title">doDraw</span><span class="params">(ShapeColor c)</span> <span class="keyword">const</span></span>;  <span class="comment">// 派生类重写这个真正的实现</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="38-通过复合塑模has-a或“根据某物实现出”"><a href="#38-通过复合塑模has-a或“根据某物实现出”" class="headerlink" title="38 通过复合塑模has-a或“根据某物实现出”"></a>38 通过复合塑模has-a或“根据某物实现出”</h2><p>复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。</p>
<h2 id="39-明智而审慎地使用private继承"><a href="#39-明智而审慎地使用private继承" class="headerlink" title="39 明智而审慎地使用private继承"></a>39 明智而审慎地使用<code>private</code>继承</h2><p>私有继承意味着条款38中的“根据某物实现出”。例如<code>D</code>私有继承自<code>B</code>，不是说<code>D</code>是某种<code>B</code>，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。<code>B</code>的每样东西在<code>D</code>中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用<code>B</code>来实现<code>D</code>的功能而已。</p>
<p>但是复合也能达到相同的效果啊~我在<code>D</code>中加入一个<code>B</code>的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。</p>
<h2 id="40-明智而审慎地使用多重继承"><a href="#40-明智而审慎地使用多重继承" class="headerlink" title="40 明智而审慎地使用多重继承"></a>40 明智而审慎地使用多重继承</h2><p>使用多重继承有可能造成歧义。例如，<code>C</code>继承自<code>A</code>和<code>B</code>，而两个基类中都含有成员函数<code>mf()</code>。那么当<code>d.mf()</code>的时候，究竟是在调用哪个呢？你必须明确地指出,<code>d.A::mf()</code>。</p>
<p>使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将<code>File</code>定为虚基类，所有直接继承自它的类采用虚继承。<br><img src="/img/effectivecpp_diamond.png" alt="钻石型继承"></p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">File</span> &#123;</span>...&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">InputFile</span>:</span> <span class="keyword">virtual</span> <span class="keyword">public</span> File &#123;...&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">OutputFile</span>:</span> <span class="keyword">virtual</span> <span class="keyword">public</span> File &#123;...&#125;;</span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">IOFile</span>:</span> <span class="keyword">public</span> InputFile, <span class="keyword">public</span> OutputFile &#123;...&#125;;</span><br></pre></td></tr></table></figure>
<p>从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。</p>
<p>所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface）</p>
<h2 id="附注-std-function的基本使用"><a href="#附注-std-function的基本使用" class="headerlink" title="附注 std::function的基本使用"></a>附注 <code>std::function</code>的基本使用</h2><p><code>std::function</code>的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为<code>std::function</code>。<br>如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;functional&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">Foo</span> &#123;</span></span><br><span class="line">    Foo(<span class="keyword">int</span> num) : num_(num) &#123;&#125;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">print_add</span><span class="params">(<span class="keyword">int</span> i)</span> <span class="keyword">const</span> </span>&#123; <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; num_+i &lt;&lt; <span class="string">'\n'</span>; &#125;</span><br><span class="line">    <span class="keyword">int</span> num_;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">print_num</span><span class="params">(<span class="keyword">int</span> i)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">struct</span> <span class="title">PrintNum</span> &#123;</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">operator</span><span class="params">()</span><span class="params">(<span class="keyword">int</span> i)</span> <span class="keyword">const</span></span></span><br><span class="line"><span class="function">    </span>&#123;</span><br><span class="line">        <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; i &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// store a free function</span></span><br><span class="line">    <span class="comment">// 函数指针</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">int</span>)&gt; f_display = print_num;</span><br><span class="line">    f_display(<span class="number">-9</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// lambda表达式</span></span><br><span class="line">    <span class="comment">// store a lambda</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>()&gt; f_display_42 = []() &#123; print_num(<span class="number">42</span>); &#125;;</span><br><span class="line">    f_display_42();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store the result of a call to std::bind</span></span><br><span class="line">    <span class="comment">// 绑定之后的函数对象</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>()&gt; f_display_31337 = <span class="built_in">std</span>::bind(print_num, <span class="number">31337</span>);</span><br><span class="line">    f_display_31337();</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store a call to a member function</span></span><br><span class="line">    <span class="comment">// 类中的成员函数，第一个参数为类实例的const reference</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">const</span> Foo&amp;, <span class="keyword">int</span>)&gt; f_add_display = &amp;Foo::print_add;</span><br><span class="line">    <span class="function"><span class="keyword">const</span> Foo <span class="title">foo</span><span class="params">(<span class="number">314159</span>)</span></span>;</span><br><span class="line">    f_add_display(foo, <span class="number">1</span>);</span><br><span class="line">    f_add_display(<span class="number">314159</span>, <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store a call to a data member accessor</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">int</span>(Foo <span class="keyword">const</span>&amp;)&gt; f_num = &amp;Foo::num_;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"num_: "</span> &lt;&lt; f_num(foo) &lt;&lt; <span class="string">'\n'</span>;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store a call to a member function and object</span></span><br><span class="line">    <span class="keyword">using</span> <span class="built_in">std</span>::placeholders::_1;</span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">int</span>)&gt; f_add_display2 = <span class="built_in">std</span>::bind( &amp;Foo::print_add, foo, _1 );</span><br><span class="line">    f_add_display2(<span class="number">2</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store a call to a member function and object ptr</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">int</span>)&gt; f_add_display3 = <span class="built_in">std</span>::bind( &amp;Foo::print_add, &amp;foo, _1 );</span><br><span class="line">    f_add_display3(<span class="number">3</span>);</span><br><span class="line"></span><br><span class="line">    <span class="comment">// store a call to a function object</span></span><br><span class="line">    <span class="built_in">std</span>::function&lt;<span class="keyword">void</span>(<span class="keyword">int</span>)&gt; f_display_obj = PrintNum();</span><br><span class="line">    f_display_obj(<span class="number">18</span>);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Silver RL课程 - DP Planning</title>
    <url>/2017/06/06/silver-rl-dp/</url>
    <content><![CDATA[<p>上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。<br><a id="more"></a></p>
<h2 id="动态规划"><a href="#动态规划" class="headerlink" title="动态规划"></a>动态规划</h2><p>动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件：</p>
<ul>
<li>最优子结构。能够分解为若干子问题。</li>
<li>子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。</li>
</ul>
<p>MDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\prime$，而在考虑状态$s^\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。</p>
<h2 id="迭代策略估计"><a href="#迭代策略估计" class="headerlink" title="迭代策略估计"></a>迭代策略估计</h2><p>给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）：</p>
<script type="math/tex; mode=display">v_{k+1}(s) = \sum_{a\in \mathcal{A}}\pi(a|s)(R_s^a+\gamma\sum_{s^\prime \in \mathcal{S}} P_{ss^\prime}^av_k(s^\prime))</script><p>我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\in \mathcal{S}$，使用上式利用上轮中的$v(s^\prime)$更新$v_{k+1}(s)$，直到收敛。</p>
<p>给出下面的算例。$4\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\pi(s)$。<br><img src="/img/silver_rl_dp_policy_evaluating_demo.png" alt="Demo"></p>
<p>这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">v = [<span class="number">0</span> <span class="keyword">for</span> _ <span class="keyword">in</span> xrange(<span class="number">16</span>)]</span><br><span class="line">line1 = range(<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">line4 = range(<span class="number">12</span>, <span class="number">15</span>)</span><br><span class="line">col1 = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>]</span><br><span class="line">col4 = [<span class="number">3</span>, <span class="number">7</span>, <span class="number">11</span>]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># the environment simulator</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_new_loc</span><span class="params">(idx, action)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> idx == <span class="number">0</span> <span class="keyword">or</span> idx == <span class="number">15</span>:</span><br><span class="line">        ret = idx</span><br><span class="line">        reward = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> ret, reward</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> action == <span class="number">0</span>:</span><br><span class="line">        <span class="comment"># up</span></span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> line1:</span><br><span class="line">            ret = idx</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret = idx<span class="number">-4</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">1</span>:</span><br><span class="line">        <span class="comment"># down</span></span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> line4:</span><br><span class="line">            ret = idx</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret = idx+<span class="number">4</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">2</span>:</span><br><span class="line">        <span class="comment"># left</span></span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> col1:</span><br><span class="line">            ret = idx</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret = idx<span class="number">-1</span></span><br><span class="line">    <span class="keyword">elif</span> action == <span class="number">3</span>:</span><br><span class="line">        <span class="comment"># right</span></span><br><span class="line">        <span class="keyword">if</span> idx <span class="keyword">in</span> col4:</span><br><span class="line">            ret = idx</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            ret = idx+<span class="number">1</span></span><br><span class="line"></span><br><span class="line">    reward = <span class="number">-1</span></span><br><span class="line">    <span class="keyword">return</span> ret, reward</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">gamma = <span class="number">1.</span></span><br><span class="line"></span><br><span class="line">K = [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">10</span>, <span class="number">100</span>]</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> xrange(<span class="number">1</span>, <span class="number">101</span>):</span><br><span class="line">    <span class="comment"># in each iteration, update v(s) via:</span></span><br><span class="line">    <span class="comment"># v(s) = \sum_a \pi(a|s) + \gamma \sum_s^\prime P_&#123;ss^\prime&#125;^a v(s^\prime)</span></span><br><span class="line">    v_aux = v[:]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(<span class="number">16</span>):</span><br><span class="line">        v_aux[i] = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> action <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">            j, r = get_new_loc(i, action)</span><br><span class="line">            v_aux[i] += <span class="number">0.25</span>*(r+gamma*v[j])</span><br><span class="line">    v = v_aux</span><br><span class="line">    <span class="keyword">if</span> k <span class="keyword">in</span> K:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">'k = &#123;&#125; '</span>.format(k),</span><br><span class="line">        <span class="keyword">print</span> <span class="string">', '</span>.join(map(<span class="keyword">lambda</span> x: <span class="string">'&#123;:.1f&#125;'</span>.format(x), v))</span><br></pre></td></tr></table></figure>
<h2 id="策略的改进"><a href="#策略的改进" class="headerlink" title="策略的改进"></a>策略的改进</h2><p>评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示：</p>
<script type="math/tex; mode=display">\pi^\prime = arg\max_{a\in \mathcal{A}}q_\pi(s,a)</script><p>以上小节中给出的算例为例，最终值函数结果为：<br><img src="/img/silver_rl_dp_policy_evaluating_demo_result.png" alt="策略估计结果"></p>
<p>那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。</p>
<p>为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。<br><img src="/img/silver_rl_dp_improve_policy_greedily_proof.png" alt="贪心方法work的证明"></p>
<p>当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。<br><img src="/img/silver_rl_dp_improve_policy_greedily_proof_2.png" alt="贪心方法的终止"></p>
<h2 id="值迭代"><a href="#值迭代" class="headerlink" title="值迭代"></a>值迭代</h2><p>首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\prime$，都有，该策略对$s^\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。</p>
<script type="math/tex; mode=display">v^\ast(s) = \max_{a\in \mathcal{A}}R_s^a+\gamma\sum_{s^\prime\in \mathcal{S}}P_{ss^\prime}^av^\ast(s^\prime)</script><p>通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）：<br><img src="/img/silver_rl_dp_value_iteration_demo.png" alt="值迭代方法示例"></p>
<p>每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。</p>
<p>注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。</p>
<h2 id="异步DP"><a href="#异步DP" class="headerlink" title="异步DP"></a>异步DP</h2><p>上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\prime$的旧值来计算$s$的新值。如下图所示：<br><img src="/img/silver_rl_dp_synchronous_value_iteration.png" alt="同步更新"><br>上面讨论了同步迭代的三个主要问题：<br><img src="/img/silver_rl_dp_synchronous_dp_algorithms.png" alt="同步更新问题"></p>
<p>我们也可以使用异步方法。主要包括以下三种：</p>
<h3 id="就地（in-place）-DP"><a href="#就地（in-place）-DP" class="headerlink" title="就地（in-place） DP"></a>就地（in-place） DP</h3><p>就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\text{new}}(s^\orime)$来更新$v(s)$。<br><img src="/img/silver_rl_dp_inplace_value_iteration.png" alt="就地更新迭代值函数"></p>
<h3 id="带有优先级的状态扫描（Prioritized-Sweeping）"><a href="#带有优先级的状态扫描（Prioritized-Sweeping）" class="headerlink" title="带有优先级的状态扫描（Prioritized Sweeping）"></a>带有优先级的状态扫描（Prioritized Sweeping）</h3><p>根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即</p>
<script type="math/tex; mode=display">|\max\_{a\in A}(R\_s^a+\gamma\sum\_{s^\prime\in S}P\_{ss^\prime}^a v(s^\prime)-v(s)|</script><p>实现的具体细节如下：<br><img src="/img/silver_rl_dp_detailed_prioritized_dp.png" alt></p>
<h3 id="实时DP"><a href="#实时DP" class="headerlink" title="实时DP"></a>实时DP</h3><p>使用智能体与环境交互的经验（experience）来挑选状态。<br><img src="/img/silver_rl_dp_realtime_dp.png" alt="实时DP"></p>
]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Silver RL课程 - MDP</title>
    <url>/2017/05/31/silver-rl-mdp/</url>
    <content><![CDATA[<p>Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。<br><img src="/img/silver_rl_mdp.png" alt="MDP"><br><a id="more"></a></p>
<h2 id="马尔科夫性质"><a href="#马尔科夫性质" class="headerlink" title="马尔科夫性质"></a>马尔科夫性质</h2><p>不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立：</p>
<script type="math/tex; mode=display">P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \dots, S_t]</script><p>定义状态转移概率（State Transition Probability）如下：</p>
<script type="math/tex; mode=display">P_{ss^\prime} = P[S_{t+1}=s^\prime|S_t=s]</script><p>前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。</p>
<p>下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。</p>
<h2 id="马尔科夫过程"><a href="#马尔科夫过程" class="headerlink" title="马尔科夫过程"></a>马尔科夫过程</h2><p>马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中，</p>
<ul>
<li>$S$是一个有限状态集合。</li>
<li>$P$是状态转移矩阵，定义如上。</li>
</ul>
<h2 id="马尔科夫奖赏过程"><a href="#马尔科夫奖赏过程" class="headerlink" title="马尔科夫奖赏过程"></a>马尔科夫奖赏过程</h2><p>马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \gamma)$来表示。其中，</p>
<ul>
<li>$R$代表奖励函数，$R_s = E[R_{t+1}|S_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。</li>
<li>$\gamma$是折旧率（discount），$\gamma \in [0,1]$</li>
</ul>
<p>定义回报（Return）为当前时刻往后得到的折旧总奖励，即：</p>
<script type="math/tex; mode=display">G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}</script><p>折旧率的引入，有以下几点考虑：</p>
<ul>
<li>在有环存在的马氏过程中，避免了无穷大回报的出现。</li>
<li>未来的不确定性对当前的影响较小。</li>
<li>事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。</li>
<li>人类行为倾向于即时奖励。</li>
<li>如果马氏过程是存在终止的，有的时候也可以使用$\gamma=1$，也就是不打折。</li>
</ul>
<h2 id="值函数"><a href="#值函数" class="headerlink" title="值函数"></a>值函数</h2><p>值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下：</p>
<script type="math/tex; mode=display">v(s) = E[G_t|S_t=s]</script><p>值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示：<br><img src="/img/silver_mdp_value_function.png" alt="推导过程"></p>
<p>最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面：</p>
<script type="math/tex; mode=display">\begin{aligned}
E[G_{t+1}|S_t = s]  &= \sum_{s^\prime\in S}E[G_{t+1}|S_{t+1}=s^\prime]P(S_{t+1}=s^\prime|S_t=s)\\
&=\sum_{s^\prime}v(S_{t+1}=s^\prime)P(S_{t+1}=s^\prime|S_t=s)\\
&=E[v(S_{t+1})|S_t=s]
\end{aligned}</script><p>上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\prime$过程中收到的奖励$r$，和从新的状态$s^\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。<br><img src="/img/silver_rl_bellman_equation_figure.png" alt="值函数的递归过程可视化"></p>
<p>把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。</p>
<script type="math/tex; mode=display">v(s) = R_s + \gamma\sum_{s^\prime \in S}P_{ss^\prime}v(s^\prime)</script><p>或者写成下面的矩阵形式，更加紧凑：</p>
<script type="math/tex; mode=display">v = R+\gamma Pv</script><p><img src="/img/silver_rl_bellman_equation_matrix.png" alt="bellman方程的矩阵形式"></p>
<p>当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下：<br><img src="/img/silver_rl_bellman_equation_solution.png" alt="Bellman方程求解"></p>
<p>对于含有$n$个状态的系统，求解复杂度是$\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有：</p>
<ul>
<li>动态规划 DP</li>
<li>蒙特卡洛仿真（Monte-Carlo evaluation）</li>
<li>时间差分学习（Temporal Difference Learning）</li>
</ul>
<h2 id="马尔科夫决策过程"><a href="#马尔科夫决策过程" class="headerlink" title="马尔科夫决策过程"></a>马尔科夫决策过程</h2><p>马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\gamma)$描述。其中，</p>
<ul>
<li>$A$是一个有限决策集合。</li>
<li>$P_{ss^\prime}^a = P(S_{t+1}=s^\prime|S_t=s, A_t=a)$是状态转移概率矩阵。</li>
<li>$R_{s}^a = E[R_{t+1}|S_t=s, A_t=a]$是奖励函数（与动作也挂钩）</li>
</ul>
<h3 id="策略"><a href="#策略" class="headerlink" title="策略"></a>策略</h3><p>策略（Policy）$\pi$是指在给定状态情况下，采取动作的概率分布，如下：</p>
<script type="math/tex; mode=display">\pi(a|s)=P(A_t=a|S_t=s)</script><p>对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。</p>
<p>当我们给定一个MDP和相应的策略$\pi$时，状态转移过程$S_1,S_2,\dots$是一个马氏过程$(S, P^\pi)$（上标$\pi$表示$P$由$\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\dots$是一个马氏奖赏过程$(S, P^\pi, R^\pi, \gamma)$。具体来说，如下（就是全概率公式）：</p>
<script type="math/tex; mode=display">\begin{aligned}
P_{ss^\prime}^\pi &= \sum_{a\in A}\pi(a|s)P_{ss^\prime}^a\\
R_s^\pi &=\sum_{a\in A}\pi(a|s)R_s^a
\end{aligned}</script><h3 id="值函数-1"><a href="#值函数-1" class="headerlink" title="值函数"></a>值函数</h3><p>MDP的值函数$v_\pi(s)$是指在当前状态$s$出发，使用策略$\pi$得到的回报期望，即，</p>
<script type="math/tex; mode=display">v_\pi(s) = E_\pi[G_t|S_t=s]</script><p>引入“动作-值”函数（action-value function）$q_\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\pi$得到的回报期望，即，</p>
<script type="math/tex; mode=display">q_\pi(s,a) = E_\pi[G_t|S_t=s,A_t=a]</script><h3 id="贝尔曼方程"><a href="#贝尔曼方程" class="headerlink" title="贝尔曼方程"></a>贝尔曼方程</h3><p>两者的关系如下如所示（通过全概率公式联系）：<br><img src="/img/silver_rl_mdp_vq_relationship.png" alt="Q函数和值函数的关系"></p>
<p>注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\prime$的值函数$v(s^\prime)$之间的关系。同样是一个全概率公式：<br><img src="/img/silver_rl_mdp_vq_relationship2.png" alt="相邻时刻Q函数和值函数的关系"></p>
<p>综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\prime)$的关系：<br><img src="/img/silver_rl_mdp_vv_relationship.png" alt="相邻时刻值函数关系"></p>
<p>同样，相邻时刻Q函数的关系：<br><img src="/img/silver_rl_mdp_qq_relationship.png" alt="相邻时刻Q函数关系"></p>
<p>写成紧凑的矩阵形式：</p>
<script type="math/tex; mode=display">v_\pi = R^\pi + \gamma P^\pi v_\pi</script><p>这个方程的解是：</p>
<script type="math/tex; mode=display">v_\pi = (1-\gamma P^\pi)^{-1}R^\pi</script><p>和上面对策略函数的分解类似，我们有下面两式成立：</p>
<script type="math/tex; mode=display">v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(sS_{t+1})]</script><h3 id="最优值函数"><a href="#最优值函数" class="headerlink" title="最优值函数"></a>最优值函数</h3><p>最优的值函数是指在所有的策略中，使得$v_\pi(s)$取得最大值的那个，即：</p>
<script type="math/tex; mode=display">v_\ast(s) = \max_\pi v_\pi(s)</script><p>最优Q函数的定义同理：</p>
<script type="math/tex; mode=display">q_\ast(s,a) = \max_\pi q_\pi(s,a)</script><p>定义策略$\pi$集合上的一个偏序为</p>
<script type="math/tex; mode=display">\pi > \pi^\prime \quad \text{if} \quad v_\pi(s) > v_{\pi^\prime}(s), \forall s</script><p>如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是：</p>
<p><img src="/img/silver_rl_mdp_optimal_policy.png" alt="最优策略的取法"><br>同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。<br><img src="/img/silver_rl_mdp_optimal_vq_relationship.png" alt><br><img src="/img/silver_rl_mdp_optimal_vq_relationship2.png" alt><br><img src="/img/silver_rl_mdp_optimal_vv_relationship.png" alt><br><img src="/img/silver_rl_mdp_optimal_qq_relationship.png" alt></p>
<p>常用的求解方法包括：</p>
<ul>
<li>值迭代（Value Iteration）</li>
<li>策略迭代（Policy Iteration）</li>
<li>Q Learning</li>
<li>Sarsa</li>
</ul>
]]></content>
      <tags>
        <tag>reinforcement learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network for Machine Learning - Lecture 02</title>
    <url>/2017/05/25/hinton-nnml-02/</url>
    <content><![CDATA[<p>这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。</p>
<p><img src="/img/hinton_02_perceptron_gragh.png" alt="Perceptron"></p>
<a id="more"></a>
<h2 id="Different-neural-network-archs"><a href="#Different-neural-network-archs" class="headerlink" title="Different neural network archs"></a>Different neural network archs</h2><h3 id="Feed-forward-neural-network-前馈神经网络"><a href="#Feed-forward-neural-network-前馈神经网络" class="headerlink" title="Feed-forward neural network 前馈神经网络"></a>Feed-forward neural network 前馈神经网络</h3><p>前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。<br><img src="/img/hinton_02_feed_forward_nn.png" alt="前馈网络"></p>
<h3 id="Recurrent-network"><a href="#Recurrent-network" class="headerlink" title="Recurrent network"></a>Recurrent network</h3><p>RNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。<br><img src="/img/hinton_02_recurrent_nn.png" alt="RNN"></p>
<p>RNN常用来对序列进行建模（modeling squence）。这里有一篇<a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/" target="_blank" rel="noopener">不错的介绍</a>。<br><img src="/img/hinton_02_rnn_app.png" alt="RNN modeling squences"></p>
<h3 id="Symmetrically-connected-network"><a href="#Symmetrically-connected-network" class="headerlink" title="Symmetrically connected network"></a>Symmetrically connected network</h3><p>这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。</p>
<h2 id="Percetron"><a href="#Percetron" class="headerlink" title="Percetron"></a>Percetron</h2><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是：</p>
<ul>
<li>将原始输入向量转化为feature activation。</li>
<li>寻找合适的权重对feature进行加权，得到某个标量。</li>
<li>如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。</li>
</ul>
<p><img src="/img/hinton_02_perceptron_paradigm_for_pattern_recong.png" alt="通用思路"></p>
<p>对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。</p>
<p>所以，感知机的数学模型可以描述如下：</p>
<script type="math/tex; mode=display">y=\begin{cases}1, \quad \text{if} \quad \sum w_ix_i+b>0 \\ 0 \quad \text{otherwise}\end{cases}</script><p>训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法：</p>
<ul>
<li>若预测值与真实值相同，则不作调整；</li>
<li>否则，若错输出为$0$，则将输入的$x$加到权重$w$上去；</li>
<li>若错输出为1，则从权重$w$上将输入的$x$减去。</li>
</ul>
<p>当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。</p>
<h3 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h3><p>下面从几何角度分析一下感知机。</p>
<p>权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。</p>
<p>让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。</p>
<p><img src="/img/hinton_02_weight_space_hyperplane.png" alt="权重空间的分类超平面"></p>
<p>当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。</p>
<p>所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。</p>
<p>下面用刚才的这种思考方式证明上述训练方法的正确性。</p>
<p>首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。<br><img src="/img/hinton_02_why_training_works_1.png" alt="why training works"></p>
<p>我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。</p>
<p>我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。）<br><img src="/img/hinton_02_margin.png" alt="带margin的可行域"></p>
<p>每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。）</p>
<h2 id="感知机的局限"><a href="#感知机的局限" class="headerlink" title="感知机的局限"></a>感知机的局限</h2><p>感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。<br><img src="/img/hinton_02_perceptron_xor.png" alt="感知机不能解决异或问题"></p>
]]></content>
      <tags>
        <tag>公开课</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu Cannot Mount exfat格式硬盘的解决办法</title>
    <url>/2017/05/04/ubuntu-cannot-mount-exfat-disk/</url>
    <content><![CDATA[<p>我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示<code>unable to mount</code>。这篇文章是对解决办法的记录。<br><img src="/img/ubuntu_exfat.png" alt="Ubuntu Exfat"></p>
<a id="more"></a>
<h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>参见<a href="https://askubuntu.com/questions/531919/ubuntu-14-04-cant-mount-exfat-external-hard-disk" target="_blank" rel="noopener">页面</a>，运行以下命令：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo -i  # 获取root权限</span><br><span class="line"># apt-get update</span><br><span class="line"># apt-get install --reinstall exfat-fuse exfat-utils</span><br><span class="line"># mkdir -p /media/user/exfat</span><br><span class="line"># chmod -Rf 777 /media/user/exfat</span><br><span class="line"># fdisk -l</span><br></pre></td></tr></table></figure>
<p>之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。</p>
<p>在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Neural Network for Machine Learning - Lecture 01</title>
    <url>/2017/05/03/hinton-nnml-01/</url>
    <content><![CDATA[<p>Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。<br><img src="/img/hinton_brainsimulator.jpg" alt="神经元"><br><a id="more"></a></p>
<h2 id="Why-do-we-need-ML"><a href="#Why-do-we-need-ML" class="headerlink" title="Why do we need ML?"></a>Why do we need ML?</h2><p>从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。<br><img src="/img/hinton_01_mnist_example.png" alt="MNIST Digit 2"></p>
<h2 id="What-are-neural-network"><a href="#What-are-neural-network" class="headerlink" title="What are neural network?"></a>What are neural network?</h2><p>为什么要研究神经网络？</p>
<ul>
<li>理解人脑机理的一个途径；</li>
<li>受到神经系统启发的并行计算</li>
<li>新的学习算法来解决现实问题（本课所关心的）</li>
</ul>
<p>（这里总结的不是很科学，勉强概括了讲义的内容）<br>神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。<br><img src="/img/hinton_01_neuron_structure.png" alt="神经元的结构"></p>
<p>神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。<br><img src="/img/hinton_01_neuron_commucation.png" alt="神经元的相互连接"></p>
<p>大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。</p>
<p>而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。</p>
<h2 id="Simple-models-of-different-neurons"><a href="#Simple-models-of-different-neurons" class="headerlink" title="Simple models of different neurons"></a>Simple models of different neurons</h2><p>我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。</p>
<h3 id="Linear-neuron"><a href="#Linear-neuron" class="headerlink" title="Linear neuron"></a>Linear neuron</h3><p>顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示：</p>
<script type="math/tex; mode=display">y = b+\sum_{i=1}^{n}w_ix_i</script><h3 id="Binary-threshold-neuron"><a href="#Binary-threshold-neuron" class="headerlink" title="Binary threshold neuron"></a>Binary threshold neuron</h3><p>这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述：</p>
<script type="math/tex; mode=display">y = \begin{cases}1 \quad \text{if} \quad z\ge \theta\\ 0\quad \text{otherwise}\end{cases}</script><p>其中，$z$是输入信号的线性组合，$z=\sum_{i}w_ix_i$</p>
<p>或者，</p>
<script type="math/tex; mode=display">y = \begin{cases}1 \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}</script><p>其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\sum_{i}w_ix_i$</p>
<h3 id="Rectified-linear-neuron"><a href="#Rectified-linear-neuron" class="headerlink" title="Rectified linear neuron"></a>Rectified linear neuron</h3><p>和上面的二值化神经元对比，有：</p>
<script type="math/tex; mode=display">y = \begin{cases}z \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}</script><h3 id="Sigmoid-neuron"><a href="#Sigmoid-neuron" class="headerlink" title="Sigmoid neuron"></a>Sigmoid neuron</h3><p>这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示：</p>
<script type="math/tex; mode=display">y = \frac{1}{1+\exp(-z)}</script><p><img src="/img/hinton_01_sigmoid_function.png" alt="Logistic函数示意"></p>
<p>由于Logistic函数将$(-\infty, +\infty)$的值压缩为S型，所以得名Sigmoid。</p>
<h3 id="Stochastic-binary-neuron"><a href="#Stochastic-binary-neuron" class="headerlink" title="Stochastic binary neuron"></a>Stochastic binary neuron</h3><p>这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是：</p>
<script type="math/tex; mode=display">P(y=1) = \frac{1}{1+\exp(-z)}</script><p>对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。</p>
<h2 id="Three-types-of-learning"><a href="#Three-types-of-learning" class="headerlink" title="Three types of learning"></a>Three types of learning</h2><p>即有监督学习，无监督学习和强化学习。</p>
<ul>
<li>有监督学习：给定输入向量，预测输出。</li>
<li>无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。</li>
<li>强化学习：学习如何决策达到最大期望奖赏。</li>
</ul>
<h3 id="有监督学习"><a href="#有监督学习" class="headerlink" title="有监督学习"></a>有监督学习</h3><p>有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。</p>
<p>所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。</p>
<h3 id="强化学习"><a href="#强化学习" class="headerlink" title="强化学习"></a>强化学习</h3><p>在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。</p>
<p>强化学习的难点如下：</p>
<ul>
<li>奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。</li>
<li>奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。</li>
</ul>
<h3 id="无监督学习"><a href="#无监督学习" class="headerlink" title="无监督学习"></a>无监督学习</h3><p>无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。</p>
]]></content>
      <tags>
        <tag>公开课</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-光流估计</title>
    <url>/2017/05/03/cs131-opticalflow/</url>
    <content><![CDATA[<p>光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。</p>
<p><del>是你让我的世界从那刻变成粉红色</del>  划掉。。。<br><img src="/img/cs131_opticalflow_demo.jpg" alt="OpticalFlow可视化"><br><a id="more"></a></p>
<h2 id="光流的计算"><a href="#光流的计算" class="headerlink" title="光流的计算"></a>光流的计算</h2><p>光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。</p>
<p>光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设：</p>
<ul>
<li>前后两帧点的位移不大（泰勒展开）</li>
<li>外界光强保持恒定。</li>
<li>空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开）</li>
</ul>
<p>其中第二条外界光强保持恒定，可以从下面的等式来理解。<br><img src="/img/cs131_opticalflow_brightnessconstancy_assumption.png" alt="光照强度保持恒定图解"></p>
<p>在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下：</p>
<script type="math/tex; mode=display">I(x,y,t-1) = I(x+u, y+v, t)</script><p>从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。</p>
<script type="math/tex; mode=display">I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t</script><p>上面两式联立，可以得到，</p>
<script type="math/tex; mode=display">I_xu+I_yv+I_t=0</script><p>上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。</p>
<p>这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。</p>
<p>上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法</p>
<h2 id="L-K方法"><a href="#L-K方法" class="headerlink" title="L-K方法"></a>L-K方法</h2><p>上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\times 5$的时候，建立的方程。<br><img src="/img/cs131_opticalflow_lkequation.png" alt="L-K方程"></p>
<p>使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。<br><img src="/img/cs131_opticalflow_lkleastsquare.png" alt="最小二乘法后的式子"></p>
<p>上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？</p>
<ul>
<li>$\mathbf{A}^\dagger \mathbf{A}$是可逆的。</li>
<li>$\mathbf{A}^\dagger \mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\lambda_1$, $\lambda_2$不应该太小。</li>
<li>$\mathbf{A}^\dagger \mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\lambda_1/\lambda_2$不应该太大。</li>
</ul>
<p>而我们在Harris角点检测的时候已经讨论过$\mathbf{A}^\dagger \mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。<br><img src="/img/cs131_opticalflow_lkrelationshipwithharris.png" alt="是不是和Harris角点更像了"></p>
<p>下面这张图就是当时的讨论结果。<br><img src="/img/cs131_opticalflow_lkharris.png" alt="不同点的分类"></p>
<p>上面就是使用L-K方法估计光流的一般思路。</p>
<h2 id="金字塔方法"><a href="#金字塔方法" class="headerlink" title="金字塔方法"></a>金字塔方法</h2><p>在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。</p>
<p><img src="/img/cs131_opticalflow_pyramid.png" alt="图像金字塔方法"></p>
<h2 id="作业：基于光流法的帧间插值"><a href="#作业：基于光流法的帧间插值" class="headerlink" title="作业：基于光流法的帧间插值"></a>作业：基于光流法的帧间插值</h2><h3 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h3><p>假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0&lt;t&lt;1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。</p>
<h3 id="简单粗暴法"><a href="#简单粗暴法" class="headerlink" title="简单粗暴法"></a>简单粗暴法</h3><p>我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说：</p>
<script type="math/tex; mode=display">I_t = (1-t)I_0+tI_1</script><p>这种方法称为”cross-fading”。效果如下。可以看到有较多的模糊抖动。<br><img src="/img/cs131_opticalflow_assignment_crossfade.gif" alt="动图"><br><img src="/img/cs131_opticalflow_assignment_crossfade.png" alt="简单粗暴法效果"></p>
<h3 id="基于光流法"><a href="#基于光流法" class="headerlink" title="基于光流法"></a>基于光流法</h3><p>使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示：</p>
<script type="math/tex; mode=display">I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)</script><p>用MATLAB实现如下：<br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> y =<span class="number">1</span>:height</span><br><span class="line">    <span class="keyword">for</span> x = <span class="number">1</span>:width</span><br><span class="line">        dy = <span class="built_in">min</span>(<span class="built_in">max</span>(<span class="built_in">round</span>(y+v0(y,x)*t), <span class="number">1</span>), height);</span><br><span class="line">        dx = <span class="built_in">min</span>(<span class="built_in">max</span>(<span class="built_in">round</span>(x+u0(y,x)*t), <span class="number">1</span>), width);</span><br><span class="line">        img(dy,dx,:) = img0(y,x,:);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>这种方法叫做”ForwardWarpping”。效果如下。可以看到，与上一种方法对比，画面有了明显的提升。<br><img src="/img/cs131_opticalflow_assignment_forwardwarped.gif" alt="动图"><br><img src="/img/cs131_opticalflow_assignment_forwardwarped.png" alt="逐帧"></p>
<h3 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h3><p>上面的方法我们假设光流一直保持不变，用$t=1$时刻的光流去代替之间所有时刻的光流。但实际上光流一定是在实时变化的。使用<em>backward warpping</em>改进，使用$t$时刻的光流反推。</p>
<script type="math/tex; mode=display">I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))</script><p>然而，我们并不能得到$t$时刻光流$u_t$和$v_t$的准确值，只能近似计算。方法如下：</p>
<script type="math/tex; mode=display">u_t(\hat{x},\hat{y}) = u_0(x,y)</script><script type="math/tex; mode=display">v_t(\hat{x},\hat{y}) = v_0(x,y)</script><p>其中，$x^\prime = x+u_0t$，$y^\prime = y+v_0t$，$\hat{x}\in\lbrace\text{floor}(x^\prime), \text{ceil}(x^\prime)\rbrace$，$\hat{y}\in\lbrace\text{floor}(y^\prime), \text{ceil}(y^\prime)\rbrace$。</p>
<p>这在一定程度上补偿了光流计算的误差。</p>
<p>如果某个点$(\hat{x}, \hat{y})$被多个初始点$(x, y)$对应，那么我们选取使得下面的式子取得最小值的那个点$(x, y)$，也就是选取那个亮度变化最小的点。</p>
<script type="math/tex; mode=display">\vert I_0(x, y) − I_1(x + u_0(x, y), y + v_0(x, y))\vert</script><p>如果某个点没有找到相对应的初始点，那么我们使用线性插值方法为其填充光流。</p>
<p>下面是这种方法的效果示意。</p>
<p><img src="/img/cs131_opticalflow_assignment_flowwarped.gif" alt="动图"><br><img src="/img/cs131_opticalflow_assignment_flowwarped.png" alt="逐帧"></p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 5 实现</title>
    <url>/2017/05/01/effective-cpp-05/</url>
    <content><![CDATA[<p>第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。</p>
<a id="more"></a>
<h2 id="26-尽可能延后变量定义式的出现时间"><a href="#26-尽可能延后变量定义式的出现时间" class="headerlink" title="26 尽可能延后变量定义式的出现时间"></a>26 尽可能延后变量定义式的出现时间</h2><p>变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。</p>
<h2 id="27-尽量少做转型动作"><a href="#27-尽量少做转型动作" class="headerlink" title="27 尽量少做转型动作"></a>27 尽量少做转型动作</h2><p>第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。</p>
<p>第二，效率问题。这里展开说。</p>
<p>C++中的转型动作有以下三种：</p>
<ul>
<li><code>(T)expression</code>，C时代的风格</li>
<li><code>T(expression)</code>，函数风格</li>
<li>新式风格，包括<code>static_cast</code>,<code>const_cast</code>,<code>dynamic_cast</code>和<code>reinterpret_cast</code>四种。</li>
</ul>
<p>作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。</p>
<p>对于 <code>const_cast</code>，用于脱掉某对象的<code>const</code>属性。</p>
<p>对于<code>static_cast</code>，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将<code>double</code>类型转换为<code>int</code>等。</p>
<p>对于<code>dynamic_cast</code>，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，<code>dynamic_cast</code>的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。</p>
<p>很多时候，之所以使用<code>dynamic_cast</code>是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。</p>
<p>对于<code>reinterpret_cast</code>，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为<code>int</code>，此转换是在bit层面的转换，更详细的信息可以参见<a href="http://en.cppreference.com/w/cpp/language/reinterpret_cast" target="_blank" rel="noopener">cpp reference的介绍</a>。</p>
<p>在它们之中，<code>reinterpret_cast</code>和<code>const_cast</code>完全是编译器层面的东西。<code>static_cast</code>会导致编译器生成对应CPU指令，但是是在编译器就能决定的。<code>dynamic_cast</code>是在运行时多态的一种手段。</p>
<h2 id="28-避免返回指向对象内部成分的句柄（handle）"><a href="#28-避免返回指向对象内部成分的句柄（handle）" class="headerlink" title="28 避免返回指向对象内部成分的句柄（handle）"></a>28 避免返回指向对象内部成分的句柄（handle）</h2><p>当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为<code>const</code>，但是实际上并不能避免通过此句柄修改资源的情况发生。</p>
<p>例如在自定义的<code>string</code>类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合<code>const</code>约定的。</p>
<p>所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回<code>const handle&amp;</code>。</p>
<p>即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。</p>
<h2 id="29-为“异常安全”努力是值得的"><a href="#29-为“异常安全”努力是值得的" class="headerlink" title="29 为“异常安全”努力是值得的"></a>29 为“异常安全”努力是值得的</h2><p>异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的<code>delete</code>操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。</p>
<p>让我们先看一个不满足异常安全的函数例子：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 自定义`Menu`类中修改背景图片的成员函数</span></span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">Menu::changeBg</span><span class="params">(<span class="built_in">std</span>::istream&amp; imgSrc)</span> </span>&#123;</span><br><span class="line">    lock(&amp;<span class="keyword">this</span>-&gt;mutex);  <span class="comment">// 互斥锁 1</span></span><br><span class="line">    <span class="keyword">delete</span> <span class="keyword">this</span>-&gt;bg;     <span class="comment">// 释放本已有的bg2</span></span><br><span class="line">    ++<span class="keyword">this</span>-&gt;imgChangeCnt;      <span class="comment">// 计数器  3</span></span><br><span class="line">    bg = <span class="keyword">new</span> Image(imgSrc);    <span class="comment">// 新图片  4</span></span><br><span class="line">    unlock(&amp;<span class="keyword">this</span>-&gt;mutex);      <span class="comment">// 释放锁  5</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>上面的代码存在以下问题，使得它不满足异常安全性。</p>
<ul>
<li>资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。</li>
<li>数据被破坏。还是上面的情况，则<code>bg</code>此时的资源已经被析构，而且计数器的值也增加了。·</li>
</ul>
<p>解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。</p>
<p>我们将异常安全分为以下三类：</p>
<ul>
<li>基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟<code>bg</code>是何值）</li>
<li>强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。</li>
</ul>
<p>在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。</p>
<ul>
<li>绝不抛出异常。作用于内置类型（<code>int</code>或指针等）身上的所有操作都提供了nothrow保证。</li>
</ul>
<h2 id="30-透彻了解内联的方方面面"><a href="#30-透彻了解内联的方方面面" class="headerlink" title="30 透彻了解内联的方方面面"></a>30 透彻了解内联的方方面面</h2><p><code>inline</code>函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用<code>inline</code>替换函数宏定义。</p>
<p>然而，<code>inline</code>不要乱用。首先，<code>inline</code>会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。</p>
<p>另一方面，<code>inline</code>只是对编译器的申请，不是真的一定内联。</p>
<p><code>inline</code>函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入<code>inline</code>关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。</p>
<p>有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。</p>
<p>作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。</p>
<h2 id="31-将文件间的编译依存关系降至最低"><a href="#31-将文件间的编译依存关系降至最低" class="headerlink" title="31 将文件间的编译依存关系降至最低"></a>31 将文件间的编译依存关系降至最低</h2><p>C++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。</p>
<p>首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。</p>
<p>另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。</p>
<p>从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。</p>
<p>使用这种思虑，定义的包含有<code>Date</code>类型对象（指明这个人的生日）的<code>Person</code>类如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;string&gt;   // for string</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;memory&gt;   // for shared_ptr</span></span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PersonImpl</span>;</span>   <span class="comment">// Person实现类的前置声明</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Date</span>;</span>         <span class="comment">// Person接口用到的类的前置声明</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Person(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name, <span class="keyword">const</span> Date&amp; birthday);</span><br><span class="line">    <span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span></span>;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">shared_ptr</span>&lt;PersonImpl&gt; pImpl;   <span class="comment">// 指向实现类</span></span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*****************实现文件****************/</span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"Person.h"</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">"PersonImpl.h"</span></span></span><br><span class="line">Person::Person(<span class="keyword">const</span> <span class="built_in">std</span>::<span class="built_in">string</span>&amp; name, <span class="keyword">const</span> Date&amp; birthday):</span><br><span class="line">    pImpl(<span class="keyword">new</span> PersonImpl(name, birthday)) &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="built_in">std</span>::<span class="built_in">string</span> <span class="title">Person::name</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> pImpl-&gt;name();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在上面的代码中，通过构造handle类<code>PersonImpl</code>，在<code>Person</code>中我们只需要前置声明<code>Date</code>，而无需包含头文件<code>date.hpp</code>。这样，即使<code>Date</code>或者<code>Person</code>有修改，影响也仅限于<code>Date</code>的实现文件和<code>PersonImpl</code>而已，不会传导到<code>Person</code>和使用了<code>Person</code>的其他代码文件。通过这种做法，实际上<code>Person</code>成为了一个单纯的接口，具体的实现在<code>PersonImpl</code>中完成，实现了“接口与实现的分离”。</p>
<p>综上：</p>
<ul>
<li>如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。</li>
<li>如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。</li>
<li>为声明式和定义式提供不同的头文件（<code>Person</code>本身和<code>PersonImpl</code>）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。</li>
</ul>
<p>除了上面的方法，也可以将<code>Person</code>定义为抽象基类（Caffe中的<code>Layer</code>就是类似的模式）。为了达成这一目标，<code>Person</code>需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">virtual</span> ~Person();</span><br><span class="line">    <span class="function"><span class="keyword">virtual</span> <span class="built_in">string</span> <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span> </span>= <span class="number">0</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为<code>static</code>。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Person</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">static</span> <span class="built_in">shared_ptr</span>&lt;Person&gt; <span class="title">create</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; name, <span class="keyword">const</span> Date&amp; birthday)</span></span>;</span><br><span class="line"><span class="comment">// ... 刚才的其他代码</span></span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>当然，要想使用，我们还必须定义派生类实现相应的接口。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RealPerson</span>:</span> <span class="keyword">public</span> Person &#123;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    RealPerson(<span class="keyword">const</span> <span class="built_in">string</span>&amp; name, <span class="keyword">const</span> Date&amp; birthday): name(name), birthDate(birthday) &#123;&#125;</span><br><span class="line">    <span class="keyword">virtual</span> ~RealPerson() &#123;&#125;</span><br><span class="line">    <span class="function"><span class="built_in">string</span> <span class="title">name</span><span class="params">()</span> <span class="keyword">const</span> </span>&#123; <span class="keyword">return</span> <span class="keyword">this</span>-&gt;name; &#125;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="built_in">string</span> name;</span><br><span class="line">    Date birthDate;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>上面的工厂函数<code>create()</code>的实现：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="built_in">shared_ptr</span>&lt;Person&gt; <span class="title">Person::create</span><span class="params">(<span class="keyword">const</span> <span class="built_in">string</span>&amp; name, <span class="keyword">const</span> Date&amp; date)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">shared_ptr</span>&lt;Person&gt;(<span class="keyword">new</span> RealPerson(name, date));</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。</p>
<p>当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。</p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 4 设计与声明</title>
    <url>/2017/04/29/effective-cpp-04/</url>
    <content><![CDATA[<p>良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。<br><img src="/img/effectivecpp_04_cwithclass.jpg" alt="带类的C?"><br><a id="more"></a></p>
<h2 id="18-让接口容易被使用，不易被误用"><a href="#18-让接口容易被使用，不易被误用" class="headerlink" title="18 让接口容易被使用，不易被误用"></a>18 让接口容易被使用，不易被误用</h2><p>首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入<code>const</code>限定修饰符。</p>
<p>其次，尽量使接口与内建类型等保持一致。例如STL中统一使用<code>size()</code>方法获取容器的大小。</p>
<p>任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。</p>
<h2 id="19-设计class犹如设计type"><a href="#19-设计class犹如设计type" class="headerlink" title="19 设计class犹如设计type"></a>19 设计<code>class</code>犹如设计<code>type</code></h2><p>设计自定义的<code>class</code>要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑：</p>
<ul>
<li>新的对象如何创建和销毁？这关系到构造和析构函数。</li>
<li>对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。</li>
<li>新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。</li>
<li>什么是新类型的合法值？可能需要对<code>setter()</code>函数进行参数检查。</li>
<li>新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。</li>
<li>新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。</li>
<li>什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。</li>
<li>什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。</li>
<li>谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为<code>friend</code>。</li>
<li>什么事新类型的未声明接口？</li>
<li>新类型有多一般化？是否建立<code>template class</code>更好？</li>
<li>新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。</li>
</ul>
<h2 id="20-宁以pass-by-reference-to-const替换pass-by-value"><a href="#20-宁以pass-by-reference-to-const替换pass-by-value" class="headerlink" title="20 宁以pass-by-reference-to-const替换pass-by-value"></a>20 宁以pass-by-reference-to-const替换pass-by-value</h2><p>对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。</p>
<p>总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。</p>
<h2 id="21-必须返回对象时，不要妄想返回其reference"><a href="#21-必须返回对象时，不要妄想返回其reference" class="headerlink" title="21 必须返回对象时，不要妄想返回其reference"></a>21 必须返回对象时，不要妄想返回其reference</h2><p>函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！</p>
<p>C++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。</p>
<h2 id="22-将成员变量声明为private"><a href="#22-将成员变量声明为private" class="headerlink" title="22 将成员变量声明为private"></a>22 将成员变量声明为<code>private</code></h2><p>封装，封装，还是封装！</p>
<p>而且，请记住，其实只有两种访问权限：<code>private</code>（提供了封装）和其他（包括<code>protected</code>，不提供封装）。</p>
<h2 id="23-宁以non-member和non-friend函数替换member函数"><a href="#23-宁以non-member和non-friend函数替换member函数" class="headerlink" title="23 宁以non-member和non-friend函数替换member函数"></a>23 宁以non-member和non-friend函数替换member函数</h2><p>对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三：</p>
<ul>
<li>封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。</li>
<li>代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。</li>
<li>编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。</li>
</ul>
<h2 id="24-若所有参数均需要类型转换，请为此采用non-member函数"><a href="#24-若所有参数均需要类型转换，请为此采用non-member函数" class="headerlink" title="24 若所有参数均需要类型转换，请为此采用non-member函数"></a>24 若所有参数均需要类型转换，请为此采用non-member函数</h2><p>作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为<code>explicit</code>，可以完成整形到有理数类的隐式类型转换。</p>
<p>重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Rational</span> &#123;</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Rational(<span class="keyword">int</span> numerator=<span class="number">0</span>, <span class="keyword">int</span> denominator=<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">const</span> Rational <span class="keyword">operator</span>*(<span class="keyword">const</span> Rational&amp; rhs) <span class="keyword">const</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure></p>
<p>然而，这样做的话，<code>auto res = 2*Rational(4,5)</code>就无法通过编译，因为<code>int</code>并没有实现<code>operator*(const Rational&amp;)</code>操作。</p>
<p>更好的方法是将其作为non-member函数，<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> Rational <span class="keyword">operator</span>*(<span class="keyword">const</span> Rational&amp; lhs, <span class="keyword">const</span> Rational&amp; rhs) &#123;</span><br><span class="line">    <span class="comment">//...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h2 id="25-考虑写出一个不抛出异常的swap-函数"><a href="#25-考虑写出一个不抛出异常的swap-函数" class="headerlink" title="25 考虑写出一个不抛出异常的swap()函数"></a>25 考虑写出一个不抛出异常的<code>swap()</code>函数</h2><p>这一条款更像是模板特化规则的大杂烩。</p>
<p>STL中的<code>swap()</code>函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(T&amp; a, T&amp;b)</span> </span>&#123;</span><br><span class="line">    <span class="function">T <span class="title">tmp</span><span class="params">(a)</span></span>;</span><br><span class="line">    a = b;</span><br><span class="line">    b = tmp;</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。</p>
<p>如何对我们的对象<code>Widget</code>实现特化？</p>
<p>如果<code>Widget</code>不是模板类，那么我们需要进行全特化。加入以下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> swap&lt;Widget&gt;(Widget&amp; a, Widget&amp; b) &#123;</span><br><span class="line">    swap(pImpl, b.pImpl);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>更好的解决方法是先将<code>swap()</code>定义为<code>Widget</code>类的公共成员函数，然后再全特化标准库的<code>swap()</code>方法时调用。这样与STL的约定保持一致。STL中<code>vector</code>等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了<code>std</code>名字空间的<code>swap()</code>方法。</p>
<p>当<code>Widget</code>是模板类时，需要进行偏特化。也许看上去是这样：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">namespace</span> <span class="built_in">std</span> &#123;</span><br><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> T&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">swap</span><span class="params">(Widget&lt;T&gt;&amp; a, Widget&lt;T&gt;&amp; b)</span> </span>&#123;</span><br><span class="line">    a.swap(b);</span><br><span class="line">&#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>但是程序员可以全特化<code>std</code>中的模板，却不能加入新的类或函数进入<code>std</code>中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。</p>
<p>所以，可以在<code>Widget</code>存在的名字空间内定义<code>swap()</code>（而不是加入<code>std</code>），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则：</p>
<ul>
<li>一般使用标准库中的<code>swap()</code>即可。</li>
<li>如果自己实现，首先提供一个<code>public</code>的<code>swap()</code>成员函数，注意这儿函数决不能抛出异常。</li>
<li>在类或者模板在的名字空间中提供一个non-member的<code>swap()</code>函数，并令它调用上述的<code>swap()</code>成员函数。</li>
<li>如果是类，而不是模板，那么特化<code>std::swap()</code>，并令它调用上述<code>swap()</code>成员函数。</li>
<li>在客户端代码调用<code>swap()</code>时，确定包含一个<code>using</code>声明式，以便让<code>std::swap()</code>在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用<code>swap()</code>。</li>
</ul>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP阅读 - Chapter 3 资源管理</title>
    <url>/2017/04/25/effective-cpp-03/</url>
    <content><![CDATA[<p>C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。<br><img src="/img/effectivecpp_02_pointers.png" alt="Pointer是什么？"><br><a id="more"></a></p>
<h2 id="13-以对象管理资源"><a href="#13-以对象管理资源" class="headerlink" title="13 以对象管理资源"></a>13 以对象管理资源</h2><p>书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。</p>
<p>本条款可以（较浅显地）归纳为：</p>
<blockquote>
<p>不要使用裸指针！要使用智能指针！</p>
</blockquote>
<p>我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。</p>
<p>许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了<code>weak_ptr</code>和<code>shared_ptr</code>，请使用它们。如果没有C++11，请使用boost）。</p>
<p>以对象管理资源的两个关键想法：</p>
<ul>
<li>获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！</li>
<li>管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。</li>
</ul>
<h2 id="14-在资源管理类中小心coping行为"><a href="#14-在资源管理类中小心coping行为" class="headerlink" title="14 在资源管理类中小心coping行为"></a>14 在资源管理类中小心coping行为</h2><p>有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。</p>
<p>如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Lock是互斥锁的资源管理类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Lock</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="function"><span class="keyword">explicit</span> <span class="title">Lock</span><span class="params">(Mutex* pm)</span>:<span class="title">mutexPtr</span><span class="params">(pm)</span> </span>&#123; <span class="comment">// 获得资源</span></span><br><span class="line">        lock(mutexPtr);</span><br><span class="line">    &#125;</span><br><span class="line">    ~Lock() &#123;unlock(mutexPtr); &#125; <span class="comment">//释放资源</span></span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    Mutex* mutexPtr;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>这样，我们期望能够使用<code>Lock</code>对象实现对互斥锁的自动管理。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Mutex m;   <span class="comment">// 互斥锁</span></span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#123;</span><br><span class="line"><span class="function">Lock <span class="title">ml</span><span class="params">(&amp;m)</span></span>;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">&#125;    <span class="comment">// 在区块外，ml自动析构，实现解锁</span></span><br></pre></td></tr></table></figure>
<p>然而，如何处理<code>Lock</code>对象的拷贝？</p>
<ul>
<li>情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。</li>
<li>情景二，对底层资源进行引用计数。也许我们可以利用<code>shared_ptr</code>，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看<code>shared_ptr</code>部分文档。</li>
<li>情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。</li>
<li>情景四，移除底层资源所有权。将所有权移至新的对象。</li>
</ul>
<h2 id="15-在资源管理类中提供对原始资源的访问"><a href="#15-在资源管理类中提供对原始资源的访问" class="headerlink" title="15 在资源管理类中提供对原始资源的访问"></a>15 在资源管理类中提供对原始资源的访问</h2><p>许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。</p>
<p>对于这种情况，智能指针提供了<code>get()</code>函数用来获取其原始指针的拷贝。同时，它们也重载了<code>-&gt;</code>和<code>*</code>操作符，允许隐式转换为原始指针。</p>
<p>我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型<code>T</code>可以通过定义<code>operator T()</code>实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。</p>
<h2 id="16-成对使用new和delete时采取相同的形式"><a href="#16-成对使用new和delete时采取相同的形式" class="headerlink" title="16 成对使用new和delete时采取相同的形式"></a>16 成对使用<code>new</code>和<code>delete</code>时采取相同的形式</h2><p>这项条款是说如果动态分配内存时候使用了<code>new T()</code>得到了单个对象的内存空间，那么销毁时应该使用<code>delete</code>销毁；如果当初使用了<code>new T[]</code>得到了对象数组空间，那么销毁时应该使用<code>delete []</code>。两者不能混用，否则会导致未定义行为。</p>
<p>另外，除非必要，不要使用原始数组。STL中的<code>vector</code>和<code>string</code>是替代数组的不错选择。</p>
<h2 id="17-以独立语句将newed对象置于智能指针"><a href="#17-以独立语句将newed对象置于智能指针" class="headerlink" title="17 以独立语句将newed对象置于智能指针"></a>17 以独立语句将newed对象置于智能指针</h2><p>以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。</p>
<p>书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。</p>
<p>如下面的函数声明：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">priority</span><span class="params">()</span> </span>&#123; <span class="comment">/*some code*/</span>&#125;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">process</span><span class="params">(<span class="built_in">shared_ptr</span>&lt;Widget&gt; pw, <span class="keyword">int</span> priority)</span> </span>&#123; <span class="comment">/*some code*/</span>&#125;</span><br></pre></td></tr></table></figure>
<p>在使用时，也许你会这样调用<code>process</code>函数。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">process(<span class="keyword">new</span> Widget(), priority());</span><br></pre></td></tr></table></figure>
<p>首先，这样是不能通过编译器的。因为<code>shared_ptr</code>的构造函数是<code>explicit</code>的，不能够隐式将原始指针转换为<code>shared_ptr</code>对象。但是改为下面的代码就没问题了吗？</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">process(<span class="built_in">shared_ptr</span>&lt;Widget&gt;(<span class="keyword">new</span> Widget()), priority());</span><br></pre></td></tr></table></figure>
<p>由于C++中函数参数的核算顺序是不确定的，所以可能发生：</p>
<ul>
<li>new出来一个Widget资源</li>
<li>调用<code>priority()</code>函数，注意此时可能引发异常，使得Widget资源无法回收</li>
<li>构造<code>shared_ptr</code>对象</li>
</ul>
<p>问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">auto</span> pw = <span class="built_in">shared_ptr</span>&lt;Widget&gt;(<span class="keyword">new</span> Widget());</span><br><span class="line">process(pw, priority());</span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 2 构造拷贝和赋值函数</title>
    <url>/2017/04/24/effective-cpp-02/</url>
    <content><![CDATA[<p>在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。<br><a id="more"></a></p>
<h2 id="05-了解C-默默编写并调用了哪些函数"><a href="#05-了解C-默默编写并调用了哪些函数" class="headerlink" title="05 了解C++默默编写并调用了哪些函数"></a>05 了解C++默默编写并调用了哪些函数</h2><p>C++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。</p>
<p>在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。</p>
<p>在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。</p>
<h2 id="06-若不想使用编译器自动生成的函数，那就明确拒绝"><a href="#06-若不想使用编译器自动生成的函数，那就明确拒绝" class="headerlink" title="06 若不想使用编译器自动生成的函数，那就明确拒绝"></a>06 若不想使用编译器自动生成的函数，那就明确拒绝</h2><p>有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为<code>private</code>，并且不提供函数的实现。</p>
<h2 id="07-为多态基类声明虚析构函数"><a href="#07-为多态基类声明虚析构函数" class="headerlink" title="07 为多态基类声明虚析构函数"></a>07 为多态基类声明虚析构函数</h2><p>多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。</p>
<p>而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为<code>virtual</code>也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。</p>
<p>由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下：</p>
<blockquote>
<p>那些有虚函数的类，几乎确定都应该有一个虚析构函数。</p>
</blockquote>
<p>对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括<code>vector</code>，<code>string</code>等。</p>
<p>有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ABC</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    <span class="keyword">virtual</span> ~ABC() = <span class="number">0</span>;    </span><br><span class="line">&#125;;</span><br><span class="line">ABC::~ABC() &#123;&#125;</span><br></pre></td></tr></table></figure>
<p>这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用<code>~ABC()</code>，所以必须为这个函数提供一份定义。</p>
<h2 id="08-别让异常逃离析构函数"><a href="#08-别让异常逃离析构函数" class="headerlink" title="08 别让异常逃离析构函数"></a>08 别让异常逃离析构函数</h2><p>如果在析构函数中抛出异常，会导致未定义的行为。</p>
<p>析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。</p>
<h2 id="09-绝不在构造函数和析构函数中调用虚函数"><a href="#09-绝不在构造函数和析构函数中调用虚函数" class="headerlink" title="09 绝不在构造函数和析构函数中调用虚函数"></a>09 绝不在构造函数和析构函数中调用虚函数</h2><p>你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。</p>
<p>为什么呢？</p>
<p>如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。</p>
<p>直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。</p>
<h2 id="10-令operator-返回一个-this的引用"><a href="#10-令operator-返回一个-this的引用" class="headerlink" title="10 令operator=返回一个*this的引用"></a>10 令<code>operator=</code>返回一个<code>*this</code>的引用</h2><p>为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。</p>
<p>所谓连锁赋值，是指下面这样的情况：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">a = b = c;</span><br></pre></td></tr></table></figure>
<p>这一条款不仅适用于赋值运算符，也适用于<code>+=</code>等。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">A</span> &#123;</span></span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    A&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> A&amp; rhs) &#123;</span><br><span class="line">        <span class="comment">// ...</span></span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<h2 id="11-在operator-中处理自我赋值"><a href="#11-在operator-中处理自我赋值" class="headerlink" title="11 在operator=中处理自我赋值"></a>11 在<code>operator=</code>中处理自我赋值</h2><p>所谓自我赋值，是指：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Widget w;</span><br><span class="line"><span class="comment">// ...</span></span><br><span class="line">w = w;</span><br></pre></td></tr></table></figure>
<p>自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。</p>
<p>一种方法是进行“证同测试”，如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">A&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> A&amp; rhs) &#123;</span><br><span class="line">    <span class="keyword">if</span>(&amp;rhs == <span class="keyword">this</span>) <span class="keyword">return</span> *<span class="keyword">this</span>; <span class="comment">//证同测试</span></span><br><span class="line">    <span class="comment">// ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。</p>
<p>还有一种方法，使用copy-swap方法，首先拷贝构造一个<code>rhs</code>的拷贝，然后交换该拷贝和<code>*this</code>，</p>
<p>毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。</p>
<h2 id="12-复制对象时勿忘每一个成分"><a href="#12-复制对象时勿忘每一个成分" class="headerlink" title="12 复制对象时勿忘每一个成分"></a>12 复制对象时勿忘每一个成分</h2><p>这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Derived</span>:</span> <span class="keyword">public</span> Base&#123;</span><br><span class="line"><span class="keyword">private</span>:</span><br><span class="line">    <span class="keyword">int</span> a;</span><br><span class="line"><span class="keyword">public</span>:</span><br><span class="line">    Derived(<span class="keyword">const</span> Derived&amp; rhs):a(rhs.a) &#123;&#125;</span><br><span class="line">    Derived&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Derived&amp; rhs) &#123;</span><br><span class="line">        a = rhs.a;</span><br><span class="line">        <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>上面的代码只是拷贝了派生类新加入的成员<code>a</code>，而对基类中已有的成员未作处理。要记住，</p>
<blockquote>
<p>任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为<code>private</code>，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为：</p>
</blockquote>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">Derived(<span class="keyword">const</span> Derived&amp; rhs):Base(rhs), a(rhs.a) &#123;&#125;</span><br><span class="line">Derived&amp; <span class="keyword">operator</span>=(<span class="keyword">const</span> Derived&amp; rhs) &#123;</span><br><span class="line">    Base::<span class="keyword">operator</span>=(rhs);</span><br><span class="line">    a = rhs.a;</span><br><span class="line">    <span class="keyword">return</span> *<span class="keyword">this</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的<code>init()</code>函数是一个更好的选择。</p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Python中的迭代器和生成器</title>
    <url>/2017/04/21/python-iter-generator/</url>
    <content><![CDATA[<p>在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程<a href="http://anandology.com/python-practice-book/iterators.html" target="_blank" rel="noopener">Iterators &amp; Generators</a>。</p>
<a id="more"></a>
<h2 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h2><p>使用<code>for</code>循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    <span class="comment"># do something 100 times</span></span><br></pre></td></tr></table></figure>
<p>在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的<code>key</code>。</p>
<p>同样，也有很多函数接受的参数为可迭代对象。例如<code>list()</code>和<code>tuple()</code>，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">list(&#123;<span class="string">'x'</span>:<span class="number">1</span>, <span class="string">'y'</span>:<span class="number">2</span>&#125;)  <span class="comment"># =&gt; ['x', 'y']</span></span><br></pre></td></tr></table></figure>
<p>为什么<code>list</code>或者<code>str</code>这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用<code>for x in XXX</code>这种方法来遍历了呢？</p>
<p>在Python中，有内建的函数<code>iter()</code>和<code>next()</code>。一般用法时，<code>iter()</code>方法接受一个可迭代对象，会调用这个对象的<code>__iter__()</code>方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现<code>next()</code>方法（Python3中改为了<code>__next__()</code>方法）。</p>
<p>如下面的例子，<code>yrange_iter</code>是<code>yrange</code>的一个迭代器。<code>yrange</code>实现了<code>__iter__()</code>方法，是一个可迭代对象。调用<code>iter(yrange object)</code>的结果就是返回一个<code>yrange_iter</code>的对象实例。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Version 1.0 使用迭代器类</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">yrange_iter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, yrange)</span>:</span></span><br><span class="line">        self.n = yrange.n</span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        v = self.i</span><br><span class="line">        self.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">yrange</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> yrange_iter(self)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> type(iter(yrange(<span class="number">5</span>))) <span class="comment"># &lt;class '__main__.yrange_iter'&gt;</span></span><br></pre></td></tr></table></figure>
<p>而不停地调用迭代器的<code>next()</code>方法，就能够不断输出迭代序列。如下所示：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">In [<span class="number">3</span>]: yiter = iter(yrange(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">4</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">4</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">5</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">5</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">6</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">6</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉<code>yrange_iter</code>，直接让<code>yrange.__iter__()</code>方法返回其自身实例。这样，我们只需要在<code>yrange</code>类中实现<code>__iter__()</code>方法和<code>next()</code>方法即可。如下所示：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Version2.0 简化版，迭代器是本身</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">yrange</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        v = self.i</span><br><span class="line">        self.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line">In [<span class="number">8</span>]: yiter = iter(yrange(<span class="number">5</span>))</span><br><span class="line"></span><br><span class="line">In [<span class="number">9</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">9</span>]: <span class="number">0</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">10</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">10</span>]: <span class="number">1</span></span><br><span class="line"></span><br><span class="line">In [<span class="number">11</span>]: yiter.next()</span><br><span class="line">Out[<span class="number">11</span>]: <span class="number">2</span></span><br></pre></td></tr></table></figure>
<p>然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是<code>self.n</code>实际上并没有用到。如果我只想产生0到10以内的序列呢？</p>
<p>我们只需要加入判断条件，当超出序列边界时，抛出Python内建的<code>StopIteration</code>异常即可。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Version3.0 加入边界判断，生成有限长度序列</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">yrange</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, n)</span>:</span></span><br><span class="line">        self.n = n</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">return</span> self</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.i == self.n:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        v = self.i</span><br><span class="line">        self.i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> v</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> yrange(<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">print</span> i</span><br></pre></td></tr></table></figure>
<h3 id="Problem-1"><a href="#Problem-1" class="headerlink" title="Problem 1"></a>Problem 1</h3><p>Write an iterator class <code>reverse_iter</code>, that takes a <code>list</code> and iterates it from the reverse direction.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">reverse_iter</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, alist)</span>:</span></span><br><span class="line">        self.container = alist</span><br><span class="line">        self.i = len(alist)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> self.i == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        self.i -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> self.container[self.i]</span><br><span class="line">it = reverse_iter([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>])</span><br></pre></td></tr></table></figure>
<h2 id="生成器"><a href="#生成器" class="headerlink" title="生成器"></a>生成器</h2><p>生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的<code>yield</code>语句。此外，生成器函数是懒惰求值，只有当调用<code>next()</code>方法时，生成器才开始顺序执行，直到遇到<code>yield</code>语句。<code>yield</code>语句就像<code>return</code>，但是并未退出，而是打上断电，等待下一次<code>next()</code>方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="function"><span class="keyword">def</span> <span class="title">foo</span><span class="params">()</span>:</span></span><br><span class="line">        <span class="keyword">print</span> <span class="string">"begin"</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"before yield"</span>, i</span><br><span class="line">            <span class="keyword">yield</span> i</span><br><span class="line">            <span class="keyword">print</span> <span class="string">"after yield"</span>, i</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"end"</span></span><br><span class="line"></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f = foo()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.next()</span><br><span class="line">begin</span><br><span class="line">before <span class="keyword">yield</span> <span class="number">0</span></span><br><span class="line"><span class="number">0</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.next()</span><br><span class="line">after <span class="keyword">yield</span> <span class="number">0</span></span><br><span class="line">before <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.next()</span><br><span class="line">after <span class="keyword">yield</span> <span class="number">1</span></span><br><span class="line">before <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>f.next()</span><br><span class="line">after <span class="keyword">yield</span> <span class="number">2</span></span><br><span class="line">end</span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">StopIteration</span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<h3 id="生成器表达式"><a href="#生成器表达式" class="headerlink" title="生成器表达式"></a>生成器表达式</h3><p>生成器表达式和列表相似，将<code>[]</code>换为<code>()</code>即可。如下所示：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> (x**<span class="number">2</span> <span class="keyword">for</span> x <span class="keyword">in</span> [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>]):</span><br><span class="line">    <span class="keyword">print</span> i</span><br><span class="line"><span class="comment"># print 1 4 9 16</span></span><br></pre></td></tr></table></figure>
<p>生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。</p>
<p>下面的代码使用生成器得到前10组勾股数。通过在调用<code>take()</code>方法时修改传入实参<code>n</code>的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边<code>x</code>的取值为$[0, \infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上<code>break</code>配合才可以达到相同的效果。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">integer</span><span class="params">(start, end=None)</span>:</span></span><br><span class="line">    <span class="string">"""Generate integer sequence [start, end)</span></span><br><span class="line"><span class="string">       If `end` is not given, then [start, \infty]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    i = start</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> end <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> i == end:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line">        <span class="keyword">yield</span> i</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">take</span><span class="params">(n, g)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="keyword">if</span> i &lt; n:</span><br><span class="line">            <span class="keyword">yield</span> g.next()</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> StopIteration</span><br><span class="line"></span><br><span class="line"><span class="comment"># 假定 x&gt;y&gt;z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6</span></span><br><span class="line">tup = ((x,y,z) <span class="keyword">for</span> x <span class="keyword">in</span> integer(<span class="number">0</span>) <span class="keyword">for</span> y <span class="keyword">in</span> integer(<span class="number">0</span>, x) <span class="keyword">for</span> z <span class="keyword">in</span> integer(<span class="number">0</span>, y) <span class="keyword">if</span> x*x==y*y+z*z)</span><br><span class="line">list(take(<span class="number">10</span>, tup))</span><br></pre></td></tr></table></figure>
<h3 id="Problem-2"><a href="#Problem-2" class="headerlink" title="Problem 2"></a>Problem 2</h3><p>Write a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">readfiles</span><span class="params">(filenames)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> filenames:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> open(f):</span><br><span class="line">            <span class="keyword">yield</span> line</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">grep</span><span class="params">(lines)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (line <span class="keyword">for</span> line <span class="keyword">in</span> lines <span class="keyword">if</span> len(line)&gt;<span class="number">40</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">printlines</span><span class="params">(lines)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">        <span class="keyword">print</span> line,</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(filenames)</span>:</span></span><br><span class="line">    lines = readfiles(filenames)</span><br><span class="line">    lines = grep(lines)</span><br><span class="line">    printlines(lines)</span><br></pre></td></tr></table></figure>
<h3 id="Problem-3"><a href="#Problem-3" class="headerlink" title="Problem 3"></a>Problem 3</h3><p>Write a function <code>findfiles</code> that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree.</p>
<p>注意<code>get_all_file()</code>方法中递归中生成器的写法，见SO的<a href="http://stackoverflow.com/questions/248830/python-using-a-recursive-algorithm-as-a-generator" target="_blank" rel="noopener">这个帖子</a>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_file</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> os.listdir(root):</span><br><span class="line">        item = os.path.join(root, item)</span><br><span class="line">        <span class="keyword">if</span> os.path.isfile(item):</span><br><span class="line">            <span class="keyword">yield</span> os.path.abspath(item)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> item <span class="keyword">in</span> generate_all_file(item):</span><br><span class="line">                <span class="keyword">yield</span> item</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">findfiles</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> item <span class="keyword">in</span> generate_all_file(root):</span><br><span class="line">        <span class="keyword">print</span> item</span><br></pre></td></tr></table></figure>
<h3 id="Problem-4"><a href="#Problem-4" class="headerlink" title="Problem 4"></a>Problem 4</h3><p>Write a function to compute the number of python files (.py extension) in a specified directory recursively.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_py_file</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (file <span class="keyword">for</span> file <span class="keyword">in</span> generate_all_file(root) <span class="keyword">if</span> os.path.splitext(file)[<span class="number">-1</span>] == <span class="string">'.py'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> len(list(generate_all_py_file(<span class="string">'./'</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="Problem-5"><a href="#Problem-5" class="headerlink" title="Problem 5"></a>Problem 5</h3><p>Write a function to compute the total number of lines of code in all python files in the specified directory recursively.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_line</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (line <span class="keyword">for</span> f <span class="keyword">in</span> generate_all_py_file(root) <span class="keyword">for</span> line <span class="keyword">in</span> open(f))</span><br><span class="line"><span class="keyword">print</span> len(list(generate_all_line(<span class="string">'./'</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="Problem-6"><a href="#Problem-6" class="headerlink" title="Problem 6"></a>Problem 6</h3><p>Write a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generate_all_no_empty_and_comment_line</span><span class="params">(root)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> (line <span class="keyword">for</span> line <span class="keyword">in</span> generate_all_line(root) <span class="keyword">if</span> <span class="keyword">not</span> (line==<span class="string">''</span> <span class="keyword">or</span> line.startswith(<span class="string">'#'</span>)))</span><br><span class="line"></span><br><span class="line"><span class="keyword">print</span> len(list(generate_all_no_empty_and_comment_line(<span class="string">'./'</span>)))</span><br></pre></td></tr></table></figure>
<h3 id="Problem-7"><a href="#Problem-7" class="headerlink" title="Problem 7"></a>Problem 7</h3><p>Write a program <code>split.py</code>, that takes an integer <code>n</code> and a <code>filename</code> as command line arguments and splits the <code>file</code> into multiple small files with each having <code>n</code> lines.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_numbered_line</span><span class="params">(filename)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> open(filename):</span><br><span class="line">        <span class="keyword">yield</span> i, line</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">split</span><span class="params">(file_name, n)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    f = open(<span class="string">'output-%d.txt'</span> %i, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> idx, line <span class="keyword">in</span> get_numbered_line(file_name):</span><br><span class="line">        f.write(line)</span><br><span class="line">        <span class="keyword">if</span> (idx+<span class="number">1</span>) % n == <span class="number">0</span>:</span><br><span class="line">            f.close()</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            f = open(<span class="string">'output-%d.txt'</span> %i, <span class="string">'w'</span>)</span><br><span class="line"></span><br><span class="line">    f.close()</span><br></pre></td></tr></table></figure>
<h3 id="Problem-9"><a href="#Problem-9" class="headerlink" title="Problem 9"></a>Problem 9</h3><p>The built-in function <code>enumerate</code> takes an <code>iteratable</code> and returns an <code>iterator</code> over pairs <code>(index, value)</code> for each value in the source.</p>
<p>Write a function <code>my_enumerate</code> that works like <code>enumerate</code>.</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_enumerate</span><span class="params">(iterable)</span>:</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    seq = iter(iterable)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        val = seq.next()</span><br><span class="line">        <span class="keyword">yield</span> i, val</span><br><span class="line">        i += <span class="number">1</span></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>Effective CPP 阅读 - Chapter 1 让自己习惯C++</title>
    <url>/2017/04/20/effective-cpp-01/</url>
    <content><![CDATA[<p>本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。<br><img src="/img/effectivecpp_01_cpp_rely_on_renpin.jpg" alt="写C++需要人品"><br><a id="more"></a></p>
<h3 id="01-将C-视作语言联邦"><a href="#01-将C-视作语言联邦" class="headerlink" title="01 将C++视作语言联邦"></a>01 将C++视作语言联邦</h3><p>C++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。</p>
<p>本条款中，将C++概括为四个次语言组成的联邦：</p>
<ul>
<li>传统C：面向过程，也规定了C++基本的语法。</li>
<li>OOP：面向对象，带类的C，加入了继承，虚函数等概念。</li>
<li>Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。</li>
<li>STL：标准模板库。使用STL要遵守它的约定。</li>
</ul>
<p>想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。</p>
<h3 id="02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）"><a href="#02-尽量使用const-enum-inline替换-define（以编译器替换预处理器）" class="headerlink" title="02 尽量使用const, enum, inline替换 #define（以编译器替换预处理器）"></a>02 尽量使用<code>const</code>, <code>enum</code>, <code>inline</code>替换 <code>#define</code>（以编译器替换预处理器）</h3><p><code>#define</code>是C时代遗留下来的预编译指令。</p>
<p>当<code>#define</code>用来定义某个常量时，通常<code>const</code>是一个更好的选择。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">define</span> PI 3.14</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">double</span> PI = <span class="number">3.14</span>;</span><br></pre></td></tr></table></figure>
<p>当此常量为整数类型（<code>int</code>, <code>char</code>, <code>bool</code>）等时，也可以使用<code>enum</code>定义常量。这种做法常常用在模板元编程中。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">enum</span> &#123;K = <span class="number">1</span>&#125;;</span><br></pre></td></tr></table></figure>
<p>对于<code>const</code>常量，你可以获取变量的地址，但是对于<code>enum</code>来说，无法获取变量的地址。对于这一点来说，<code>enum</code>和<code>#define</code>相类似。</p>
<p>另一种可能使用<code>#define</code>的场景是宏定义。这种情形可以使用<code>inline</code>声明内联函数解决。</p>
<p>总之，尽可能相信编译器的力量。使用<code>#define</code>将遮蔽编译器的视野，带来奇怪的问题。</p>
<h3 id="03-尽可能使用const"><a href="#03-尽可能使用const" class="headerlink" title="03 尽可能使用const"></a>03 尽可能使用<code>const</code></h3><p><code>const</code>不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。</p>
<p>所以，如果某一变量满足<code>const</code>的要求，那么请加上<code>const</code>，和编译器签订一份契约，保护你的行为。</p>
<p>这里不再讨论<code>const</code>的寻常用法。提示一下：当修饰指针变量时，<code>const</code>在星号左边，是指指针所指物是常量；当<code>const</code>在星号右边，是指指针本身是常量。如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* p = &amp;a;</span><br><span class="line">*p = <span class="number">5</span>;   <span class="comment">// 非法</span></span><br><span class="line">p = &amp;b;   <span class="comment">// 合法</span></span><br><span class="line"><span class="keyword">int</span>* <span class="keyword">const</span> p = &amp;a;</span><br><span class="line">p = &amp;b;    <span class="comment">// 非法</span></span><br><span class="line">*p = <span class="number">5</span>;    <span class="comment">// 合法</span></span><br></pre></td></tr></table></figure>
<p>STL中，如果声明某个迭代器为<code>const</code>，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用<code>const_iterator</code>。</p>
<p><code>const</code>更丰富的用法是用于函数声明中，</p>
<ul>
<li>当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span> (fun(a, b) = c)  <span class="comment">// 这里错把 == 打成了 =</span></span><br></pre></td></tr></table></figure>
<ul>
<li>当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。</li>
<li>当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。</li>
</ul>
<p>这种情况下，可能会有<code>const</code>重载现象。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_string</span>&#123;</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">size_t</span> pos) &#123;</span><br><span class="line">	  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;ptr[pos];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">char</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">size_t</span> pos) &#123;</span><br><span class="line">	  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;ptr[pos];</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>实际调用时，根据调用该函数的对象是否是<code>const</code>的来决定究竟调用哪个版本。</p>
<p>上面的实现未免过于复杂，我们还可以改成下面的形式：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">my_string</span>&#123;</span></span><br><span class="line">  <span class="keyword">const</span> <span class="keyword">char</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">size_t</span> pos) &#123;</span><br><span class="line">	  <span class="keyword">return</span> <span class="keyword">this</span>-&gt;ptr[pos];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">char</span>&amp; <span class="keyword">operator</span>[](<span class="keyword">size_t</span> pos) &#123;</span><br><span class="line">	  <span class="keyword">return</span> <span class="keyword">const_cast</span>&lt;<span class="keyword">char</span>&amp;&gt;(</span><br><span class="line">	         <span class="keyword">static_cast</span>&lt;<span class="keyword">const</span> my_string&amp;&gt;(*<span class="keyword">this</span>)[pos]);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure>
<p>注意上面的代码进行了两次类型转换。由<code>non-const reference</code>转为<code>const reference</code>是类型安全的，使用<code>static_cast</code>进行。最后我们要脱掉<code>const char&amp;</code>的<code>const</code>属性，使用了<code>const_cast</code>。</p>
<p>对于<code>const</code>成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为<code>mutable</code>。</p>
<h3 id="04-确保对象在使用前已经被初始化"><a href="#04-确保对象在使用前已经被初始化" class="headerlink" title="04 确保对象在使用前已经被初始化"></a>04 确保对象在使用前已经被初始化</h3><p>使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。</p>
<p>对于内建类型，需要手动初始化。</p>
<p>对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="title">A</span><span class="params">(name, age)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">this</span>-&gt;name = name; <span class="comment">// 这是赋值，不是初始化！</span></span><br><span class="line">  <span class="keyword">this</span>-&gt;age = age;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>如果在类<code>A</code>的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用<code>copy-construct</code>实现。</p>
<p>需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。</p>
<p>讨论完上述情况，再来看一种特殊变量：不同编译单元<code>non-local static</code>变量，是指不在某个函数scope下的<code>static</code>变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。</p>
]]></content>
      <tags>
        <tag>cpp</tag>
      </tags>
  </entry>
  <entry>
    <title>Caffe中的底层数学计算函数</title>
    <url>/2017/03/08/mathfunctions-in-caffe/</url>
    <content><![CDATA[<p>Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对<a href="https://github.com/BVLC/caffe/blob/master/include/caffe/util/math_functions.hpp" target="_blank" rel="noopener">mathfunction.hpp 文件</a>中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。<br><img src="/img/caffe_mathfunctions_gpuisnuclearweapon.jpg" alt="一卡一栋楼，双卡灭地球，三卡银河系，四卡创世纪"></p>
<a id="more"></a>
<h2 id="BLAS介绍"><a href="#BLAS介绍" class="headerlink" title="BLAS介绍"></a>BLAS介绍</h2><p>以下内容参考<a href="https://en.wikipedia.org/wiki/Basic_Linear_Algebra_Subprograms" target="_blank" rel="noopener">BLAS wiki页面</a>整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。</p>
<p>BLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。</p>
<p>不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如<del>AMD的ACML</del>（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。</p>
<p>在BLAS中，实现了矩阵与矩阵相乘的函数<code>gemm</code>（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数<code>gemv</code>，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。<br><img src="/img/mathfunctions_time_distribution.png" alt="前向计算中的典型时间分布"></p>
<p>可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。<br><img src="/img/mathfunctions_im2col.png" alt="im2col的原理"></p>
<p>对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章<a href="https://petewarden.com/2015/04/20/why-gemm-is-at-the-heart-of-deep-learning/" target="_blank" rel="noopener">Why GEMM is at the heart of deep learning</a>的分析。上面的图也都来源于这篇博客。</p>
<h2 id="矩阵运算函数"><a href="#矩阵运算函数" class="headerlink" title="矩阵运算函数"></a>矩阵运算函数</h2><p>矩阵运算函数在文件<code>math_functions.hpp</code>中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。</p>
<h3 id="矩阵与矩阵，矩阵与向量的乘法"><a href="#矩阵与矩阵，矩阵与向量的乘法" class="headerlink" title="矩阵与矩阵，矩阵与向量的乘法"></a>矩阵与矩阵，矩阵与向量的乘法</h3><p>函数<code>caffe_cpu_gemm()</code>是对BLAS中矩阵与矩阵相乘函数<code>gemm</code>的包装。与之对应的<code>caffe_cpu_gemv()</code>是对矩阵与向量相乘<code>gemv</code>函数的包装。以前者为例，其实现代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span>&lt;&gt;</span><br><span class="line"><span class="keyword">void</span> caffe_cpu_gemm&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> CBLAS_TRANSPOSE TransA,</span><br><span class="line">    <span class="keyword">const</span> CBLAS_TRANSPOSE TransB, <span class="keyword">const</span> <span class="keyword">int</span> M, <span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> K,</span><br><span class="line">    <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">const</span> <span class="keyword">float</span>* A, <span class="keyword">const</span> <span class="keyword">float</span>* B, <span class="keyword">const</span> <span class="keyword">float</span> beta,</span><br><span class="line">    <span class="keyword">float</span>* C) &#123;</span><br><span class="line">  <span class="keyword">int</span> lda = (TransA == CblasNoTrans) ? K : M;</span><br><span class="line">  <span class="keyword">int</span> ldb = (TransB == CblasNoTrans) ? N : K;</span><br><span class="line">  cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B,</span><br><span class="line">      ldb, beta, C, N);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的<code>cblas_sgemm()</code>函数。其功能是计算<code>C = alpha * A * B + beta * C</code>。参数的具体含义可以查看BLAS的相关文档。</p>
<h3 id="矩阵-向量的加减"><a href="#矩阵-向量的加减" class="headerlink" title="矩阵/向量的加减"></a>矩阵/向量的加减</h3><p>下面的函数都是将<code>X</code>指针所指的数据作为<code>src</code>，将<code>Y</code>指针所指的数据为<code>dst</code>。同时，第一个参数统一是向量的长度。</p>
<ul>
<li><code>caffe_axpy(N, alpha, x, mutable y)</code>实现向量加法<code>Y = alpha * X + Y</code>。</li>
<li><code>caffe_axpby(N, alpha, x, beta, mutable y)</code>实现向量加法<code>Y = alpha * X + beta * Y</code></li>
</ul>
<p>这两个函数的用法可以参见欧氏距离loss函数中的梯度计算：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">caffe_cpu_axpby(</span><br><span class="line">    bottom[i]-&gt;count(),              <span class="comment">// count</span></span><br><span class="line">    alpha,                              <span class="comment">// alpha</span></span><br><span class="line">    diff_.cpu_data(),                   <span class="comment">// a</span></span><br><span class="line">    Dtype(<span class="number">0</span>),                           <span class="comment">// beta</span></span><br><span class="line">    bottom[i]-&gt;mutable_cpu_diff());  <span class="comment">// b</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>其中，<code>bottom[i]-&gt;count()</code>给定了<code>blob</code>的大小，也就是向量的长度。<code>alpha</code>实际是由顶层<code>top_blob</code>传来的<code>loss_weight</code>，也即是<code>*top_blob-&gt;cpu_diff()/batch_size</code>。由于是直接将加权后的<code>diff</code>直接赋给<code>bottom_blob</code>的<code>cpu_diff</code>，所以，将<code>beta</code>赋值为0。</p>
<h3 id="内存相关"><a href="#内存相关" class="headerlink" title="内存相关"></a>内存相关</h3><p>和C语言中的<code>memset()</code>和<code>memcpy()</code>类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似：</p>
<ul>
<li><code>caffe_copy(N, x, mutable y)</code>实现向量拷贝。源地址和目标地址服从上小节的约定。</li>
<li><code>caffe_set(N, alpha, mutable x)</code>实现向量的置位，将向量分量填充为值<code>alpha</code>。</li>
</ul>
<p>查看其实现可以知道，这里Caffe中直接调用了<code>memset()</code>完成任务。<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_set</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype alpha, Dtype* Y)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (alpha == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="built_in">memset</span>(Y, <span class="number">0</span>, <span class="keyword">sizeof</span>(Dtype) * N);  <span class="comment">// NOLINT(caffe/alt_fn)</span></span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; N; ++i) &#123;</span><br><span class="line">    Y[i] = alpha;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 模板的特化</span></span><br><span class="line"><span class="keyword">template</span> <span class="keyword">void</span> caffe_set&lt;<span class="keyword">int</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">int</span> alpha, <span class="keyword">int</span>* Y);</span><br><span class="line"><span class="keyword">template</span> <span class="keyword">void</span> caffe_set&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">float</span> alpha, <span class="keyword">float</span>* Y);</span><br><span class="line"><span class="keyword">template</span> <span class="keyword">void</span> caffe_set&lt;<span class="keyword">double</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> <span class="keyword">double</span> alpha, <span class="keyword">double</span>* Y);</span><br></pre></td></tr></table></figure></p>
<p>而<code>caffe_copy()</code>中则是直接实现了CPU和GPU的功能。注意到下面代码中调用<code>cudaMemcpy()</code>的时候，使用了参数<code>cudaMemcpyDefault</code>。通过查阅文档，这个变量的含义是<a href="http://horacio9573.no-ip.org/cuda/group__CUDART__TYPES_g18fa99055ee694244a270e4d5101e95b.html" target="_blank" rel="noopener">cudaMemcpyDefault: Default based unified virtual address space</a>。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;<span class="keyword">typename</span> Dtype&gt;</span><br><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">caffe_copy</span><span class="params">(<span class="keyword">const</span> <span class="keyword">int</span> N, <span class="keyword">const</span> Dtype* X, Dtype* Y)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (X != Y) &#123;</span><br><span class="line">    <span class="keyword">if</span> (Caffe::mode() == Caffe::GPU) &#123;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifndef</span> CPU_ONLY</span></span><br><span class="line">      <span class="comment">// NOLINT_NEXT_LINE(caffe/alt_fn)</span></span><br><span class="line">      CUDA_CHECK(cudaMemcpy(Y, X, <span class="keyword">sizeof</span>(Dtype) * N, cudaMemcpyDefault));</span><br><span class="line"><span class="meta">#<span class="meta-keyword">else</span></span></span><br><span class="line">      NO_GPU;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="built_in">memcpy</span>(Y, X, <span class="keyword">sizeof</span>(Dtype) * N);  <span class="comment">// NOLINT(caffe/alt_fn)</span></span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所谓的unified virtual address（UVA）就是下图这个意思（见<a href="http://on-demand.gputechconf.com/gtc-express/2011/presentations/cuda_webinars_GPUDirect_uva.pdf" target="_blank" rel="noopener">P2P&amp;UVA</a>）。<br><img src="/img/caffe_mathfunctions_whatisuva.png" alt="UVA图示"></p>
<p>有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示：<br><img src="/img/caffe_mathfunctions_useuva.png" alt="How to use UVA"></p>
<p>其使用条件如下：<br><img src="/img/caffe_mathfunctions_uvarequirement.png" alt="UVA Requirement"></p>
<h3 id="向量逐元素运算"><a href="#向量逐元素运算" class="headerlink" title="向量逐元素运算"></a>向量逐元素运算</h3><ul>
<li><code>caffe_add(N, a, b, y)</code>函数实现<code>Y[i] = a[i] + b[i]</code>。</li>
<li><code>caffe_sub</code>, <code>caffe_div</code>, <code>caffe_mul</code>同理。</li>
<li><code>caffe_exp</code>, <code>caffe_powx</code>, <code>caffe_abs</code>, <code>caffe_sqr</code>, <code>caffe_log</code>相似，这里只将<code>caffe_exp()</code>的实现复制如下：</li>
</ul>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 又是模板特化</span></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> caffe_exp&lt;<span class="keyword">float</span>&gt;(<span class="keyword">const</span> <span class="keyword">int</span> n, <span class="keyword">const</span> <span class="keyword">float</span>* a, <span class="keyword">float</span>* y) &#123;</span><br><span class="line">  vsExp(n, a, y);   <span class="comment">// 返回 y[i] = exp(a[i])</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<ul>
<li><code>caffe_scal</code>实现向量的数乘。这个函数常常用在<code>loss_layer</code>中计算反传的梯度，常常要乘上一个标量<code>loss_weight</code>。</li>
<li><code>caffe_add_scalar</code>实现向量每个分量与标量相加。</li>
</ul>
<h2 id="GPU版本"><a href="#GPU版本" class="headerlink" title="GPU版本"></a>GPU版本</h2><p>相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了<code>math_functions.hpp</code>中，而相应的实现代码在<code>math_functions.cu</code>中。</p>
<h2 id="随机数产生器"><a href="#随机数产生器" class="headerlink" title="随机数产生器"></a>随机数产生器</h2><p>Caffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。</p>
<p>这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数<code>caffe_rng_distribution_name</code>即可。</p>
<h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>【1】<a href="http://blog.csdn.net/seven_first/article/details/47378697" target="_blank" rel="noopener">seven-first 的博客</a></p>
]]></content>
      <tags>
        <tag>caffe</tag>
      </tags>
  </entry>
  <entry>
    <title>Residual Net论文阅读 - Identity Mapping in Deep Residual Networks</title>
    <url>/2017/03/07/residualnet-paper2-identitymapping/</url>
    <content><![CDATA[<p>这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。<br><a id="more"></a></p>
<h2 id="从残差到残差"><a href="#从残差到残差" class="headerlink" title="从残差到残差"></a>从残差到残差</h2><p>在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出，$\mathcal{F}$为残差函数。</p>
<script type="math/tex; mode=display">y_l = h(x_l) + \mathcal{F}(x_l, W_l)</script><script type="math/tex; mode=display">x_{l+1} = f(y_l)</script><p>在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即，</p>
<script type="math/tex; mode=display">h(x_l) = x_l</script><p>在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。<br><img src="/img/residualnet_improved_structure.png" alt="新的网络结构，将BN和ReLU看做是前激活"></p>
<p>作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。</p>
<p>从右图可以看到，本单元的输入$x_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y_l$，直接和$x_l$相加，得到该层的输出$x_{l+1}$。</p>
<p>利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。</p>
<h2 id="Identity-Mapping-Why-Always-me"><a href="#Identity-Mapping-Why-Always-me" class="headerlink" title="Identity Mapping: Why Always me?"></a>Identity Mapping: Why Always me?</h2><p>作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。</p>
<p>在使用了这种新结构之后，我们有，</p>
<script type="math/tex; mode=display">x_{l+1} = x_l + \mathcal{F}(x_l, W_l)</script><p>如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则：</p>
<script type="math/tex; mode=display">x_L = x_l +\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i)</script><p>从上面的式子可以看出，</p>
<ul>
<li>深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\mathcal{F}$。</li>
<li>我们有，$x_L=x_0+\sum_{i=0}^{L-1}\mathcal{F}(x_i, W_i)$，而普通网络$x_L$和$x_l$的关系比较复杂，$x_L = \prod_{i=0}^{L-1}W_ix_0$。看上去，前者的优化应该更加简单。</li>
</ul>
<p>计算bp的时候，有，</p>
<script type="math/tex; mode=display">\frac{\partial \epsilon}{\partial x_l} = \frac{\partial \epsilon} {\partial x_L}\frac{\partial x_L}{\partial x_l} = \frac{\partial \epsilon}{\partial x_L}(1+\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i))</script><p>上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。</p>
<p>如果不做Identity Mapping，而是乘上一个系数$\lambda$呢？作者发现这会在上面的式子上出现$\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。</p>
<p>所以，Identity Mapping是坠吼的！</p>
<h2 id="花式跑实验"><a href="#花式跑实验" class="headerlink" title="花式跑实验"></a>花式跑实验</h2><p>论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：<a href="https://gist.github.com/xmfbit/67c407e34cbaf56e7820f09e774e56d8" target="_blank" rel="noopener">Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch</a>。</p>
<p>下面的可视化结果由<a href="https://github.com/dmlc/tensorboard" target="_blank" rel="noopener">DMLC/tensorboard</a>实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。<br><img src="/img/resnet-164layer-cifar10-training.jpg" alt="训练集损失和精度"><br><img src="/img/resnet-164layer-cifar10-testing.jpg" alt="测试集损失和精度"></p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLO网络参数的解析与存储</title>
    <url>/2017/03/06/yolo-cfg-parser/</url>
    <content><![CDATA[<p>YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。<br><a id="more"></a></p>
<h2 id="YOLO网络结构定义的CFG文件"><a href="#YOLO网络结构定义的CFG文件" class="headerlink" title="YOLO网络结构定义的CFG文件"></a>YOLO网络结构定义的CFG文件</h2><p>YOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。</p>
<p>CFG文件的格式可以归纳如下（可以打开某个<a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolo-voc.cfg" target="_blank" rel="noopener">CFG文件</a>进行对照）：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[net]</span><br><span class="line"># 这里会对net的参数进行配置</span><br><span class="line"># 同时YOLO将对net的求解器的参数也放在了这里</span><br><span class="line">[conv]</span><br><span class="line"># 一些conv层的参数描述</span><br><span class="line">[maxpool]</span><br><span class="line"># 一些池化层的参数描述</span><br><span class="line"></span><br><span class="line"># 顺序堆叠的其他layer描述</span><br></pre></td></tr></table></figure></p>
<p>在Darknet的代码中，将每个<code>[]</code>符号导引的参数列表叫做section。</p>
<h2 id="网络结构解析器-Parser"><a href="#网络结构解析器-Parser" class="headerlink" title="网络结构解析器 Parser"></a>网络结构解析器 Parser</h2><p>具体的解析实现参见<a href="https://github.com/pjreddie/darknet/blob/master/src/parser.c" target="_blank" rel="noopener">parser.c文件</a>。我们先以<code>convolutional_layer parse_convolutional(list *options, size_params params)</code>函数为例，看一下Darknet是如何完成对卷积层参数的解析的。</p>
<p>从函数签名可以看出，这个函数接受一个<code>list</code>的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而<code>size_params</code>类型的变量<code>params</code>指示了该层上一层的参数情况，其具体定义如下：<br><figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">typedef</span> <span class="class"><span class="keyword">struct</span> <span class="title">size_params</span>&#123;</span></span><br><span class="line">    <span class="keyword">int</span> batch;</span><br><span class="line">    <span class="keyword">int</span> inputs;</span><br><span class="line">    <span class="keyword">int</span> h;</span><br><span class="line">    <span class="keyword">int</span> w;</span><br><span class="line">    <span class="keyword">int</span> c;</span><br><span class="line">    <span class="keyword">int</span> index;</span><br><span class="line">    <span class="keyword">int</span> time_steps;</span><br><span class="line">    network net;</span><br><span class="line">&#125; size_params;</span><br></pre></td></tr></table></figure></p>
<p>这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。</p>
<p>进入函数内部，会发现频繁出现<code>option_find_int</code>这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。</p>
<p>我们首先来看一下这个函数的定义吧~这个函数并不在<code>parser.c</code>中，而是在<a href="https://github.com/pjreddie/darknet/blob/master/src/option_list.c" target="_blank" rel="noopener">option_list.c 文件</a>中。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// l: data pointer to the list</span></span><br><span class="line"><span class="comment">// key: the key to find, example: "filters", "padding"</span></span><br><span class="line"><span class="comment">// def: default value</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">option_find_int</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key, <span class="keyword">int</span> def)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="comment">// 去找到该key对应的数值，使用atoi转换为整型数</span></span><br><span class="line">    <span class="keyword">char</span> *v = option_find(l, key);</span><br><span class="line">    <span class="keyword">if</span>(v) <span class="keyword">return</span> atoi(v);</span><br><span class="line">    <span class="comment">// 使用XXX_quiet版本可以不打印此信息</span></span><br><span class="line">    <span class="built_in">fprintf</span>(<span class="built_in">stderr</span>, <span class="string">"%s: Using default '%d'\n"</span>, key, def);</span><br><span class="line">    <span class="comment">// 没有找到key，返回默认值</span></span><br><span class="line">    <span class="keyword">return</span> def;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>而其中的<code>option_find</code>函数则是逐项顺序查找，匹配字符串来实现的。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">char</span> *<span class="title">option_find</span><span class="params">(<span class="built_in">list</span> *l, <span class="keyword">char</span> *key)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    node *n = l-&gt;front;</span><br><span class="line">    <span class="keyword">while</span>(n)&#123;</span><br><span class="line">        kvp *p = (kvp *)n-&gt;val;</span><br><span class="line">        <span class="keyword">if</span>(<span class="built_in">strcmp</span>(p-&gt;key, key) == <span class="number">0</span>)&#123;</span><br><span class="line">            p-&gt;used = <span class="number">1</span>;</span><br><span class="line">            <span class="keyword">return</span> p-&gt;val;</span><br><span class="line">        &#125;</span><br><span class="line">        n = n-&gt;next;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h2 id="构建conv层"><a href="#构建conv层" class="headerlink" title="构建conv层"></a>构建conv层</h2><p>由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line">   <span class="comment">// 首先得到参数</span></span><br><span class="line">   <span class="keyword">int</span> n = option_find_int(options, <span class="string">"filters"</span>,<span class="number">1</span>);</span><br><span class="line">   <span class="keyword">int</span> size = option_find_int(options, <span class="string">"size"</span>,<span class="number">1</span>);</span><br><span class="line">   <span class="keyword">int</span> stride = option_find_int(options, <span class="string">"stride"</span>,<span class="number">1</span>);</span><br><span class="line">   <span class="keyword">int</span> pad = option_find_int_quiet(options, <span class="string">"pad"</span>,<span class="number">0</span>);</span><br><span class="line">   <span class="keyword">int</span> padding = option_find_int_quiet(options, <span class="string">"padding"</span>,<span class="number">0</span>);</span><br><span class="line">   <span class="keyword">if</span>(pad) padding = size/<span class="number">2</span>;</span><br><span class="line"><span class="comment">// 激活函数是通过匹配其名称的方法得到的</span></span><br><span class="line">   <span class="keyword">char</span> *activation_s = option_find_str(options, <span class="string">"activation"</span>, <span class="string">"logistic"</span>);</span><br><span class="line">   ACTIVATION activation = get_activation(activation_s);</span><br><span class="line">   <span class="comment">// 通过上层的信息得到batch size，做参数检查</span></span><br><span class="line">   <span class="keyword">int</span> batch,h,w,c;</span><br><span class="line">   h = params.h;</span><br><span class="line">   w = params.w;</span><br><span class="line">   c = params.c;</span><br><span class="line">   batch=params.batch;</span><br><span class="line">   <span class="keyword">if</span>(!(h &amp;&amp; w &amp;&amp; c)) error(<span class="string">"Layer before convolutional layer must output image."</span>);</span><br><span class="line">   <span class="keyword">int</span> batch_normalize = option_find_int_quiet(options, <span class="string">"batch_normalize"</span>, <span class="number">0</span>);</span><br><span class="line">   <span class="keyword">int</span> binary = option_find_int_quiet(options, <span class="string">"binary"</span>, <span class="number">0</span>);</span><br><span class="line">   <span class="keyword">int</span> xnor = option_find_int_quiet(options, <span class="string">"xnor"</span>, <span class="number">0</span>);</span><br><span class="line">   <span class="comment">// 调用初始化函数</span></span><br><span class="line">   convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam);</span><br><span class="line">   layer.flipped = option_find_int_quiet(options, <span class="string">"flipped"</span>, <span class="number">0</span>);</span><br><span class="line">   layer.dot = option_find_float_quiet(options, <span class="string">"dot"</span>, <span class="number">0</span>);</span><br></pre></td></tr></table></figure>
<p>所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。</p>
<h2 id="构建网络"><a href="#构建网络" class="headerlink" title="构建网络"></a>构建网络</h2><p>有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。</p>
<p>Darknet中对应的函数为<code>network parse_network_cfg(char *filename)</code>，这个函数接受文件名为参数，进行网络结构的解析。</p>
<p>首先，调用<code>read_cfg(filename)</code>得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是<code>[net]</code> section，要特殊对待。这里不再多说了。</p>
<h2 id="保存参数信息"><a href="#保存参数信息" class="headerlink" title="保存参数信息"></a>保存参数信息</h2><p>Darknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">save_convolutional_weights</span><span class="params">(layer l, FILE *fp)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">if</span>(l.binary)&#123;</span><br><span class="line">        <span class="comment">//save_convolutional_weights_binary(l, fp);</span></span><br><span class="line">        <span class="comment">//return;</span></span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">ifdef</span> GPU</span></span><br><span class="line">    <span class="keyword">if</span>(gpu_index &gt;= <span class="number">0</span>)&#123;</span><br><span class="line">        pull_convolutional_layer(l);</span><br><span class="line">    &#125;</span><br><span class="line"><span class="meta">#<span class="meta-keyword">endif</span></span></span><br><span class="line">    <span class="keyword">int</span> num = l.n*l.c*l.size*l.size;</span><br><span class="line">    fwrite(l.biases, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), l.n, fp);</span><br><span class="line">    <span class="comment">// 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论</span></span><br><span class="line">    <span class="keyword">if</span> (l.batch_normalize)&#123;</span><br><span class="line">        fwrite(l.scales, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), l.n, fp);</span><br><span class="line">        fwrite(l.rolling_mean, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), l.n, fp);</span><br><span class="line">        fwrite(l.rolling_variance, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), l.n, fp);</span><br><span class="line">    &#125;</span><br><span class="line">    fwrite(l.weights, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), num, fp);</span><br><span class="line">    <span class="keyword">if</span>(l.adam)&#123;</span><br><span class="line">        fwrite(l.m, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), num, fp);</span><br><span class="line">        fwrite(l.v, <span class="keyword">sizeof</span>(<span class="keyword">float</span>), num, fp);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>yolo</tag>
      </tags>
  </entry>
  <entry>
    <title>Residual Net论文阅读 - Deep Residual Learning for Image Recongnition</title>
    <url>/2017/03/05/residualnet-paper/</url>
    <content><![CDATA[<p>Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。<br><img src="/img/residualnet_unit.png" alt="ResidualNet Unit"><br><a id="more"></a></p>
<h2 id="更深的网络-gt-更好的性能"><a href="#更深的网络-gt-更好的性能" class="headerlink" title="更深的网络 -&gt; 更好的性能"></a>更深的网络 -&gt; 更好的性能</h2><p>在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。</p>
<p>在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？</p>
<blockquote>
<p> Is learning better networks as easy as stacking more layers?</p>
</blockquote>
<p>很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。</p>
<p>另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。<br><img src="/img/residualnet_deepnet_problem.png" alt="更深的网络表现反而不好"></p>
<p>个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。</p>
<h2 id="残差单元"><a href="#残差单元" class="headerlink" title="残差单元"></a>残差单元</h2><p>也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。<br><img src="/img/residualnet_unit.png" alt="残差结构单元"></p>
<p>使用残差单元时，我们不再让网络去直接学映射$\mathcal{H}(x)$，而是学习映射$\mathcal{F}(x) = \mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。</p>
<p>应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。</p>
<h2 id="残差学习"><a href="#残差学习" class="headerlink" title="残差学习"></a>残差学习</h2><p>从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\mathcal{H}$，而是这个映射和输入的残差$\mathcal{F}$。这里叫做残差学习（Residual Learning）。</p>
<p>神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。</p>
<p>上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。</p>
<script type="math/tex; mode=display">y = \mathcal{F(x,\lbrace W_i\rbrace)}+x</script><p>上面的式子要求$\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。</p>
<p>同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\mathcal{F}$只剩一个线性映射可以学了。。。）。</p>
<h2 id="ImageNet实验和比较"><a href="#ImageNet实验和比较" class="headerlink" title="ImageNet实验和比较"></a>ImageNet实验和比较</h2><p>由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。</p>
<p>这里着重介绍作者在ImageNet上的实验结果。</p>
<p>作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。</p>
<p><img src="/img/residualnet_comparison_with_plainnet.png" alt="残差网络和普通网络不同深度的比较"></p>
<p>另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。</p>
<p>同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。</p>
<ul>
<li>方案A使用zero-padding的方法</li>
<li>方案B使用乘上权重矩阵的方法</li>
<li>方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式</li>
</ul>
<p>实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。</p>
<h2 id="Bottleneck结构"><a href="#Bottleneck结构" class="headerlink" title="Bottleneck结构"></a>Bottleneck结构</h2><p>为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\times 1$的卷积核，来进行channel的变形。通过前面的$1\times 1$卷积核，将channel降下来。和$3\times 3$卷积核作用后，再用最后的$1\times 1$卷积核升上去。</p>
<p><img src="/img/residualnet_bottleneck_unit.png" alt="Bottleneck单元结构"></p>
<p>使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。</p>
<h2 id="附录"><a href="#附录" class="headerlink" title="附录"></a>附录</h2><p>在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
      </tags>
  </entry>
  <entry>
    <title>toy demo - PyTorch + MNIST</title>
    <url>/2017/03/04/pytorch-mnist-example/</url>
    <content><![CDATA[<p>本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。<br><img src="/img/mnist_example.png" alt="MNIST"><br><a id="more"></a></p>
<h2 id="加载MNIST数据集"><a href="#加载MNIST数据集" class="headerlink" title="加载MNIST数据集"></a>加载MNIST数据集</h2><p>PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。<code>MNIST</code>是<code>torchvision.datasets</code>包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将<code>download</code>参数设置为<code>True</code>，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过<code>root</code>传入即可。</p>
<p>在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在<code>torchvision.transforms</code>包中找到对应的操作。在下面的代码中，通过使用<code>transforms.Compose()</code>，我们构造了对数据进行预处理的复合操作序列，<code>ToTensor</code>负责将PIL图像转换为Tensor数据（RGB通道从<code>[0, 255]</code>范围变为<code>[0, 1]</code>）， <code>Normalize</code>负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入<code>tuple</code>。</p>
<p>之后，我们通过<code>DataLoader</code>返回一个数据集上的可迭代对象。一会我们通过<code>for</code>循环，就可以遍历数据集了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line"><span class="keyword">import</span> torchvision.datasets <span class="keyword">as</span> dset</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line"><span class="comment"># use cuda or not</span></span><br><span class="line">use_cuda = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((<span class="number">0.5</span>,), (<span class="number">1.0</span>,))])</span><br><span class="line"></span><br><span class="line">train_set = dset.MNIST(root=root, train=<span class="literal">True</span>, transform=trans, download=download)</span><br><span class="line">test_set = dset.MNIST(root=root, train=<span class="literal">False</span>, transform=trans)</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">128</span></span><br><span class="line"></span><br><span class="line">train_loader = torch.utils.data.DataLoader(</span><br><span class="line">                 dataset=train_set,</span><br><span class="line">                 batch_size=batch_size,</span><br><span class="line">                 shuffle=<span class="literal">True</span>)</span><br><span class="line">test_loader = torch.utils.data.DataLoader(</span><br><span class="line">                dataset=test_set,</span><br><span class="line">                batch_size=batch_size,</span><br><span class="line">                shuffle=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>
<h2 id="网络构建"><a href="#网络构建" class="headerlink" title="网络构建"></a>网络构建</h2><p>在进行网络构建时，主要通过<code>torch.nn</code>包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。<code>nn.Linear</code>负责构建全连接层，需要提供输入和输出的通道数，也就是<code>y = wx+b</code>中<code>x</code>和<code>y</code>的维度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">MLPNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(MLPNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">28</span>*<span class="number">28</span>, <span class="number">500</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>, <span class="number">256</span>)</span><br><span class="line">        self.fc3 = nn.Linear(<span class="number">256</span>, <span class="number">10</span>)</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">28</span>*<span class="number">28</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<p>由于PyTorch可以实现自动求导，所以我们只需实现<code>forward</code>过程即可。这里由于池化层和非线性变换都没有参数，所以使用了<code>nn.functionals</code>中的对应操作实现。通过看文档，可以发现，一般<code>nn</code>里面的各种层，都会在<code>nn.functionals</code>里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># With square kernels and equal stride</span></span><br><span class="line">filters = autograd.Variable(torch.randn(<span class="number">8</span>,<span class="number">4</span>,<span class="number">3</span>,<span class="number">3</span>))</span><br><span class="line">inputs = autograd.Variable(torch.randn(<span class="number">1</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">5</span>))</span><br><span class="line">F.conv2d(inputs, filters, padding=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>
<p>同样地，我们可以实现LeNet的结构如下。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LeNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LeNet, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">20</span>, <span class="number">50</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>, <span class="number">500</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">500</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">50</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>
<h2 id="训练与测试"><a href="#训练与测试" class="headerlink" title="训练与测试"></a>训练与测试</h2><p>在训练时，我们首先应确定优化方法。这里我们使用带动量的<code>SGD</code>方法。下面代码中的<code>optim.SGD</code>初始化需要接受网络中待优化的<code>Parameter</code>列表（或是迭代器），以及学习率<code>lr</code>，动量<code>momentum</code>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## training</span></span><br><span class="line">model = LeNet()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> use_cuda:</span><br><span class="line">    model = model.cuda()</span><br><span class="line"></span><br><span class="line">optimizer = optim.SGD(model.parameters(), lr=<span class="number">0.01</span>, momentum=<span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">ceriation = nn.CrossEntropyLoss()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># trainning</span></span><br><span class="line">    ave_loss = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (x, target) <span class="keyword">in</span> enumerate(train_loader):</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            x, target = x.cuda(), target.cuda()</span><br><span class="line">        x, target = Variable(x), Variable(target)</span><br><span class="line">        out = model(x)</span><br><span class="line">        loss = ceriation(out, target)</span><br><span class="line">        ave_loss = ave_loss * <span class="number">0.9</span> + loss.data[<span class="number">0</span>] * <span class="number">0.1</span></span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> (batch_idx+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> (batch_idx+<span class="number">1</span>) == len(train_loader):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'</span>.format(</span><br><span class="line">                epoch, batch_idx+<span class="number">1</span>, ave_loss)</span><br><span class="line">    <span class="comment"># testing</span></span><br><span class="line">    correct_cnt, ave_loss = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    total_cnt = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch_idx, (x, target) <span class="keyword">in</span> enumerate(test_loader):</span><br><span class="line">        <span class="keyword">if</span> use_cuda:</span><br><span class="line">            x, targe = x.cuda(), target.cuda()</span><br><span class="line">        x, target = Variable(x, volatile=<span class="literal">True</span>), Variable(target, volatile=<span class="literal">True</span>)</span><br><span class="line">        out = model(x)</span><br><span class="line">        loss = ceriation(out, target)</span><br><span class="line">        _, pred_label = torch.max(out.data, <span class="number">1</span>)</span><br><span class="line">        total_cnt += x.data.size()[<span class="number">0</span>]</span><br><span class="line">        correct_cnt += (pred_label == target.data).sum()</span><br><span class="line">        <span class="comment"># smooth average</span></span><br><span class="line">        ave_loss = ave_loss * <span class="number">0.9</span> + loss.data[<span class="number">0</span>] * <span class="number">0.1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span>(batch_idx+<span class="number">1</span>) % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> (batch_idx+<span class="number">1</span>) == len(test_loader):</span><br><span class="line">            <span class="keyword">print</span> <span class="string">'==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, test loss: &#123;:.6f&#125;, acc: &#123;:.3f&#125;'</span>.format(</span><br><span class="line">                epoch, batch_idx+<span class="number">1</span>, ave_loss, correct_cnt * <span class="number">1.0</span> / total_cnt)</span><br></pre></td></tr></table></figure>
<p>当优化完毕后，需要保存模型。这里<a href="http://pytorch.org/docs/notes/serialization.html#recommend-saving-models" target="_blank" rel="noopener">官方文档</a>给出了推荐的方法，如下所示：<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line">torch.save(model.state_dict(), PATH)   <span class="comment">#保存网络参数</span></span><br><span class="line">the_model = TheModelClass(*args, **kwargs)</span><br><span class="line">the_model.load_state_dict(torch.load(PATH))  <span class="comment">#读取网络参数</span></span><br></pre></td></tr></table></figure></p>
<p>该博客的完整代码可以见：<a href="https://gist.github.com/xmfbit/b27cdbff68870418bdb8cefa86a2d558" target="_blank" rel="noopener">PyTorch MNIST demo</a>。</p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>远程登录Jupyter笔记本</title>
    <url>/2017/02/26/jupyternotebook-remote-useage/</url>
    <content><![CDATA[<p>Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用<code>jupyter notebook</code>命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。<br><img src="/img/jupyternotebook_logo.png" alt="jupyternotebook"><br><a id="more"></a></p>
<h2 id="配置jupter-notebook"><a href="#配置jupter-notebook" class="headerlink" title="配置jupter notebook"></a>配置jupter notebook</h2><p>登录远程服务器后，使用如下命令生成配置文件。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">jupyter notebook --generate-config</span><br></pre></td></tr></table></figure>
<p>并对其内容进行修改。打开配置文件：</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">vim ~/.jupyter/jupyter_notebook_config.py</span><br></pre></td></tr></table></figure>
<p>主要修改两处地方：</p>
<ul>
<li><code>c.NotebookApp.ip=&#39;*&#39;</code>，即不限制ip访问</li>
<li><code>c.NotebookApp.password = u&#39;hash_value&#39;</code></li>
</ul>
<p>上面的<code>hash_value</code>是由用户给定的密码生成的。可以使用<code>ipython</code>中的命令轻松搞定。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> notebook.auth <span class="keyword">import</span> passwd</span><br><span class="line">passwd()</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">这里会要求用户输入密码并确认，生成的hash值要填写到上面</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成<code>False</code>即可。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">c.NotebookApp.open_browser = True</span><br></pre></td></tr></table></figure>
<h2 id="启动notebook"><a href="#启动notebook" class="headerlink" title="启动notebook"></a>启动notebook</h2><p>之后，在远程服务器上启动笔记本<code>jupyter notebook</code>。接着，在本地机器上访问<code>远程服务器ip:8888</code>（默认端口为<code>8888</code>，也可以在配置文件中修改），输入密码即可访问远程笔记本。</p>
<h2 id="Jupyter-Notebook的常用快捷键"><a href="#Jupyter-Notebook的常用快捷键" class="headerlink" title="Jupyter Notebook的常用快捷键"></a>Jupyter Notebook的常用快捷键</h2><p>类似Vim编辑器，Jupyter Notebook有两种键盘输入模式：</p>
<ul>
<li>编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过<code>Esc</code>可以切换至命令模式。</li>
<li>命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过<code>Enter</code>可以切换至编辑模式。</li>
</ul>
<p>常用的快捷键有：</p>
<ul>
<li>h：弹出快捷键列表</li>
<li>m：将该Cell转换为Markdown输入状态</li>
<li>y：将该Cell转换为代码输入状态</li>
<li>shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建）</li>
<li>Ctrl+Enter：执行该Cell</li>
</ul>
<p>本篇内容参考自博客<a href="http://blog.leanote.com/post/jevonswang/远程访问jupyter-notebook" target="_blank" rel="noopener">远程访问jupyter notebook</a>以及<a href="http://www.cnblogs.com/weidiao/p/7792885.html" target="_blank" rel="noopener">Jupyter Notebook快捷键
</a>。</p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>PyTorch简介</title>
    <url>/2017/02/25/pytorch-tutor-01/</url>
    <content><![CDATA[<p>这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于<a href="https://github.com/pytorch/tutorials/blob/master/Deep%20Learning%20with%20PyTorch.ipynb" target="_blank" rel="noopener">GitHub repo</a>。<br><img src="/img/pytorch_logo.png" alt="PyTorch Logo"></p>
<a id="more"></a>
<h2 id="PyTorch简介"><a href="#PyTorch简介" class="headerlink" title="PyTorch简介"></a>PyTorch简介</h2><p><a href="https://github.com/pytorch/pytorch" target="_blank" rel="noopener">PyTorch</a>是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的<code>numpy</code>，另一方面，PyTorch也是强大的深度学习框架。</p>
<p>目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的<code>prototxt</code>进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。</p>
<h2 id="Tensors"><a href="#Tensors" class="headerlink" title="Tensors"></a>Tensors</h2><p><code>Tensor</code>，即<code>numpy</code>中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的<code>Tensor</code>可以与<code>numpy</code>中的<code>array</code>很方便地进行互相转换。</p>
<p>通过<code>Tensor(shape)</code>便可以创建所需要大小的<code>tensor</code>。如下所示。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">x = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)  <span class="comment"># construct a 5x3 matrix, uninitialized</span></span><br><span class="line"><span class="comment"># 或者随机填充</span></span><br><span class="line">y = torch.rand(<span class="number">5</span>, <span class="number">3</span>)    <span class="comment"># construct a randomly initialized matrix</span></span><br><span class="line"><span class="comment"># 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuple</span></span><br><span class="line">x.size()                <span class="comment"># out: torch.Size([5, 3])</span></span><br></pre></td></tr></table></figure>
<p>PyTorch中已经实现了很多常用的<code>op</code>，如下所示。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># addition: syntax 1</span></span><br><span class="line">x + y                  <span class="comment"># out: [torch.FloatTensor of size 5x3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># addition: syntax 2</span></span><br><span class="line">torch.add(x, y)        <span class="comment"># 或者使用torch包中的显式的op名称</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># addition: giving an output tensor</span></span><br><span class="line">result = torch.Tensor(<span class="number">5</span>, <span class="number">3</span>)  <span class="comment"># 预先定义size</span></span><br><span class="line">torch.add(x, y, out=result)  <span class="comment"># 结果被填充到变量result</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 对于加法运算，其实没必要这么复杂</span></span><br><span class="line">out = x + y                  <span class="comment"># 无需预先定义size</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># torch包中带有下划线的op说明是就地进行的，如下所示</span></span><br><span class="line"><span class="comment"># addition: in-place</span></span><br><span class="line">y.add_(x)              <span class="comment"># 将x加到y上</span></span><br><span class="line"><span class="comment"># 其他的例子: x.copy_(y), x.t_().</span></span><br></pre></td></tr></table></figure>
<p>PyTorch中的元素索引方式和<code>numpy</code>相同。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># standard numpy-like indexing with all bells and whistles</span></span><br><span class="line">x[:,<span class="number">1</span>]                 <span class="comment"># out: [torch.FloatTensor of size 5]</span></span><br></pre></td></tr></table></figure>
<p>对于更多的<code>op</code>，可以参见PyTorch的<a href="http://pytorch.org/docs/torch.html" target="_blank" rel="noopener">文档页面</a>。</p>
<p><code>Tensor</code>可以和<code>numpy</code>中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># Tensor 转为 np.array</span></span><br><span class="line">a = torch.ones(<span class="number">5</span>)    <span class="comment"># out: [torch.FloatTensor of size 5]</span></span><br><span class="line"><span class="comment"># 使用 numpy方法即可实现转换</span></span><br><span class="line">b = a.numpy()        <span class="comment"># out: array([ 1.,  1.,  1.,  1.,  1.], dtype=float32)</span></span><br><span class="line"><span class="comment"># 注意！a的值的变化同样引起b的变化</span></span><br><span class="line">a.add_(<span class="number">1</span>)</span><br><span class="line">print(a)</span><br><span class="line">print(b)             <span class="comment"># a b的值都变成2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># np.array 转为Tensor</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a = np.ones(<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 使用torch.from_numpy即可实现转换</span></span><br><span class="line">b = torch.from_numpy(a)  <span class="comment"># out: [torch.DoubleTensor of size 5]</span></span><br><span class="line">np.add(a, <span class="number">1</span>, out=a)</span><br><span class="line">print(a)</span><br><span class="line">print(b)            <span class="comment"># a b的值都变为2</span></span><br></pre></td></tr></table></figure>
<p>PyTorch中使用GPU计算很简单，通过调用<code>.cuda()</code>方法，很容易实现GPU支持。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># let us run this cell only if CUDA is available</span></span><br><span class="line"><span class="keyword">if</span> torch.cuda.is_available():</span><br><span class="line">    print(<span class="string">'cuda is avaliable'</span>)</span><br><span class="line">    x = x.cuda()</span><br><span class="line">    y = y.cuda()</span><br><span class="line">    x + y          <span class="comment"># 在GPU上进行计算</span></span><br></pre></td></tr></table></figure>
<h2 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h2><p>说完了数据类型<code>Tensor</code>，下一步便是如何实现一个神经网络。首先，对<a href="http://pytorch.org/docs/autograd.html" target="_blank" rel="noopener">自动求导</a>做一说明。</p>
<p>我们需要关注的是<code>autograd.Variable</code>。这个东西包装了<code>Tensor</code>。一旦你完成了计算，就可以使用<code>.backward()</code>方法自动得到（以该<code>Variable</code>为叶子节点的那个）网络中参数的梯度。<code>Variable</code>有一个名叫<code>data</code>的字段，可以通过它获得被包装起来的那个原始的<code>Tensor</code>数据。同时，使用<code>grad</code>字段，可以获取梯度（也是一个<code>Variable</code>）。</p>
<p><code>Variable</code>是计算图的节点，同时<code>Function</code>实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个<code>Variable</code>有一个<code>creator</code>的字段，表明了它是由哪个<code>Function</code>创建的（除了用户自己显式创建的那些，这时候<code>creator</code>是<code>None</code>）。</p>
<p>当进行反向传播计算梯度时，如果<code>Variable</code>是标量（比如最终的<code>loss</code>是欧氏距离或者交叉熵），那么<code>backward()</code>函数不需要参数。然而如果<code>Variable</code>有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和<code>Variable</code>shape匹配的<code>Tensor</code>）。看下面的说明代码。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.autograd <span class="keyword">import</span> Variable</span><br><span class="line">x = Variable(torch.ones(<span class="number">2</span>, <span class="number">2</span>), requires_grad = <span class="literal">True</span>)</span><br><span class="line">x     <span class="comment"># x 包装了一个2x2的Tensor</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 1  1</span></span><br><span class="line"><span class="string"> 1  1</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># Variable进行计算</span></span><br><span class="line"><span class="comment"># y was created as a result of an operation,</span></span><br><span class="line"><span class="comment"># so it has a creator</span></span><br><span class="line">y = x + <span class="number">2</span></span><br><span class="line">y.creator    <span class="comment"># out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;</span></span><br><span class="line"></span><br><span class="line">z = y * y * <span class="number">3</span>  </span><br><span class="line">out = z.mean()   <span class="comment"># out: Variable containing: 27 [torch.FloatTensor of size 1]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># let's backprop now</span></span><br><span class="line">out.backward()  <span class="comment"># 其实相当于 out.backward(torch.Tensor([1.0]))</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># print gradients d(out)/dx</span></span><br><span class="line">x.grad</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 4.5000  4.5000</span></span><br><span class="line"><span class="string"> 4.5000  4.5000</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 2x2]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>下面的代码就是结果不是标量，而是普通的<code>Tensor</code>的例子。<br><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 也可以通过Tensor显式地创建Variable</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>)</span><br><span class="line">x = Variable(x, requires_grad = <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 一个更复杂的 op例子</span></span><br><span class="line">y = x * <span class="number">2</span></span><br><span class="line"><span class="keyword">while</span> y.data.norm() &lt; <span class="number">1000</span>:</span><br><span class="line">    y = y * <span class="number">2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 dy/dx</span></span><br><span class="line">gradients = torch.FloatTensor([<span class="number">0.1</span>, <span class="number">1.0</span>, <span class="number">0.0001</span>])</span><br><span class="line">y.backward(gradients)</span><br><span class="line">x.grad</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string">  204.8000</span></span><br><span class="line"><span class="string"> 2048.0000</span></span><br><span class="line"><span class="string">    0.2048</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 3]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure></p>
<p>说完了NN的构成元素<code>Variable</code>，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了<code>torch.nn</code>包。我们自定义的网络结构是由若干的<code>layer</code>组成的，我们将其设置为 <code>nn.Module</code>的子类，只要使用方法<code>forward(input)</code>就可以返回网络的<code>output</code>。下面的代码展示了如何建立一个包含有<code>conv</code>和<code>max-pooling</code>和<code>fc</code>层的简单CNN网络。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn                 <span class="comment"># 以我的理解，貌似有参数的都在nn里面</span></span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F       <span class="comment"># 没有参数的（如pooling和relu）都在functional里面？</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">6</span>, <span class="number">5</span>) <span class="comment"># 1 input image channel, 6 output channels, 5x5 square convolution kernel</span></span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        <span class="comment"># 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。</span></span><br><span class="line">        <span class="comment"># 所以fc层的第一个参数是 16x5x5</span></span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>) <span class="comment"># an affine operation: y = Wx + b</span></span><br><span class="line">        self.fc2   = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3   = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        <span class="comment"># 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构</span></span><br><span class="line">        <span class="comment"># 同时，我们无需实现 backward，这是被自动求导实现的</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv1(x)), (<span class="number">2</span>, <span class="number">2</span>)) <span class="comment"># Max pooling over a (2, 2) window</span></span><br><span class="line">        x = F.max_pool2d(F.relu(self.conv2(x)), <span class="number">2</span>) <span class="comment"># If the size is a square you can only specify a single number</span></span><br><span class="line">        x = x.view(<span class="number">-1</span>, self.num_flat_features(x))  <span class="comment"># 把它拉直</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">num_flat_features</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        size = x.size()[<span class="number">1</span>:] <span class="comment"># all dimensions except the batch dimension</span></span><br><span class="line">        num_features = <span class="number">1</span></span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> size:</span><br><span class="line">            num_features *= s</span><br><span class="line">        <span class="keyword">return</span> num_features</span><br><span class="line"></span><br><span class="line"><span class="comment"># 实例化Net对象</span></span><br><span class="line">net = Net()</span><br><span class="line">net     <span class="comment"># 给出了网络结构</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Net (</span></span><br><span class="line"><span class="string">  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))</span></span><br><span class="line"><span class="string">  (fc1): Linear (400 -&gt; 120)</span></span><br><span class="line"><span class="string">  (fc2): Linear (120 -&gt; 84)</span></span><br><span class="line"><span class="string">  (fc3): Linear (84 -&gt; 10)</span></span><br><span class="line"><span class="string">)</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>我们可以列出网络中的所有参数。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">params = list(net.parameters())</span><br><span class="line">print(len(params))      <span class="comment"># out: 10, 5个权重，5个bias</span></span><br><span class="line">print(params[<span class="number">0</span>].size())  <span class="comment"># conv1's weight out: torch.Size([6, 1, 5, 5])</span></span><br><span class="line">print(params[<span class="number">1</span>].size())  <span class="comment"># conv1's bias, out: torch.Size([6])</span></span><br></pre></td></tr></table></figure>
<p>给出网络的输入，得到网络的输出。并进行反向传播梯度。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">input = Variable(torch.randn(<span class="number">1</span>, <span class="number">1</span>, <span class="number">32</span>, <span class="number">32</span>))</span><br><span class="line">out = net(input)         <span class="comment"># 重载了()运算符？</span></span><br><span class="line">net.zero_grad()          <span class="comment"># bp前，把所有参数的grad buffer清零</span></span><br><span class="line">out.backward(torch.randn(<span class="number">1</span>, <span class="number">10</span>))</span><br></pre></td></tr></table></figure>
<p>注意一点，<code>torch.nn</code>只支持mini-batch。所以如果你的输入只有一个样例的时候，使用<code>input.unsqueeze(0)</code>人为给它加上一个维度，让它变成一个4-D的<code>Tensor</code>。</p>
<h2 id="网络训练"><a href="#网络训练" class="headerlink" title="网络训练"></a>网络训练</h2><p>给定target和网络的output，就可以计算loss函数了。在<code>torch.nn</code>中已经<a href="http://pytorch.org/docs/nn.html#loss-functions" target="_blank" rel="noopener">实现好了一些loss函数</a>。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">output = net(input)</span><br><span class="line">target = Variable(torch.range(<span class="number">1</span>, <span class="number">10</span>))  <span class="comment"># a dummy target, for example</span></span><br><span class="line"><span class="comment"># 使用平均平方误差，即欧几里得距离</span></span><br><span class="line">criterion = nn.MSELoss()</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">Variable containing:</span></span><br><span class="line"><span class="string"> 38.6049</span></span><br><span class="line"><span class="string">[torch.FloatTensor of size 1]</span></span><br><span class="line"><span class="string">"""</span></span><br></pre></td></tr></table></figure>
<p>网络的整体结构如下所示。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d  </span><br><span class="line">      -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear</span><br><span class="line">      -&gt; MSELoss</span><br><span class="line">      -&gt; loss</span><br></pre></td></tr></table></figure>
<p>我们可以使用<code>previous_functions</code>来获得该节点前面<code>Function</code>的信息。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># For illustration, let us follow a few steps backward</span><br><span class="line">print(loss.creator) # MSELoss</span><br><span class="line">print(loss.creator.previous_functions[0][0]) # Linear</span><br><span class="line">print(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU</span><br><span class="line">&quot;&quot;&quot;</span><br><span class="line">&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;</span><br><span class="line">&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;</span><br><span class="line">&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;</span><br><span class="line">&quot;&quot;&quot;</span><br></pre></td></tr></table></figure>
<p>进行反向传播后，让我们查看一下参数的变化。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment"># now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.</span></span><br><span class="line">net.zero_grad() <span class="comment"># zeroes the gradient buffers of all parameters</span></span><br><span class="line">print(<span class="string">'conv1.bias.grad before backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br><span class="line">loss.backward()</span><br><span class="line">print(<span class="string">'conv1.bias.grad after backward'</span>)</span><br><span class="line">print(net.conv1.bias.grad)</span><br></pre></td></tr></table></figure>
<p>计算梯度后，自然需要更新参数了。简单的方法可以自己手写：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">learning_rate = <span class="number">0.01</span></span><br><span class="line"><span class="keyword">for</span> f <span class="keyword">in</span> net.parameters():</span><br><span class="line">    f.data.sub_(f.grad.data * learning_rate)</span><br></pre></td></tr></table></figure>
<p>不过，<code>torch.optim</code>中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="comment"># create your optimizer</span></span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr = <span class="number">0.01</span>)</span><br><span class="line"><span class="comment"># in your training loop:</span></span><br><span class="line">optimizer.zero_grad() <span class="comment"># zero the gradient buffers</span></span><br><span class="line">output = net(input)</span><br><span class="line">loss = criterion(output, target)</span><br><span class="line">loss.backward()</span><br><span class="line">optimizer.step() <span class="comment"># Does the update</span></span><br></pre></td></tr></table></figure>
<h2 id="数据载入"><a href="#数据载入" class="headerlink" title="数据载入"></a>数据载入</h2><p>由于PyTorch的Python接口和<code>np.array</code>之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了<code>torchvision</code>包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> torchvision</span><br><span class="line"><span class="keyword">import</span> torchvision.transforms <span class="keyword">as</span> transforms</span><br><span class="line"></span><br><span class="line"><span class="comment"># The output of torchvision datasets are PILImage images of range [0, 1].</span></span><br><span class="line"><span class="comment"># We transform them to Tensors of normalized range [-1, 1]</span></span><br><span class="line"><span class="comment"># Compose: Composes several transforms together.</span></span><br><span class="line"><span class="comment"># see http://pytorch.org/docs/torchvision/transforms.html?highlight=transforms</span></span><br><span class="line">transform=transforms.Compose([transforms.ToTensor(),</span><br><span class="line">                              transforms.Normalize((<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>), (<span class="number">0.5</span>, <span class="number">0.5</span>, <span class="number">0.5</span>)),</span><br><span class="line">                             ])   <span class="comment"># torchvision.transforms.Normalize(mean, std)</span></span><br><span class="line"><span class="comment"># 读取CIFAR10数据集                             </span></span><br><span class="line">trainset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">True</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line"><span class="comment"># 使用DataLoader</span></span><br><span class="line">trainloader = torch.utils.data.DataLoader(trainset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">True</span>, num_workers=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># Test集，设置train = False</span></span><br><span class="line">testset = torchvision.datasets.CIFAR10(root=<span class="string">'./data'</span>, train=<span class="literal">False</span>, download=<span class="literal">True</span>, transform=transform)</span><br><span class="line">testloader = torch.utils.data.DataLoader(testset, batch_size=<span class="number">4</span>,</span><br><span class="line">                                          shuffle=<span class="literal">False</span>, num_workers=<span class="number">2</span>)</span><br><span class="line">classes = (<span class="string">'plane'</span>, <span class="string">'car'</span>, <span class="string">'bird'</span>, <span class="string">'cat'</span>,</span><br><span class="line">           <span class="string">'deer'</span>, <span class="string">'dog'</span>, <span class="string">'frog'</span>, <span class="string">'horse'</span>, <span class="string">'ship'</span>, <span class="string">'truck'</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们对上面部分的CNN网络进行小修，设置第一个<code>conv</code>层接受3通道的输入。并使用交叉熵定义loss。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">6</span>, <span class="number">5</span>)</span><br><span class="line">        self.pool  = nn.MaxPool2d(<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">6</span>, <span class="number">16</span>, <span class="number">5</span>)</span><br><span class="line">        self.fc1   = nn.Linear(<span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>, <span class="number">120</span>)</span><br><span class="line">        self.fc2   = nn.Linear(<span class="number">120</span>, <span class="number">84</span>)</span><br><span class="line">        self.fc3   = nn.Linear(<span class="number">84</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = self.pool(F.relu(self.conv1(x)))</span><br><span class="line">        x = self.pool(F.relu(self.conv2(x)))</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">16</span>*<span class="number">5</span>*<span class="number">5</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = F.relu(self.fc2(x))</span><br><span class="line">        x = self.fc3(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line"></span><br><span class="line">net = Net()</span><br><span class="line"><span class="comment"># use a Classification Cross-Entropy loss</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=<span class="number">0.001</span>, momentum=<span class="number">0.9</span>)</span><br></pre></td></tr></table></figure>
<p>接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">2</span>): <span class="comment"># loop over the dataset multiple times</span></span><br><span class="line"></span><br><span class="line">    running_loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i, data <span class="keyword">in</span> enumerate(trainloader, <span class="number">0</span>):</span><br><span class="line">        <span class="comment"># get the inputs</span></span><br><span class="line">        inputs, labels = data</span><br><span class="line"></span><br><span class="line">        <span class="comment"># wrap them in Variable</span></span><br><span class="line">        inputs, labels = Variable(inputs), Variable(labels)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># zero the parameter gradients</span></span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># forward + backward + optimize</span></span><br><span class="line">        outputs = net(inputs)</span><br><span class="line">        loss = criterion(outputs, labels)</span><br><span class="line">        loss.backward()        </span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># print statistics</span></span><br><span class="line">        running_loss += loss.data[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">1999</span>: <span class="comment"># print every 2000 mini-batches</span></span><br><span class="line">            print(<span class="string">'[%d, %5d] loss: %.3f'</span> % (epoch+<span class="number">1</span>, i+<span class="number">1</span>, running_loss / <span class="number">2000</span>))</span><br><span class="line">            running_loss = <span class="number">0.0</span></span><br><span class="line">print(<span class="string">'Finished Training'</span>)</span><br></pre></td></tr></table></figure>
<p>我们在测试集上选取一个mini-batch（也就是4张，见上面<code>testloader</code>的定义），进行测试。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">dataiter = iter(testloader)</span><br><span class="line">images, labels = dataiter.next()   <span class="comment"># 得到image和对应的label</span></span><br><span class="line">outputs = net(Variable(images))</span><br><span class="line"></span><br><span class="line"><span class="comment"># the outputs are energies for the 10 classes.</span></span><br><span class="line"><span class="comment"># Higher the energy for a class, the more the network</span></span><br><span class="line"><span class="comment"># thinks that the image is of the particular class</span></span><br><span class="line"><span class="comment"># So, let's get the index of the highest energy</span></span><br><span class="line">_, predicted = torch.max(outputs.data, <span class="number">1</span>)   <span class="comment"># 找出分数最高的对应的channel，即为top-1类别</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Predicted: '</span>, <span class="string">' '</span>.join(<span class="string">'%5s'</span>% classes[predicted[j][<span class="number">0</span>]] <span class="keyword">for</span> j <span class="keyword">in</span> range(<span class="number">4</span>)))</span><br></pre></td></tr></table></figure>
<p>测试一下整个测试集合上的表现。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">correct = <span class="number">0</span></span><br><span class="line">total = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:     <span class="comment"># 每一个test mini-batch</span></span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">    total += labels.size(<span class="number">0</span>)</span><br><span class="line">    correct += (predicted == labels).sum()</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Accuracy of the network on the 10000 test images: %d %%'</span> % (<span class="number">100</span> * correct / total))</span><br></pre></td></tr></table></figure>
<p>对哪一类的预测精度更高呢？</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">class_correct = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line">class_total = list(<span class="number">0.</span> <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>))</span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> testloader:</span><br><span class="line">    images, labels = data</span><br><span class="line">    outputs = net(Variable(images))</span><br><span class="line">    _, predicted = torch.max(outputs.data, <span class="number">1</span>)</span><br><span class="line">    c = (predicted == labels).squeeze()</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">4</span>):</span><br><span class="line">        label = labels[i]</span><br><span class="line">        class_correct[label] += c[i]</span><br><span class="line">        class_total[label] += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用<code>.cuda()</code>方法就行了。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">net.cuda()</span><br></pre></td></tr></table></figure>
<p>不过记得在每次训练测试的迭代中，<code>images</code>和<code>label</code>也要传送到GPU上才可以。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())</span><br></pre></td></tr></table></figure>
<h2 id="更多的例子和教程"><a href="#更多的例子和教程" class="headerlink" title="更多的例子和教程"></a>更多的例子和教程</h2><p><a href="https://github.com/pytorch/examples" target="_blank" rel="noopener">更多的例子</a><br><a href="https://github.com/pytorch/tutorials" target="_blank" rel="noopener">更多的教程</a></p>
]]></content>
      <tags>
        <tag>pytorch</tag>
      </tags>
  </entry>
  <entry>
    <title>在Caffe中使用Baidu warpctc实现CTC Loss的计算</title>
    <url>/2017/02/22/warpctc-caffe/</url>
    <content><![CDATA[<p>CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的<a href="https://github.com/baidu-research/warp-ctc" target="_blank" rel="noopener">warp-ctc</a>，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的<a href="https://github.com/baidu-research/warp-ctc" target="_blank" rel="noopener">项目页面</a>。本文介绍内容的相关代码可以参见我的GitHub项目<a href="https://github.com/xmfbit/warpctc-caffe" target="_blank" rel="noopener">warpctc-caffe</a><br><img src="/img/warpctc_intro.png" alt="CTC Loss"><br><a id="more"></a></p>
<h2 id="移植warp-ctc"><a href="#移植warp-ctc" class="headerlink" title="移植warp-ctc"></a>移植warp-ctc</h2><p>本节介绍了如何将<code>warp-ctc</code>的源码在Caffe中进行编译。</p>
<p>首先，我们将<code>warp-ctc</code>的项目代码从GitHub上clone下来。在Caffe的<code>include/caffe</code>和<code>src/caffe</code>下分别创建名为<code>3rdparty</code>的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。</p>
<p>由于<code>warp-ctc</code>中使用了<code>C++11</code>的相关技术，所以需要修改Caffe的<code>Makefile</code>文件，添加<code>C++11</code>支持，可以参见<a href="https://github.com/xmfbit/warpctc-caffe/blob/master/Makefile" target="_blank" rel="noopener">Makefile</a>。</p>
<p>对Caffe的修改就是这么简单，之后我们需要修改<code>warp-ctc</code>中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。</p>
<p><code>warp-ctc</code>提供了CPU多线程的计算，这里我直接将相应的<code>openmp</code>并行化语句删掉了。</p>
<p>另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为<code>cuh</code>，这样才能够通过编译。否则编译器会给出找不到<code>__host__</code>和<code>__device__</code>等等关键字的错误。</p>
<p>对于详细的修改配置，还请参见GitHub相应的<a href="https://github.com/xmfbit/warpctc-caffe/blob/master/include/caffe/3rdparty/detail/hostdevice.cuh" target="_blank" rel="noopener">代码文件</a>。</p>
<h2 id="实现CTC-Loss计算"><a href="#实现CTC-Loss计算" class="headerlink" title="实现CTC Loss计算"></a>实现CTC Loss计算</h2><p>编译没有问题后，我们可以编写<code>ctc_loss_layer</code>实现CTC Loss的计算。在实现时，注意参考文件<code>ctc.h</code>。这个文件中给出了使用<code>warp-ctc</code>进行CTC Loss计算的全部API接口。</p>
<p><code>ctc_loss_layer</code>继承自<code>loss_layer</code>，主要是前向和反向计算的实现。由于<code>warp-ctc</code>中只对单精度浮点数<code>float</code>进行支持，所以，对于双精度网络参数，直接将其设置为<code>NOT_IMPLEMENTED</code>，如下所示。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> CtcLossLayer&lt;<span class="keyword">double</span>&gt;::Forward_cpu(</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;<span class="keyword">double</span>&gt;*&gt;&amp; bottom, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;<span class="keyword">double</span>&gt;*&gt;&amp; top) &#123;</span><br><span class="line">    NOT_IMPLEMENTED;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">template</span> &lt;&gt;</span><br><span class="line"><span class="keyword">void</span> CtcLossLayer&lt;<span class="keyword">double</span>&gt;::Backward_cpu(<span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;<span class="keyword">double</span>&gt;*&gt;&amp; top,</span><br><span class="line">    <span class="keyword">const</span> <span class="built_in">vector</span>&lt;<span class="keyword">bool</span>&gt;&amp; propagate_down, <span class="keyword">const</span> <span class="built_in">vector</span>&lt;Blob&lt;<span class="keyword">double</span>&gt;*&gt;&amp; bottom) &#123;</span><br><span class="line">    NOT_IMPLEMENTED;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>使用<code>warp-ctc</code>相关接口进行CTC Loss计算的步骤如下：</p>
<ul>
<li>设置<code>ctcOptions</code>，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。</li>
<li>调用<code>get_workspace_size()</code>函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。</li>
<li>调用<code>compute_ctc_loss()</code>函数，计算<code>loss</code>和<code>gradient</code>。</li>
</ul>
<p>其中，在第三步中计算<code>gradient</code>时，可以直接将对应<code>blob</code>的<code>cpu/gpu_diff</code>指针传入，作为<code>gradient</code>。</p>
<p>这部分的实现代码分别位于<code>include/caffe/layers</code>和<code>src/caffe/layers/</code>下。</p>
<h2 id="验证码数字识别"><a href="#验证码数字识别" class="headerlink" title="验证码数字识别"></a>验证码数字识别</h2><p>本部分相关代码位于<code>examples/warpctc</code>文件夹下。实验方案如下。</p>
<ul>
<li>使用<code>Python</code>中的<code>capycha</code>进行包含<code>0-9</code>数字的验证码图片的产生，图片中数字个数从<code>1</code>到<code>MAX_LEN</code>不等。</li>
<li>使用<code>10</code>作为<code>blank_label</code>，将所有的标签序列在后面补<code>blank_label</code>以达到同样的长度<code>MAX_LEN</code>。</li>
<li>将图像的每一列看做一个time step，网络模型使用<code>image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss</code>，简单粗暴。</li>
<li>模型训练过程中，数据输入使用<code>HDF5</code>格式。</li>
</ul>
<h3 id="数据产生"><a href="#数据产生" class="headerlink" title="数据产生"></a>数据产生</h3><p>使用<code>captcha</code>生成验证码图片。<a href="https://pypi.python.org/pypi/captcha/0.1.1" target="_blank" rel="noopener">这里</a>是一个简单的API demo。默认生成的图片大小为<code>160x60</code>。我们将其长宽缩小一半，使用<code>80x30</code>的彩色图片作为输入。</p>
<p>使用<code>python</code>中的<code>h5py</code>模块生成<code>HDF5</code>格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。</p>
<h3 id="LSTM的输入"><a href="#LSTM的输入" class="headerlink" title="LSTM的输入"></a>LSTM的输入</h3><p>在Caffe中已经有了<code>lstm_layer</code>的实现。<code>lstm_layer</code>要求输入的序列<code>blob</code>为<code>TxNx...</code>，也就是说我们需要将输入的image进行转置。</p>
<p>Caffe中Batch的内存布局顺序为<code>NxCxHxW</code>。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了<a href="https://github.com/weiliu89/caffe/blob/ssd/include/caffe/layers/permute_layer.hpp" target="_blank" rel="noopener">liuwei的SSD工作中实现的<code>permute_layer</code></a>进行转置，将<code>W</code>维度放到最前方。与之对应的参数定义如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">layer &#123;</span><br><span class="line">    name: &quot;permuted_data&quot;</span><br><span class="line">    type: &quot;Permute&quot;</span><br><span class="line">    bottom: &quot;data&quot;</span><br><span class="line">    top: &quot;permuted_data&quot;</span><br><span class="line">    permute_param &#123;</span><br><span class="line">        order: 3   # W</span><br><span class="line">        order: 0   # N</span><br><span class="line">        order: 1   # C</span><br><span class="line">        order: 2   # H</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为<code>ContinuationIndicator</code>的layer，产生对应的time indicator序列。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>在某次试验中，迭代50,000次，实验过程中的损失函数变化如下：</p>
<p><img src="/img/captcha_train_loss.png" alt="train loss"></p>
<p>在验证集上的精度变化如下：</p>
<p><img src="/img/captcha_test_accuracy.png" alt="test accuracy"></p>
<p>最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。</p>
<p>至于该实验的具体细节，可以参考repo的相关具体代码实现。</p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>deep learning</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-MeanShift</title>
    <url>/2017/02/12/cs131-mean-shift/</url>
    <content><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Mean_shift" target="_blank" rel="noopener">MeanShift</a>最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文<a href="http://courses.csail.mit.edu/6.869/handouts/PAMIMeanshift.pdf" target="_blank" rel="noopener">Mean Shift: A Robust Approach Toward Feature Space Analysis</a>，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。</p>
<p>MeanShift是一种用来寻找特征空间内<a href="https://en.wikipedia.org/wiki/Mode_(statistics" target="_blank" rel="noopener">模态</a>的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。<br><img src="/img/meanshift_basics.jpg" alt="MeanShift"></p>
<a id="more"></a>
<h2 id="核密度估计"><a href="#核密度估计" class="headerlink" title="核密度估计"></a>核密度估计</h2><p>上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了<a href="https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/" target="_blank" rel="noopener">这篇博客</a>和<a href="https://saravananthirumuruganathan.wordpress.com/2010/04/01/introduction-to-mean-shift-algorithm/" target="_blank" rel="noopener">这篇讲义</a>。</p>
<p>注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\mathbb{R}^n\rightarrow \mathbb{R}$的函数满足以下条件，就能将其作为核函数。<br><img src="/img/meanshift_kernel_function.png" alt="kernel"></p>
<p>比如高斯核函数：</p>
<script type="math/tex; mode=display">K(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{x^2}{2\sigma^2})</script><p>核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。</p>
<script type="math/tex; mode=display">f(x) = \frac{1}{nh^d}\sum_{i=1}^{n}K(\frac{x-x_i}{h})</script><p>如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。</p>
<script type="math/tex; mode=display">K(x) = c_{k,d}k(\Arrowvert x\Arrowvert ^2)</script><h2 id="mean-shift向量"><a href="#mean-shift向量" class="headerlink" title="mean shift向量"></a>mean shift向量</h2><p>那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\prime(s)$。<br><img src="/img/meanshift_gradient_of_density.png" alt="密度函数的梯度"></p>
<p>观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\Arrowvert x\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。</p>
<script type="math/tex; mode=display">m_h(x) = \frac{\sum_{i=1}^{n}x_i g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}{\sum_{i=1}^{n}g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}-x</script><p>所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% generate data</span></span><br><span class="line">mu = [<span class="number">1</span> <span class="number">2</span>];</span><br><span class="line">Sigma = [<span class="number">1</span> <span class="number">0</span>; <span class="number">0</span> <span class="number">2</span>]; R = chol(Sigma);</span><br><span class="line">N = <span class="number">250</span>;</span><br><span class="line">data = <span class="built_in">repmat</span>(mu, N, <span class="number">1</span>) + <span class="built_in">randn</span>(N, <span class="number">2</span>)*R;</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line"><span class="built_in">scatter</span>(data(:, <span class="number">1</span>), data(:, <span class="number">2</span>), <span class="number">50</span>, <span class="string">'filled'</span>);</span><br><span class="line"><span class="comment">%% meanshift</span></span><br><span class="line">mu0 = <span class="built_in">rand</span>(<span class="number">1</span>,<span class="number">2</span>) * <span class="number">5</span>;</span><br><span class="line">mu = mean_shift(mu0, <span class="number">10</span>, data);</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">gaussian_kernel</span><span class="params">(x, sigma)</span></span></span><br><span class="line"><span class="comment">% gauss kernel, g(x) = \exp(-x^2/2\sigma^2)</span></span><br><span class="line">out = <span class="built_in">exp</span>(-x.*x/(<span class="number">2</span>*sigma*sigma));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">mu</span> = <span class="title">mean_shift</span><span class="params">(mu0, h, data)</span></span></span><br><span class="line"><span class="comment">% implementation of meanshift algorithm</span></span><br><span class="line"><span class="comment">% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \frac&#123;\sum_i=1^n xg&#125;&#123;\sum_i=1^n g&#125;</span></span><br><span class="line">mu = mu0;</span><br><span class="line">sigma = <span class="number">1</span>;    <span class="comment">% parameter for gaussian kernel function</span></span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:<span class="number">20</span>    </span><br><span class="line">    fprintf(<span class="string">'iter = %d, mu = [%f, %f]\n'</span>, iter, mu(<span class="number">1</span>), mu(<span class="number">2</span>));</span><br><span class="line">    <span class="built_in">scatter</span>(mu(<span class="number">1</span>), mu(<span class="number">2</span>), <span class="number">50</span>, [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>], <span class="string">'d'</span>, <span class="string">'filled'</span>);</span><br><span class="line">    offset = <span class="built_in">bsxfun</span>(@minus, mu, data);    <span class="comment">% offset = x-x_i</span></span><br><span class="line">    dis = sum(offset.^<span class="number">2</span>, <span class="number">2</span>);              <span class="comment">% dis = ||x-x_i||^2</span></span><br><span class="line">    x = data(dis &lt; h, :);                 <span class="comment">% neighborhood with bandwidth = h</span></span><br><span class="line">    g = gaussian_kernel(offset(dis &lt; h), sigma);</span><br><span class="line">    xg = x.*g;</span><br><span class="line">    mu_prev = mu;</span><br><span class="line">    mu = sum(xg, <span class="number">1</span>) / sum(g, <span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> norm(mu_prev - mu, <span class="number">2</span>) &lt; <span class="number">1E-2</span></span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    <span class="built_in">plot</span>([mu_prev(<span class="number">1</span>) mu(<span class="number">1</span>)], [mu_prev(<span class="number">2</span>), mu(<span class="number">2</span>)], <span class="string">'b-.'</span>, <span class="string">'linewidth'</span>, <span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="built_in">scatter</span>(mu(<span class="number">1</span>), mu(<span class="number">2</span>), <span class="number">50</span>, [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>], <span class="string">'d'</span>, <span class="string">'filled'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p><img src="/img/meanshift_simple_demo.png" alt></p>
<p>同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。</p>
<script type="math/tex; mode=display">K(x) = \frac{1}{e^x+e^{-x}+2}</script><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">out</span> = <span class="title">logistic_kernel</span><span class="params">(x)</span></span></span><br><span class="line">out = <span class="number">1.</span>/(<span class="built_in">exp</span>(x) + <span class="built_in">exp</span>(-x) + <span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>在Ubuntu14.04构建Caffe</title>
    <url>/2017/02/09/build-caffe-ubuntu/</url>
    <content><![CDATA[<p>Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。</p>
<p><img src="/img/caffe_image.jpg" alt="caffe"></p>
<a id="more"></a>
<h2 id="修改Makefile-config"><a href="#修改Makefile-config" class="headerlink" title="修改Makefile.config"></a>修改Makefile.config</h2><p>当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment">## Refer to http://caffe.berkeleyvision.org/installation.html</span></span><br><span class="line"><span class="comment"># Contributions simplifying and improving our build system are welcome!</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># cuDNN acceleration switch (uncomment to build with cuDNN).</span></span><br><span class="line">USE_CUDNN := 1    <span class="comment"># 这里我们使用cudnn加速</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CPU-only switch (uncomment to build without GPU support).</span></span><br><span class="line"><span class="comment"># CPU_ONLY := 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># uncomment to disable IO dependencies and corresponding data layers</span></span><br><span class="line"><span class="comment"># USE_OPENCV := 0</span></span><br><span class="line"><span class="comment"># USE_LEVELDB := 0</span></span><br><span class="line"><span class="comment"># USE_LMDB := 0</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)</span></span><br><span class="line"><span class="comment">#	You should not set this flag if you will be reading LMDBs with any</span></span><br><span class="line"><span class="comment">#	possibility of simultaneous read and write</span></span><br><span class="line"><span class="comment"># ALLOW_LMDB_NOLOCK := 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment if you're using OpenCV 3</span></span><br><span class="line"><span class="comment"># OPENCV_VERSION := 3</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># To customize your choice of compiler, uncomment and set the following.</span></span><br><span class="line"><span class="comment"># N.B. the default for Linux is g++ and the default for OSX is clang++</span></span><br><span class="line"><span class="comment"># CUSTOM_CXX := g++</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CUDA directory contains bin/ and lib/ directories that we need.</span></span><br><span class="line">CUDA_DIR := /usr/<span class="built_in">local</span>/cuda</span><br><span class="line"><span class="comment"># On Ubuntu 14.04, if cuda tools are installed via</span></span><br><span class="line"><span class="comment"># "sudo apt-get install nvidia-cuda-toolkit" then use this instead:</span></span><br><span class="line"><span class="comment"># CUDA_DIR := /usr</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># CUDA architecture setting: going with all of them.</span></span><br><span class="line"><span class="comment"># For CUDA &lt; 6.0, comment the *_50 lines for compatibility.</span></span><br><span class="line"><span class="comment"># 这里可以去掉sm_20和21，因为实在是已经太老了</span></span><br><span class="line"><span class="comment"># 如果保留的话，编译时nvcc会给出警告</span></span><br><span class="line">CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \</span><br><span class="line">		-gencode arch=compute_35,code=sm_35 \</span><br><span class="line">		-gencode arch=compute_50,code=sm_50 \</span><br><span class="line">		-gencode arch=compute_50,code=compute_50</span><br><span class="line"></span><br><span class="line"><span class="comment"># BLAS choice:</span></span><br><span class="line"><span class="comment"># atlas for ATLAS (default)</span></span><br><span class="line"><span class="comment"># mkl for MKL</span></span><br><span class="line"><span class="comment"># open for OpenBlas</span></span><br><span class="line">BLAS := atlas</span><br><span class="line"><span class="comment"># Custom (MKL/ATLAS/OpenBLAS) include and lib directories.</span></span><br><span class="line"><span class="comment"># Leave commented to accept the defaults for your choice of BLAS</span></span><br><span class="line"><span class="comment"># (which should work)!</span></span><br><span class="line"><span class="comment"># BLAS_INCLUDE := /path/to/your/blas</span></span><br><span class="line"><span class="comment"># BLAS_LIB := /path/to/your/blas</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Homebrew puts openblas in a directory that is not on the standard search path</span></span><br><span class="line"><span class="comment"># BLAS_INCLUDE := $(shell brew --prefix openblas)/include</span></span><br><span class="line"><span class="comment"># BLAS_LIB := $(shell brew --prefix openblas)/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># This is required only if you will compile the matlab interface.</span></span><br><span class="line"><span class="comment"># MATLAB directory should contain the mex binary in /bin.</span></span><br><span class="line"><span class="comment"># MATLAB_DIR := /usr/local</span></span><br><span class="line"><span class="comment"># MATLAB_DIR := /Applications/MATLAB_R2012b.app</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># <span class="doctag">NOTE:</span> this is required only if you will compile the python interface.</span></span><br><span class="line"><span class="comment"># We need to be able to find Python.h and numpy/arrayobject.h.</span></span><br><span class="line">PYTHON_INCLUDE := /usr/include/python2.7 \</span><br><span class="line">		/usr/lib/python2.7/dist-packages/numpy/core/include</span><br><span class="line"><span class="comment"># Anaconda Python distribution is quite popular. Include path:</span></span><br><span class="line"><span class="comment"># Verify anaconda location, sometimes it's in root.</span></span><br><span class="line"><span class="comment"># 这里我们使用Anaconda</span></span><br><span class="line">ANACONDA_HOME := $(HOME)/anaconda2</span><br><span class="line"> PYTHON_INCLUDE := $(ANACONDA_HOME)/include \</span><br><span class="line">		 $(ANACONDA_HOME)/include/python2.7 \</span><br><span class="line">		 $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to use Python 3 (default is Python 2)</span></span><br><span class="line"><span class="comment"># PYTHON_LIBRARIES := boost_python3 python3.5m</span></span><br><span class="line"><span class="comment"># PYTHON_INCLUDE := /usr/include/python3.5m \</span></span><br><span class="line"><span class="comment">#                 /usr/lib/python3.5/dist-packages/numpy/core/include</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># We need to be able to find libpythonX.X.so or .dylib.</span></span><br><span class="line"><span class="comment">#PYTHON_LIB := /usr/lib</span></span><br><span class="line"> PYTHON_LIB := $(ANACONDA_HOME)/lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># Homebrew installs numpy in a non standard path (keg only)</span></span><br><span class="line"><span class="comment"># PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include</span></span><br><span class="line"><span class="comment"># PYTHON_LIB += $(shell brew --prefix numpy)/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to support layers written in Python (will link against Python libs)</span></span><br><span class="line">WITH_PYTHON_LAYER := 1</span><br><span class="line"></span><br><span class="line"><span class="comment"># Whatever else you find you need goes here.</span></span><br><span class="line">INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/<span class="built_in">local</span>/include</span><br><span class="line">LIBRARY_DIRS := $(PYTHON_LIB) /usr/<span class="built_in">local</span>/lib /usr/lib</span><br><span class="line"></span><br><span class="line"><span class="comment"># If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies</span></span><br><span class="line"><span class="comment"># INCLUDE_DIRS += $(shell brew --prefix)/include</span></span><br><span class="line"><span class="comment"># LIBRARY_DIRS += $(shell brew --prefix)/lib</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># NCCL acceleration switch (uncomment to build with NCCL)</span></span><br><span class="line"><span class="comment"># https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)</span></span><br><span class="line"><span class="comment"># USE_NCCL := 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment to use `pkg-config` to specify OpenCV library paths.</span></span><br><span class="line"><span class="comment"># (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)</span></span><br><span class="line"><span class="comment"># USE_PKG_CONFIG := 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># N.B. both build and distribute dirs are cleared on `make clean`</span></span><br><span class="line">BUILD_DIR := build</span><br><span class="line">DISTRIBUTE_DIR := distribute</span><br><span class="line"></span><br><span class="line"><span class="comment"># Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171</span></span><br><span class="line"><span class="comment"># DEBUG := 1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># The ID of the GPU that 'make runtest' will use to run unit tests.</span></span><br><span class="line">TEST_GPUID := 0</span><br><span class="line"></span><br><span class="line"><span class="comment"># enable pretty build (comment to see full commands)</span></span><br><span class="line">Q ?= @</span><br></pre></td></tr></table></figure>
<p>对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将<code>include</code>中的头文件放于<code>/usr/local/cuda-8.0/include</code>下，将<code>lib</code>中的库文件放于<code>/usr/loca/cuda-8.0/lib64</code>文件夹下即可。</p>
<h2 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h2><p>使用<code>make -j8</code>进行编译，并使用<code>make pycaffe</code>生成python接口。并在<code>.bashrc</code>中添加内容：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH</span><br></pre></td></tr></table></figure></p>
<p>结果在<code>import caffe</code>时出现问题如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure></p>
<p>解决方法如下，详见GitHub issue<a href="https://github.com/NVIDIA/DIGITS/issues/8" target="_blank" rel="noopener">讨论</a>。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo ldconfig /usr/local/cuda/lib64</span><br></pre></td></tr></table></figure></p>
<p>然而仍有问题，如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ImportError: No module named google.protobuf.internal</span><br></pre></td></tr></table></figure></p>
<p>解决方法如下，详见G+ caffe-user group的<a href="https://groups.google.com/forum/#!topic/caffe-users/9Q10WkpCGxs" target="_blank" rel="noopener">帖子</a>。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install protobuf</span><br></pre></td></tr></table></figure></p>
<p>不过仍然存在的问题是远程SSH登录时，不能在<code>ipython</code>环境下导入caffe，不知为何。</p>
<p>使用<code>make test; make runtest</code>进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory</span><br></pre></td></tr></table></figure>
<p>解决方法为手动添加符号链接，详见GitHub<a href="https://github.com/BVLC/caffe/issues/1463" target="_blank" rel="noopener">讨论帖</a>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd /usr/lib/x86_64-linux-gnu</span><br><span class="line">sudo ln -s libhdf5.so.7 libhdf5.so.10</span><br><span class="line">sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10</span><br></pre></td></tr></table></figure>
<p>另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录<code>/opt/intel/mkl</code>下。使用<code>sudo</code>权限，在目录<code>/etc/ld.so.conf.d/</code>下建立一个名为<code>intel_mkl_setttings.conf</code>的文件，将MKL安装位置下的链接库目录添加进去，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/opt/intel/mkl/lib/intel64_lin/</span><br></pre></td></tr></table></figure></p>
<p>接着，运行<code>sudo ldconfig</code>命令，就可以了。</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>首先，通过<code>make runtest</code>看是否全部test可以通过。其次，可以试运行<code>example</code>下的LeNet训练。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd $CAFFE_ROOT</span><br><span class="line">./data/mnist/get_mnist.sh</span><br><span class="line">./examples/mnist/create_mnist.sh</span><br><span class="line">./examples/mnist/train_lenet.sh</span><br></pre></td></tr></table></figure></p>
]]></content>
      <tags>
        <tag>caffe</tag>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙</title>
    <url>/2017/02/08/digitalocean-shadowsocks/</url>
    <content><![CDATA[<p>之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。</p>
<p>之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。<br><img src="/img/god_use_vpn.png" alt="佛跳墙"></p>
<a id="more"></a>
<h2 id="申请机器"><a href="#申请机器" class="headerlink" title="申请机器"></a>申请机器</h2><p>在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。</p>
<p>申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。</p>
<h2 id="安装ss"><a href="#安装ss" class="headerlink" title="安装ss"></a>安装ss</h2><p>远程登录后，我们需要安装ss。安装命令很简单。<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">apt-get install python-pip</span><br><span class="line">pip install shadowsocks</span><br></pre></td></tr></table></figure></p>
<p>然而，在安装时，我遇到了一个奇怪的问题，提示我<code>unsupported locale setting</code>，后来搜索得知，是语言配置的问题，见<a href="http://www.linfuyan.com/locale_error_unsupported_locale_setting/" target="_blank" rel="noopener">这篇博文</a>，解决办法如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="built_in">export</span> LC_ALL=C</span><br></pre></td></tr></table></figure></p>
<h2 id="编辑配置文件"><a href="#编辑配置文件" class="headerlink" title="编辑配置文件"></a>编辑配置文件</h2><p>之后，进入<code>/etc</code>目录，建立一个名叫<code>shadowsocks.json</code>的文件（文件名任意，一会对应即可），文件配置内容如下：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">&quot;server&quot;:&quot;::&quot;,  </span><br><span class="line">&quot;server_port&quot;:8388,</span><br><span class="line">&quot;local_address&quot;: &quot;127.0.0.1&quot;,</span><br><span class="line">&quot;local_port&quot;: 1080,</span><br><span class="line">&quot;password&quot;:&quot;your_password（任写）&quot;,</span><br><span class="line">&quot;timeout&quot;:600,</span><br><span class="line">&quot;method&quot;:&quot;aes-256-cfb&quot;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>其中第一行写成<code>::</code>即是为了IPV6连接。</p>
<h2 id="编辑启动项，设置自动启动"><a href="#编辑启动项，设置自动启动" class="headerlink" title="编辑启动项，设置自动启动"></a>编辑启动项，设置自动启动</h2><p>之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。</p>
<p>编辑<code>/etc/rc.local</code>文件，在<code>exit 0</code>之前添加如下命令。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssserver -c /etc/shadowsocks.json -d start  # 这里的json文件名要相对应</span><br></pre></td></tr></table></figure></p>
<p>之后，使用<code>reboot</code>命令重启即可。</p>
<h2 id="客户端配置"><a href="#客户端配置" class="headerlink" title="客户端配置"></a>客户端配置</h2><p>客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。</p>
<h2 id="IOS平台设置"><a href="#IOS平台设置" class="headerlink" title="IOS平台设置"></a>IOS平台设置</h2><p>终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。</p>
<ul>
<li><a href="https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/README-zh.md" target="_blank" rel="noopener">IPsec VPN 服务器一键安装脚本</a></li>
<li><a href="https://github.com/hwdsl2/setup-ipsec-vpn/blob/master/docs/clients-zh.md" target="_blank" rel="noopener">配置 IPsec/L2TP VPN 客户端</a></li>
</ul>
<p>首先，使用如下命令在VPN服务器上搭建IPsec服务：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wget https://git.io/vpnsetup -O vpnsetup.sh &amp;&amp; sudo sh vpnsetup.sh</span><br></pre></td></tr></table></figure>
<p>然后按照下面的步骤在IOS平台上进行设置。<br><img src="/img/ipsec_ios_vpn_setting.png" alt></p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-KMeans聚类</title>
    <url>/2017/02/05/cs131-kmeans/</url>
    <content><![CDATA[<p><a href="https://zh.wikipedia.org/wiki/K-平均算法" target="_blank" rel="noopener">K-Means聚类</a>是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。</p>
<script type="math/tex; mode=display">\text{SSD} = \sum_{i=1}^{k}\sum_{x\in c_i}(x-c_i)^2</script><p><img src="/img/kmeans_demo.png" alt="K-Means Demo"></p>
<a id="more"></a>
<h2 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h2><p>K-Means方法实际上需要确定两个参数，$c^\ast$和$\delta^\ast$。其中$c_{i}^\ast$代表各个聚类中心的位置，$\delta_{ij}^\ast$的取值为$\lbrace 0,1\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。</p>
<p>那么，目标函数可以写成如下的形式。</p>
<script type="math/tex; mode=display">c^\ast, \delta^\ast = \arg\min_{c,\delta} \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{k}\delta_{i,j}(c_i-x_j)^2</script><p>然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。</p>
<p>实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。</p>
<h2 id="算法流程"><a href="#算法流程" class="headerlink" title="算法流程"></a>算法流程</h2><p>K-Means算法的流程如下所示。<br><img src="/img/kmeans_algorithm.png" alt="K-Means算法流程"></p>
<p>假设我们有$N$个样本点，$\lbrace x_1, \dots, x_N\rbrace, x_i\in\mathbb{R}^D$，并给出聚类数目$k$。</p>
<p>首先，随机选取一系列的聚类中心点$\mu_i, i = 1,\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。</p>
<h2 id="算法细节"><a href="#算法细节" class="headerlink" title="算法细节"></a>算法细节</h2><h3 id="初始化"><a href="#初始化" class="headerlink" title="初始化"></a>初始化</h3><p>上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。</p>
<ul>
<li><p>kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。<br>这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \omega(x-c_i)^2$选取其他的聚类中心点。其中$\omega$是归一化系数。</p>
</li>
<li><p>多次初始化，保留最好的结果。</p>
</li>
</ul>
<h3 id="K值的选取"><a href="#K值的选取" class="headerlink" title="K值的选取"></a>K值的选取</h3><p>在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？</p>
<p>我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。<br><img src="/img/kmeans_object_fun_vs_k.png" alt="参数K的确定"></p>
<h3 id="距离的度量"><a href="#距离的度量" class="headerlink" title="距离的度量"></a>距离的度量</h3><p>目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。</p>
<ul>
<li>欧几里得距离（最为常用）</li>
<li>余弦距离（向量的夹角）</li>
<li>核函数（<a href="http://www.public.asu.edu/~jye02/CLASSES/Fall-2005/PAPERS/kdd_spectral_kernelkmeans.pdf" target="_blank" rel="noopener">Kernel K-Means</a>）</li>
</ul>
<h3 id="迭代终止条件"><a href="#迭代终止条件" class="headerlink" title="迭代终止条件"></a>迭代终止条件</h3><p>当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下：</p>
<ul>
<li>达到了预先给定的最大迭代次数</li>
<li>在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛）</li>
<li>目标函数（平均的距离）下降小于阈值</li>
</ul>
<h2 id="基于K-Means的图像分割"><a href="#基于K-Means的图像分割" class="headerlink" title="基于K-Means的图像分割"></a>基于K-Means的图像分割</h2><p>图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。<br><img src="/img/kmeans_image_seg_via_intensity.png" alt="图像分割结果1"></p>
<p>然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。</p>
<p>在2012年PAMI上有一篇文章<a href="https://infoscience.epfl.ch/record/177415/files/Superpixel_PAMI2011-2.pdf" target="_blank" rel="noopener">SLIC Superpixels Compared to State-of-the-art Superpixel Methods</a>介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。</p>
<h2 id="优点和不足"><a href="#优点和不足" class="headerlink" title="优点和不足"></a>优点和不足</h2><p>作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。</p>
<p>它的缺点主要有：</p>
<ul>
<li>对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。<br><img src="/img/kmeans_sensitive_to_outlier.png" alt="outlier是个大麻烦"></li>
<li>每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。</li>
<li>在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。</li>
<li>如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。</li>
</ul>
<p>针对K-Means，也有不少相关改进工作，参考下面这幅图吧。<br><img src="/img/kmeans_scaling_up.png" alt="K-Means Scaling Up"></p>
<h2 id="MATLAB实验"><a href="#MATLAB实验" class="headerlink" title="MATLAB实验"></a>MATLAB实验</h2><p>下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用<code>scatter</code>函数做出散点图。</p>
<p>代码中的主要部分为<code>my_kmeans</code>函数的实现（为了不与内建的kmeans函数重名，故加上了<code>my</code>前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。</p>
<p>注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% generate data</span></span><br><span class="line">K = <span class="number">3</span>;   <span class="comment">% number of clusters</span></span><br><span class="line">pos = [<span class="number">-5</span>, <span class="number">5</span>; <span class="number">0</span>, <span class="number">1</span>; <span class="number">3</span>, <span class="number">6</span>];  <span class="comment">% position of cluster centers</span></span><br><span class="line">N = <span class="number">20</span>;    <span class="comment">% number of data points</span></span><br><span class="line">R = <span class="number">3</span>;     <span class="comment">% radius of clusters</span></span><br><span class="line">data = <span class="built_in">zeros</span>(N, <span class="number">2</span>);    <span class="comment">% data</span></span><br><span class="line">class = <span class="built_in">zeros</span>(N, <span class="number">1</span>);   <span class="comment">% index of cluster</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:N</span><br><span class="line">    idx = randi(<span class="number">3</span>, <span class="number">1</span>);</span><br><span class="line">    dr = R*<span class="built_in">rand</span>();</span><br><span class="line">    data(<span class="built_in">i</span>, :) = pos(idx, :) + [dr*<span class="built_in">cos</span>(<span class="built_in">rand</span>()*<span class="number">2</span>*<span class="built_in">pi</span>), dr*<span class="built_in">sin</span>(<span class="built_in">rand</span>()*<span class="number">2</span>*<span class="built_in">pi</span>)];</span><br><span class="line">    class(<span class="built_in">i</span>) = idx;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% visualization data points</span></span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">color = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>; <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>];</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:K</span><br><span class="line">    x = data(class == <span class="built_in">i</span>, <span class="number">1</span>);</span><br><span class="line">    y = data(class == <span class="built_in">i</span>, <span class="number">2</span>);</span><br><span class="line">    <span class="built_in">scatter</span>(x, y, <span class="number">150</span>, <span class="built_in">repmat</span>(color(<span class="built_in">i</span>,:), [<span class="built_in">length</span>(x), <span class="number">1</span>]), <span class="string">'filled'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="comment">%% K-Means</span></span><br><span class="line">best_J = <span class="number">1E100</span>;</span><br><span class="line">best_idx = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> times = <span class="number">1</span>:<span class="number">5</span>  <span class="comment">% 5 times experiments to choose the best result</span></span><br><span class="line">    [mu, assignment, J] = my_kmeans(data, K);</span><br><span class="line">    <span class="keyword">if</span> best_J &gt; J</span><br><span class="line">        best_idx = times;</span><br><span class="line">        best_J = J;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    fprintf(<span class="string">'%d experiment: J = %f\n'</span>, times, J);</span><br><span class="line">    <span class="built_in">disp</span>(mu);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">fprintf(<span class="string">'best: %d experiment: J = %f\n'</span>, best_idx, best_J);</span><br><span class="line"></span><br><span class="line"><span class="comment">%% basic functions</span></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">J</span> = <span class="title">ssd</span><span class="params">(X, mu, assignment)</span></span></span><br><span class="line"><span class="comment">% sum of square distance</span></span><br><span class="line"><span class="comment">% X -- data, N*D matrix</span></span><br><span class="line"><span class="comment">% mu -- centers of clusters, K*D matrix</span></span><br><span class="line"><span class="comment">% assignment -- current assignment of data to clusters</span></span><br><span class="line">J = <span class="number">0</span>;</span><br><span class="line">K = <span class="built_in">size</span>(mu, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:K</span><br><span class="line">    x_k = X(assignment == k, :);</span><br><span class="line">    mu_k = mu(k, :);</span><br><span class="line">    err2 = <span class="built_in">bsxfun</span>(@minus, x_k, mu_k).^<span class="number">2</span>;</span><br><span class="line">    J = J + sum(err2(:));</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">J = J / <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">mu</span> = <span class="title">compute_mu</span><span class="params">(X, assignment, K)</span></span></span><br><span class="line">mu = <span class="built_in">zeros</span>(K, <span class="built_in">size</span>(X, <span class="number">2</span>));</span><br><span class="line"><span class="keyword">for</span> k = <span class="number">1</span>:K</span><br><span class="line">    x_k = X(assignment == k, :);</span><br><span class="line">    mu(k, :) = <span class="built_in">mean</span>(x_k, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">assignment</span> = <span class="title">assign</span><span class="params">(X, mu)</span></span></span><br><span class="line"><span class="comment">% assign data points to clusters</span></span><br><span class="line">N = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">assignment = <span class="built_in">zeros</span>(N, <span class="number">1</span>);</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:N</span><br><span class="line">    x = X(<span class="built_in">i</span>, :);</span><br><span class="line">    err2 = <span class="built_in">bsxfun</span>(@minus, x, mu).^<span class="number">2</span>;</span><br><span class="line">    dis = sum(err2, <span class="number">2</span>);</span><br><span class="line">    [~, idx] = <span class="built_in">min</span>(dis);</span><br><span class="line">    assignment(<span class="built_in">i</span>) = idx;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[mu, assignment, J]</span> = <span class="title">my_kmeans</span><span class="params">(X, K)</span></span></span><br><span class="line">N = <span class="built_in">size</span>(X, <span class="number">1</span>);</span><br><span class="line">assignment = <span class="built_in">zeros</span>(N, <span class="number">1</span>);</span><br><span class="line">idx = randsample(N, K);</span><br><span class="line">mu = X(idx, :);</span><br><span class="line"></span><br><span class="line"><span class="comment">% for i = 1:K</span></span><br><span class="line"><span class="comment">%     for j = 1:N</span></span><br><span class="line"><span class="comment">%         if assignment_gt(j) == i</span></span><br><span class="line"><span class="comment">%             mu(i,:) = X(j,:);</span></span><br><span class="line"><span class="comment">%             break;</span></span><br><span class="line"><span class="comment">%         end</span></span><br><span class="line"><span class="comment">%     end</span></span><br><span class="line"><span class="comment">% end</span></span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">color = [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>; <span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>; <span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>];</span><br><span class="line"><span class="built_in">scatter</span>(mu(:,<span class="number">1</span>), mu(:,<span class="number">2</span>), <span class="number">200</span>, color, <span class="string">'d'</span>);</span><br><span class="line"><span class="keyword">for</span> iter = <span class="number">1</span>:<span class="number">20</span></span><br><span class="line">    assignment_prev = assignment;</span><br><span class="line">    assignment = assign(X, mu);</span><br><span class="line">    <span class="keyword">if</span> assignment == assignment_prev</span><br><span class="line">        <span class="keyword">break</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    mu_prev = mu;</span><br><span class="line">    mu = compute_mu(X, assignment, K);</span><br><span class="line">    <span class="built_in">scatter</span>(mu(:, <span class="number">1</span>), mu(:, <span class="number">2</span>), <span class="number">200</span>, color, <span class="string">'d'</span>);</span><br><span class="line">    MU = <span class="built_in">zeros</span>(<span class="number">2</span>*K, <span class="number">2</span>);</span><br><span class="line">    MU(<span class="number">1</span>:<span class="number">2</span>:<span class="keyword">end</span>, :) = mu_prev;</span><br><span class="line">    MU(<span class="number">2</span>:<span class="number">2</span>:<span class="keyword">end</span>, :) = mu;</span><br><span class="line">    mu_x = <span class="built_in">reshape</span>(MU(:, <span class="number">1</span>), [], K);</span><br><span class="line">    mu_y = <span class="built_in">reshape</span>(MU(:, <span class="number">2</span>), [], K);</span><br><span class="line">    <span class="built_in">plot</span>(mu_x, mu_y, <span class="string">'k-.'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:K</span><br><span class="line">    x = X(assignment == <span class="built_in">i</span>, <span class="number">1</span>);</span><br><span class="line">    y = X(assignment == <span class="built_in">i</span>, <span class="number">2</span>);</span><br><span class="line">    <span class="built_in">scatter</span>(x, y, <span class="number">150</span>, <span class="built_in">repmat</span>(color(<span class="built_in">i</span>,:), [<span class="built_in">length</span>(x), <span class="number">1</span>]), <span class="string">'filled'</span>);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">J = ssd(X, mu, assignment);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。<br><img src="/img/kmeans_data_demo.png" alt="K-Means聚类"></p>
<p>下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。</p>
<p><img src="/img/kmeans_success.png" alt="K-Means聚类"></p>
<p>再换个大点的数据集来做，效果貌似还不错~<br><img src="/img/kmeans_bigger_demo.png" alt="大一些"></p>
<h2 id="PS"><a href="#PS" class="headerlink" title="PS"></a>PS</h2><p>这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$c_&#123;i&#125;^\ast$ XXX $\delta_&#123;ij&#125;^\ast$</span><br></pre></td></tr></table></figure></p>
<p>它的显示效果为$c<em>{i}^\ast$ XXX $\delta</em>{ij}^\ast$。</p>
<p>这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$c\_&#123;i&#125;^\ast$ XXX $\delta\_&#123;ij&#125;^\ast$</span><br></pre></td></tr></table></figure></p>
<p>它的显示效果为$c_{i}^\ast$ XXX $\delta_{ij}^\ast$。</p>
<p>具体分析可以参见<a href="http://lukang.me/2014/mathjax-for-hexo.html" target="_blank" rel="noopener">博客</a>。</p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>B站视频“线性代数的本质”观后感</title>
    <url>/2017/02/05/video-linear-alg-essential-property/</url>
    <content><![CDATA[<p>线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：<a href="http://www.bilibili.com/video/av6731067/index_1.html" target="_blank" rel="noopener">线性代数的本质</a>。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。<br><img src="/img/video_linear_alg_essential.png" alt></p>
<a id="more"></a>
<h2 id="从线性空间和线性变换讲起"><a href="#从线性空间和线性变换讲起" class="headerlink" title="从线性空间和线性变换讲起"></a>从线性空间和线性变换讲起</h2><p>BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。</p>
<p>而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的<strong>线性变换</strong>。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件：</p>
<ul>
<li>变换前后原点不动</li>
<li>变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。</li>
</ul>
<h2 id="线性变换与矩阵的关系"><a href="#线性变换与矩阵的关系" class="headerlink" title="线性变换与矩阵的关系"></a>线性变换与矩阵的关系</h2><p>视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。</p>
<p>在由一组基向量$\alpha_i, i = 1,2,\dots,n$张成的线性空间$\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是</p>
<script type="math/tex; mode=display">v = \sum_{i=1}^{n}k_i\alpha_i</script><p>则线性变换$\mathcal{T}$对$v$作用之后，有，</p>
<script type="math/tex; mode=display">u = \mathcal{T}(v) = \mathcal{T}(\sum_{i=1}^{n}k_i\alpha_i)</script><p>根据线性变换的叠加性，有，</p>
<script type="math/tex; mode=display">u = \sum_{i=1}^{n}k_i\mathcal{T}(\alpha_i)</script><p>设$\alpha_i$经过线性变换$\mathcal{T}$作用后，变换为$\beta_i$，那么，</p>
<script type="math/tex; mode=display">u = \sum_{i=1}^{n}k_i\beta_i</script><p>也就是说，</p>
<script type="math/tex; mode=display">u = \begin{bmatrix}\mathcal{T}(\alpha_1), \mathcal{T}(\alpha_2), \cdots, \mathcal{T}(\alpha_n)\end{bmatrix}
\begin{bmatrix}k_1\\\\ k_2\\\\ \vdots\\\\ k_n\end{bmatrix}</script><p>上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。</p>
<p>举个例子，旋转变换。如果旋转$\frac{\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$和$(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为</p>
<script type="math/tex; mode=display">A = \begin{bmatrix}\frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}& \frac{\sqrt{2}}{2}\end{bmatrix}</script><p>矩阵$A$的两列分别为变换后的基向量坐标。</p>
<h2 id="矩阵乘法"><a href="#矩阵乘法" class="headerlink" title="矩阵乘法"></a>矩阵乘法</h2><p>那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \begin{bmatrix}-1\\ 0\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以，</p>
<script type="math/tex; mode=display">Ax = -1\begin{bmatrix}\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}\end{bmatrix} + 0\begin{bmatrix}-\frac{\sqrt{2}}{2}\\\\\frac{\sqrt{2}}{2}\end{bmatrix}</script><p>而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。</p>
<p>所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。</p>
<p>而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。</p>
<p>矩阵的秩的意义就是矩阵列空间的维数。</p>
<p>同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示：<br><img src="/img/video_linear_alg_essential_linear_equation.png" alt="线性方程组与矩阵乘法"></p>
<p>既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。</p>
<h2 id="行列式"><a href="#行列式" class="headerlink" title="行列式"></a>行列式</h2><p>仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。</p>
<p>我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况：</p>
<ul>
<li>当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。</li>
<li>当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。</li>
</ul>
<p>你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。</p>
<h2 id="点积叉积和对偶性"><a href="#点积叉积和对偶性" class="headerlink" title="点积叉积和对偶性"></a>点积叉积和对偶性</h2><p>这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道，</p>
<script type="math/tex; mode=display">\langle v, u \rangle = \sum_{i=1}^{n}v_iu_i</script><p>从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。</p>
<p>按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\mathbb{R}^2$到$\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。</p>
]]></content>
      <tags>
        <tag>math</tag>
      </tags>
  </entry>
  <entry>
    <title>YOLO 论文阅读</title>
    <url>/2017/02/04/yolo-paper/</url>
    <content><![CDATA[<p>YOLO(<strong>Y</strong>ou <strong>O</strong>nly <strong>L</strong>ook <strong>O</strong>nce)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">YOLO V1</a>和<a href="https://arxiv.org/abs/1612.08242" target="_blank" rel="noopener">YOLO V2</a>。YOLO V2的代码目前作为<a href="http://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">Darknet</a>的一部分开源在<a href>GitHub</a>。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。</p>
<p>Update@2018/04: <a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="noopener">YOLO v3</a>已经发布！可以参考我的博客<a href="https://xmfbit.github.io/2018/04/01/paper-yolov3/">论文 - YOLO v3</a>。</p>
<p><img src="/img/yolo2_result.png" alt="YOLO V2的检测效果示意"><br><a id="more"></a></p>
<h2 id="YOLO-V1"><a href="#YOLO-V1" class="headerlink" title="YOLO V1"></a>YOLO V1</h2><p>这里不妨把YOLO V1论文<a href="https://arxiv.org/abs/1506.02640" target="_blank" rel="noopener">“You Only Look Once: Unitied, Real-Time Object Detection”</a>的摘要部分意译如下：</p>
<blockquote>
<p>我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。<br>YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。</p>
</blockquote>
<p>和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。<br><img src="/img/yolo1_detection_system.png" alt="YOLO V1检测系统示意图"></p>
<h3 id="基本思路"><a href="#基本思路" class="headerlink" title="基本思路"></a>基本思路</h3><p><img src="/img/yolo1_basic_idea.png" alt="基础思路示意图"></p>
<ul>
<li>网格划分：将输入image划分为$S \times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示：</li>
</ul>
<script type="math/tex; mode=display">\text{confidence} = P(\text{Object})\times \text{IoU}_{\text{pred}}^{\text{truth}}</script><ul>
<li><p>网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\text{Class}_i|\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。</p>
<script type="math/tex; mode=display">\text{confidence}\times P(\text{Class}_i|\text{Object}) = P(\text{Class}_i)\times \text{IoU}_{\text{pred}}^{\text{truth}}</script></li>
<li><p>实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\times 7 \times 30$</p>
</li>
</ul>
<h3 id="网络模型结构"><a href="#网络模型结构" class="headerlink" title="网络模型结构"></a>网络模型结构</h3><p>Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。<br><img src="/img/yolo1_network_arch.png" alt="YOLO的网络结构示意图"></p>
<p>另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。</p>
<h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。</p>
<p>由于<a href="https://arxiv.org/abs/1504.06066" target="_blank" rel="noopener">Ren的论文</a>提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\times 224$提升到了$448 \times 448$。</p>
<p>在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示：</p>
<script type="math/tex; mode=display">
f(x)=
\begin{cases}
x, &\text{if}\ x > 0 \\\\
0.1x, &\text{otherwise}
\end{cases}</script><p>很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明：</p>
<ul>
<li>loss的形式采用误差平方和的形式（真是把回归进行到底了。。。）</li>
<li>由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，<script type="math/tex; mode=display">\lambda_{\text{coord}} = 5，\lambda_{\text{noobj}} = 0.5</script></li>
<li>直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\sqrt{w}$和$\sqrt{h}$。</li>
<li>上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。</li>
</ul>
<p>loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。</p>
<p>$\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。<br><img src="/img/yolo1_loss_fun.png" alt="YOLO的损失函数定义"></p>
<p>在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中<a href="https://github.com/pjreddie/darknet/blob/master/src/detection_layer.c" target="_blank" rel="noopener">detection_layer.c</a>中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数），</p>
<figure class="highlight c"><table><tr><td class="code"><pre><span class="line"><span class="keyword">if</span>(state.train)&#123;</span><br><span class="line">    <span class="keyword">float</span> avg_iou = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> avg_cat = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> avg_allcat = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> avg_obj = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">float</span> avg_anyobj = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line">    *(l.cost) = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">int</span> size = l.inputs * l.batch;</span><br><span class="line">    <span class="built_in">memset</span>(l.delta, <span class="number">0</span>, size * <span class="keyword">sizeof</span>(<span class="keyword">float</span>));</span><br><span class="line">    <span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">int</span> index = b*l.inputs;</span><br><span class="line">        <span class="comment">// for each grid cell</span></span><br><span class="line">        <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; locations; ++i) &#123;   <span class="comment">// locations = S * S = 49</span></span><br><span class="line">            <span class="keyword">int</span> truth_index = (b*locations + i)*(<span class="number">1</span>+l.coords+l.classes);</span><br><span class="line">            <span class="keyword">int</span> is_obj = state.truth[truth_index];</span><br><span class="line">            <span class="comment">// for each bbox</span></span><br><span class="line">            <span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; l.n; ++j) &#123;     <span class="comment">// l.n = B = 2</span></span><br><span class="line">                <span class="keyword">int</span> p_index = index + locations*l.classes + i*l.n + j;</span><br><span class="line">                l.delta[p_index] = l.noobject_scale*(<span class="number">0</span> - l.output[p_index]);</span><br><span class="line">                <span class="comment">// 因为no obj对应的bbox很多，而responsible的只有一个</span></span><br><span class="line">                <span class="comment">// 这里统一加上，如果一会判断该bbox responsible for object，再把它减去</span></span><br><span class="line">                *(l.cost) += l.noobject_scale*<span class="built_in">pow</span>(l.output[p_index], <span class="number">2</span>);  </span><br><span class="line">                avg_anyobj += l.output[p_index];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> best_index = <span class="number">-1</span>;</span><br><span class="line">            <span class="keyword">float</span> best_iou = <span class="number">0</span>;</span><br><span class="line">            <span class="keyword">float</span> best_rmse = <span class="number">20</span>;</span><br><span class="line">            <span class="comment">// 该grid cell没有目标，直接返回</span></span><br><span class="line">            <span class="keyword">if</span> (!is_obj)&#123;</span><br><span class="line">                <span class="keyword">continue</span>;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="comment">// 否则，找出responsible的bounding box，计算其他几项的loss</span></span><br><span class="line">            <span class="keyword">int</span> class_index = index + i*l.classes;</span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; l.classes; ++j) &#123;</span><br><span class="line">                l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+<span class="number">1</span>+j] - l.output[class_index+j]);</span><br><span class="line">                *(l.cost) += l.class_scale * <span class="built_in">pow</span>(state.truth[truth_index+<span class="number">1</span>+j] - l.output[class_index+j], <span class="number">2</span>);</span><br><span class="line">                <span class="keyword">if</span>(state.truth[truth_index + <span class="number">1</span> + j]) avg_cat += l.output[class_index+j];</span><br><span class="line">                avg_allcat += l.output[class_index+j];</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            box truth = float_to_box(state.truth + truth_index + <span class="number">1</span> + l.classes);</span><br><span class="line">            truth.x /= l.side;</span><br><span class="line">            truth.y /= l.side;</span><br><span class="line">            <span class="comment">// 找到最好的IoU，对应的bbox是responsible的，记录其index</span></span><br><span class="line">            <span class="keyword">for</span>(j = <span class="number">0</span>; j &lt; l.n; ++j)&#123;</span><br><span class="line">                <span class="keyword">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords;</span><br><span class="line">                box out = float_to_box(l.output + box_index);</span><br><span class="line">                out.x /= l.side;</span><br><span class="line">                out.y /= l.side;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">if</span> (l.<span class="built_in">sqrt</span>)&#123;</span><br><span class="line">                    out.w = out.w*out.w;</span><br><span class="line">                    out.h = out.h*out.h;</span><br><span class="line">                &#125;</span><br><span class="line"></span><br><span class="line">                <span class="keyword">float</span> iou  = box_iou(out, truth);</span><br><span class="line">                <span class="comment">//iou = 0;</span></span><br><span class="line">                <span class="keyword">float</span> rmse = box_rmse(out, truth);</span><br><span class="line">                <span class="keyword">if</span>(best_iou &gt; <span class="number">0</span> || iou &gt; <span class="number">0</span>)&#123;</span><br><span class="line">                    <span class="keyword">if</span>(iou &gt; best_iou)&#123;</span><br><span class="line">                        best_iou = iou;</span><br><span class="line">                        best_index = j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    <span class="keyword">if</span>(rmse &lt; best_rmse)&#123;</span><br><span class="line">                        best_rmse = rmse;</span><br><span class="line">                        best_index = j;</span><br><span class="line">                    &#125;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(l.forced)&#123;</span><br><span class="line">                <span class="keyword">if</span>(truth.w*truth.h &lt; <span class="number">.1</span>)&#123;</span><br><span class="line">                    best_index = <span class="number">1</span>;</span><br><span class="line">                &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">                    best_index = <span class="number">0</span>;</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">if</span>(l.random &amp;&amp; *(state.net.seen) &lt; <span class="number">64000</span>)&#123;</span><br><span class="line">                best_index = rand()%l.n;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">int</span> box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords;</span><br><span class="line">            <span class="keyword">int</span> tbox_index = truth_index + <span class="number">1</span> + l.classes;</span><br><span class="line"></span><br><span class="line">            box out = float_to_box(l.output + box_index);</span><br><span class="line">            out.x /= l.side;</span><br><span class="line">            out.y /= l.side;</span><br><span class="line">            <span class="keyword">if</span> (l.<span class="built_in">sqrt</span>) &#123;</span><br><span class="line">                out.w = out.w*out.w;</span><br><span class="line">                out.h = out.h*out.h;</span><br><span class="line">            &#125;</span><br><span class="line">            <span class="keyword">float</span> iou  = box_iou(out, truth);</span><br><span class="line"></span><br><span class="line">            <span class="comment">//printf("%d,", best_index);</span></span><br><span class="line">            <span class="keyword">int</span> p_index = index + locations*l.classes + i*l.n + best_index;</span><br><span class="line">            *(l.cost) -= l.noobject_scale * <span class="built_in">pow</span>(l.output[p_index], <span class="number">2</span>);  <span class="comment">// 还记得我们曾经统一加过吗？这里需要减去了</span></span><br><span class="line">            *(l.cost) += l.object_scale * <span class="built_in">pow</span>(<span class="number">1</span>-l.output[p_index], <span class="number">2</span>);</span><br><span class="line">            avg_obj += l.output[p_index];</span><br><span class="line">            l.delta[p_index] = l.object_scale * (<span class="number">1.</span>-l.output[p_index]);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span>(l.rescore)&#123;</span><br><span class="line">                l.delta[p_index] = l.object_scale * (iou - l.output[p_index]);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            l.delta[box_index+<span class="number">0</span>] = l.coord_scale*(state.truth[tbox_index + <span class="number">0</span>] - l.output[box_index + <span class="number">0</span>]);</span><br><span class="line">            l.delta[box_index+<span class="number">1</span>] = l.coord_scale*(state.truth[tbox_index + <span class="number">1</span>] - l.output[box_index + <span class="number">1</span>]);</span><br><span class="line">            l.delta[box_index+<span class="number">2</span>] = l.coord_scale*(state.truth[tbox_index + <span class="number">2</span>] - l.output[box_index + <span class="number">2</span>]);</span><br><span class="line">            l.delta[box_index+<span class="number">3</span>] = l.coord_scale*(state.truth[tbox_index + <span class="number">3</span>] - l.output[box_index + <span class="number">3</span>]);</span><br><span class="line">            <span class="keyword">if</span>(l.<span class="built_in">sqrt</span>)&#123;</span><br><span class="line">                l.delta[box_index+<span class="number">2</span>] = l.coord_scale*(<span class="built_in">sqrt</span>(state.truth[tbox_index + <span class="number">2</span>]) - l.output[box_index + <span class="number">2</span>]);</span><br><span class="line">                l.delta[box_index+<span class="number">3</span>] = l.coord_scale*(<span class="built_in">sqrt</span>(state.truth[tbox_index + <span class="number">3</span>]) - l.output[box_index + <span class="number">3</span>]);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            *(l.cost) += <span class="built_in">pow</span>(<span class="number">1</span>-iou, <span class="number">2</span>);</span><br><span class="line">            avg_iou += iou;</span><br><span class="line">            ++count;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br></pre></td></tr></table></figure>
<h2 id="YOLO-V2"><a href="#YOLO-V2" class="headerlink" title="YOLO V2"></a>YOLO V2</h2><p>YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。</p>
<ul>
<li>受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中；</li>
<li>修改了网络结构，去掉了全连接层，改成了全卷积结构；</li>
<li>引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。</li>
</ul>
<p>下面，还是先把论文的摘要意译如下：</p>
<blockquote>
<p>我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。</p>
</blockquote>
<p>根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。</p>
<h2 id="Better"><a href="#Better" class="headerlink" title="Better"></a>Better</h2><p>在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。</p>
<h3 id="改进1：引入BN层（Batch-Normalization）"><a href="#改进1：引入BN层（Batch-Normalization）" class="headerlink" title="改进1：引入BN层（Batch Normalization）"></a>改进1：引入BN层（Batch Normalization）</h3><p>Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。</p>
<h3 id="改进2：高分辨率分类器（High-Resolution-Classifier）"><a href="#改进2：高分辨率分类器（High-Resolution-Classifier）" class="headerlink" title="改进2：高分辨率分类器（High Resolution Classifier）"></a>改进2：高分辨率分类器（High Resolution Classifier）</h3><p>YOLO V1首先在ImageNet上以$224\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。</p>
<h3 id="改进3：引入Anchor-Box"><a href="#改进3：引入Anchor-Box" class="headerlink" title="改进3：引入Anchor Box"></a>改进3：引入Anchor Box</h3><p>YOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。</p>
<p>作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。</p>
<p>与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。</p>
<p>使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。</p>
<h3 id="改进4：Dimension-Cluster"><a href="#改进4：Dimension-Cluster" class="headerlink" title="改进4：Dimension Cluster"></a>改进4：Dimension Cluster</h3><p>在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。</p>
<p>这里对作者使用的方法不再过多赘述，强调以下两点：</p>
<ul>
<li>作者使用的聚类方法是K-Means；</li>
<li>相似性度量不用欧氏距离，而是用IoU，定义如下：<script type="math/tex; mode=display">d(\text{box}, \text{centroid}) = 1-\text{IoU}(\text{box}, \text{centroid})</script></li>
</ul>
<p>使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。<br><img src="/img/yolo2_cluster_result.png" alt></p>
<h3 id="改进5：直接位置预测（Direct-Location-Prediction）"><a href="#改进5：直接位置预测（Direct-Location-Prediction）" class="headerlink" title="改进5：直接位置预测（Direct Location Prediction）"></a>改进5：直接位置预测（Direct Location Prediction）</h3><p>我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。</p>
<p>在output的feature map上，对于每个cell（共计$13\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。</p>
<p><img src="/img/yolo2_bbox_location.png" alt="确定bbox的位置"></p>
<p>设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。<br><img src="/img/yolo2_bbox_param.png" alt="bounding box参数的计算方法"></p>
<p>Darknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// get bounding box</span></span><br><span class="line"><span class="comment">// x: data pointer of feature map</span></span><br><span class="line"><span class="comment">// biases: data pointer of anchor box data</span></span><br><span class="line"><span class="comment">// biases[2*n] = width of anchor box</span></span><br><span class="line"><span class="comment">// biases[2*n+1] = height of anchor box</span></span><br><span class="line"><span class="comment">// n: output bounding box for each cell in the feature map</span></span><br><span class="line"><span class="comment">// index: output bounding box index in the cell</span></span><br><span class="line"><span class="comment">// i: `cx` in the paper</span></span><br><span class="line"><span class="comment">// j: 'cy' in the paper</span></span><br><span class="line"><span class="comment">// (cx, cy) is the offset from the left top corner of the feature map</span></span><br><span class="line"><span class="comment">// (w, h) is the size of feature map (do normalization in the code)</span></span><br><span class="line"><span class="function">box <span class="title">get_region_box</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *biases, <span class="keyword">int</span> n, <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> w, <span class="keyword">int</span> h)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    box b;</span><br><span class="line">    <span class="comment">// i &lt;- cx, j &lt;- cy</span></span><br><span class="line">    <span class="comment">// index + 0: tx</span></span><br><span class="line">    <span class="comment">// index + 1: ty</span></span><br><span class="line">    <span class="comment">// index + 2: tw</span></span><br><span class="line">    <span class="comment">// index + 3: th</span></span><br><span class="line">    <span class="comment">// index + 4: to   // not used here</span></span><br><span class="line">    <span class="comment">// index + 5, +6, ..., +(C+4)   // confidence of P(class c|Object), not used here</span></span><br><span class="line">    b.x = (i + logistic_activate(x[index + <span class="number">0</span>])) / w;    <span class="comment">// bx = cx+sigmoid(tx)</span></span><br><span class="line">    b.y = (j + logistic_activate(x[index + <span class="number">1</span>])) / h;    <span class="comment">// by = cy+sigmoid(ty)</span></span><br><span class="line">    b.w = <span class="built_in">exp</span>(x[index + <span class="number">2</span>]) * biases[<span class="number">2</span>*n]   / w;        <span class="comment">// bw = exp(tw) * pw</span></span><br><span class="line">    b.h = <span class="built_in">exp</span>(x[index + <span class="number">3</span>]) * biases[<span class="number">2</span>*n+<span class="number">1</span>] / h;        <span class="comment">// bh = exp(th) * ph</span></span><br><span class="line">    <span class="comment">// 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到</span></span><br><span class="line">    <span class="comment">// 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值</span></span><br><span class="line">    <span class="keyword">return</span> b;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>顺便说一下对bounding box的bp实现。具体代码如下：</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// truth: ground truth</span></span><br><span class="line"><span class="comment">// x: data pointer of feature map</span></span><br><span class="line"><span class="comment">// biases: data pointer of anchor box data</span></span><br><span class="line"><span class="comment">// n, index, i, j, w, h: same meaning with `get_region_box`</span></span><br><span class="line"><span class="comment">// delta: data pointer of gradient</span></span><br><span class="line"><span class="comment">// scale: just a weight, given by user</span></span><br><span class="line"><span class="function"><span class="keyword">float</span> <span class="title">delta_region_box</span><span class="params">(box truth, <span class="keyword">float</span> *x, <span class="keyword">float</span> *biases,</span></span></span><br><span class="line"><span class="function"><span class="params">                       <span class="keyword">int</span> n, <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> w, <span class="keyword">int</span> h,</span></span></span><br><span class="line"><span class="function"><span class="params">                       <span class="keyword">float</span> *delta, <span class="keyword">float</span> scale)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    box pred = get_region_box(x, biases, n, index, i, j, w, h);</span><br><span class="line">    <span class="comment">// get iou of the bbox and truth</span></span><br><span class="line">    <span class="keyword">float</span> iou = box_iou(pred, truth);</span><br><span class="line">    <span class="comment">// ground truth of the parameters (tx, ty, tw, th)</span></span><br><span class="line">    <span class="keyword">float</span> tx = (truth.x*w - i);</span><br><span class="line">    <span class="keyword">float</span> ty = (truth.y*h - j);</span><br><span class="line">    <span class="keyword">float</span> tw = <span class="built_in">log</span>(truth.w*w / biases[<span class="number">2</span>*n]);</span><br><span class="line">    <span class="keyword">float</span> th = <span class="built_in">log</span>(truth.h*h / biases[<span class="number">2</span>*n + <span class="number">1</span>]);</span><br><span class="line">    <span class="comment">// 这里是欧式距离损失的梯度回传</span></span><br><span class="line">    <span class="comment">// 以tx为例。</span></span><br><span class="line">    <span class="comment">// loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx)</span></span><br><span class="line">    <span class="comment">// d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx)</span></span><br><span class="line">    <span class="comment">// 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx)</span></span><br><span class="line">    <span class="comment">// 也就是(bx^hat-bx) * d(bx)/d(tx)</span></span><br><span class="line">    <span class="comment">// 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的）</span></span><br><span class="line">    <span class="comment">// 后面相当于是求sigmoid函数对输入自变量的梯度。</span></span><br><span class="line">    <span class="comment">// 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里</span></span><br><span class="line">    delta[index + <span class="number">0</span>] = scale * (tx - logistic_activate(x[index + <span class="number">0</span>]))</span><br><span class="line">                             * logistic_gradient(logistic_activate(x[index + <span class="number">0</span>]));</span><br><span class="line"></span><br><span class="line">    delta[index + <span class="number">1</span>] = scale * (ty - logistic_activate(x[index + <span class="number">1</span>]))</span><br><span class="line">                             * logistic_gradient(logistic_activate(x[index + <span class="number">1</span>]));</span><br><span class="line">    <span class="comment">// tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw</span></span><br><span class="line">    delta[index + <span class="number">2</span>] = scale * (tw - x[index + <span class="number">2</span>]);</span><br><span class="line">    delta[index + <span class="number">3</span>] = scale * (th - x[index + <span class="number">3</span>]);</span><br><span class="line">    <span class="keyword">return</span> iou;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个</span></span><br><span class="line"><span class="comment">// 首先，我们认为当前bounding box没有responsible for any groundtruth，</span></span><br><span class="line"><span class="comment">// 那么，loss = 1/2*(0-sigmoid(to))^2</span></span><br><span class="line"><span class="comment">// =&gt; d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)</span></span><br><span class="line"><span class="comment">// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码</span></span><br><span class="line"><span class="comment">// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*y</span></span><br><span class="line">l.delta[index + <span class="number">4</span>] = l.noobject_scale * ((<span class="number">0</span> - l.output[index + <span class="number">4</span>]) * logistic_gradient(l.output[index + <span class="number">4</span>]));</span><br><span class="line"><span class="comment">// 如果best iou &gt; thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可</span></span><br><span class="line"><span class="keyword">if</span> (best_iou &gt; l.thresh) &#123;</span><br><span class="line">    l.delta[index + <span class="number">4</span>] = <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// for each class</span><br><span class="line">for(n = 0; n &lt; classes; ++n)&#123;</span><br><span class="line">    // P_i = \frac&#123;exp^out_i&#125;&#123;sum of exp^out_j&#125;</span><br><span class="line">    // SoftmaxLoss = -logP(class)</span><br><span class="line">    // ∂SoftmaxLoss/∂output = -(1(n==class)-P)</span><br><span class="line">    delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]);</span><br><span class="line">    if(n == class) *avg_cat += output[index + n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<h3 id="改进6：Fine-Gained-Features"><a href="#改进6：Fine-Gained-Features" class="headerlink" title="改进6：Fine-Gained Features"></a>改进6：Fine-Gained Features</h3><p>这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\times 26$大小的feature map加进来。</p>
<p>在具体实现时，是将higher resolution（也就是$26\times 26$）的feature map stacking在一起。比如，原大小为$26\times 26 \times 512$的feature map，因为我们要将其变为$13\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中<code>reorg_layer</code>的实现。</p>
<p>使用这一扩展之后的feature map，提高了1%的性能提升。</p>
<h3 id="改进7：多尺度训练（Multi-Scale-Training）"><a href="#改进7：多尺度训练（Multi-Scale-Training）" class="headerlink" title="改进7：多尺度训练（Multi-Scale Training）"></a>改进7：多尺度训练（Multi-Scale Training）</h3><p>在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。</p>
<p>具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\lbrace 320, 352, \dots, 608\rbrace$。</p>
<p>在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。</p>
<p><img src="/img/yolo2_different_methods_comparation.png" alt="不同检测方法的对比"><br><img src="/img/yolo2_different_methods_comparation_in_table.png" alt="不同检测方法的对比"></p>
<h3 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h3><p>在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。<br><img src="/img/yolo2_different_methods_improvement.png" alt="不同改进措施的影响"></p>
<h2 id="Faster"><a href="#Faster" class="headerlink" title="Faster"></a>Faster</h2><p>这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。</p>
<p>在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。<br><img src="/img/yolo2_dartnet_19_structure.png" alt="Darknet-19的网络结构"></p>
<p>在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\times 224$大小的图像进行训练，再使用$448\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。</p>
<p>然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\times(5+20)=125$。从YOLO V2的<code>yolo_voc.cfg</code><a href="https://github.com/pjreddie/darknet/blob/master/cfg/yolo.cfg" target="_blank" rel="noopener">文件</a>中，我们也可以看到如下的对应结构：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[convolutional]</span><br><span class="line">batch_normalize=1</span><br><span class="line">size=3</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=1024</span><br><span class="line">activation=leaky</span><br><span class="line"></span><br><span class="line">[convolutional]</span><br><span class="line">size=1</span><br><span class="line">stride=1</span><br><span class="line">pad=1</span><br><span class="line">filters=125</span><br><span class="line">activation=linear</span><br></pre></td></tr></table></figure>
<p>同时，加上上文提到的pass-through结构。</p>
<h2 id="Stronger"><a href="#Stronger" class="headerlink" title="Stronger"></a>Stronger</h2><p>未完待续</p>
]]></content>
      <tags>
        <tag>deep learning</tag>
        <tag>paper</tag>
        <tag>yolo</tag>
        <tag>detection</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-立体视觉基础</title>
    <url>/2017/02/02/cs131-camera/</url>
    <content><![CDATA[<p>数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。<br><img src="/img/camera_geometry_application.png" alt="立体视觉应用"><br><a id="more"></a></p>
<h2 id="针孔相机模型（Pinhole-Camera）"><a href="#针孔相机模型（Pinhole-Camera）" class="headerlink" title="针孔相机模型（Pinhole Camera）"></a>针孔相机模型（Pinhole Camera）</h2><p>针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。</p>
<h3 id="投影几何的重要性质"><a href="#投影几何的重要性质" class="headerlink" title="投影几何的重要性质"></a>投影几何的重要性质</h3><p>在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。</p>
<p>在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。<br><img src="/img/projective_geometry_property_1.png" alt="性质1"></p>
<p>另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。<br><img src="/img/projective_geometry_property_2.png" alt="性质2"></p>
<h3 id="针孔相机模型"><a href="#针孔相机模型" class="headerlink" title="针孔相机模型"></a>针孔相机模型</h3><p>如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\Pi^\prime$的点$P^\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。<br><img src="/img/pinhole_camera_model.png" alt="针孔相机模型示意图"></p>
<p>由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\prime$坐标之间的数量关系为：</p>
<script type="math/tex; mode=display">\left\{\begin{matrix}
x^\prime = fx/z \\
y^\prime = fy/z
\end{matrix}\right.</script><p>可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。<br><img src="/img/qicizuobiao.png" alt="齐次坐标"></p>
<p>这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。<br><img src="/img/qicizuobiao_transform.png" alt></p>
<p>上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设：</p>
<ol>
<li>内假设（和相机本身有关）<ul>
<li>不同方向上焦距相同；</li>
<li>光学中心在相平面的坐标原点$(0, 0)$</li>
<li>没有倾斜（no skew）</li>
</ul>
</li>
<li>外假设（和相机位姿有关，和相机本身参数无关）<ul>
<li>相机没有旋转（坐标轴与世界坐标系方向重合）</li>
<li>相机没有平移（相机中心与世界坐标系中心重合）</li>
</ul>
</li>
</ol>
<p>其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。<br><img src="/img/camera_skew.png" alt="what is &quot;skew&quot;?"></p>
<p>下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。</p>
<h4 id="理想情况"><a href="#理想情况" class="headerlink" title="理想情况"></a>理想情况</h4><p>理想情况以上假设全部满足，矩阵$M$如下所示。<br><img src="/img/case_1_m.png" alt="case 1: M"></p>
<h4 id="光学中心不在像平面的坐标原点"><a href="#光学中心不在像平面的坐标原点" class="headerlink" title="光学中心不在像平面的坐标原点"></a>光学中心不在像平面的坐标原点</h4><p>假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为：<br><img src="/img/case_2_m.png" alt="case 2: M"></p>
<h4 id="像素非正方形"><a href="#像素非正方形" class="headerlink" title="像素非正方形"></a>像素非正方形</h4><p>由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下：<br><img src="/img/case_3_m.png" alt="case 3: M"></p>
<h4 id="no-skew"><a href="#no-skew" class="headerlink" title="no skew"></a>no skew</h4><p>这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下：<br><img src="/img/case_4_m.png" alt="case 4: M"></p>
<h4 id="相机的旋转和平移"><a href="#相机的旋转和平移" class="headerlink" title="相机的旋转和平移"></a>相机的旋转和平移</h4><p>相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。<br><img src="/img/camera_translation_rotation.png" alt="相机旋转和平移"></p>
<p>所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\in \mathbb{R}^{3\times 4}$。</p>
<script type="math/tex; mode=display">P^\prime = MHP</script><p>首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\mathbb{0}$矩阵变为了一个平移向量。<br><img src="/img/case_m_5.png" alt="case 5: M"></p>
<p>进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示：<br><img src="/img/rotation_matrix.png" alt="绕单轴的旋转矩阵"></p>
<p>将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下：<br><img src="/img/case_6_m.png" alt="case 6: M"></p>
<h4 id="最终形式"><a href="#最终形式" class="headerlink" title="最终形式"></a>最终形式</h4><p>综上所示，变换矩阵的最终形式为：<br><img src="/img/generic_projection_matrix.png" alt="最终的投影变换矩阵"></p>
<p>其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。</p>
<p>上面的内容总结起来，如下图所示。<br><img src="/img/camera_model_things_to_remember.png" alt="things to remember"></p>
<h2 id="对极几何"><a href="#对极几何" class="headerlink" title="对极几何"></a>对极几何</h2><h3 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h3><p>如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。</p>
<p><img src="/img/epipolar_fig.png" alt="对极几何概念图示"></p>
<ul>
<li>极点：$e$和$e^\prime$点分别是$OO^\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。</li>
<li>极平面：点$O$，$O^\prime$，$P$点共同确定的平面（灰色）</li>
<li>极线：极平面与两个成像平面的交线，即$pe$和$p^\prime e^\prime$（蓝色）</li>
<li>基线：两个相机中心的连线（黄色）</li>
</ul>
<h3 id="极线约束"><a href="#极线约束" class="headerlink" title="极线约束"></a>极线约束</h3><p>从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？</p>
<p>如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\prime$有向线段，表明相机中心的位移。<br><img src="/img/epipolar_constraint_1.png" alt="极线约束1"></p>
<p>（下面的推导参考了<a href="http://www.cnblogs.com/gemstone/articles/2294551.html" target="_blank" rel="noopener">博客：计算机视觉基础4——对极几何</a>）。在下面的推导中，我们使用$p^\prime$表示在相机$O^\prime$下的向量$O\prime P$，符号$p$同理。那么，有如下关系成立：<br>$R(p-T) = p^\prime$</p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-描述图像的特征(SIFT)</title>
    <url>/2017/01/30/cs131-sift/</url>
    <content><![CDATA[<p><a href="https://en.wikipedia.org/wiki/Scale-invariant_feature_transform" target="_blank" rel="noopener">SIFT(尺度不变特征变换，Scale Invariant Feature Transform)</a>,最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下：</p>
<ul>
<li>scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。</li>
<li>interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。</li>
<li>确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。</li>
<li>确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。</li>
</ul>
<p><img src="/img/sift_picture.jpg" alt="SIFT图示"></p>
<a id="more"></a>
<h2 id="SIFT介绍"><a href="#SIFT介绍" class="headerlink" title="SIFT介绍"></a>SIFT介绍</h2><p>上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。<br><img src="/img/harris_non_scale_constant.png" alt="harris的尺度变换不满足尺度不变性"></p>
<p>而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。<br><img src="/img/patch_average_intensity_scale_constant.png" alt="平均亮度满足尺度变化呢不变性"></p>
<p>而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。</p>
<p><a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank" rel="noopener">Lowe的论文</a>中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。</p>
<p>这篇博客主要是<a href="http://www.cs.ubc.ca/~lowe/papers/ijcv04.pdf" target="_blank" rel="noopener">Lowe上述论文</a>的读书笔记，按照SIFT特征的计算步骤进行组织。</p>
<h2 id="尺度空间极值的检测方法"><a href="#尺度空间极值的检测方法" class="headerlink" title="尺度空间极值的检测方法"></a>尺度空间极值的检测方法</h2><p>前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\sigma)$的卷积结果。如下式所示：</p>
<script type="math/tex; mode=display">L(x,y,\sigma) = G(x,y,\sigma)\ast I(x,y)</script><p>其中，$G(x,y, \sigma) = \frac{1}{2\pi\sigma^2}\exp(-(x^2+y^2)/2\sigma^2)$。不同的$\sigma$代表不同的尺度。</p>
<p>DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即，</p>
<script type="math/tex; mode=display">D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)</script><p>如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\sigma$最终变成了2倍（即$\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。<br><img src="/img/sift_dog.png" alt="DoG的计算"></p>
<p>为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\sigma^2\Delta G$提供了足够的近似。其中前面的$\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\sigma \Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。</p>
<p>对于高斯核函数，有以下性质：</p>
<script type="math/tex; mode=display">\frac{\partial G}{\partial \sigma} = \sigma \Delta G</script><p>我们将式子左侧的微分变成差分，得到了下式：</p>
<script type="math/tex; mode=display">\sigma\Delta G \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}</script><p>也就是：</p>
<script type="math/tex; mode=display">G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \Delta G</script><p>当$k=1$时，上式的近似误差为0（即上面的$s=\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。</p>
<p>构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。<br><img src="/img/sift_detection_maximum.png" alt="检测极值"></p>
<p>另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。</p>
<p>此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。</p>
<h2 id="128维feature的获取"><a href="#128维feature的获取" class="headerlink" title="128维feature的获取"></a>128维feature的获取</h2><p>我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引<code>pyramid{scale}(y, x)</code>就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。</p>
<p>我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量<code>patch_mag</code>和<code>patch_theta</code>分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">patch_mag = <span class="built_in">sqrt</span>(patch_dx.^<span class="number">2</span> + patch_dy.^<span class="number">2</span>);</span><br><span class="line">patch_theta = <span class="built_in">atan2</span>(patch_dy, patch_dx);  <span class="comment">% atan2的返回结果在区间[-pi, pi]上。</span></span><br><span class="line">patch_theta = <span class="built_in">mod</span>(patch_theta, <span class="number">2</span>*<span class="built_in">pi</span>);   <span class="comment">% 这里我们要将其转换为[0, 2pi]</span></span><br></pre></td></tr></table></figure>
<p>之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。<br><img src="/img/sift_dominant_orientation.png" alt="何为主方向"></p>
<p>所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将<code>[0, 2pi]</code>区间划分为若干个<code>bin</code>，并将patch内的每个点使用其梯度大小向对应的<code>bin</code>内投票即可。如下所示：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[histogram, angles]</span> = <span class="title">ComputeGradientHistogram</span><span class="params">(num_bins, gradient_magnitudes, gradient_angles)</span></span></span><br><span class="line"><span class="comment">% Compute a gradient histogram using gradient magnitudes and directions.</span></span><br><span class="line"><span class="comment">% Each point is assigned to one of num_bins depending on its gradient</span></span><br><span class="line"><span class="comment">% direction; the gradient magnitude of that point is added to its bin.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% INPUT</span></span><br><span class="line"><span class="comment">% num_bins: The number of bins to which points should be assigned.</span></span><br><span class="line"><span class="comment">% gradient_magnitudes, gradient angles:</span></span><br><span class="line"><span class="comment">%       Two arrays of the same shape where gradient_magnitudes(i) and</span></span><br><span class="line"><span class="comment">%       gradient_angles(i) give the magnitude and direction of the gradient</span></span><br><span class="line"><span class="comment">%       for the ith point. gradient_angles ranges from 0 to 2*pi</span></span><br><span class="line"><span class="comment">%                                      </span></span><br><span class="line"><span class="comment">% OUTPUT</span></span><br><span class="line"><span class="comment">% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is</span></span><br><span class="line"><span class="comment">%       the sum of entries in gradient_magnitudes whose corresponding</span></span><br><span class="line"><span class="comment">%       gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for</span></span><br><span class="line"><span class="comment">%       angles between angle_step and 2*angle_step. Angle_step is calculated as</span></span><br><span class="line"><span class="comment">%       2*pi/num_bins.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">% angles: A 1 x num_bins array which holds the histogram bin lower bounds.</span></span><br><span class="line"><span class="comment">%       In other words, histogram(i) contains the sum of the</span></span><br><span class="line"><span class="comment">%       gradient magnitudes of all points whose gradient directions fall</span></span><br><span class="line"><span class="comment">%       in the range [angles(i), angles(i + 1))</span></span><br><span class="line"></span><br><span class="line">    angle_step = <span class="number">2</span> * <span class="built_in">pi</span> / num_bins;</span><br><span class="line">    angles = <span class="number">0</span> : angle_step : (<span class="number">2</span>*<span class="built_in">pi</span>-angle_step);</span><br><span class="line"></span><br><span class="line">    histogram = <span class="built_in">zeros</span>(<span class="number">1</span>, num_bins);</span><br><span class="line">    num = <span class="built_in">numel</span>(gradient_angles);</span><br><span class="line">    <span class="keyword">for</span> n = <span class="number">1</span>:num</span><br><span class="line">        index = <span class="built_in">floor</span>(gradient_angles(n) / angle_step) + <span class="number">1</span>;</span><br><span class="line">        histogram(index) = histogram(index) + gradient_magnitudes(n);</span><br><span class="line">    <span class="keyword">end</span>    </span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>Lowe论文中推荐的<code>bin</code>数目为36个，计算主方向的函数如下：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">direction</span> = <span class="title">ComputeDominantDirection</span><span class="params">(gradient_magnitudes, gradient_angles)</span></span></span><br><span class="line"><span class="comment">% Computes the dominant gradient direction for the region around a keypoint</span></span><br><span class="line"><span class="comment">% given the scale of the keypoint and the gradient magnitudes and gradient</span></span><br><span class="line"><span class="comment">% angles of the pixels in the region surrounding the keypoint.</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% INPUT</span></span><br><span class="line"><span class="comment">% gradient_magnitudes, gradient_angles:</span></span><br><span class="line"><span class="comment">%   Two arrays of the same shape where gradient_magnitudes(i) and</span></span><br><span class="line"><span class="comment">%   gradient_angles(i) give the magnitude and direction of the gradient for</span></span><br><span class="line"><span class="comment">%   the ith point.</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Compute a gradient histogram using the weighted gradient magnitudes.</span></span><br><span class="line">    <span class="comment">% In David Lowe's paper he suggests using 36 bins for this histogram.</span></span><br><span class="line">    num_bins = <span class="number">36</span>;</span><br><span class="line">    <span class="comment">% Step 1:</span></span><br><span class="line">    <span class="comment">% compute the 36-bin histogram of angles using ComputeGradientHistogram()</span></span><br><span class="line">    [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles);</span><br><span class="line">    <span class="comment">% Step 2:</span></span><br><span class="line">    <span class="comment">% Find the maximum value of the gradient histogram, and set "direction"</span></span><br><span class="line">    <span class="comment">% to the angle corresponding to the maximum. (To match our solutions,</span></span><br><span class="line">    <span class="comment">% just use the lower-bound angle of the max histogram bin. (E.g. return</span></span><br><span class="line">    <span class="comment">% 0 radians if it's bin 1.)</span></span><br><span class="line">    [~, max_index] = <span class="built_in">max</span>(histogram);</span><br><span class="line">    direction = angle_bound(max_index);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;</span><br><span class="line">patch_theta = <span class="built_in">mod</span>(patch_theta, <span class="number">2</span>*<span class="built_in">pi</span>);</span><br><span class="line">patch_mag = patch_mag .* fspecial(<span class="string">'gaussian'</span>, patch_size, patch_size / <span class="number">2</span>); <span class="comment">% patch_size = 16</span></span><br></pre></td></tr></table></figure>
<p>遍历cell，计算feature如下：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">feature = [];</span><br><span class="line">row_iter = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> y = <span class="number">1</span>:num_histograms</span><br><span class="line">    col_iter = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> x = <span class="number">1</span>:num_histograms</span><br><span class="line">        cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - <span class="number">1</span>, ...</span><br><span class="line">                             col_iter: col_iter + pixelsPerHistogram - <span class="number">1</span>);</span><br><span class="line">        cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - <span class="number">1</span>, ...</span><br><span class="line">                             col_iter: col_iter + pixelsPerHistogram - <span class="number">1</span>);</span><br><span class="line">        [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta);</span><br><span class="line">        feature = [feature, histogram];</span><br><span class="line">        col_iter = col_iter + pixelsPerHistogram;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    row_iter = row_iter + pixelsPerHistogram;</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。</p>
<p>这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。</p>
<h2 id="应用：图像特征点匹配"><a href="#应用：图像特征点匹配" class="headerlink" title="应用：图像特征点匹配"></a>应用：图像特征点匹配</h2><p>和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中<code>descriptor</code>是两幅图像的SIFT特征向量。阈值默认为取做0.7。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">match</span> = <span class="title">SIFTSimpleMatcher</span><span class="params">(descriptor1, descriptor2, thresh)</span></span></span><br><span class="line"><span class="comment">% SIFTSimpleMatcher</span></span><br><span class="line"><span class="comment">%   Match one set of SIFT descriptors (descriptor1) to another set of</span></span><br><span class="line"><span class="comment">%   descriptors (decriptor2). Each descriptor from descriptor1 can at</span></span><br><span class="line"><span class="comment">%   most be matched to one member of descriptor2, but descriptors from</span></span><br><span class="line"><span class="comment">%   descriptor2 can be matched more than once.</span></span><br><span class="line"><span class="comment">%   </span></span><br><span class="line"><span class="comment">%   Matches are determined as follows:</span></span><br><span class="line"><span class="comment">%   For each descriptor vector in descriptor1, find the Euclidean distance</span></span><br><span class="line"><span class="comment">%   between it and each descriptor vector in descriptor2. If the smallest</span></span><br><span class="line"><span class="comment">%   distance is less than thresh*(the next smallest distance), we say that</span></span><br><span class="line"><span class="comment">%   the two vectors are a match, and we add the row [d1 index, d2 index] to</span></span><br><span class="line"><span class="comment">%   the "match" array.</span></span><br><span class="line"><span class="comment">%   </span></span><br><span class="line"><span class="comment">% INPUT:</span></span><br><span class="line"><span class="comment">%   descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.</span></span><br><span class="line"><span class="comment">%   descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.</span></span><br><span class="line"><span class="comment">%   thresh: a given threshold of ratio. Typically 0.7</span></span><br><span class="line"><span class="comment">%</span></span><br><span class="line"><span class="comment">% OUTPUT:</span></span><br><span class="line"><span class="comment">%   Match: N * 2 matrix, each row is a match.</span></span><br><span class="line"><span class="comment">%          For example, Match(k, :) = [i, j] means i-th descriptor in</span></span><br><span class="line"><span class="comment">%          descriptor1 is matched to j-th descriptor in descriptor2.</span></span><br><span class="line">    <span class="keyword">if</span> ~exist(<span class="string">'thresh'</span>, <span class="string">'var'</span>),</span><br><span class="line">        thresh = <span class="number">0.7</span>;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    match = [];</span><br><span class="line">    [N1, ~] = <span class="built_in">size</span>(descriptor1);</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:N1</span><br><span class="line">        fea = descriptor1(<span class="built_in">i</span>, :);</span><br><span class="line">        err = <span class="built_in">bsxfun</span>(@minus, fea, descriptor2);</span><br><span class="line">        dis = <span class="built_in">sqrt</span>(sum(err.^<span class="number">2</span>, <span class="number">2</span>));</span><br><span class="line">        [sorted_dis, ind] = <span class="built_in">sort</span>(dis, <span class="number">1</span>);</span><br><span class="line">        <span class="keyword">if</span> sorted_dis(<span class="number">1</span>) &lt; thresh * sorted_dis(<span class="number">2</span>)</span><br><span class="line">            match = [match; [<span class="built_in">i</span>, ind(<span class="number">1</span>)]];</span><br><span class="line">        <span class="keyword">end</span></span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足：</p>
<script type="math/tex; mode=display">Hp_{\text{before}} = p_{\text{after}}</script><p>其中</p>
<script type="math/tex; mode=display">p = \begin{bmatrix}x \\\\ y \\\\ 1\end{bmatrix}</script><p>对上式稍作变形，有</p>
<script type="math/tex; mode=display">p_{\text{before}}^\dagger H^\dagger = p_{\text{after}}\dagger</script><p>就可以使用标准的最小二乘正则方程进行求解了。代码如下：</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">H</span> = <span class="title">ComputeAffineMatrix</span><span class="params">( Pt1, Pt2 )</span></span></span><br><span class="line"><span class="comment">%ComputeAffineMatrix</span></span><br><span class="line"><span class="comment">%   Computes the transformation matrix that transforms a point from</span></span><br><span class="line"><span class="comment">%   coordinate frame 1 to coordinate frame 2</span></span><br><span class="line"><span class="comment">%Input:</span></span><br><span class="line"><span class="comment">%   Pt1: N * 2 matrix, each row is a point in image 1</span></span><br><span class="line"><span class="comment">%       (N must be at least 3)</span></span><br><span class="line"><span class="comment">%   Pt2: N * 2 matrix, each row is the point in image 2 that</span></span><br><span class="line"><span class="comment">%       matches the same point in image 1 (N should be more than 3)</span></span><br><span class="line"><span class="comment">%Output:</span></span><br><span class="line"><span class="comment">%   H: 3 * 3 affine transformation matrix,</span></span><br><span class="line"><span class="comment">%       such that H*pt1(i,:) = pt2(i,:)</span></span><br><span class="line"></span><br><span class="line">    N = <span class="built_in">size</span>(Pt1,<span class="number">1</span>);</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">size</span>(Pt1, <span class="number">1</span>) ~= <span class="built_in">size</span>(Pt2, <span class="number">1</span>),</span><br><span class="line">        error(<span class="string">'Dimensions unmatched.'</span>);</span><br><span class="line">    <span class="keyword">elseif</span> N&lt;<span class="number">3</span></span><br><span class="line">        error(<span class="string">'At least 3 points are required.'</span>);</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">% Convert the input points to homogeneous coordintes.</span></span><br><span class="line">    P1 = [Pt1';<span class="built_in">ones</span>(<span class="number">1</span>,N)];</span><br><span class="line">    P2 = [Pt2';<span class="built_in">ones</span>(<span class="number">1</span>,N)];</span><br><span class="line"></span><br><span class="line">    H = P1*P1'\P1*P2';</span><br><span class="line">    H = H';</span><br><span class="line"></span><br><span class="line">    <span class="comment">% Sometimes numerical issues cause least-squares to produce a bottom</span></span><br><span class="line">    <span class="comment">% row which is not exactly [0 0 1], which confuses some of the later</span></span><br><span class="line">    <span class="comment">% code. So we'll ensure the bottom row is exactly [0 0 1].</span></span><br><span class="line">    H(<span class="number">3</span>,:) = [<span class="number">0</span> <span class="number">0</span> <span class="number">1</span>];</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure>
<p>作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~<br><img src="/img/sift_experiment_1.png" alt="result 1"><br><img src="/img/sift_experiment_2.png" alt="result 2"></p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-描述图像的特征(Harris 角点)</title>
    <url>/2017/01/25/cs131-finding-features/</url>
    <content><![CDATA[<p>feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。</p>
<p>那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。</p>
<p><img src="/img/image_matching_hard.png" alt="image matching example"></p>
<a id="more"></a>
<h2 id="Harris角点"><a href="#Harris角点" class="headerlink" title="Harris角点"></a>Harris角点</h2><p>角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。<br><img src="/img/what_is_corner.png" alt="what is corner"></p>
<p><a href="http://www.bmva.org/bmvc/1988/avc-88-023.pdf" target="_blank" rel="noopener">Harris角点</a>得名于其发明者Harris，是一种常见的角点检测方法。<br>给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。</p>
<script type="math/tex; mode=display">E(u,v) = \sum_x\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2</script><p>其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。<br><img src="/img/corner_window_fun.png" alt="window function"></p>
<p>使用泰勒级数展开，并忽略非线性项，我们有</p>
<script type="math/tex; mode=display">I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v</script><p>所以上式可以写成（线性二次型写成了矩阵形式），</p>
<script type="math/tex; mode=display">E(u,v) = \sum_{x,y}w(I_xu+I_yv)^2 = \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\\\v\end{bmatrix}</script><p>其中，</p>
<script type="math/tex; mode=display">M = w\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y&I_y^2\end{bmatrix}</script><p>当使用门限函数时，权值$w_{i,j} = 1$，则，</p>
<script type="math/tex; mode=display">M = \begin{bmatrix}\sum I_xI_x& \sum I_xI_y\\\\\sum I_xI_y&\sum I_yI_y\end{bmatrix} = \sum \begin{bmatrix}I_x \\\\I_y\end{bmatrix}\begin{bmatrix}I_x &I_y\end{bmatrix}</script><p>当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵</p>
<script type="math/tex; mode=display">M = \begin{bmatrix}\lambda_1 & 0 \\\\ 0&\lambda_2 \end{bmatrix}</script><p><img src="/img/corner_type_1.png" alt="M为对角阵"></p>
<p>当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。</p>
<script type="math/tex; mode=display">M = R^{-1}\Sigma R, \text{其中}\Sigma = \begin{bmatrix}\lambda_1&0\\\\0&\lambda_2\end{bmatrix}</script><p>所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\lambda_1$和$\lambda_2$）。<br><img src="/img/corner_judge.png" alt="使用M矩阵特征值判定"></p>
<p> 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。</p>
<script type="math/tex; mode=display">\theta = \det(M)-\alpha\text{trace}(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2</script><p><img src="/img/corner_judge_2.png" alt="使用theta判定"></p>
<p>为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示：</p>
<script type="math/tex; mode=display">w(x,y) = \exp(-(x^2+y^2)/2\sigma^2)</script>]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-边缘检测</title>
    <url>/2017/01/24/cs131-edge-detection/</url>
    <content><![CDATA[<p>边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。</p>
<p><img src="/img/edge_camera_man.png" alt="边缘检测图示"><br><a id="more"></a></p>
<h2 id="边缘的产生"><a href="#边缘的产生" class="headerlink" title="边缘的产生"></a>边缘的产生</h2><p>若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点：</p>
<ul>
<li>物体表面不平造成灰度值的不连续；</li>
<li>深度值不同造成灰度值不连续；</li>
<li>物体表面颜色的突变造成灰度值不连续</li>
</ul>
<h2 id="朴素思想"><a href="#朴素思想" class="headerlink" title="朴素思想"></a>朴素思想</h2><p>利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。<br><img src="/img/edge_deriative.png" alt="边缘点处导数很大"></p>
<p>问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。</p>
<p>在$x$方向上，令$g_x = \frac{\partial f}{\partial x}$；在$y$方向上，令$g_y = \frac{\partial f}{\partial y}$。梯度的大小和方向为</p>
<script type="math/tex; mode=display">g = \lbrack g_x, g_y\rbrack, \theta = \arctan(g_y/g_x)</script><p>通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。</p>
<p>只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。<br><img src="/img/fun_noise.png" alt="噪声影响湮没了边缘点"></p>
<h2 id="改进1：先平滑"><a href="#改进1：先平滑" class="headerlink" title="改进1：先平滑"></a>改进1：先平滑</h2><p>改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有：</p>
<script type="math/tex; mode=display">\frac{d}{dx}(f\ast g) = f\ast\frac{d}{dx}g</script><p>所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。<br><img src="/img/dog_x.png" alt="x方向的DoG"></p>
<p>进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。<br><img src="/img/dog_different_size.png" alt="不同"></p>
<h2 id="改进2：Canny检测子"><a href="#改进2：Canny检测子" class="headerlink" title="改进2：Canny检测子"></a>改进2：Canny检测子</h2><p>改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下：</p>
<ul>
<li>使用DoG计算梯度幅值和方向。</li>
<li>非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。</li>
<li>利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。</li>
</ul>
<p><img src="/img/canny_nms.png" alt="nms示意图"><br><img src="/img/canny_linking.png" alt="linking示意图"></p>
<p>同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定<code>low</code>和<code>high</code>两个阈值，来判定某个点是否属于<strong>强</strong>或<strong>弱</strong>边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比<code>low</code>还要小，则在此停止。</p>
<h2 id="改进3：RANSAC方法"><a href="#改进3：RANSAC方法" class="headerlink" title="改进3：RANSAC方法"></a>改进3：RANSAC方法</h2><p>有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。</p>
<p>RANSAC方法的思想在于，认为已有的feature大部分都是<strong>好的</strong>。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。</p>
<p>以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。<br><img src="/img/ransac_step.png" alt="ransac step"></p>
<p>上述RANSAC方法进行直线拟合的过程可以总结如下：<br><img src="/img/ransac_line_fit.png" alt="ransac line fit alg"></p>
<p>按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。</p>
<p>而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：<br><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">least square: a = 3.319566, b = -1.446528</span><br><span class="line">ransac method: a = 1.899640, b= 1.298608</span><br></pre></td></tr></table></figure></p>
<p><img src="/img/line_fit_demo.png" alt="demo result"></p>
<p>实验使用的MATLAB代码如下：<br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% generate data</span></span><br><span class="line">x = <span class="number">0</span>:<span class="number">1</span>:<span class="number">10</span>;</span><br><span class="line">y_gt = <span class="number">2</span>*x+<span class="number">1</span>;</span><br><span class="line">y = y_gt + <span class="built_in">randn</span>(<span class="built_in">size</span>(y_gt));</span><br><span class="line"><span class="built_in">scatter</span>(x, y, [], [<span class="number">1</span>,<span class="number">0</span>,<span class="number">0</span>]);</span><br><span class="line"><span class="built_in">hold</span> on</span><br><span class="line">out_x = <span class="number">0</span>:<span class="number">1</span>:<span class="number">10</span>;</span><br><span class="line">out_y = <span class="number">5</span>*<span class="built_in">rand</span>(<span class="built_in">size</span>(out_x)).*out_x + <span class="number">4</span>*<span class="built_in">rand</span>(<span class="built_in">size</span>(out_x));</span><br><span class="line"><span class="built_in">scatter</span>(out_x, out_y, [], [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>]);</span><br><span class="line">X = [x, out_x]';</span><br><span class="line">Y = [y, out_y]';</span><br><span class="line">X = [X, <span class="built_in">ones</span>(<span class="built_in">length</span>(X), <span class="number">1</span>)];</span><br><span class="line">[a, b] = ls_fit(X, Y);</span><br><span class="line"><span class="built_in">plot</span>(x, a*x+b, <span class="string">'linestyle'</span>, <span class="string">'--'</span>, <span class="string">'color'</span>, <span class="string">'r'</span>);</span><br><span class="line"></span><br><span class="line">[ra, rb] = ransac_fit(X, Y, <span class="number">100</span>, <span class="number">2</span>, <span class="number">0.5</span>, <span class="number">3</span>);</span><br><span class="line"><span class="built_in">plot</span>(x, ra*x+rb, <span class="string">'linestyle'</span>, <span class="string">'-.'</span>, <span class="string">'color'</span>, <span class="string">'g'</span>);</span><br><span class="line">fprintf(<span class="string">'least square: a = %f, b = %f\n'</span>,a, b);</span><br><span class="line">fprintf(<span class="string">'ransac method: a = %f, b= %f\n'</span>, ra, rb)</span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[a, b]</span> = <span class="title">ransac_fit</span><span class="params">(X, Y, k, n, t ,d)</span></span></span><br><span class="line"><span class="comment">% ransac fit</span></span><br><span class="line"><span class="comment">% k -- maximum iteration number</span></span><br><span class="line"><span class="comment">% n -- smallest point numer required</span></span><br><span class="line"><span class="comment">% t -- threshold to identify a point is fit well</span></span><br><span class="line"><span class="comment">% d -- the number of nearby points to assert a model is fine</span></span><br><span class="line">data = [X, Y];</span><br><span class="line">N = <span class="built_in">size</span>(data, <span class="number">1</span>);</span><br><span class="line">best_good_cnt = <span class="number">-1</span>;</span><br><span class="line">best_a = <span class="number">0</span>;</span><br><span class="line">best_b = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span>:k</span><br><span class="line">    <span class="comment">% sample point</span></span><br><span class="line">    idx = randsample(N, n);</span><br><span class="line">    data_sampled = data(idx, :);</span><br><span class="line">    <span class="comment">% fit with least square</span></span><br><span class="line">    [a, b] = ls_fit(data_sampled(:, <span class="number">1</span>:<span class="number">2</span>), data_sampled(:, <span class="number">3</span>));</span><br><span class="line">    <span class="comment">% test model</span></span><br><span class="line">    not_sampled = <span class="built_in">ones</span>(N, <span class="number">1</span>);</span><br><span class="line">    not_sampled(idx) = <span class="number">0</span>;</span><br><span class="line">    not_sampled_data = data(not_sampled == <span class="number">1</span>, :);</span><br><span class="line">    distance = <span class="built_in">abs</span>(not_sampled_data(:, <span class="number">1</span>:<span class="number">2</span>) * [a; b] - not_sampled_data(:, <span class="number">3</span>)) / <span class="built_in">sqrt</span>(a^<span class="number">2</span>+<span class="number">1</span>);</span><br><span class="line">    inner_flag = distance &lt; t;</span><br><span class="line">    good_cnt = sum(inner_flag);</span><br><span class="line">    <span class="keyword">if</span> good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt</span><br><span class="line">        best_good_cnt = good_cnt;</span><br><span class="line">        data_refine = data(<span class="built_in">find</span>(inner_flag), :);</span><br><span class="line">        [a, b] = ls_fit(data_refine(:, <span class="number">1</span>:<span class="number">2</span>), data_refine(:, <span class="number">3</span>));</span><br><span class="line">        best_a = a;</span><br><span class="line">        best_b = b;</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line">    fprintf(<span class="string">'iteration %d, best_a = %f, best_b = %f\n'</span>, <span class="built_in">i</span>, best_a, best_b);</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line">a = best_a;</span><br><span class="line">b = best_b;</span><br><span class="line"><span class="keyword">end</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">function</span> <span class="params">[a, b]</span> = <span class="title">ls_fit</span><span class="params">(X, Y)</span></span></span><br><span class="line"><span class="comment">% least square fit</span></span><br><span class="line">A = X'*X\X'*Y;</span><br><span class="line">a = A(<span class="number">1</span>);</span><br><span class="line">b = A(<span class="number">2</span>);</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<p>我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。</p>
<p>仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。<br><img src="/img/ransac_k.png" alt="k"></p>
<p>RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。</p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-线性滤波器和矩阵的SVD分解</title>
    <url>/2017/01/23/cs131-filter-svd/</url>
    <content><![CDATA[<p>数字图像可以看做$\mathbb{R}^2 \rightarrow \mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。</p>
<p><img src="/img/svd_picture.jpg" alt="SVD图示"></p>
<a id="more"></a>
<h2 id="卷积"><a href="#卷积" class="headerlink" title="卷积"></a>卷积</h2><p>卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自<a href="http://blog.csdn.net/zouxy09/article/details/49080029" target="_blank" rel="noopener">博客《图像卷积与滤波的一些知识点》</a>）<br><img src="/img/convolution.png" alt="卷积操作示意图"></p>
<p>在卷积操作时，常常需要对图像做padding，常用的padding方法有：</p>
<ul>
<li>zero padding，也就是填充0值。</li>
<li>edge replication，也就是复制边缘值进行填充。</li>
<li>mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。</li>
</ul>
<h2 id="作业1"><a href="#作业1" class="headerlink" title="作业1"></a>作业1</h2><h3 id="调整图像灰度值为0到255"><a href="#调整图像灰度值为0到255" class="headerlink" title="调整图像灰度值为0到255"></a>调整图像灰度值为0到255</h3><p>计算相应的k和offset值即可。另外MATLAB中的<code>uint8</code>函数可以将结果削顶与截底为0到255之间。<br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">scale_ratio = <span class="number">255.0</span> / (max_val - min_val);</span><br><span class="line">offset = -min_val * scale_ratio;</span><br><span class="line">fixedimg = scale_ratio * dark + offset;</span><br></pre></td></tr></table></figure></p>
<h3 id="SVD图像压缩"><a href="#SVD图像压缩" class="headerlink" title="SVD图像压缩"></a>SVD图像压缩</h3><p>使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。<br><img src="/img/svd_ranking.png" alt="SVD值大小示意图"></p>
<h4 id="MATLAB实现"><a href="#MATLAB实现" class="headerlink" title="MATLAB实现"></a>MATLAB实现</h4><p>分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。<br><img src="/img/svd_flower.png" alt="不同分量个数的图像压缩"></p>
<p>MATLAB代码如下：<br><figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line"><span class="comment">%% read image</span></span><br><span class="line">im = imread(<span class="string">'./flower.bmp'</span>);</span><br><span class="line">im_gray = double(rgb2gray(im));</span><br><span class="line">[u, s, v] = svd(im_gray);</span><br><span class="line"><span class="comment">%% get sigular value</span></span><br><span class="line">sigma = <span class="built_in">diag</span>(s);</span><br><span class="line">top_k = sigma(<span class="number">1</span>:<span class="number">10</span>);</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line"><span class="built_in">plot</span>(<span class="number">1</span>:<span class="built_in">length</span>(sigma), sigma, <span class="string">'r-'</span>, <span class="string">'marker'</span>, <span class="string">'s'</span>, <span class="string">'markerfacecolor'</span>, <span class="string">'g'</span>);</span><br><span class="line"></span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line">subplot(<span class="number">2</span>, <span class="number">2</span>, <span class="number">1</span>);</span><br><span class="line">imshow(uint8(im_gray));</span><br><span class="line">title(<span class="string">'flower.bmp'</span>)</span><br><span class="line">index = <span class="number">2</span>;</span><br><span class="line"><span class="keyword">for</span> k = [<span class="number">10</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line">    uk = u(:, <span class="number">1</span>:k);</span><br><span class="line">    sk = s(<span class="number">1</span>:k, <span class="number">1</span>:k);</span><br><span class="line">    vk = v(:, <span class="number">1</span>:k);</span><br><span class="line">    im_rec = uk * sk * vk';</span><br><span class="line">    subplot(<span class="number">2</span>, <span class="number">2</span>, index);</span><br><span class="line">    index = index + <span class="number">1</span>;</span><br><span class="line">    imshow(uint8(im_rec));</span><br><span class="line">    title(sprintf(<span class="string">'k = %d'</span>, k));</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure></p>
<h4 id="图像SVD压缩中的误差分析"><a href="#图像SVD压缩中的误差分析" class="headerlink" title="图像SVD压缩中的误差分析"></a>图像SVD压缩中的误差分析</h4><p>完全是个人随手推导，不严格的说明：</p>
<p>将矩阵分块。由SVD分解公式$\mathbf{U}\mathbf{\Sigma} \mathbf{V^\dagger} = \mathbf{A}$，把$\mathbf{U}$按列分块，$\mathbf{V^\dagger}$按行分块，有下式成立：</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
u_1 & u_2 &\vdots  &u_n
\end{bmatrix}
\begin{bmatrix}
\sigma_1 &  &  & \\\\
 &  \sigma_2&  & \\\\
 &  &  \ddots& \\\\
 &  &  &\sigma_m
\end{bmatrix}
\begin{bmatrix}
v_1^\dagger\\\\
v_2^\dagger\\\\
\dots\\\\
v_m^\dagger
\end{bmatrix}=\mathbf{A}</script><p>由于</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
u_1 & u_2 &\vdots  &u_n
\end{bmatrix}
\begin{bmatrix}
\sigma_1 &  &  & \\\\
 &  \sigma_2&  & \\\\
 &  &  \ddots& \\\\
 &  &  &\sigma_m
\end{bmatrix}
=
\begin{bmatrix}
\sigma_1u_1 & \sigma_2u_2 &\vdots  &\sigma_nu_n
\end{bmatrix}</script><p>所以，</p>
<script type="math/tex; mode=display">\mathbf{A} = \sum_{i = 1}^{r}\sigma_iu_iv_i^\dagger</script><p>上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\sigma_k = 0$。</p>
<p>所以<script type="math/tex">\mathbf{A} - \hat{\mathbf{A}} = \sum_{i = k+1}^{r}\sigma_iu_iv_i^\dagger</script></p>
<p>根绝矩阵范数的<a href="https://zh.wikipedia.org/wiki/矩陣範數" target="_blank" rel="noopener">性质</a>，我们有，</p>
<script type="math/tex; mode=display">\left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i\left\lVert u_i\right\rVert\left\lVert v_i^\dagger\right\rVert</script><p>由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故，</p>
<script type="math/tex; mode=display">\left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i</script><p>取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有：</p>
<script type="math/tex; mode=display">e \le \sum_{i=k+1}^{r}\sigma_i</script><h3 id="SVD与矩阵范数"><a href="#SVD与矩阵范数" class="headerlink" title="SVD与矩阵范数"></a>SVD与矩阵范数</h3><p>如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。</p>
<ul>
<li>$f(\mathbf{A}) = \mathbf{0} \Leftrightarrow \mathbf{A} = \mathbf{0}$</li>
<li>$f(c\mathbf{A}) = c f(\mathbf{A}), \forall c \in \mathbb{R}$</li>
<li>$f(\mathbf{A+b}) \le f(\mathbf{A}) + f(\mathbf{B})$</li>
</ul>
<p>其中，矩阵的2范数可以定义为</p>
<script type="math/tex; mode=display">\left\lVert\mathbf{A}\right\rVert_2 = \max{\sqrt{(\mathbf{A}x)^\dagger\mathbf{A}x}}</script><p>其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。</p>
<p>下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。</p>
<p>对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）：</p>
<script type="math/tex; mode=display">(Ax)^\dagger Ax = x^\dagger V \Sigma^\dagger \Sigma V^\dagger x</script><p>其中，$U^\dagger U = I$，已经被消去了。</p>
<p>进一步化简，我们将$V^\dagger x$看做一个整体，令$\omega = V\dagger x$，那么有，</p>
<script type="math/tex; mode=display">(Ax)^\dagger Ax = (\Sigma \omega)^\dagger \Sigma \omega</script><p>也就是说，矩阵的2范转换为了$\Sigma \omega$的幅值的最大值。由于$\omega$是酉矩阵和一个单位向量的乘积，所以$\omega$仍然是单位阵。</p>
<p>由于$\Sigma$是对角阵，所以$\omega$与其相乘后，相当于每个分量分别被放大了$\sigma_i$倍。即</p>
<script type="math/tex; mode=display">\Sigma \omega =
\begin{bmatrix}
\sigma_1 \omega_1\\\\
\sigma_2 \omega_2\\\\
\cdots\\\\
\sigma_n \omega_n
\end{bmatrix}</script><p>它的幅值平方为</p>
<script type="math/tex; mode=display">\left\lVert \Sigma \omega \right \rVert ^2 = \sum_{i=1}^{n}\sigma_i^2 \omega_i^2 \le \sigma_{1} \sum_{i=1}^{n}\omega_i^2 = \sigma_1^2</script><p>当且仅当，$\omega_1 = 1$, $\omega_k = 0, k &gt; 1$时取得等号。</p>
<p>综上所述，矩阵2范数的值等于其最大的奇异值。</p>
<p>矩阵的另一种范数定义方法Frobenius norm定义如下：</p>
<script type="math/tex; mode=display">\left\lVert A \right\rVert_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}\left\vert a_{i,j}\right\rvert}</script><p>如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式：</p>
<script type="math/tex; mode=display">\left\lVert A\right \rVert_F^2 = \text{trace}(A^\dagger A)</script><p>利用矩阵的SVD分解，可以很容易得出，$\text{trace}(A^\dagger A) = \sum_{i=1}^{r}\sigma_i^2$</p>
<p>说明如下：</p>
<script type="math/tex; mode=display">\text{trace}(A^\dagger A) = \text{trace}(V\Sigma^\dagger\Sigma V^\dagger)</script><p>由于$V^\dagger = V^{-1}$，而且$\text{trace}(BAB^{-1}) = \text{trace}(A)$，所以，</p>
<script type="math/tex; mode=display">\text{trace}(A^\dagger A) = \text{trace}(\Sigma^\dagger \Sigma) = \sum_{i=1}^{r}\sigma_i^2</script><p>也就是说，矩阵的F范数等于它的奇异值平方和的平方根。</p>
<script type="math/tex; mode=display">\left\lVert A\right\rVert_F= \sqrt{\sum_{i=1}^{r}\sigma_i^2}</script>]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>CS131-线代基础</title>
    <url>/2017/01/22/cs131-linear-alg/</url>
    <content><![CDATA[<p>CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，<a href="http://vision.stanford.edu/teaching/cs131_fall1617/index.html" target="_blank" rel="noopener">该课程</a>目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。</p>
<p><em>2018/03/20 Update: 这门课的2017秋季课程已经全部放出来，和上个版本相比，作业采用Python实现，同时加入了更多机器学习的内容。详细内容见：<a href="http://vision.stanford.edu/teaching/cs131_fall1718/" target="_blank" rel="noopener">CS131 Computer Vision@Fall 2017</a></em></p>
<p>由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前<a href="http://vision.stanford.edu/teaching/cs131_fall1617/lectures/lecture2_linalg_review_cs131_2016.pdf" target="_blank" rel="noopener">线代基础</a>的复习与整理。<br><img src="/img/cs131_linear_algebra.jpg" alt="线性代数词云"></p>
<a id="more"></a>
<h2 id="向量与矩阵"><a href="#向量与矩阵" class="headerlink" title="向量与矩阵"></a>向量与矩阵</h2><p>数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。<br>slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。</p>
<h2 id="矩阵作为线性变换"><a href="#矩阵作为线性变换" class="headerlink" title="矩阵作为线性变换"></a>矩阵作为线性变换</h2><p>通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。</p>
<h3 id="scale变换"><a href="#scale变换" class="headerlink" title="scale变换"></a>scale变换</h3><p>对角阵可以用来表示放缩变换。</p>
<script type="math/tex; mode=display">
\begin{bmatrix}
s_x & 0\\\\
0 & s_y
\end{bmatrix}\begin{bmatrix}
x\\\\
y
\end{bmatrix} = \begin{bmatrix}
s_xx\\\\
s_yy
\end{bmatrix}</script><h3 id="旋转变换"><a href="#旋转变换" class="headerlink" title="旋转变换"></a>旋转变换</h3><p>如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为：<br><img src="/img/rotation.png" alt="旋转变换"></p>
<script type="math/tex; mode=display">
\mathbf{R} = \begin{bmatrix}
\cos\theta &-\sin\theta \\\\
\sin\theta &\cos\theta
\end{bmatrix}</script><p>旋转矩阵是<a href="https://zh.wikipedia.org/wiki/酉矩阵" target="_blank" rel="noopener">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>
<script type="math/tex; mode=display">
\mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}</script><p>由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$<br>旋转矩阵是<a href="https://zh.wikipedia.org/wiki/酉矩阵" target="_blank" rel="noopener">酉矩阵</a>，矩阵内的各列（或者各行）相互正交。满足如下的关系式：</p>
<script type="math/tex; mode=display">
\mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}</script><p>由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$.</p>
<h3 id="齐次变换-Homogeneous-Transform"><a href="#齐次变换-Homogeneous-Transform" class="headerlink" title="齐次变换(Homogeneous Transform)"></a>齐次变换(Homogeneous Transform)</h3><p>只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。</p>
<script type="math/tex; mode=display">
\mathbf{H} =\begin{bmatrix}
a & b & t_x\\\\
c & d & t_y\\\\
0 & 0 & 1
\end{bmatrix},\mathbf{H}\begin{bmatrix}
x\\\\
y\\\\
1\\\\
\end{bmatrix}=\begin{bmatrix}
ax+by+t_x\\\\
cx+dy+t_y\\\\
1
\end{bmatrix}</script><h3 id="SVD分解"><a href="#SVD分解" class="headerlink" title="SVD分解"></a>SVD分解</h3><p>可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积：</p>
<script type="math/tex; mode=display">\mathbf{U}\mathbf{\Sigma}\mathbf{V^\dagger} = \mathbf{A}</script><p>其中矩阵$\mathbf{A}$大小为$m\times n$，矩阵$\mathbf{U}$是大小为$m\times m$的酉矩阵，$\mathbf{V}$是大小为$n \times n$的酉矩阵，$\mathbf{\Sigma}$是大小为$m \times n$的旋转矩阵，即只有主对角元素不为0.</p>
<p>SVD分解在主成分分析中年很有用。由于矩阵$\mathbf{\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。</p>
<p>如下图，是使用前10个分量对原图片进行压缩的效果。</p>
<figure class="highlight matlab"><table><tr><td class="code"><pre><span class="line">im = imread(<span class="string">'./superman.png'</span>);</span><br><span class="line">im_gray = rbg2gray(im);</span><br><span class="line">[u, s, v] = svd(double(im_gray));</span><br><span class="line">k = <span class="number">10</span>;</span><br><span class="line">uk = u(:, <span class="number">1</span>:k);</span><br><span class="line">sigma = <span class="built_in">diag</span>(s);</span><br><span class="line">sk = <span class="built_in">diag</span>(sigma(<span class="number">1</span>:k));</span><br><span class="line">vk = v(:, <span class="number">1</span>:k);</span><br><span class="line">im_k = uk*sk*vk';</span><br><span class="line">imshow(uint8(im_k))</span><br></pre></td></tr></table></figure>
<p><img src="/img/original_superman.png" alt="原始图像"><br><img src="/img/svd_superman.png" alt="压缩图像"></p>
]]></content>
      <tags>
        <tag>cs131</tag>
        <tag>公开课</tag>
      </tags>
  </entry>
  <entry>
    <title>Windows环境下使用Doxygen生成注释文档</title>
    <url>/2016/12/16/use-doxygen/</url>
    <content><![CDATA[<p>Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。</p>
<p><img src="/img/doxygen_picture.png" alt="Doxygen"><br><a id="more"></a></p>
<h2 id="安装-Doxygen"><a href="#安装-Doxygen" class="headerlink" title="安装 Doxygen"></a>安装 Doxygen</h2><p>Doxygen 在Windows平台下的安装比较简单，<a href="http://www.doxygen.nl/" target="_blank" rel="noopener">Doxygen的项目主页</a>提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。</p>
<p>安装成功后，使用命令行命令</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">doxygen --<span class="built_in">help</span></span><br></pre></td></tr></table></figure>
<p>就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。</p>
<p>使用命令，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">doxygen -g doxygen_filename</span><br></pre></td></tr></table></figure>
<p>就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。</p>
<p>使用命令，</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">doxygen doxygen_filename</span><br></pre></td></tr></table></figure>
<p>就可以生成注释文档了。</p>
<p>下面就来说一说对中文的支持。</p>
<h2 id="生成-HTML-格式文档"><a href="#生成-HTML-格式文档" class="headerlink" title="生成 HTML 格式文档"></a>生成 HTML 格式文档</h2><p>中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。</p>
<p>我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。</p>
<p>这样一来，编译出来的 HTML 页面就不会有中文乱码了。</p>
<h2 id="生成Latex-格式文档"><a href="#生成Latex-格式文档" class="headerlink" title="生成Latex 格式文档"></a>生成Latex 格式文档</h2><p>生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。</p>
<p>可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。</p>
<p>打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 <code>\begin{document}</code>一行，将其改为</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\begin&#123;document&#125;</span><br><span class="line">\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125;</span><br></pre></td></tr></table></figure>
<p>也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。</p>
<p>相应的，我们要将结尾的 <code>\end{document)</code>改为：<br><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">\end&#123;CJK&#125;</span><br><span class="line">\end&#123;document&#125;</span><br></pre></td></tr></table></figure></p>
<p>这样，运行make命令之后，就可以看到中文的注释文档了。</p>
]]></content>
      <tags>
        <tag>tool</tag>
        <tag>doxygen</tag>
      </tags>
  </entry>
  <entry>
    <title>使用 Visual Studio 编译 GSL 科学计算库</title>
    <url>/2016/12/16/gsl-with-vs/</url>
    <content><![CDATA[<p>GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。</p>
<p><img src="/img/gsl_picture.jpg" alt="GSL is GNU Sentific Library"><br><a id="more"></a><br><a href="http://www.gnu.org/software/gsl/" target="_blank" rel="noopener">GSL 的项目主页</a>提供的说明来看，GSL支持如下的科学计算：</p>
<p>（下面的这张表格的HTML使用的是<a href="http://pressbin.com/tools/excel_to_html_table/index.html" target="_blank" rel="noopener">No-Cruft Excel to HTML Table Converter</a>生成的）<br>
<table>
   <tr>
      <td>Complex Numbers </td>
      <td>Roots of Polynomials</td>
   </tr>
   <tr>
      <td>Special Functions </td>
      <td>Vectors and Matrices</td>
   </tr>
   <tr>
      <td>Permutations </td>
      <td>Sorting</td>
   </tr>
   <tr>
      <td>BLAS Support </td>
      <td>Linear Algebra</td>
   </tr>
   <tr>
      <td>Eigensystems </td>
      <td>Fast Fourier Transforms</td>
   </tr>
   <tr>
      <td>Quadrature </td>
      <td>Random Numbers</td>
   </tr>
   <tr>
      <td>Quasi-Random Sequences </td>
      <td>Random Distributions</td>
   </tr>
   <tr>
      <td>Statistics </td>
      <td>Histograms</td>
   </tr>
   <tr>
      <td>N-Tuples </td>
      <td>Monte Carlo Integration</td>
   </tr>
   <tr>
      <td>Simulated Annealing </td>
      <td>Differential Equations</td>
   </tr>
   <tr>
      <td>Interpolation </td>
      <td>Numerical Differentiation</td>
   </tr>
   <tr>
      <td>Chebyshev Approximation </td>
      <td>Series Acceleration</td>
   </tr>
   <tr>
      <td>Discrete Hankel Transforms </td>
      <td>Root-Finding</td>
   </tr>
   <tr>
      <td>Minimization </td>
      <td>Least-Squares Fitting</td>
   </tr>
   <tr>
      <td>Physical Constants </td>
      <td>IEEE Floating-Point</td>
   </tr>
   <tr>
      <td>Discrete Wavelet Transforms </td>
      <td>Basis splines</td>
   </tr>
</table>
</p>
<p>GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO!</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">./configure</span><br><span class="line">make</span><br><span class="line">make install</span><br><span class="line">make clean</span><br></pre></td></tr></table></figure>
<p>同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。</p>
<h2 id="使用CMAKE编译成-SLN文件"><a href="#使用CMAKE编译成-SLN文件" class="headerlink" title="使用CMAKE编译成.SLN文件"></a>使用CMAKE编译成.SLN文件</h2><p>打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。</p>
<h2 id="使用Visual-Studio生成解决方案"><a href="#使用Visual-Studio生成解决方案" class="headerlink" title="使用Visual Studio生成解决方案"></a>使用Visual Studio生成解决方案</h2><p>使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。</p>
<p>当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\bin，\gsl，\Debug和\Release。</p>
<h2 id="加入环境变量"><a href="#加入环境变量" class="headerlink" title="加入环境变量"></a>加入环境变量</h2><p>修改环境变量的Path，将\GSL_Build_Path\bin\Debug加入，这主要是为了\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。</p>
<p>这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。</p>
<h2 id="建立Visual-Studio属性表"><a href="#建立Visual-Studio属性表" class="headerlink" title="建立Visual Studio属性表"></a>建立Visual Studio属性表</h2><p>Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 <a href="http://my.phirobot.com/blog/2014-02-opencv_configuration_in_vs.html" target="_blank" rel="noopener">Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)</a>。</p>
<p>配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。</p>
<figure class="highlight html"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="utf-8"?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Project</span> <span class="attr">ToolsVersion</span>=<span class="string">"4.0"</span> <span class="attr">xmlns</span>=<span class="string">"http://schemas.microsoft.com/developer/msbuild/2003"</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ImportGroup</span> <span class="attr">Label</span>=<span class="string">"PropertySheets"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">PropertyGroup</span> <span class="attr">Label</span>=<span class="string">"UserMacros"</span> /&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">PropertyGroup</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">IncludePath</span>&gt;</span>$(OPENCV249)\include;E:\GSLCode\gsl-build\;$(IncludePath)<span class="tag">&lt;/<span class="name">IncludePath</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">LibraryPath</span> <span class="attr">Condition</span>=<span class="string">"'$(Platform)'=='Win32'"</span>&gt;</span>$(OPENCV249)\x86\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)<span class="tag">&lt;/<span class="name">LibraryPath</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">LibraryPath</span> <span class="attr">Condition</span>=<span class="string">"'$(Platform)'=='X64'"</span>&gt;</span>$(OPENCV249)\x64\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)<span class="tag">&lt;/<span class="name">LibraryPath</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">PropertyGroup</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ItemDefinitionGroup</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Link</span> <span class="attr">Condition</span>=<span class="string">"'$(Configuration)'=='Debug'"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">AdditionalDependencies</span>&gt;</span>opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class="tag">&lt;/<span class="name">AdditionalDependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Link</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">Link</span> <span class="attr">Condition</span>=<span class="string">"'$(Configuration)'=='Release'"</span>&gt;</span></span><br><span class="line">          <span class="tag">&lt;<span class="name">AdditionalDependencies</span>&gt;</span>opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)<span class="tag">&lt;/<span class="name">AdditionalDependencies</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">Link</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;/<span class="name">ItemDefinitionGroup</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">ItemGroup</span> /&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">Project</span>&gt;</span></span><br></pre></td></tr></table></figure>
<p>在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！</p>
<h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;gsl/gsl_sf_bessel.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">(<span class="keyword">void</span>)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">	<span class="keyword">double</span> x = <span class="number">5.0</span>;</span><br><span class="line">	<span class="keyword">double</span> y = gsl_sf_bessel_J0(x);</span><br><span class="line">	<span class="built_in">printf</span>(<span class="string">"J0(%g) = %.18e\n"</span>, x, y);</span><br><span class="line">	<span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>控制台输出正确：<br>
</p><p><img src="http://i.imgur.com/uXhVvwS.jpg" width="600" height="200"></p>
<p></p>
]]></content>
      <tags>
        <tag>tool</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2016/12/16/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<p><img src="/img/helloworld_hexo.png" alt="Hexo"><br><a id="more"></a></p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<h3 id="Code-highlight"><a href="#Code-highlight" class="headerlink" title="Code highlight"></a>Code highlight</h3><p>Hello World!</p>
<figure class="highlight cpp"><table><tr><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="built_in">std</span>::<span class="built_in">cout</span> &lt;&lt; <span class="string">"HelloWorld\n"</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="keyword">print</span> <span class="string">'HelloWorld'</span></span><br></pre></td></tr></table></figure>
<h3 id="Latex-Support-by-Mathjax"><a href="#Latex-Support-by-Mathjax" class="headerlink" title="Latex Support by Mathjax"></a>Latex Support by Mathjax</h3><p>Mass-energy equation by Einstein: $E = mc^2$</p>
<p>a linear equation:</p>
<script type="math/tex; mode=display">\mathbf{A}\mathbf{v} = \mathbf{y}</script>]]></content>
      <tags>
        <tag>扯淡</tag>
      </tags>
  </entry>
  <entry>
    <title>Python Regular Expressions （Python 正则表达式)</title>
    <url>/2014/07/17/python-reg-exp/</url>
    <content><![CDATA[<p>本文来自于Google Developers中对于Python的介绍。<a href="https://developers.google.com/edu/python/regular-expressions" title="Google Python Class, Regular Expression" target="_blank" rel="noopener">https://developers.google.com/edu/python/regular-expressions</a>。</p>
<p><img src="/img/regex_picture.jpg" alt="regex"><br><a id="more"></a></p>
<h2 id="认识正则表达式"><a href="#认识正则表达式" class="headerlink" title="认识正则表达式"></a>认识正则表达式</h2><p>Python的正则表达式是使用 <strong>re 模块</strong>的。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">match = re.search(pattern,str)</span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">	<span class="keyword">print</span> <span class="string">'found'</span>,match.group()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">'NOT Found!'</span></span><br></pre></td></tr></table></figure>
<h2 id="正则表达式的规则"><a href="#正则表达式的规则" class="headerlink" title="正则表达式的规则"></a>正则表达式的规则</h2><h3 id="基本规则"><a href="#基本规则" class="headerlink" title="基本规则"></a>基本规则</h3><ul>
<li>a, x, 9 都是普通字符 (ordinary characters)</li>
<li>. (一个点)可以匹配任何单个字符（除了’\n’）</li>
<li>\w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\W （大写的W）可以匹配非单词里的这些元素</li>
<li>\b 匹配单词与非单词的分界</li>
<li>\s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\n\r\t\f)；\S（大写的S）匹配一个非 whitespace character</li>
<li>\d 匹配十进制数字 [0-9]</li>
<li>^=start，$=end 用来匹配字符串的开始和结束</li>
<li>\ 是转义字符，用 . 来匹配串里的’.’，等<h3 id="一些基本的例子"><a href="#一些基本的例子" class="headerlink" title="一些基本的例子"></a>一些基本的例子</h3></li>
</ul>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## 在字符串'piiig'中查找'iii'</span></span><br><span class="line">match = re.search(<span class="string">r'iii'</span>, <span class="string">'piiig'</span>)  <span class="comment"># found, match.group() == "iii"</span></span><br><span class="line">match = re.search(<span class="string">r'igs'</span>, <span class="string">'piiig'</span>)  <span class="comment">#  not found, match == None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## . 匹配除了\n的任意字符</span></span><br><span class="line">match = re.search(<span class="string">r'..g'</span>, <span class="string">'piiig'</span>)  <span class="comment">#  found, match.group() == "iig"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## \d 匹配0-9的数字字符, \w 匹配单词里的字符</span></span><br><span class="line">match = re.search(<span class="string">r'\d\d\d'</span>, <span class="string">'p123g'</span>) <span class="comment">#  found, match.group() == "123"</span></span><br><span class="line">match = re.search(<span class="string">r'\w\w\w'</span>, <span class="string">'@@abcd!!'</span>) <span class="comment">#  found, match.group() == "abc"</span></span><br></pre></td></tr></table></figure>
<h3 id="重复"><a href="#重复" class="headerlink" title="重复"></a>重复</h3><p>可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。</p>
<ul>
<li>‘+’ 用来匹配1个或者多个字符</li>
<li>‘*’ 用来匹配0个或者多个字符</li>
<li>‘?’ 用来匹配0个或1个字符</li>
</ul>
<p>注意，’+’和’*’会匹配尽可能多的字符。</p>
<h3 id="一些重复字符的例子"><a href="#一些重复字符的例子" class="headerlink" title="一些重复字符的例子"></a>一些重复字符的例子</h3><figure class="highlight py"><table><tr><td class="code"><pre><span class="line"><span class="comment">## i+  匹配1个或者多个'i'</span></span><br><span class="line">match = re.search(<span class="string">r'pi+'</span>, <span class="string">'piiig'</span>) <span class="comment">#  found, match.group() == "piii"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## 找到字符串中最左边尽可能长的模式。</span></span><br><span class="line"><span class="comment">## 注意，并没有匹配到第二个 'i+'</span></span><br><span class="line">match = re.search(<span class="string">r'i+'</span>, <span class="string">'piigiiii'</span>)  <span class="comment">#  found, match.group() == "ii"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## \s*  匹配0个或1个空白字符 whitespace</span></span><br><span class="line">match = re.search(<span class="string">r'\d\s*\d\s*\d'</span>, <span class="string">'xx1 2   3xx'</span>)  <span class="comment">#  found, match.group() == "1 2   3"</span></span><br><span class="line">match = re.search(<span class="string">r'\d\s*\d\s*\d'</span>, <span class="string">'xx12  3xx'</span>)    <span class="comment">#  found, match.group() == "12  3"</span></span><br><span class="line">match = re.search(<span class="string">r'\d\s*\d\s*\d'</span>, <span class="string">'xx123xx'</span>)      <span class="comment"># found, match.group() == "123"</span></span><br><span class="line"></span><br><span class="line"><span class="comment">## ^ 匹配字符串的第一个字符</span></span><br><span class="line">match = re.search(<span class="string">r'^b\w+'</span>, <span class="string">'foobar'</span>)  <span class="comment"># not found, match == None</span></span><br><span class="line"><span class="comment">## 与上例对比</span></span><br><span class="line">match = re.search(<span class="string">r'b\w+'</span>, <span class="string">'foobar'</span>)   <span class="comment"># found, match.group() == "bar"</span></span><br></pre></td></tr></table></figure>
<h3 id="Email"><a href="#Email" class="headerlink" title="Email"></a>Email</h3><p>考虑一个典型的Email地址：<code>someone@host.com</code>，可以用如下的方式匹配：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">match = re.search(<span class="string">r'\w+@\w+'</span>,str)</span><br></pre></td></tr></table></figure>
<p>但是，对于这种Email地址 <code>xyz alice-b@google.com purple monkey</code>则不能奏效。</p>
<h3 id="使用方括号"><a href="#使用方括号" class="headerlink" title="使用方括号"></a>使用方括号</h3><p>方括号里面的字符表示一个字符集合。[abc]可以被用来匹配’a’或者’b’或者’c’。\w \s等都可以用在方括号里，除了’.’以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下：</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">match = re.search(<span class="string">'r[\w.-]+@[\w.-]+'</span>,str)</span><br></pre></td></tr></table></figure>
<p>你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如<sup><a href="#fn_ab" id="reffn_ab">ab</a></sup>表示除了’a’和’b’之外的其他字符。</p>
<h2 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h2><p>以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？<br>可以在模式中用圆括号指定。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">str = <span class="string">'purple alice-b@google.com monkey dishwasher'</span></span><br><span class="line">match = re.search(<span class="string">'([\w.-]+)@([\w.-]+)'</span>, str)   <span class="comment">#用圆括号指定分割</span></span><br><span class="line"><span class="keyword">if</span> match:</span><br><span class="line">    <span class="keyword">print</span> match.group()   <span class="comment">## 'alice-b@google.com' (the whole match)</span></span><br><span class="line">    <span class="keyword">print</span> match.group(<span class="number">1</span>)  <span class="comment">## 'alice-b' (the username, group 1)</span></span><br><span class="line">  	<span class="keyword">print</span> match.group(<span class="number">2</span>)  <span class="comment">## 'google.com' (the host, group 2)</span></span><br></pre></td></tr></table></figure>
<h3 id="findall-函数"><a href="#findall-函数" class="headerlink" title="findall 函数"></a>findall 函数</h3><p>与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">str = <span class="string">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></span><br><span class="line"><span class="comment">## findall返回一个包含所有匹配结果的 list</span></span><br><span class="line">emails = re.findall(<span class="string">r'[\w\.-]+@[\w\.-]+'</span>, str) <span class="comment">## ['alice@google.com', 'bob@abc.com']</span></span><br><span class="line"><span class="keyword">for</span> email <span class="keyword">in</span> emails:</span><br><span class="line">    <span class="keyword">print</span> email</span><br></pre></td></tr></table></figure>
<h3 id="在文件中使用findall"><a href="#在文件中使用findall" class="headerlink" title="在文件中使用findall"></a>在文件中使用findall</h3><p>当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">f = open(filename.txt,<span class="string">'r'</span>)</span><br><span class="line">matches = re.findall(pattern,f.read())</span><br></pre></td></tr></table></figure>
<h3 id="findall-和分组"><a href="#findall-和分组" class="headerlink" title="findall 和分组"></a>findall 和分组</h3><p>和group的用法相似，也可以指定分组。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">str = <span class="string">'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'</span></span><br><span class="line"><span class="comment">##　返回了一个list</span></span><br><span class="line">tuples = re.findall(<span class="string">r'([\w\.-]+)@([\w\.-]+)'</span>, str)</span><br><span class="line"><span class="keyword">print</span> tuples  <span class="comment">## [('alice', 'google.com'), ('bob', 'abc.com')]</span></span><br><span class="line"><span class="comment">##　list中的元素是tuple</span></span><br><span class="line"><span class="keyword">for</span> tuple <span class="keyword">in</span> tuples:</span><br><span class="line">  <span class="keyword">print</span> tuple[<span class="number">0</span>]  <span class="comment">## username</span></span><br><span class="line">  <span class="keyword">print</span> tuple[<span class="number">1</span>]  <span class="comment">## host</span></span><br></pre></td></tr></table></figure>
<h2 id="调试"><a href="#调试" class="headerlink" title="调试"></a>调试</h2><p>正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。</p>
<h2 id="其他选项"><a href="#其他选项" class="headerlink" title="其他选项"></a>其他选项</h2><p>正则表达式还可以设置“选项”。</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">match = re.search(pat,str,opt)</span><br></pre></td></tr></table></figure>
<p>这些可选项如下：</p>
<ul>
<li>IGNORECASE  忽视大小写</li>
<li>DOTALL  允许’.’匹配’\n’</li>
<li>MULTILINE  在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束</li>
</ul>
]]></content>
      <tags>
        <tag>python</tag>
      </tags>
  </entry>
</search>
