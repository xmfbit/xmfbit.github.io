<?xml version="1.0" encoding="utf-8"?>
<search>
  
    
    <entry>
      <title><![CDATA[MIT Missing Semester - Shell]]></title>
      <url>%2F2020%2F03%2F13%2Fmit-missing-semester-02-shell%2F</url>
      <content type="text"><![CDATA[工欲善其事，必先利其器。MIT Missing Semester就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！ 这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。 这里我跳过了第一节课，直接从bash shell开始。 一些零散的点 bash中双引号和单引号的区别 双引号会发生变量替换，单引号不会。 123456foo=bar# output: hello, barecho "hello, $foo"# output: hello, $fooecho 'hello, $foo' bangbang 使用!!可以执行上一条命令。 bash 中的特殊变量 123456789101112$? # 上一条命令的返回值，正常退出是0，否则是非0$@ # 所有输入的参数$# # 输入参数的个数$$ # pid of current script# 检查上条命令是否正常退出if [ $? -ne 0 ]; then echo "fail"else echo "success"fi 如何忽略命令的输出 有的时候，我们只想要命令的返回值。例如使用grep foo bar来查看文件bar中是否含有字符串foo，可以将标准输出和标准错误重定向到/dev/null。 123456789101112# 第一个是标准输出，第二个是标准错误grep "foo" bar &gt; /dev/null 2&gt; /dev/null# 或者可以这样：# grep "name" test_lazy.cpp 2&gt;&amp;1 &gt; /dev/nullif [ "$?" -ne 0 ]; then echo "found foo in bar"else echo "not found foo in bar"fi globbing 任意字符：* 单个字符：? 使用{}给定可选元素的集合。 123456a.&#123;hpp,cpp&#125; =&gt; a.hpp a.cppa&#123;,0,1,2&#125; =&gt; a a0 a1 a2# 支持多层级touch proj&#123;1,2&#125;/&#123;a,b&#125;.txt# 还支持rangetouch proj&#123;1,2&#125;/&#123;a..g&#125;.txt bash 中的函数 如何写函数 1234mycd () &#123; cd $1 pwd&#125; 如何在bash中导入脚本中的函数 123source your_bash_script.sh# then use the function defined in the bash script# 你可以这样理解：from your_bash_script import * for-loop遍历给定的元素序列使用for item in xxx; do yyy; done来遍历给定的序列，并施加具体操作于序列元素： 1234567for i in 1 2 3do echo "welcome $i"done# welcome 1# welcome 2# welcome 3 注意，列表元素是通过空格来隔离的。如果这样写 1for i in 1, 2, 3 那么最终输出也是welcome 1, welcome 2, welcome 3 还可以使用for-range的方法： 1for i in &#123;1..3&#125; c-style for-loop也可以像C语言那样使用for-loop： 1for (( Exp1; Exp2; Exp3)); do xxx; done 例如： 1for (( c=1; c&lt;=3; c++ )); do echo "welcome $c"; done 还可以使用这种风格构造无穷循环，for (( ; ; )); do xxx; done。 break / continue当满足一定条件时，使用break退出循环，或使用continue继续循环。 while除了for-loop，还可以使用while。 1while CONDITION; do xxx; done untiluntil和while的用法一致，不同点在于： while是CONDITION为true执行，当false是退出循环 until是CONDITION为false执行，当true时退出循环 12345c=1until [ $c -gt 3 ]; do echo "welcome $c" ((c++))done 数学表达式在上面for-loop中，已经看到了我们使用((exp))的形式进行数学表达式运算。一般来说，在bash shell中进行数学表达式的运算可以采用： 使用expr，如c=$(expr 1 + 1); echo $c，注意操作数与操作符之间都是有空格的。 使用let，如c=1; let c=$c+1; echo $c，注意操作数与操作符之间没有空格。 使用双括号(())，就像上面看到的那样：c=1; echo $((c += 1)); echo $c。这时候，操作数与操作符之间的空格可有可无。 最后一种双括号可能更为常用，支持的操作符：+/-/++/--/*/%/**，也支持逻辑运算符：&gt;=/&lt;=/&gt;/&lt;/==/!=/!/||/&amp;&amp;。 如果希望进行浮点数运算，bash本身是不支持的，可以使用bc命令，将表达式作为字符串传入就可以了： 12echo "1.0+2.0" | bcc=$(r=1.5;echo "$r + 2.5"|bc); echo $c 调试工具shellcheck可以用来帮助静态分析shell脚本。用法： 1shellcheck your_bash_script.sh 可以去网站上试用：Shellcheck 几个有用的命令这里列出一些常用的命令工具，都是和查找有关。更多内容，可以通过man或者tldr查看。 查找文件 find最常用的用法： 123456789# 递归地查找当前目录及子目录下所有的python文件find . -name="*.py"# -type d 表示过滤结果为所有目录# -type f 表示过滤结果为所有文件find . -name="test" -type d# Find all files modified in the last dayfind . -mtime -1# Find all zip files with size in range 500k to 10Mfind . -size +500k -size -10M -name '*.tar.gz' find还可以通过-exec来接后续处理，如： 12345# Delete all files with .tmp extension# 注意最后的 \find . -name '*.tmp' -exec rm &#123;&#125; \;# Find all PNG files and convert them to JPGfind . -name '*.png' -exec convert &#123;&#125; &#123;.&#125;.jpg \; 你也可以用fd作为find的改进版。具体用法可以参考fd，这里不再多说了。 locate如果你想按照名字去查找文件，还可以试试locate。一个简单的比较： locate只支持按名字查找，find可以更加多样 locate通过周期性更新的database来查找，时效性不如find locate使用更简单，默认会查找所有符合要求的文件，find一般是查找给定路径下的文件 由于上述原因，我一般是使用locate查找系统自带的某个lib等文件。比如有时候我可能不知道libcudart.so在哪里，这时候就可以通过locate libcudart.so来查找。 123456locate libcudart.so | grep "/usr"# output:# /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7# /usr/local/cuda-10.0/lib64/libcudart.so# ... 在文件中查找字符串 grepgrep 用来在文件中正则匹配字符串，比如某个变量或函数定义啥的。grep命令很有用，在后续课程中还会着重介绍。 12345# 在文件中查找xxx，并打印其所在的行grep xxx file# 在所有文件中递归地查找grep -R xxx . 常用的一些flag，可以是-C +number（用来显示match的context，number是行数），-v是反转（不包含所给pattern的行） 和find一样，grep也有一些更好用的替代品，如rg，ag，ack等。 12345678# Find all python files where I used the requests libraryrg -t py 'import requests'# Find all files (including hidden files) without a shebang linerg -u --files-without-match "^#!"# Find all matches of foo and print the following 5 linesrg foo -A 5# Print statistics of matches (# of matched lines and files )rg --stats PATTERN 查找历史命令 historyhistory可以打印历史的shell命令，和grep配合能够找到历史上曾经用过的某给定命令。不过这个我在使用zsh的时候，一般是通过光标上下键来联想查找的。 一个有用的工具：fzf（which means 模糊查找）。 另外，这里讲师推荐了一款基于历史命令的自动补全（看lecture时候觉得很酷）。如果你和我一样使用zsh，可以参考这个插件：zsh-autosuggestions。 关于目录因为shell环境下没有GUI，所以查看一个目录内的内容，包括跳转目录都很不方便。对此也有一些好用的工具： 查看目录内容：tree（最经典），broot，nnn，ranger 跳转目录：autojump（在用），fasd 课后习题关于ls的用法 Includes all files, including hidden files：ls -al Sizes are listed in human readable format (e.g. 454M instead of 454279954)：ls -lh Files are ordered by recency：ls -lt Output is colorized：ls -l --color （zsh自动colorized，所以这个没有验证） bash函数Write bash functions marco and polo that do the following. Whenever you execute marco the current working directory should be saved in some manner, then when you execute polo, no matter what directory you are in, polo should cd you back to the directory where you executed marco. For ease of debugging you can write the code in a file marco.sh and (re)load the definitions to your shell by executing source marco.sh. 12345678910#!/bin/bash# 使用文件存储要cd的pathmacro () &#123; echo "$(pwd)" &gt; $HOME/.macro.history&#125;polo () &#123; cd "$(cat "$HOME/.macro.history")" || exit 1&#125; 循环和程序返回值判断Say you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail. 123456789101112#!/bin/bashfor ((i=1; ; i++)); do # save the script to `fail_rarely.sh` ./fail_rarely.sh 2&amp;&gt; out.log if [ $? -ne 0 ]; then echo "fail after run $i times" echo "stdout and stderr message:" cat out.log break fidone xargs和管道As we covered in lecture find’s -exec can be very powerful for performing operations over the files we are searching for. However, what if we want to do something with all the files, like creating a zip file? As you have seen so far commands will take input from both arguments and STDIN. When piping commands, we are connecting STDOUT to STDIN, but some commands like tar take inputs from arguments. To bridge this disconnect there’s the xargs command which will execute a command using STDIN as arguments. For example ls | xargs rm will delete the files in the current directory. Your task is to write a command that recursively finds all HTML files in the folder and makes a zip with them. Note that your command should work even if the files have spaces (hint: check -d flag for xargs) xargs先来看下xargs和管道的区别。这里已经给了一个例子：ls | xargs rm。由于rm命令比较危险，所以下面会换成cat（删除文件变成了打印文件内容）。 为什么不能用管道呢，比如ls | cat。我们先建立一个空目录作为playground： 123mkdir testcd testecho "simgple test" &gt; a.txt 执行ls | cat，会发现它只是把当前目录下的所有文件名打印了出来，并没有打印a.txt的内容： 12ls | cat# a.txt 这是因为管道只是把STDOUT作为cat的STDIN。在linux中，STDOUT和STDIN是两个特殊的文件，ls将把它的输出结果写入到STDOUT中，同时我们就会在屏幕上看到对应输出。而cat从STDIN中接受输入。当没有管道时，由用户输入并写入STDIN。由于管道，cat将直接从STDOUT中读取。也就是ls的输出，也就是当前目录下的文件列表。拆解后想当于下面： 12ls &gt; stdout_lscat &lt; stdout_ls 所以，如果我们想要打印a.txt的内容，管道就不够用了。也就是上面说的，我们要把ls的输出作为cat的参数。这时候需要使用xargs： 12ls | xargs cat# simple test 准备先准备一些测试文件 123456mkdir htmlscd htmlsmkdir htmls/&#123;1..3&#125;touch htmls/1/1.htmltouch htmls/2/2\ 2.htmltouch htmls/root.html 实现题目说明中的-d没找到，在12 Practical Examples of Linux Xargs Command for Beginners找到了如下用法，使用-print0和-0（是数字0）配合，具体可以参考man xargs中的内容。 1234#-0 Change xargs to expect NUL (``\0'') characters as separators, instead of spaces and newlines. # This is expected to be used in concert with the -print0 function in find(1).find htmls -name "*.html" -print0 | xargs -0 tar vcf html.zip 命令组合Write a command or script to recursively find the most recently modified file in a directory. More generally, can you list all files by recency? 首先递归地列出当前目录下的所有文件，再使用ls -lt将其按照时间排序。 1234find -L . -type f -print0 | xargs -0 ls -lt# 如果只需要最新的那个，使用 head命令只打印第一行find -L . -type f -print0 | xargs -0 ls -lt | head -1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Debian9 编译Caffe的一个坑]]></title>
      <url>%2F2019%2F11%2F24%2Fcaffe-compiling-debian9%2F</url>
      <content type="text"><![CDATA[记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题： 12345In file included from /usr/local/cuda/include/cuda_runtime.h:120:0, from &lt;command-line&gt;:0:/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &quot;&quot;__CUDACC_VER__ is no longer supported. Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&quot;&quot; is not valid in preprocessor expressions #define __CUDACC_VER__ &quot;__CUDACC_VER__ is no longer supported. Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&quot; 如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。 解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个github issue，修改include/caffe/common.hpp即可。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - MetaPruning：Meta Learning for Automatic Neural Network Channel Pruning]]></title>
      <url>%2F2019%2F10%2F26%2Fpaper-meta-pruning%2F</url>
      <content type="text"><![CDATA[这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：MetaPruning。 这篇文章首发于Paper Weekly公众号，欢迎关注。 Motivation模型剪枝是一种能够减少模型大小和计算量的方法。模型剪枝一般可以分为三个步骤： 训练一个参数量较多的大网络 将不重要的权重参数剪掉 剪枝后的小网络做fine tune 其中第二步是模型剪枝中的关键。有很多paper围绕“怎么判断权重是否重要”以及“如何剪枝”等问题进行讨论。困扰模型剪枝落地的一个问题就是剪枝比例的确定。传统的剪枝方法常常需要人工layer by layer地去确定每层的剪枝比例，然后进行fine tune，用起来很耗时，而且很不方便。不过最近的Rethinking the Value of Network Pruning指出，剪枝后的权重并不重要，对于channel pruning来说，更重要的是找到剪枝后的网络结构，具体来说就是每层留下的channel数量。受这个发现启发，文章提出可以用一个PruningNet，对于给定的剪枝网络，自动生成weight，无需进行retrain，然后评测剪枝网络在验证集上的性能，从而选出最优的网络结构。 具体来说，PruningNet的输入是剪枝后的网络结构，必须首先对网络结构进行编码，转换为coding vector。这里可以直接用剪枝后网络每层的channel数来编码。在搜索剪枝网络的时候，我们可以尝试各种coding vector，用PruningNet生成剪枝后的网络权重。网络结构和权重都有了，就可以去评测网络的性能。进而用进化算法搜索最优的coding vector，也就是最优的剪枝结构。在用进化算法搜索的时候，可以使用自定义的目标函数，包括将网络的accuracy，latency，FLOPS等考虑进来。 PruningNet从上一小节已经可以知道，PruningNet是整个算法的关键。那么怎么才能找到这样一个“神奇网络”呢？ 先做一下符号约定，使用$c_i$表示剪枝之后第$i$层的channel数量，$l$为网络的层数，$W$表示剪枝后网络的权重。那么PruningNet的输入输出如下所示： $W = \text{PruningNet}(c_1, c_2, \dots, c_l)$ 训练先结合下图看一下forward部分。PruningNet是由$l$个PruningBlock组成的，每个PruningBlock是一个两层的MLP。首先看图b，编码着网络结构信息的coding vector输入到当前block后，输出经过Reshape，成了一个Weight Matrix。注意哦，这里的WeightMatrix是固定大小的（也就是未剪枝的原始Weight shape大小），和剪枝网络结构无关。再看图a，因为要对网络进行剪枝，所以WeightMatrix要进行Crop。对应到图b，可以看到，Crop是在两个维度上进行的。首先，由于上一层也进行了剪枝，所以input channel数变少了；其次，由于当前层进行了剪枝，所以output channel数变少了。这样经过Crop，就生成了剪枝后的网络weight。我们再输入一个mini batch的训练图片，就可以得到剪枝后的网络的loss。 在backward部分，我们不更新剪枝后网络的权重，而是更新PruningNet的权重。由于上面的操作都是可微分的，所以直接用链式法则传过去就行。如果你使用PyTorch等支持自动微分的框架，这是很容易的。 下图所示是训练过程的整个PruningNet（左侧）和剪枝后网络（右侧，即PrunedNet）。训练过程中的coding vector在状态空间里随机采样，随机选取每层的channel数量。 PS：和原始论文相比，下图和上图顺序是颠倒的。这里从底向上介绍了PruningNet的训练，而论文则是自顶向下。 搜索训练好PruningNet后，就可以用它来进行搜索了！我们只需要输入某个coding vector，PruningNet就会为我们生成对应每层的WeightMatrix。别忘了coding vector是编码的网络结构，现在又有了weight，我们就可以在验证集上测试网络的性能了。进而，可以使用进化算法等优化方法去搜索最优的coding vector。当我们得到了最优结构的剪枝网络后，再from scratch地训练它。 进化算法这里不再赘述，很多优化的书中包括网上都有资料。这里把整个算法流程贴出来： 实验作者在ImageNet上用MobileNet和ResNet进行了实验。训练PruningNet用了$\frac{1}{4}$的原模型的epochs。数据增强使用常见的标准流程，输入image大小为$224\times 224$。 将原始ImageNet的训练集做分割，每个类别选50张组成sub-validation（共计50000），其余作为sub-training。在训练时，我们使用sub-training训练PruningNet。在搜索时，使用sub-validation评估剪枝网络的性能。不过，还要注意，在搜索时，使用20000张sub-training中的图片重新计算BatchNorm layer中的running mean和running variance。 shortcut剪枝在进行模型剪枝时，一个比较难处理的问题是ResNet中的shortcut结构。因为最后有一个element-wise的相加操作，必须保证两路feature map是严格shape相同的，所以不能随意剪枝，否则会造成channel不匹配。下面对几种论文中用到的网络结构分别讨论。 MobileNet-v1MobileNet-v1是没有shortcut结构的。我们为每个conv layer都配上相应的PruningBlock——一个两层的MLP。PruningNet的输入coding vector中的元素是剪枝后每层的channel数量。而输入第$i$个PruningBlock的是一个2D vector，由归一化的第$i-1$层和第$i$层的剪枝比例构成。这部分可以结合代码MetaPruning来看。注意第$1$个conv layer的输入是1D vector，因为它是第一个被剪枝的layer。在训练时，coding vector的搜索空间被以一定步长划分为grid，采样就是在这些格点上进行的。 MobileNet-v2MobileNet-v2引入了类似ResNet的shortcut结构，这种resnet block必须统一看待。具体来说，对于没有在resnet block中的conv，处理方法如MobileNet-v1。对每个resnet block，配上一个相应的PruningBlock。由于每个resnet block中只有一个中间层（$3\times 3$的conv），所以输出第$i$个PruningBlock的是一个3D vector，由归一化的第$i-1$个resnet block，第$i$个resnet block和中间conv层的剪枝比例构成。其他设置和MobileNet-v1相同。这里可以结合代码MetaPruning来看。 ResNet处理方法如MobileNet-v2所示。可以结合代码MetaPruning来看。 实验结果在相近FLOPS情况下，和MobileNet论文中改变ratio参数得到的模型比较，MetaPruning得到的模型accuracy更高。尤其是压缩比例更大时，该方法更有优势。 和其他剪枝方法（如AMC）等方法比较，该方法也得到了SOTA的结果。MetaPruning方法能够以一种统一的方法处理ResNet中的shortcut结构，并且不需要人工调整太多的参数。 上面的比较都是基于理论FLOPS，现在更多人在关注网络在实际硬件上的latency怎么样。文章对此也进行了讨论。如何测试网络的latency？当然可以每个网络都实际跑一下，不过有些麻烦。基于每个layer的inference时间是互相独立的这个假设，作者首先构造了各个layer inference latency的查找表（参见论文Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search），以此来估计实际网络的latency。作者这里和MobileNet baseline做了比较，结果也证明了该方法更优。 PruningNet结果分析此外，作者还对PruningNet的预测结果进行可视化，试图找出一些可解释性，并找出剪枝参数的一些规律。 down-sampling的部分PruningNet倾向于保留更多的channel，如MobileNet-v2 block中间的那个conv 优先剪浅层layer的channel，FLOPS约束太强剪深层的channel，但可能会造成网络accuracy下降比较多 结论这篇文章从“剪枝后的weight作用不大”的现象出发，将剪枝和NAS结合，提出了PruningNet为剪枝后的网络预测weight，避免了网络的retrain，从而可以快速衡量剪枝网络的性能。并在编码网络信息的coding vector状态空间进行搜索，找到给定约束条件下的最优网络结构，在ImageNet数据集和ResNet/MobileNet-v1/v2上取得了比之前剪枝算法更好的效果。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Bag of Tricks for Image Classification with Convolutional Neural Networks]]></title>
      <url>%2F2019%2F07%2F06%2Fbag-of-tricks-for-image-cls%2F</url>
      <content type="text"><![CDATA[这是Bag of Tricks for Image Classification with Convolutional Neural Networks的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。 简介这篇文章主要讨论了训练图片分类模型的tricks，包括data augmentation，（lr，batch size等）超参设置，模型架构微调和模型蒸馏等技术。可以在增加少许计算量的情况下，把ResNet-50的top 1 acc提升4个点，从而打败许多后起之秀。Talk is cheap, show me the code. 论文讨论的方法对应代码，都已经在GluonCV中开源，所以建议在阅读论文的时候，对照代码进行学习。 Baseline Training这里介绍了一些（已经不算trick的）训练ResNet-50可以注意的地方。使用这些方法，应该可以复现论文中给出的结果。 Data Argumentation这里都是老生常谈了，可以直接参看代码gluon cv/image classification。 12345678910111213141516171819202122232425262728jitter_param = 0.4lighting_param = 0.1mean_rgb = [123.68, 116.779, 103.939]std_rgb = [58.393, 57.12, 57.375]train_data = mx.io.ImageRecordIter( path_imgrec = rec_train, path_imgidx = rec_train_idx, preprocess_threads = num_workers, shuffle = True, batch_size = batch_size, data_shape = (3, input_size, input_size), mean_r = mean_rgb[0], mean_g = mean_rgb[1], mean_b = mean_rgb[2], std_r = std_rgb[0], std_g = std_rgb[1], std_b = std_rgb[2], rand_mirror = True, random_resized_crop = True, max_aspect_ratio = 4. / 3., min_aspect_ratio = 3. / 4., max_random_area = 1, min_random_area = 0.08, brightness = jitter_param, saturation = jitter_param, contrast = jitter_param, pca_noise = lighting_param,) 参数初始化 使用Xavier初始化卷积层和全连接层的权重，也就是$w\sim \mathcal{U}(-a, a)$，其中$a = \sqrt{6/(d_{in} + d_{out})}$，$d$是输入和输出的channel size。偏置项初始化为$0$。 BatchNorm的$\gamma$初始化为$1$，偏置项为$0$。 训练参数8卡V100，batch size = 256，使用NAG梯度下降，lr从0.1，在30，60，90epoch处除以10。 使用上述设置，得到的ResNet-50模型比原始论文更好，不过Inception-V3（输入为$229\times 229$大小）和MobileNet稍差于原始论文。 更快地训练主要讨论使用低精度（FP16）和大batch size对训练的影响。 大batch size大的batch size经常会导致模型的val acc降低（一个简单的解释是，大batch size造成iteration次数减少，导致模型效果变差。当然，实际训练中，大batch size常常搭配较大的lr，所以问题并不是这么简单），可以考虑使用下面的方法解决这个问题。 （成比例）提高lr上面说的iteration次数减少是一个方面。另一个考虑是大的batch size会造成对梯度的估计方差变小，我们可以乘上一个较大的lr，让方差的不确定性增大一些。一个经验之谈是，lr随着batch size成比例扩大。比如在训练ResNet-50的时候，He给出的在$B = 256$时，lr取为$0.1$。那么如果$B = 512$，那么lr也相应扩大为$0.2$。 lr warm up如果lr初始设置的很大，可能会带来数值不稳定。因为刚开始的时候权重是随机初始化的，gradient也比较大。可以给lr做warm up，也就是开始若干个迭代用较小的lr，等训练稳定了再用回那个大的lr。一种方法是线性warm up，也就是在warm up阶段，lr是线性地从0涨到给定的那个大lr。 设置$\gamma = 0$这个操作比较新奇，在初始阶段，BN的$\beta$参数是设置为$0$的。如果我们再设置$\gamma = 0$，说明BN的输出就是$0$了。这是什么操作？！ 作者指出，可以在ResNet这种有by-pass的结构中使用这个trick。在ResNet block的最后一层，我们经常做$y = x + res(x)$，可以考虑将res这一路的最后一个BN层的$\gamma$参数设置为0。这时候，相当于只有输入$x$传到后面，相当于减少了网络的层深。之后的训练中，$\gamma$会逐渐变大，也就逐渐恢复了res通路。 这种方法也是试图解决网络训练初始阶段不稳定的问题。不过这个操作还是挺骚的。。。类似的方法（利用BN层的$\gamma$参数）也见到过被用在模型剪枝上，如Net Sliming等方法。可以参见博客中的相关文章讨论。 weight decay给weight加上L2 norm来做weight decay，是缓解网络过拟合的标准解决办法之一。不过，最好只对conv和fc的kernel做，而不要对它们的bias，BN的$\gamma$和$\beta$做。 上面的方法，在batch size不大于2K的时候，应该是够用了的。 低精度很多新GPU都加入了FP16的硬件支持，例如V100上使用FP16比FP32，训练能够加速$2$到$3$倍。FP16的问题是表示范围变小了，同时分辨率变小。对应地会造成两个问题，溢出和无法更新（梯度过小，不到FP16的最小表示）。一种解决办法是使用FP16来做forward和backward，但是在FP32上更新梯度（防止梯度过小）。同时给loss乘上一个系数，让它更好地契合FP16能表示的数据范围。 这里简要介绍下FP16精度的相关内容。关于Nvidia GPU FP16的更多信息，可以参考Nvidia文档混合精度训练。 FP16数据表示FP16，顾名思义，就是使用16个bit表示浮点数。具体编码方式上，和FP32基本一致，只不过位数有了缩水。 IEEE 754 standard defines the following 16-bit half-precision floating point format: 1 sign bit, 5 exponent bits, and 10 fractional bits. TODO: FP32和FP16的比较 FP16 in MXNet在MXNet中，使用混合精度训练还是挺简单的。具体可以参考Mixed precision training using float16 下面是使用gluon训练时候要注意的几个地方： 12345678910111213## optimizer 开启混合精度选项## 这会使optimizer为参数保存一份FP32拷贝，在上面进行梯度的更新，## 防止梯度过小无法更新FP16if opt.dtype != 'float32': optimizer_params['multi_precision'] = True## net cast到给定的数值精度net = get_model(model_name, **kwargs)net.cast(opt.dtype)## 训练过程中，将输入也cast到指定精度while in_training: ## blablabla outputs = [net(X.astype(opt.dtype, copy=False)) for X in data] ## 计算loss也把label cast到指定精度 使用MXNet老的symbolic接口时候，因为静态图一旦写好就固定了，所以我们需要在建图的时候，考虑FP16精度。 在原始输入node后面接一个cast op，将FP32转成FP16。 最好在SoftmaxOutput之前，插入一个cast op，将FP16转回FP32，以便有更高的精度。 optimizer打开multi_precision开关，这里和上面gluon是一致的。 1234567891011## 建图data = mx.sym.Variable(name="data")if dtype == 'float16': data = mx.sym.Cast(data=data, dtype=np.float16)# ... the rest of the networknet_out = net(data)if dtype == 'float16': net_out = mx.sym.Cast(data=net_out, dtype=np.float32)output = mx.sym.SoftmaxOutput(data=net_out, name='softmax')## 优化器设置optimizer = mx.optimizer.create('sgd', multi_precision=True, lr=0.01) 下面有几条额外的建议： FP16加速主要来源于新GPU上的Tensor Core计算$D = A * B + C$这种运算，且它们的维度是$8$的倍数。所以如果不满足$8$倍数这个条件，FP16的计算速度可能不会很快，或者说和FP32相比没多少优势。尤其是当你在CIFAR10这种输入图片size比较小的数据集上训练的时候。 针对上面这种情况，你可以使用nvprof工具来check是否Tensor Core被使用了，那些名字里面带有s884cudnn的操作就是了。 确保data io和preprocessing不要成为瓶颈，不然面对这些扯后腿的地方，FP16男默女泪。 batch size最好设置为8的倍数，2的幂次是坠吼的。 如果GPU memory还算充足，可以设置MXNET_CUDNN_AUTOTUNE_DEFAULT = 2，来让MXNet有更多的测试来选用最快的卷积算法，代价就是更多的显存占用。 最好为BatchNorm和SoftmaxOutput使用FP32精度。Gluon里面这些都是自动的，MXNet中BN层是自动的，但是SoftmaxOutput需要自己设置一下，见上。 loss scaling再说一下上面提到的loss scaling。 为啥要做loss scaling呢？主要是由于FP16的精度比较差，而能够表示的较大的数对于CNN网络来说又基本用不到（虽然说FP16的表示范围相比FP32已经缩水不少了），所以可能出现这样一种情形，loss对FP16 weight或activation求梯度，梯度太小，以至于FP16无法表示。那其实我们可以给loss乘上一个系数，放大gradient，以便FP16能够表示。在梯度更新之前，再把这个梯度scale回去，就可以了。如下图所示。 使用gluon或MXNet设置loss scaling的方法如下： 12345678910## gluonloss = gluon.loss.SoftmaxCrossEntropyLoss(weight=128)optimizer = mx.optimizer.create('sgd', multi_precision=True, rescale_grad=1.0/128)## mxnetmxnet.sym.SoftmaxOutput(other_args, grad_scale=128.0)optimizer = mx.optimizer.create('sgd', multi_precision=True, rescale_grad=1.0/128) 经验来看，对于Multibox SSD, R-CNN, bigLSTM and Seq2seq这些任务，loss scaling是比较有必要的。这里有个疑问，loss scaling应该是在训练过程中不断变化的，但上面的使用都是直接把loss scaling写死了（gluon还好，再手动给loss乘上一个因子），那如何修改loss scaling呢？后面指出可以使用constant的loss scaling（一般取2的幂次64，128等），但是不知道实际训练会不会有问题。Nvidia guide中给出的建议是： If you encounter precision problems, it is beneficial to scale the loss up by 128, and scale the application of the gradients down by 128. 当然，最好的办法是自己看一下FP32 gradient的分布。 当当当。。。说了这么多，那么具体加速效果如何呢？使用batch size = $1024$，和batch size = $256$的baseline相比，从下表可知，三种不同的网络结构，分别加速了$1.6X$到$3X$，而且acc还涨了一些。 具体的acc影响的ablation实验如下。可以看到，只是使用lr线性增大的情况下，大（batch size的）网络稍逊于小（batch size的）网络。不过当使用上面几个技术综合来看的时候，大小网络的性能差异已经抹去了，而且大网络的训练速度更快。 更好的网络TODO: 未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello TVM]]></title>
      <url>%2F2019%2F06%2F29%2Ftvm-helloworld%2F</url>
      <content type="text"><![CDATA[TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！) TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends. compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。 我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM! 背景介绍随着深度学习逐渐从研究所的“伊甸园”迅速在工业界的铺开，摆在大家面前的问题是如何将深度学习模型部署到目标硬件平台上，能够多快好省地完成前向计算，从而提供更好的用户体验，同时为老板省钱，还能减少碳排放来造福子孙。 和单纯做研究相比，在工业界我们主要遇到了两个问题： 深度学习框架实在是太$^{\text{TM}}$多了。caffe / mxnet / tensorflow / pytorch训练出来的模型都彼此有不同的分发格式。如果你和我一样，做过不同框架的TensorRT的部署，我想你会懂的。。。 GPU实在是太$^{\text{TM}}$贵了。深度学习春风吹满地，老黄股票真争气。另一方面，一些嵌入式平台没有使用GPU的条件。同时一些人也开始在做FPGA/ASIC的深度学习加速卡。如何将深度学习模型部署适配到多样的硬件平台上？ 为了解决第一个问题，TVM内部实现了自己的IR，可以将上面这些主流深度学习框架的模型转换为统一的内部表示，以便后续处理。若想要详细了解，可以看下NNVM这篇博客：NNVM Compiler: Open Compiler for AI Frameworks。这张图应该能够说明NNVM在TVM中起到的作用。 为了解决第二个问题，TVM内部有多重机制来做优化。其中一个特点是，使用机器学习（结合专家知识）的方法，通过在目标硬件上跑大量trial，来获得该硬件上相关运算（例如卷积）的最优实现。这使得TVM能够做到快速为新型硬件或新的op做优化。我们知道，在GPU上我们站在Nvidia内部专家的肩膀上，使用CUDA / CUDNN / CUBLAS编程。但相比于Conv / Pooling等Nvidia已经优化的很好了的op，我们自己写的op很可能效率不高。或者在新的硬件上，没有类似CUDA的生态，如何对网络进行调优？TVM这种基于机器学习的方法给出了一个可行的方案。我们只需给定参数的搜索空间（少量的人类专家知识），就可以将剩下的工作交给TVM。如果对此感兴趣，可以阅读TVM中关于AutoTuner的介绍和tutorial：Auto-tuning a convolutional network for ARM CPU。 编译我的环境为Debian 8，CUDA 9。 准备代码1234git clone https://github.com/dmlc/tvm.gitcd tvmgit checkout e22b5802git submodule update --init --recursive config文件1234cd tvmmkdir buildcp ../cmake/config.cmake ./buildcd build 编辑config文件，打开CUDA / BLAS / cuBLAS / CUDNN的开关。注意下LLVM的开关。LLVM可以从这个页面LLVM Download下载，我之前就已经下载好，版本为7.0。如果你像我一样是Debian8，可以使用for Ubuntu14.04的那个版本。由于是已经编译好的二进制包，下载之后解压即可。 找到这一行，改成1set(USE_LLVM /path/to/llvm/bin/llvm-config) 编译这里有个坑，因为我们使用了LLVM，最好使用LLVM中的clang。否则可能导致tvm生成的代码无法二次导入。见这个讨论帖：_cc.create_shared error while run tune_simple_template。 1234export LLVM=/path/to/llvmcmake -DCMAKE_C_COMPILER=$LLVM/bin/clang -DCMAKE_CXX_COMPILER=$LLVM/bin/clang++ ..# 火力全开，let's rockmake -j$(nproc) python包安装1234567cd /path/to/tvm# 我一般用清华的镜像，你呢。。。export THU_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simplepip install tornado tornado psutil xgboost numpy decorator attrs --user -i $THU_MIRRORcd python; python setup.py install --user; cd ..cd topi/python; python setup.py install --user; cd ../..cd nnvm/python; python setup.py install --user; cd ../.. demo使用tvm为mxnet symbol计算图生成CUDA代码，并进行前向计算。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364656667686970717273747576import numpyimport tvmfrom tvm import relayfrom tvm.relay import testingfrom tvm.contrib import graph_runtimeimport mxnet as mx## load mxnet modelprefix = '/your/mxnet/checkpoint/prefix'epoch = 0mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)## import model into tvm from mxnetshape_dict = &#123;'data': (1, 3, 224, 224)&#125;## tvm提供了 frontend.from_XXX 接口，从不同的框架中导入模型relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict, arg_params=arg_params, aux_params=aux_params)# 设定目标硬件为 GPU，生成TVM模型## ---------------------------- # graph：execution graph in json format# lib: tvm module library of compiled functions for the graph on the target hardware# params: parameter blobs## ---------------------------target = 'cuda'with relay.build_config(opt_level=3): graph, lib, params = relay.build(relay_func, target, params=relay_params)# run forward## 直接使用tvm提供的cat示例图片from tvm.contrib.download import download_testdataimg_url = 'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'img_path = download_testdata(img_url, 'cat.png', module='data')from PIL import Imageimage = Image.open(img_path).resize((224, 224))def transform_image(im): im = np.array(im).astype(np.float32) im = np.transpose(im, [2, 0, 1]) im = im[np.newaxis, :] return imx = transform_image(image)# let's goctx = tvm.gpu(0)dtype = 'float32'## 加载模型m = graph_runtime.create(graph, lib, ctx)## set input datam.set_input('data', tvm.nd.array(x.astype(dtype)))## set input paramsm.set_input(**params)m.run()# get outputoutputs = m.get_output(0)top1 = np.argmax(outputs.asnumpy()[0])# save model## lib存为tar包文件，解压后可以发现，就是打包了动态链接库path_lib = './deploy_resnet50_v2_lib.tar'lib.export_library(path_lib)## 计算图存为json文件with open('./deploy_resnet50_v2_graph.json', 'w') as f: f.write(graph)## 权重存为二进制文件with open('./deploy_params', 'wb') as f: f.write(relay.save_param_dict(params))# load model backloaded_json = open('./deploy_resnet50_v2_graph.json').read()loaded_lib = tvm.module.load(path_lib)loaded_params = bytearray(open('./deploy_params', 'rb').read())module = graph_runtime.create(loaded_json, loaded_lib, ctx)## 好了，剩下的就都一样了 最后的话我个人的观点，TVM是一个很有意思的项目。在深度学习模型的优化和部署上做了很多探索，在官方放出的benchmark上表现还是不错的。如果使用非GPU进行模型的部署，TVM值得一试。不过在GPU上，得益于Nvidia的CUDA生态，目前TensorRT仍然用起来更方便，综合性能更好。如果你和我一样，主要仍然在GPU上搞事情，可以密切关注TVM的发展，并尝试使用在自己的项目中，不过我觉得还是优先考虑TensorRT。另一方面，TVM的代码实在是看不太懂啊。。。 想要更多 TVM paper：TVM: An Automated End-to-End Optimizing Compiler for Deep Learning TVM 项目主页：TVM 后续TVM的介绍，不知道啥时候有时间再写。。。随缘吧。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[重读 C++ Primer]]></title>
      <url>%2F2019%2F05%2F01%2Fcpp-primer-review%2F</url>
      <content type="text"><![CDATA[重读C++ Primer第五版，整理一些糊涂的语法知识点。 基础语法总结一些比较容易搞乱的基础语法。 const 限定说明符 const对象一般只在当前文件可见，如果希望在其他文件访问，在声明和定义时，均需加上extern关键字。 1extern const int BUF_SIZE = 100; 顶层const和底层const 指针本身也是对象，所以有所谓的“常量指针”（指针本身不能赋值）和“指向常量的指针”（指针指向的那个对象不能赋值）。 12345678int a = 10;// 指针指向的对象不能经由指针赋值const int* p1 = &amp;a;*p = 0; // 错误// 指针本身不能再赋值int* const p2 = &amp;a;int b = 0;p2 = &amp;p; // 错误 如何记住这条规则？c++中类型说明从右向左读即可。例如p1，其左侧首先遇到int*，故其是个“普通”指针（没有被const修饰），再往左才读到const，故这个指针指向的内容是常量，不能修改。p2同理。 把“指针本身是常量”的行为称为“顶层const”（top-level），把“指针指向内容是常量”的行为称为“底层const”（low-level）。 auto 和 decltype auto类型推断的规则 编译器推断auto声明变量的类型时，可能和初始值类型不一样。当初始值类型为引用时，编译器以被引用对象的类型作为auto的类型，除非显式指明。 123456int i = 0;int&amp; ri = i;// type of j: intauto j = ri;// type of rj: int&amp;auto&amp; rj = ri; 另外，auto只会保留底层const，忽略顶层const，除非显式指定。 1234567891011int a = 0;const int* const p = &amp;a;// type of b: intauto b = a;// type of p1: const int*auto p1 = p;// type of p2: const int* constconst auto p2 = p;p1 = &amp;b; // ok, p1 本身已经不是const的了p2 = &amp;b; // wrong! 显式指定了 p2 本身是 const*p1 = 10; // wrong! p1 保留了底层const，指向的内容仍然不可改变 decltype 类型推断规则 和auto不同，decltype保留表达式的顶层const和引用。 如果表达式是变量，那么返回该变量的类型； 如果表达式不是纯变量，返回表达式结果的类型； 如果表达式是解引用，返回引用类型。 12345int i = 42, *p = &amp;i, &amp;r = i;decltype(i) j; // ok, j is a intdecltype(r) y; // wrong! y是引用类型，必须初始化decltype(r + 0) z; // ok, r+0 返回值是intdecltype(*p) c; // wrong! 解引用的结果是引用，必须初始化 有一种情况特殊，如果是春变量，但是变量名加上括号，结果将是引用。原因：变量加上括号，将会被当做表达式。而变量又可以被赋值，所以得到了引用。 1dectype((i)) d; // wrong! d是引用 泛型算法C++的标准库中实现了很多泛型算法，如find, sort等。它们大多定义在&lt;algorithm&gt;头文件中，一些数值相关的定义在&lt;numeric&gt;中。通过“迭代器”这一层抽象，泛型算法可以不关心所操作数据实际储存的容器，不过仍然受制于实际数据类型。例如find中，为了比较当前元素是否为所求值，要求元素类型实现==运算。好在这些算法大多支持自定义操作。 迭代器在标准库的&lt;iterator&gt;中，定义了如下几种通用迭代器。 插入迭代器 插入器是一个迭代器的适配器，接受一个容器，生成一个用于该容器的迭代器，能够实现向该容器插入元素。插入迭代器有三种，区别在于插入元素的位置： back_inserter，创建一个使用push_back插入的迭代器 front_inserter，创建一个使用push_front插入的迭代器 inserter，创建一个使用insert的迭代器，在给定的迭代器前面插入元素 123456789101112131415161718192021222324252627282930313233#include &lt;iostream&gt;#include &lt;vector&gt;#include &lt;iterator&gt;#include &lt;algorithm&gt;using namespace std;// 使用back_inserter插入数据int main() &#123; int a[] = &#123;1,2,3,4,5&#125;; vector&lt;int&gt; b; // copy a -&gt; b, 动态改变b的大小 copy(begin(a), end(a), back_inserter(b)); for (auto v: b) &#123; cout &lt;&lt; v &lt;&lt; endl; &#125; // b: 1, 2, 3, 4, 5 return 0;&#125;// 使用inserter，将数据插入指定位置int main() &#123; int a[] = &#123;1,2,3,4,5&#125;; vector&lt;int&gt; b &#123;6,7,8&#125;; // find iter of value 8 auto iter = find(b.begin(), b.end(), 8); // copy a -&gt; b before value 8 copy(begin(a), end(a), inserter(b, iter)); for (auto v : b) &#123; cout &lt;&lt; v &lt;&lt; endl; &#125; // b: 6, 7, 1, 2, 3, 4, 5, 8 return 0;&#125; 这里要注意的是，当使用front_inserter时，由于插入总是在容器头部发生，所以最后的插入结果是原始数据序列的逆序。 流迭代器 虽然输入输出流不是容器，不过也有用于这些IO对象的迭代器：istream_iterator和ostream_iterator。这样，我们可以通过它们向对应的输入输出流读写数据， 创建输入流迭代器时，必须指定其要操作的数据类型，并将其绑定到某个流（标准输入输出流或文件流），或使用默认初始化，得到当做尾后值使用的迭代器。 1234istream_iterator&lt;int&gt; in_iter(cin);istream_iterator&lt;int&gt; in_eof;// 使用迭代器构建vectorvector&lt;int&gt; values(in_iter, in_eof); 创建输出流迭代器时，必须指定其要操作的数据类型，并向其绑定到某个流，还可以传入第二个参数，类型是C风格的字符串（字符串字面常量或指向\0结尾的字符数组指针），表示在输出数据之后，还会输出此字符串。 123vector&lt;int&gt; v&#123;1,2,3,4,5&#125;;// 输出：1 2 3 4 5 copy(v.begin(), v.end(), ostream_iterator&lt;int&gt;(cout, "\t")); 反向迭代器 顾名思义，反向迭代器的迭代顺序和正常的迭代器是相反的。使用rbegin和rend可以获得绑定在该容器的反向迭代器。不过forward_list和流对象，由于没有同时实现++和--，所以没有反向迭代器。 反向迭代器常常用来在容器中查找最后一个满足条件的元素。这时候要注意，如果继续使用该迭代器，顺序仍然是反向的。如果需要正向迭代器，可以使用.base()方法得到对应的正向迭代器。不过要注意，正向迭代器和反向迭代器的位置会不一样哦~ 12345678// 找到数组中最后一个5,并将其后数字打印出来vector&lt;int&gt; v &#123;10, 5, 4, 5, 1, 2&#125;;auto iter = find(v.rbegin(), v.rend(), 5);// 输出：5,4,5,10,copy(iter, v.rend(), ostream_iterator&lt;int&gt;(cout, ","));cout &lt;&lt; "\n";// 输出：1,2, 注意并没有输出5copy(iter.base(), v.end(), ostream_iterator&lt;int&gt;(cout, ",")); 未完待续拖延症发作。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO Caffe模型转换BN的坑]]></title>
      <url>%2F2019%2F03%2F09%2Fdarknet-caffe-converter%2F</url>
      <content type="text"><![CDATA[YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。 Darknet中BN的计算以CPU代码为例，在Darknet中，BN做normalization的操作如下，normalize_cpu 123456789101112void normalize_cpu(float *x, float *mean, float *variance, int batch, int filters, int spatial)&#123; int b, f, i; for(b = 0; b &lt; batch; ++b)&#123; for(f = 0; f &lt; filters; ++f)&#123; for(i = 0; i &lt; spatial; ++i)&#123; int index = b*filters*spatial + f*spatial + i; x[index] = (x[index] - mean[f])/(sqrt(variance[f]) + .000001f); &#125; &#125; &#125;&#125; 可以看到，Darknet中的BN计算如下： \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2} + \epsilon}而且，$\epsilon$参数是固定的，为$1\times 10^{-6}$。 问题和解决然而，在Caffe（以及大部分其他框架）中，$\epsilon$的位置是在根号里面的，也就是： \hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}另外，查看caffe.proto可以知道，Caffe默认的$\epsilon$值为$1\times 10^{-5}$。 所以，在转换为caffe prototxt时，需要设置batch_norm_param如下： 1234batch_norm_param &#123; use_global_stats: true eps: 1e-06&#125; 另外，需要重新求解$\sigma^2$，按照layer输出要相等的等量关系，可以求得： 12def convert_running_var(var, eps=DARKNET_EPS): return np.square(np.sqrt(var) + eps) - eps 这里调整之后，转换后的Caffe模型和原始Darknet模型的输出误差已经是$1\times 10^{-7}$量级，可以认为转换成功。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MacOS Mojave更新之后一定要做这几件事！]]></title>
      <url>%2F2018%2F10%2F27%2Fmac-update-mojave%2F</url>
      <content type="text"><![CDATA[很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。 Git等工具试图使用git是出现了如下错误： 12git clone xx.gitxcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun 解决办法参考macOS Mojave: invalid active developer path中的最高赞回答：1xcode-select --install osxfuse参考Github讨论帖osxfuse not compatible with MacOS Mojave，从官网下载最新的3.8.2版本安装即可。 VSCode等编辑器字体变“瘦”更新之后，发现VSCode编辑器中的字体变得“很瘦”，不美观。执行下面的命令，并重启机器，应该可以恢复。1defaults write -g CGFontRenderingFontSmoothingDisabled -bool NO Mos Caffine IINA 等APPMos可以平滑Mac上外接鼠标的滚动，并调整鼠标滚动方向和Windows相同。更新后发现Mos失灵。这应该是和新版本中更强的权限管理有关，解决办法是在”安全隐私设置” -&gt; “辅助功能”中，先把Mos的勾勾去掉，然后重新勾选。Caffine同样的操作。 IINA是一款Mac上的播放器软件，是我在Mac上的默认播放器。更新后点击媒体文件，发现只是弹出IINA软件的界面，却没有自动播放。解决办法是在媒体文件上右键，在打开方式中重新选择IINA，并勾选默认打开方式选项。 更新新系统后，遇到的坑暂时就这么多。希望能够帮助到需要的人。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Rethinking The Value of Network Pruning]]></title>
      <url>%2F2018%2F10%2F22%2Fpaper-rethinking-the-value-of-network-pruning%2F</url>
      <content type="text"><![CDATA[这篇文章是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考论文总结 - 模型剪枝 Model Pruning。 这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。 然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：Eric-mingjie/rethinking-network-pruning。 FLOP的计算代码中有关于PyTorch模型的FLOPs的计算，见compute_flops.py。可以很方便地应用到自己的代码中。 ThiNet的实现实验比较结论几个仍然有疑问的地方： 作者已经证明在ImageNet/CIFAR等样本分布均衡的数据集上的结论，如果样本分布不均衡呢？有三种思路有待验证： prune模型需要从大模型处继承权重，然后直接在不均衡数据集上训练即可； prune模型不需要从大模型处继承权重， 但是需要先在ImageNet数据集上训练，然后再在不均衡数据集上训练； prune模型直接在不均衡数据集上训练（以我的经验，这种思路应该是不work的） prune前的大模型权重不重要，结构重要，这是本文的结论之一。自动搜索树的prune算法可以看做是模型结构搜索，但是大模型给出了搜索空间的一个很好的初始点。这个初始点是否是任务无关的？也就是说，对A任务有效的小模型，是否在B任务上也是很work的？ 现在的网络搜索中应用了强化学习/遗传算法等方法，这些方法怎么能够和prune结合？ECCV 2018中HanSong和He Yihui发表了AMC方法。 总之，作者用自己辛勤的实验，给我们指出了一个”可能的”（毕竟文章还没被接收）误区，但是仍然有很多乌云漂浮在上面，需要更多的实验。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文总结 - 模型剪枝 Model Pruning]]></title>
      <url>%2F2018%2F10%2F03%2Fpaper-summary-model-pruning%2F</url>
      <content type="text"><![CDATA[模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。 Deep Compression, Han Song抛去LeCun等人在90年代初的几篇论文，HanSong是这个领域的先行者。发表了一系列关于模型压缩的论文。其中NIPS 2015上的这篇Learning both weights and connections for efficient neural network着重讨论了对模型进行剪枝的方法。这篇论文之前我已经写过了阅读总结，比较详细。 概括来说，作者提出的主要观点包括，L1 norm作为neuron是否重要的metric，train -&gt; pruning -&gt; retrain三阶段方法以及iteratively pruning。需要注意的是，作者的方法只能得到非结构化的稀疏，对于作者的专用硬件EIE可能会很有帮助。但是如果想要在通用GPU或CPU上用这种方法做加速，是不太现实的。 SSL，WenWei既然非结构化稀疏对现有的通用GPU/CPU不友好，那么可以考虑构造结构化的稀疏。将Conv中的某个filter或filter的某个方形区域甚至是某个layer直接去掉，应该是可以获得加速效果的。WenWei论文Learning Structured Sparsity in Deep Neural Networks发表在NIPS 2016上，介绍了如何使用LASSO，给损失函数加入相应的惩罚，进行结构化稀疏。这篇论文之前也已经写过博客，可以参考博客文章。 概括来说，作者引入LASSO正则惩罚项，通过不同的具体形式，构造了对不同结构化稀疏的损失函数。 L1-norm Filter Pruning，Li Hao在通用GPU/CPU上，加速效果最好的还是整个Filter直接去掉。作者发表在ICLR 2017上的论文Pruning Filters for Efficient ConvNets提出了一种简单的对卷积层的filter进行剪枝的方法。 这篇论文真的很简单。。。主要观点就是通过Filter的L1 norm来判断这个filter是否重要。人为设定剪枝比例后，将该层不重要的那些filter直接去掉，并进行fine tune。在确定剪枝比例的时候，假定每个layer都是互相独立的，分别对其在不同剪枝比例下进行剪枝，并评估模型在验证集上的表现，做sensitivity分析，然后确定合理的剪枝比例。在实现的时候要注意，第$i$个layer中的第$j$个filter被去除，会导致其输出的feature map中的第$j$个channel缺失，所以要相应调整后续的BN层和Conv层的对应channel上的参数。 另外，实现起来还有一些细节，这些可以参见原始论文。提一点，在对ResNet这种有旁路结构的网络进行剪枝时，每个block中的最后一个conv不太好处理。因为它的输出要与旁路做加和运算。如果channel数量不匹配，是没法做的。作者在这里的处理方法是，听identity那一路的。如果那一路确定了剪枝后剩余的index是多少，那么$\mathcal{F}(x)$那一路的最后那个conv也这样剪枝。 这里给出一张在ImageNet上做sensitivity analysis的图表。需要对每个待剪枝的layer进行类似的分析。 Automated Gradual Pruning, Gupta这篇文章发表在NIPS 2017的一个关于移动设备的workshop上，名字很有意思（这些人起名字为什么都这么熟练啊）：To prune, or not to prune: exploring the efficacy of pruning for model compression。TensorFlow的repo中已经有了对应的实现（亲儿子。。）：Model pruning: Training tensorflow models to have masked connections。哈姆雷特不能回答的问题，作者的答案则是Yes。 这篇文章主要有两个贡献。一是比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但是compact-small模型的性能，得出结论：对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，都是前者更好。具体的数据参考论文。 二是提出了一个渐进的自动调节的pruning策略。首先，作者也着眼于非结构化稀疏。同时和上面几篇文章一样，作者也使用绝对值大小作为衡量importance的标准，作者提出，sparsity可以按照下式自动调节： s_t = s_f + (s_i-s_f)(1-\frac{t-t_0}{n\Delta t})^3 \quad \text{for}\quad t \in \{t_0, t_0+\Delta t,\dots,t_0+n\Delta t\}其中，$s_i$是初始剪枝比例，一般为$0$。$s_f$为最终的剪枝比例，开始剪枝的迭代次数为$t_0$，剪枝间隔为$\Delta t$，共进行$n$次。 Net Sliming, Liu Zhuang &amp; Huang Gao这篇文章Learning Efficient Convolutional Networks through Network Slimming发表在ICCV 2017，利用CNN网络中的必备组件——BN层中的gamma参数，实现端到端地学习剪枝参数，决定某个layer中该去除掉哪些channel。作者中有DenseNet的作者——姚班学生刘壮和康奈尔大学博士后黄高。代码已经开源：liuzhuang13/slimming。 作者的主要贡献是提出可以使用BN层的gamma参数，标志其前面的conv输出的feature map的某个channel是否重要，相应地，也是conv参数中的那个filter是否重要。 首先，需要给BN的gamma参数加上L1 正则惩罚训练模型，新的损失函数变为$L= \sum_{(x,y)}l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma}g(\gamma)$。 接着将该网络中的所有gamma进行排序，根据人为给出的剪枝比例，去掉那些gamma很小的channel，也就是对应的filter。最后进行finetune。这个过程可以反复多次，得到更好的效果。如下所示： 还是上面遇到过的问题，如果处理ResNet或者DenseNet Feature map会多路输出的问题。这里作者提出使用一个”channel selection layer”，统一对该feature map的输出进行处理，只选择没有被mask掉的那些channel输出。具体实现可以参见开源代码channel selection layer： 123456789101112131415161718192021222324class channel_selection(nn.Module): """ Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer. The output shape of this layer is determined by the number of 1 in `self.indexes`. """ def __init__(self, num_channels): """ Initialize the `indexes` with all one vector with the length same as the number of channels. During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0. """ super(channel_selection, self).__init__() self.indexes = nn.Parameter(torch.ones(num_channels)) def forward(self, input_tensor): """ Parameter --------- input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer. """ selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy())) if selected_index.size == 1: selected_index = np.resize(selected_index, (1,)) output = input_tensor[:, selected_index, :, :] return output 略微解释一下：在开始加入L1正则，惩罚gamma的时候，相当于identity变换；当确定剪枝参数后，相应index会被置为$0$，被mask掉，这样输出就没有这个channel了。后面的几路都可以用这个共同的输出。 AutoPruner, Wu Jianxin这篇文章AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference是南大Wu Jianxin组新进发的文章，还没有投稿到任何学术会议或期刊，只是挂在了Arvix上，应该是还不够完善。他们还有一篇文章ThiNet：ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression发表在ICCV 2017上。 这篇文章的主要贡献是提出了一种端到端的模型剪枝方法，如下图所示。为第$i$个Conv输出加上一个旁路，输入为其输出的Feature map，依次经过Batch-wise Pooling -&gt; FC -&gt; scaled sigmoid的变换，按channel输出取值在$[0,1]$范围的向量作为mask，与Feature map做积，mask掉相应的channel。通过学习FC的参数，就可以得到适当的mask，判断该剪掉第$i$个Conv的哪个filter。其中，scaled sigmoid变换是指$y = \sigma(\alpha x)$。通过训练过程中不断调大$\alpha$，就可以控制sigmoid的“硬度”，最终实现$0-1$门的效果。 构造损失函数$\mathcal{L} = \mathcal{L}_{\text{cross-entropy}} + \lambda \Vert \frac{\Vert v \Vert_1}{C} - r \Vert_2^2$。其中，$v$是sigmoid输出的mask，$C$为输出的channel数量，$r$为目标稀疏度。 不过在具体的细节上，作者表示要注意的东西很多。主要是FC层的初始化和几个超参数的处理。作者在论文中提出了相应想法： FC层初始化权重为$0$均值，方差为$10\sqrt{\frac{2}{n}}$的高斯分布，其中$n = C\times H \times W$。 上述$\alpha$的控制，如何增长$\alpha$。作者设计了一套if-else的规则。 上述损失函数中的比例$\lambda$，作者使用了$\lambda = 100 \vert r_b - r\vert$的自适应调节方法。 Rethinking Net Pruning, 匿名这篇文章Rethinking the Value of Network Pruning有意思了。严格说来，它还在ICLR 2019的匿名评审阶段，并没有被接收。不过这篇文章的炮口已经瞄准了之前提出的好几个model pruning方法，对它们的结果提出了质疑。上面的链接中，也有被diss的方法之一的作者He Yihui和本文作者的交流。 之前的剪枝算法大多考虑两个问题： 怎么求得一个高效的剪枝模型结构，如何确定剪枝方式和剪枝比例：在哪里剪，剪多少 剪枝模型的参数求取：如何保留原始模型中重要的weight，对进行补偿，使得accuracy等性能指标回复到原始模型 而本文的作者check了六种SOA的工作，发现：在剪枝算法得到的模型上进行finetune，只比相同结构，但是使用random初始化权重的网络performance好了一点点，甚至有的时候还不如。作者的结论是： 训练一个over parameter的model对最终得到一个efficient的小模型不是必要的 为了得到剪枝后的小模型，求取大模型中的important参数其实并不打紧 剪枝得到的结构，相比求得的weight，更重要。所以不如将剪枝算法看做是网络结构搜索的一种特例。 作者立了两个论点来打： 要先训练一个over-parameter的大模型，然后在其基础上剪枝。因为大模型有更强大的表达能力。 剪枝之后的网络结构和权重都很重要，是剪枝模型finetune的基础。 作者试图通过实验证明，很多剪枝方法并没有他们声称的那么有效，很多时候，无需剪枝之后的权重，而是直接随机初始化并训练，就能达到这些论文中的剪枝方法的效果。当然，这些论文并不是一无是处。作者提出，是剪枝之后的结构更重要。这些剪枝方法可以看做是网络结构的搜索。 论文的其他部分就是对几种现有方法的实验和diss。我还没有细看，如果后续这篇论文得到了接收，再做总结吧~夹带一些私货，基于几篇论文的实现经验和在真实数据集上的测试，这篇文章的看法我是同意的。 更新：这篇文章的作者原来正是Net Sliming的作者Liu Zhuang和Huang Gao，那实验和结论应该是很有保障的。最近这篇文章确实也引起了大家的注意，值得好好看一看。 其他论文等资源 Distiller：一个使用PyTorch实现的剪枝工具包]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[VIM安装YouCompleteMe和Jedi进行自动补全]]></title>
      <url>%2F2018%2F10%2F02%2Fvim-you-complete-me%2F</url>
      <content type="text"><![CDATA[这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~ 使用Jedi和YouCompleteMe配置Vim在远程开发机上调试代码时，我的习惯是大型项目使用sshfs将其镜像到本地，然后使用VSCode打开编辑。VSCode中有终端可以方便的ssh到远端开发机，我将”CTRL+`”配置成了编辑器和终端之间的切换快捷键。加上vim插件，就可以实现不用鼠标，不离开当前编辑环境进行代码编写和调试了。 然而，如果是想在开发机上写一段小的代码，上述方法就显得太麻烦了。 编译Vim编译Vim，注意我们要设定其安装目录为anaconda下的bin目录： 1./configure --with-features=huge --enable-multibyte --enable-pythoninterp=yes --with-python-config-dir=/path/to/anaconda/bin/python-config --enable-gui=gtk2 --prefix=/path/to/anaconda 编译并安装：12make -j4 VIMRUNTIMEDIR=/path/to/anaconda/share/vim/vim81make install 安装后，可以查看vim的version进行确认。安装没有问题，会提示刚才编译的版本信息。1vim --version 使用Vundle管理插件，这个没有什么问题，直接按照README提示即可，见：Vundle@Github。 使用Vundle进行插件管理，只需要以下面的形式指明插件目录或Github仓库名称，进入vim后，在Normal状态，输入:PluginInstall即可。 Jedi首先需要安装jedi的python包：1pip install jedi 使用Vbudle安装jedi-vim，并在.vimrc中添加以下内容。1let g:jedi#force_py_version=2.7 YouCompleteMe使用Vundle安装YouCompleteMe。 之后，进入目录.vim/bundle/YouCompleteMe，执行./install.py。如果需要C++支持，执行./install.py --clang-completer。 但是，其中遇到了问题，找不到Python.h文件。使用locate Python.h，明确该文件确实存在，且其位于/path/to/anaconda/include/python2.7后，手动修改CMakeLists.txt，指定该文件目录位置即可。 修改这个：.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/CMakeLists.txt和.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/CMakeLists.txt，向其中添加： 1set( CMAKE_CXX_FLAGS "$&#123;CMAKE_CXX_FLAGS&#125; -I/path/to/anaconda/include/python2.7" ) 强行指定头文件包含目录。 括号自动补全虽然SO上有人指出可以直接通过设置.vimrc的方法实现，不过还是直接用现成的插件吧。推荐使用jiangmiao/auto-pairs。可以按照README的说明进行安装。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[MXNet fit介绍]]></title>
      <url>%2F2018%2F10%2F02%2Fmxnet-fit-usage%2F</url>
      <content type="text"><![CDATA[在MXNet中，Module提供了训练模型的方便接口。使用symbol将计算图建好之后，用Module包装一下，就可以通过fit()方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用fit()接口就不太合适。这里把fit()代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。 其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个Slide分享，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。 准备工作首先，需要将数据绑定到计算图上，并初始化模型的参数，并初始化求解器。这些是求解模型必不可少的。 其次，还会建立训练的metric，方便我们掌握训练进程和当前模型在训练任务的表现。 这些是在为后续迭代进行梯度下降更新做准备。 迭代更新使用SGD进行训练的时候，我们需要不停地从数据迭代器中获取包含data和label的batch，并将其feed到网络模型中。进行forward computing后进行bp，获得梯度，并根据具体的优化方法（SGD, SGD with momentum, RMSprop等）进行参数更新。 这部分可以抽成：12345678910# in an epochwhile not end_epoch: batch = next(train_iter) m.forward_backward(batch) m.update() try: next_batch = next(data_iter) m.prepare(next_batch) except StopIteration: end_epoch = True metric在训练的时候，观察输出的各种metric是必不可少的。我们对训练过程的把握就是通过metric给出的信息。通常在分类任务中常用到的metric有Accuracy，TopK-Accuracy以及交叉熵损失等，这些已经在MXNet中有了现成的实现。而在fit中，调用了m.update_metric(eval_metric, data_batch.label)实现。这里的eval_metric就是我们指定的metric，而label是batch提供的label。注意，在MXNet中，label一般都是以list的形式给出（对应于多任务学习），也就是说这里的label是list of NDArray。当自己魔改的时候要注意。 logging计算了eval_metric等信息，我们需要将其在屏幕上打印出来。MXNet中可以通过callback实现。另外，保存模型checkpoint这样的功能也是通过callback实现的。一种常用的场景是每过若干个batch，做一次logging，打印当前的metric信息，如交叉熵损失降到多少了，准确率提高到多少了等。MXNet会将以下信息打包成BatchEndParam类型（其实是一个自定义的namedtuple）的变量，包括当前epoch，当前迭代次数，评估的metric。如果你需要更多的信息或者更自由的logging监控，也可以参考代码自己实现。 我们以常用的Speedometer看一下如何使用这些信息，其功能如下，将训练的速度和metric打印出来。 Logs training speed and evaluation metrics periodically PS:这里有个隐藏的坑。MXNet中的Speedometer每回调一次，会把metric的内容清除。这在训练的时候当然没问题。但是如果是在validation上跑，就会有问题了。这样最终得到的只是最后一个回调周期那些batch的metric，而不是整个验证集上的。如果在fit方法中传入了eval_batch_end_callback参数就要注意这个问题了。解决办法一是在Speedometer实例初始化时传入auto_reset=False，另一种干脆就不要加这个参数，默认为None好了。同样的问题也发生在调用Module.score()方法来获取模型在验证集上metric的时候。 可以在Speedometer代码中寻找下面这几行，会更清楚： 1234if param.eval_metric is not None: name_value = param.eval_metric.get_name_value() if self.auto_reset: param.eval_metric.reset() 在验证集上测试当在训练集上跑过一个epoch后，如果提供了验证集的迭代器，会在验证集上对模型进行测试。这里，MXNet直接封装了score()方法。在score中，基本流程和fit()相同，只是我们只需要forward computing即可。 附用了一段时间的MXNet，给我的最大的感觉是MXNet就像一个写计算图的前端，提供了很方便的python接口生成静态图，以及很多“可插拔”的插件（虽然可能不是很全，更像是一份guide而不是拿来即用的tool），如上文中的metric等，使其更适合做成流程化的基础DL平台，供给更上层方便地配置使用。缺点就是隐藏了比较多的实现细节（当然，你完全可以从代码中自己学习，比如从fit()代码了解神经网络的大致训练流程）。至于MXNet宣扬的诸如速度快，图优化，省计算资源等优点，因为我没有过数据对比，就不说了。 缺点就是写图的时候有时不太灵活（可能也是我写的看的还比较少），即使是和TensorFlow这种同为静态图的DL框架比。另外，貌似MXNet中很多东西都没有跟上最新的论文等，比如Cosine的learning rate decay就没有。Model Zoo也比较少(gluon可能会好一点，Gluon-CV和Gluon-NLP貌似是在搞一些论文复现的工作)。对开发来讲，很多东西都需要阅读代码才能知道是怎么回事，只是读文档的话容易踩坑。 说到这里，感觉MXNet的python训练接口（包括module，optimizer，metric等）更像是一份example代码，是在教你怎么去用MXNet，而不像一个灵活地强大的工具箱。当然，很多东西不能得兼，希望MXNet越来越好。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Like What You Like - Knowledge Distill via Neuron Selectivity Transfer]]></title>
      <url>%2F2018%2F10%2F02%2Fpaper-knowledge-transfer-neural-selectivity-transfer%2F</url>
      <content type="text"><![CDATA[好长时间没有写博客了，国庆假期把最近看的东西整理一下。Like What You Like: Knowledge Distill via Neuron Selectivity Transfer这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。 另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：Data-driven sparse structure selection for deep neural networks。 另另外，其实这两篇文章挂在Arxiv的时间很接近，知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。 MotivationKD的一个痛点在于其只适用于softmax分类问题。这样，对于Detection。Segmentation等一系列问题，没有办法应用。另一个问题在于当分类类别数较少时，KD效果不理想。这个问题比较好理解，假设我们面对一个二分类问题，那么我们并不关心类间的similarity，而是尽可能把两类分开即可。同时，这篇文章的实验部分也验证了这个猜想：当分类问题分类类别数较多时，使用KD能够取得best的结果。 作者联想到，我们是否可以把CNN中某个中间层输出的feature map利用起来呢？Student输出的feature map要和Teacher的相似，相当于是Student学习到了Teacher提取特征的能力。在CNN中，每个filter都是在和一个feature map上的patch做卷积得到输出，很多个filter都做卷积运算，就得到了feature（map）。另外，当filter和patch有相似的结构时，得到的激活比较大。举个例子，如果filter是Sobel算子，那么当输入image是边缘的时候，得到的响应是最大的。filter学习出来的是输入中的某些模式。当模式匹配上时，激活。这里也可以参考一些对CNN中filter做可视化的研究。 顺着上面的思路，有人提出了Attention Transfer的方法，可以参见这篇文章：Improving the Performance of Convolutional Neural Networks via Attention Transfer。而在NST这篇文章中，作者引入了新的损失函数，用于衡量Student和Teacher对相同输入的激活Feature map的不同，可以说除了下面要介绍的数学概念以外，没有什么难理解的地方。整个训练的网络结构如下所示： Maximum Mean DiscrepancyMMD 是用来衡量sampled data之间分布差异的距离量度。如果有两个不同的分布$p$和$q$，以及从两个分布中采样得到的Data set$\mathcal{X}$和$\mathcal{Y}$。那么MMD距离如下： \mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2其中，$\phi$表示某个mapping function。变形之后（内积打开括号），可以得到： \mathcal{L}(\mathcal{X}, \mathcal{Y}) = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)其中，$k$是某个kernel function，$k(x, y) = \phi(x)^{T}\phi(y)$。 我们可以使用MMD来衡量Student模型和Teacher模型中间输出的激活feature map的相似程度。通过优化这个损失函数，使得S的输出分布接近T。通过引入MMD，将NST loss定义如下，下标$S$表示Student的输出，$T$表示Teacher的输出。第一项$\mathcal{H}$是指由样本类别标签计算的CrossEntropy Loss。第二项即为上述的MMD Loss。 \mathcal{L} = \mathcal{H}(y, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)注意，为了确保后一项有意义，需要保证$F_T$和$F_S$有相同长度。具体来说，对于网络中间输出的feature map，我们将每个channel上的$HW$维的feature vector作为分布$\mathcal{X}$的一个采样。按照作者的设定，我们需要保证S和T对应的feature map在spatial dimension上必须一样大。如果不一样，可以使用插值方法进行扩展。 为了不受相对幅值大小的影响，需要对feature vector做normalization。 对于kernal的选择，作者提出了三种可行方案：线性，多项式和高斯核。在后续通过实验对比了它们的性能。 和其他方法的关联如果使用线性核函数，也就是$\phi$是一个identity mapping，那么MMD就成了直接比较两个样本分布质心的距离。这时候，和上文提到的AT方法的一种形式是类似的。（这个我觉得有点强行扯关系。。。） 如果使用二次多项式核函数，可以得到，$\mathcal{L}_{MMD}(F_T, F_S) = \Vert G_T - G_S\Vert_F^2$。其中，$G \in \mathbb{R}^{HW\times HW}$为Gram矩阵，其中的元素$g_{ij} = (f^i)^Tf^j$。 实验作者在CIFAR10，ImageNet等数据集上进行了实验。Student均使用Inception-BN网络，Teacher分别使用了ResNet-1001和ResNet-101。一些具体的参数设置参考论文即可。 下面是CIFAR10上的结果。可以看到，单一方法下，CIFAR10分类NST效果最好，CIFAR100分类KD最好。组合方法中，KD+NST最好。 下面是ImageNet上的结果。KD+NST的组合仍然是效果最好的。 作者还对NST前后，Student和Teacher的输出Feature map做了聚类，发现NST确实能够使得S的输出去接近T的输出分布。如下图所示： 此外，作者还实验了在Detection任务上的表现。在PASCAL VOC2007数据集上基于Faster RCNN方法进行了实验。backbone网络仍然是Inception BN，从4blayer获取feature map，此时stide为16。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Distilling the Knowledge in a Neural Network]]></title>
      <url>%2F2018%2F06%2F07%2Fknowledge-distilling%2F</url>
      <content type="text"><![CDATA[知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文Distilling the Knowledge in a Neural Network。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。 背景这里我将Wang Naiyang在知乎相关问题的回答粘贴如下，将KD方法的motivation讲的很清楚。图森也发了论文对KD进行了改进，下篇笔记总结。 Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。 蒸馏这篇论文很好阅读。论文中实现蒸馏是靠soften softmax prob实现的。在分类任务中，常常使用交叉熵作为损失函数，使用one-hot编码的标注好的类别标签${1,2,\dots,K}$作为target，如下所示： \mathcal{L} = -\sum_{i=1}^{K}t_i\log p_i作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，$2$和$3$就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字$2$时，很有可能它给出的概率是：数字$2$为$0.99$，数字$3$为$10^{-2}$，数字$7$为$10^{-4}$。如何能够利用训练好的Teacher模型给出的这种信息呢？ 可以使用带温度的softmax函数。对于softmax的输入（下文统一称为logit），我们按照下式给出输出： q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}其中，当$T = 1$时，就是普通的softmax变换。这里令$T &gt; 1$，就得到了软化的softmax。（这个很好理解，除以一个比$1$大的数，相当于被squash了，线性的sqush被指数放大，差距就不会这么大了）。OK，有了这个东西，我们将Teacher网络和Student的最后充当分类器的那个全连接层的输出都做这个处理。 对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失： \mathcal{L}_{soft}=-\sum_{i=1}^{K}p_i\log q_i其中，$p_i$为Teacher模型给出的soft target，$q_i$为Student模型给出的soft output。作者发现，最好的方式是做一个multi task learning，将上面这个损失函数和真正的交叉熵损失加权相加。相应地，我们将其称为hard target。 \mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}其中，$\mathcal{L}_{hard}$是分类问题中经典的交叉熵损失。由于做softened softmax计算时，需要除以$T$，导致soft target关联的梯度幅值被缩小了$T^2$倍，所以有必要在$\lambda$中预先考虑到$T^2$这个因子。 PS:这里有一篇地平线烫叔关于多任务中loss函数设计的回答：神经网络中，设计loss function有哪些技巧? - Alan Huang的回答 - 知乎。 实现这里给出一个开源的MXNet的实现:kd loss by mxnet。MXNet中的SoftmaxOutput不仅能直接支持one-hot编码类型的array作为label输入，甚至label的dtype也可以不是整型！ 12345678def kd(student_hard_logits, teacher_hard_logits, temperature, weight_lambda, prefix): student_soft_logits = student_hard_logits / temperature teacher_soft_logits = teacher_hard_logits / temperature teacher_soft_labels = mx.symbol.SoftmaxActivation(teacher_soft_logits, name="teacher%s_soft_labels" % prefix) kd_loss = mx.symbol.SoftmaxOutput(data=student_soft_logits, label=teacher_soft_labels, grad_scale=weight_lambda, name="%skd_loss" % prefix) return kd_loss matching logit是特例（这部分没什么用，练习推导了一下交叉熵损失的梯度计算） 在Hinton之前，有学者提出可以匹配Teacher和Student输出的logit，Hinton指出这是本文方法在一定假设下的近似。为了和论文中的符号相同，下面我们使用$C$表示soft target带来的loss，Teacher和Student第$i$个神经元输出的logit分别为$v_i$和$z_i$，输出的softened softmax分别为$p_i$和$q_i$。那么我们有： C = -\sum_{j=1}^{C}p_j \log q_j而且， p_i = \frac{\exp(v_i/T)}{\sum_j \exp(v_j/T)}q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}让我们暂时忽略$T$（最后我们乘上$\frac{1}{T}$即可），我们有： \frac{\partial C}{\partial z_i} = -\sum_{j=1}^{K}p_j\frac{1}{q_j}\frac{\partial q_j}{\partial z_i}分情况讨论，当$i = j$时，有： \frac{\partial q_j}{\partial z_i} = q_i (1-q_i)当$i \neq j$时，有： \begin{aligned} \frac{\partial q_j}{\partial z_i} &= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2} \\ &=-q_iq_j \end{aligned}这样，我们有： \begin{aligned} \frac{\partial C}{\partial z_i} &= - p_i\frac{1}{q_i}q_i(1-q_i) + \sum_{j=1, j\neq i}^{K}p_j\frac{1}{q_j}q_iq_j \\ &= -p_i + p_iq_i + \sum_{j=1, j\neq i}^K p_jq_i \\ &= q_i -p_i \end{aligned}当然，其实上面的推导过程只不过是重复了一遍one-hot编码的交叉熵损失的计算。 这样，如果我们假设logit是零均值的，也就是说$\sum_j z_j = \sum_j v_j = 0$，那么有： \frac{\partial C}{\partial z_i} \sim \frac{1}{NT^2}(z_i - v_i)所以说，MSE下进行logit的匹配，是本文方法的一个特例。 实验作者使用了MNIST进行图片分类的实验，一个有趣的地方在于（和论文前半部分举的$2$和$3$识别的例子呼应），作者在数据集中有意地去除了标签为$3$的样本。没有KD的student网络不能识别测试时候提供的$3$，有KD的student网络能够识别一些$3$（虽然它从来没有在训练样本中出现过！）。后面，作者在语音识别和一个Google内部的很大的图像分类数据集（JFT dataset）上做了实验， 附 知乎上关于soft target的讨论，有Wang Naiyan和Zhou Bolei的分析：如何理解soft target这一做法？]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[（译）PyTorch 0.4.0 Migration Guide]]></title>
      <url>%2F2018%2F04%2F27%2Fpytorch-040-migration-guide%2F</url>
      <content type="text"><![CDATA[PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个转换指导，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有： Tensor和Variable合并，autograd的机制有所不同，变得更简单，使用requires_grad和上下文相关环境管理。 Numpy风格的Tensor构建。 提出了device，更简单地在cpu和gpu中移动数据。 概述在0.4.0版本中，PyTorch引入了许多令人兴奋的新特性和bug fixes。为了方便以前版本的使用者转换到新的版本，我们编写了此指导，主要包括以下几个重要的方面： Tensors 和 Variables 已经merge到一起了 支持0维的Tensor（即标量scalar） 弃用了 volatile 标志 dtypes, devices, 和 Numpy 风格的 Tensor构造函数 （更好地编写）设备无关代码 下面分条介绍。 Tensor 和 Variable 合并在PyTorch以前的版本中，Tensor类似于numpy中的ndarray，只是对多维数组的抽象。为了能够使用自动求导机制，必须使用Variable对其进行包装。而现在，这两个东西已经完全合并成一个了，以前Variable的使用情境都可以使用Tensor。所以以前训练的时候总要额外写的warpping语句用不到了。 1234for data, target in data_loader: ## 用不到了 data, target = Variable(data), Variable(target) loss = criterion(model(data), target) Tensor的类型type()以前我们可以使用type()获取Tensor的data type（FloatTensor，LongTensor等）。现在需要使用x.type()获取类型或isinstance()判别类型。 1234567&gt;&gt;&gt; x = torch.DoubleTensor([1, 1, 1])&gt;&gt;&gt; print(type(x)) # 曾经会给出 torch.DoubleTensor"&lt;class 'torch.Tensor'&gt;"&gt;&gt;&gt; print(x.type()) # OK: 'torch.DoubleTensor''torch.DoubleTensor'&gt;&gt;&gt; print(isinstance(x, torch.DoubleTensor)) # OK: TrueTrue autograd现在如何追踪计算图的历史Tensor和Variable的合并，简化了计算图的构建，具体规则见本条和以下几条说明。 requires_grad, 这个autograd中的核心标志量,现在成了Tensor的属性。之前的Variable使用规则可以同样应用于Tensor，autograd自动跟踪那些至少有一个input的requires_grad==True的计算节点构成的图。 12345678910111213141516171819202122232425262728&gt;&gt;&gt; x = torch.ones(1) ## 默认requires_grad = False&gt;&gt;&gt; x.requires_gradFalse&gt;&gt;&gt; y = torch.ones(1) ## 同样，y的requires_grad标志也是False&gt;&gt;&gt; z = x + y&gt;&gt;&gt; ## 所有的输入节点都不要求梯度，所以z的requires_grad也是False&gt;&gt;&gt; z.requires_gradFalse&gt;&gt;&gt; ## 所以如果试图对z做梯度反传，会抛出Error&gt;&gt;&gt; z.backward()RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn&gt;&gt;&gt;&gt;&gt;&gt; ## 通过手动指定的方式创建 requires_grad=True 的Tensor&gt;&gt;&gt; w = torch.ones(1, requires_grad=True)&gt;&gt;&gt; w.requires_gradTrue&gt;&gt;&gt; ## 把它和之前requires_grad=False的节点相加，得到输出&gt;&gt;&gt; total = w + z&gt;&gt;&gt; ## 由于w需要梯度，所以total也需要&gt;&gt;&gt; total.requires_gradTrue&gt;&gt;&gt; ## 可以做bp&gt;&gt;&gt; total.backward()&gt;&gt;&gt; w.gradtensor([ 1.])&gt;&gt;&gt; ## 不用有时间浪费在求取 x y z的梯度上，因为它们没有 require grad，它们的grad == None&gt;&gt;&gt; z.grad == x.grad == y.grad == NoneTrue 操作 requires_grad 标志除了直接设置这个属性，你可以使用my_tensor.requires_grad_()就地修改这个标志（还记得吗，以_结尾的方法名表示in-place的操作）。或者就在构造的时候传入此参数。 123456&gt;&gt;&gt; existing_tensor.requires_grad_()&gt;&gt;&gt; existing_tensor.requires_gradTrue&gt;&gt;&gt; my_tensor = torch.zeros(3, 4, requires_grad=True)&gt;&gt;&gt; my_tensor.requires_gradTrue .data怎么办？What about .data?原来版本中，对于某个Variable，我们可以通过x.data的方式获取其包装的Tensor。现在两者已经merge到了一起，如果你调用y = x.data仍然和以前相似，y现在会共享x的data，并与x的计算历史无关，且其requires_grad标志为False。 然而，.data有的时候可能会成为代码中不安全的一个点。对x.data的任何带动都不会被aotograd跟踪。所以，当做反传的时候，计算的梯度可能会不对，一种更安全的替代方法是调用x.detach()，仍然会返回一个共享xdata的Tensor，且requires_grad=False，但是当x需要bp的时候，会报告那些in-place的操作。 However, .data can be unsafe in some cases. Any changes on x.data wouldn’t be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward. 这里有些绕，可以看下下面的示例代码： 12345678910111213141516171819202122# 一个简单的计算图：y = sum(x**2)x = torch.ones((1 ,2))x.requires_grad_()y = torch.sum(x**2)y.backward()x.grad # grad: [2, 2, 2]# 使用.data，在计算完y之后，又改动了x，会造成梯度计算错误x.grad.zero_()y = torch.sum(x**2)data = x.datadata[0, 0] = 2y.backward()x.grad # grad: [4, 2, 2] 错了哦~# 使用detach，同样的操作，会抛出异常x.grad.zero_()y = torch.sum(x**2)data = x.detach()data[0, 0] = 2y.backward()# 抛出如下异常# RuntimeError: one of the variables needed for gradient # computation has been modified by an inplace operation 支持0维(scalar)的Tensor原来的版本中，对Tensor vector（1D Tensor）做索引得到的结果是一个python number，但是对一个Variable vector来说，得到的就是一个size(1,)的vector!对于reduction function（如torch.sum，torch.max）也有这样的问题。 所以我们引入了scalar（0D Tensor）。它可以使用torch.tensor() 函数来创建，现在你可以这样做： 123456789101112131415161718192021&gt;&gt;&gt; torch.tensor(3.1416) # 直接创建scalartensor(3.1416)&gt;&gt;&gt; torch.tensor(3.1416).size() # scalar 是 0Dtorch.Size([])&gt;&gt;&gt; torch.tensor([3]).size() # 和1D对比torch.Size([1])&gt;&gt;&gt;&gt;&gt;&gt; vector = torch.arange(2, 6) # 1D的vector&gt;&gt;&gt; vectortensor([ 2., 3., 4., 5.])&gt;&gt;&gt; vector.size()torch.Size([4])&gt;&gt;&gt; vector[3] # 对1D的vector做indexing，得到的是scalartensor(5.)&gt;&gt;&gt; vector[3].item() # 使用.item()获取python number5.0&gt;&gt;&gt; mysum = torch.tensor([2, 3]).sum()&gt;&gt;&gt; mysumtensor(5)&gt;&gt;&gt; mysum.size()torch.Size([]) 累积losses我们在训练的时候，经常有这样的用法：total_loss += loss.data[0]。loss通常都是由损失函数计算出来的一个标量，也就是包装了(1,)大小Tensor的Variable。在新的版本中，loss则变成了0D的scalar。对一个scalar做indexing是没有意义的，应该使用loss.item()获取python number。 注意，如果你在做累加的时候没有转换为python number，你的程序可能会出现不必要的内存占用。因为autograd会记录调用过程，以便做反向传播。所以，你现在应该写成 total_loss += loss.item()。 弃用volatile标志volatile 标志被弃用了，现在没有任何效果。以前的版本中，一个设置volatile=True的Variable 表明其不会被autograd追踪。现在，被替换成了一个更灵活的上下文管理器，如torch.no_grad()，torch.set_grad_enable(grad_mode)等。 12345678910111213141516171819&gt;&gt;&gt; x = torch.zeros(1, requires_grad=True)&gt;&gt;&gt; with torch.no_grad(): # 使用 torch,no_grad()构建不需要track的上下文环境... y = x * 2&gt;&gt;&gt; y.requires_gradFalse&gt;&gt;&gt;&gt;&gt;&gt; is_train = False&gt;&gt;&gt; with torch.set_grad_enabled(is_train): # 在inference的时候，设置不要track... y = x * 2&gt;&gt;&gt; y.requires_gradFalse&gt;&gt;&gt; torch.set_grad_enabled(True) # 当然也可以不用with构建上下文环境，而单独这样用&gt;&gt;&gt; y = x * 2&gt;&gt;&gt; y.requires_gradTrue&gt;&gt;&gt; torch.set_grad_enabled(False)&gt;&gt;&gt; y = x * 2&gt;&gt;&gt; y.requires_gradFalse dtypes, devices 和NumPy风格的构建函数以前的版本中，我们需要以”tensor type”的形式给出对data type（如float或double），device type（如cpu或gpu）以及layout（dense或sparse）的限定。例如，torch.cuda.sparse.DoubleTensor用来构造一个data type是double，在GPU上以及sparse的tensor。 现在我们引入了torch.dtype，torch.device和torch.layout来更好地使用Numpy风格的构建函数。 torch.dtype下面是可用的 torch.dtypes (data types) 和它们对应的tensor types。可以使用x.dtype获取。 data type torch.dtype Tensor types 32-bit floating point torch.float32 or torch.float torch.*.FloatTensor 64-bit floating point torch.float64 or torch.double torch.*.DoubleTensor 16-bit floating point torch.float16 or torch.half torch.*.HalfTensor 8-bit integer (unsigned) torch.uint8 torch.*.ByteTensor 8-bit integer (signed) torch.int8 torch.*.CharTensor 16-bit integer (signed) torch.int16 or torch.short torch.*.ShortTensor 32-bit integer (signed) torch.int32 or torch.int torch.*.IntTensor 64-bit integer (signed) torch.int64 or torch.long torch.*.LongTensor torch.devicetorch.device包含了device type（如cpu或cuda）和可能的设备id。使用torch.device(&#39;{device_type}&#39;)或torch.device(&#39;{device_type}:{device_ordinal}&#39;)的方式来初始化。 如果没有指定device ordinal，那么默认是当前的device。例如，torch.device(&#39;cuda&#39;)相当于torch.device(&#39;cuda:X&#39;)，其中，X是torch.cuda.current_device()的返回结果。 使用x.device来获取。 torch.layouttorch.layout代表了Tensor的data layout。 目前支持的是torch.strided (dense，也是默认的) 和 torch.sparse_coo (COOG格式的稀疏tensor)。 使用x.layout来获取。 创建Tensor（Numpy风格）你可以使用dtype，device，layout和requires_grad更好地控制Tensor的创建。 12345678910&gt;&gt;&gt; device = torch.device("cuda:1") &gt;&gt;&gt; x = torch.randn(3, 3, dtype=torch.float64, device=device)tensor([[-0.6344, 0.8562, -1.2758], [ 0.8414, 1.7962, 1.0589], [-0.1369, -1.0462, -0.4373]], dtype=torch.float64, device='cuda:1')&gt;&gt;&gt; x.requires_grad # default is FalseFalse&gt;&gt;&gt; x = torch.zeros(3, requires_grad=True)&gt;&gt;&gt; x.requires_gradTrue torch.tensor(data, ...)torch.tensor是新加入的Tesnor构建函数。它接受一个”array-like”的参数，并将其value copy到一个新的Tensor中。可以将它看做numpy.array的等价物。不同于torch.*Tensor方法，你可以创建0D的Tensor（也就是scalar）。此外，如果dtype参数没有给出，它会自动推断。推荐使用这个函数从已有的data，如Python List创建Tensor。 1234567891011&gt;&gt;&gt; cuda = torch.device("cuda")&gt;&gt;&gt; torch.tensor([[1], [2], [3]], dtype=torch.half, device=cuda)tensor([[ 1], [ 2], [ 3]], device='cuda:0')&gt;&gt;&gt; torch.tensor(1) # scalartensor(1)&gt;&gt;&gt; torch.tensor([1, 2.3]).dtype # type inferecetorch.float32&gt;&gt;&gt; torch.tensor([1, 2]).dtype # type inferecetorch.int64 我们还加了更多的Tensor创建方法。其中有一些torch.*_like，tensor.new_*这样的形式。 torch.*_like的参数是一个input tensor， 它返回一个相同属性的tensor，除非有特殊指定。 12345&gt;&gt;&gt; x = torch.randn(3, dtype=torch.float64)&gt;&gt;&gt; torch.zeros_like(x)tensor([ 0., 0., 0.], dtype=torch.float64)&gt;&gt;&gt; torch.zeros_like(x, dtype=torch.int)tensor([ 0, 0, 0], dtype=torch.int32) tensor.new_*类似，不过它通常需要接受一个指定shape的参数。 12345&gt;&gt;&gt; x = torch.randn(3, dtype=torch.float64)&gt;&gt;&gt; x.new_ones(2)tensor([ 1., 1.], dtype=torch.float64)&gt;&gt;&gt; x.new_ones(4, dtype=torch.int)tensor([ 1, 1, 1, 1], dtype=torch.int32) 为了指定shape参数，你可以使用tuple，如torch.zeros((2, 3))（Numpy风格）或者可变数量参数torch.zeros(2, 3)（以前的版本只支持这种）。 Name Returned Tensor torch.*_likevariant tensor.new_*variant torch.empty unintialized memory ✔ ✔ torch.zeros all zeros ✔ ✔ torch.ones all ones ✔ ✔ torch.full filled with a given value ✔ ✔ torch.rand i.i.d. continuous Uniform[0, 1) ✔ torch.randn i.i.d. Normal(0, 1) ✔ torch.randint i.i.d. discrete Uniform in given range ✔ torch.randperm random permutation of {0, 1, ..., n - 1} torch.tensor copied from existing data (list, NumPy ndarray, etc.) ✔ torch.from_numpy* from NumPy ndarray (sharing storage without copying) torch.arange, torch.range and torch.linspace uniformly spaced values in a given range torch.logspace logarithmically spaced values in a given range torch.eye identity matrix 注：torch.from_numpy只接受NumPy ndarray作为输入参数。 书写设备无关代码（device-agnostic code）以前版本很难写设备无关代码。我们使用两种方法使其变得简单： Tensor的device属性可以给出其torch.device（get_device只能获取CUDA tensor） 使用x.to()方法，可以很容易将Tensor或者Module在devices间移动（而不用调用x.cpu()或者x.cuda()。 推荐使用下面的模式： 123456789# 在脚本开始的地方，指定devicedevice = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")## 一些代码# 当你想创建新的Tensor或者Module时候，使用下面的方法# 如果已经在相应的device上了，将不会发生copyinput = data.to(device)model = MyModule(...).to(device) 在nn.Module中对于submodule，parameter和buffer名字新的约束当使用module.add_module(name, value), module.add_parameter(name, value) 或者 module.add_buffer(name, value)时候不要使用空字符串或者包含.的字符串，可能会导致state_dict中的数据丢失。如果你在load这样的state_dict，注意打补丁，并且应该更新代码，规避这个问题。 一个具体的例子下面是一个code snippet，展示了从0.3.1跨越到0.4.0的不同。 0.3.1 version1234567891011121314151617181920model = MyRNN()if use_cuda: model = model.cuda()# traintotal_loss = 0for input, target in train_loader: input, target = Variable(input), Variable(target) hidden = Variable(torch.zeros(*h_shape)) # init hidden if use_cuda: input, target, hidden = input.cuda(), target.cuda(), hidden.cuda() ... # get loss and optimize total_loss += loss.data[0]# evaluatefor input, target in test_loader: input = Variable(input, volatile=True) if use_cuda: ... ... 0.4.0 version1234567891011121314151617# torch.device object used throughout this scriptdevice = torch.device("cuda" if use_cuda else "cpu")model = MyRNN().to(device)# traintotal_loss = 0for input, target in train_loader: input, target = input.to(device), target.to(device) hidden = input.new_zeros(*h_shape) # has the same device &amp; dtype as `input` ... # get loss and optimize total_loss += loss.item() # get Python number from 1-element Tensor# evaluatewith torch.no_grad(): # operations inside don't track history for input, target in test_loader: ... 附 Release Note Documentation]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[JupyterNotebook设置Python环境]]></title>
      <url>%2F2018%2F04%2F09%2Fset-env-in-jupyternotebook%2F</url>
      <content type="text"><![CDATA[使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。 conda的虚拟环境在Anaconda中，我们可以使用conda create -n your_env_name python=your_python_version的方法创建虚拟环境，并使用source activate your_env_name方式激活该虚拟环境，并在其中安装与默认（主）python环境不同的软件包等。 当激活该虚拟环境时，ipython下是可以正常加载的。但是打开Jupyter Notebook，会发现其加载的仍然是默认的Python kernel，而我们需要在notebook中也能使用新添加的虚拟环境。 解决方法解决方法见这个帖子：Conda environments not showing up in Jupyter Notebook. 首先，安装nb_conda_kernels包：1conda install nb_conda_kernels 然后，打开Notebook，点击New，会出现当前所有安装的虚拟环境以供选择，如下所示。 如果是已经编辑过的notebook，只需要打开该笔记本，在菜单栏中选择Kernel -&gt; choose kernel -&gt; your env kernel即可。 关于nb_conda_kernels的详细信息，可以参考其GitHub页面：nb_conda_kernels。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[数值优化之牛顿方法]]></title>
      <url>%2F2018%2F04%2F03%2Fnewton-method%2F</url>
      <content type="text"><![CDATA[简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于Wiki页面。 介绍牛顿方法，是一种用来求解方程$f(x) = 0$的根的方法。从题图可以看出它是如何使用的。 首先，需要给定根的初始值$x_0$。接下来，在函数曲线上找到其所对应的点$(x_0, f(x_0))$，并过该点做切线交$x$轴于一点$x_1$。从$x_1$出发，重复上述操作，直至收敛。 根据图上的几何关系和导数的几何意义，有： x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}优化上的应用做优化的时候，我们常常需要的是求解某个损失函数$L$的极值。在极值点处，函数的导数为$0$。所以这个问题被转换为了求解$L$的导数的零点。我们有 \theta_{n+1} = \theta_n - \frac{L^\prime(\theta_n)}{L^{\prime\prime}(\theta_n)}推广到向量形式机器学习中的优化问题常常是在高维空间进行，可以将其推广到向量形式： \theta_{n+1} = \theta_n - H^{-1}\nabla_\theta L(\theta_n)其中，$H$表示海森矩阵，是一个$n\times n$的矩阵，其中元素为： H_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}特别地，当海森矩阵为正定时，此时的极值为极小值（可以使用二阶的泰勒展开式证明）。 PS:忘了什么是正定矩阵了吗？想想二次型的概念，对于$\forall x$不为$0$向量，都有$x^THx &gt; 0$。 优缺点牛顿方法的收敛速度较SGD为快（二阶收敛），但是会涉及到求解一个$n\times n$的海森矩阵的逆，所以虽然需要的迭代次数更少，但反而可能比较耗时（$n$的大小）。 L-BFGS由于牛顿方法中需要计算海森矩阵的逆，所以很多时候并不实用。大家就想出了一些近似计算$H^{-1}$的方法，如L-BFGS等。 推导过程待续。。。 L-BFGS的资料网上还是比较多的，这里有一个PyTorch中L-BFGS方法的实现：optim.lbfgs。 这里有一篇不错的文章数值优化：理解L-BFGS算法，本博客写作过程参考很多。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Feature Pyramid Networks for Object Detection (FPN)]]></title>
      <url>%2F2018%2F04%2F02%2Fpaper-fpn%2F</url>
      <content type="text"><![CDATA[图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取SIFT特征就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。 Pyramid or not? It’s a question.在DL席卷CV之前，特征大多需要研究人员手工设计，如SIFT/Harr/HoG等。人们在使用这些特征的时候发现，往往需要使用图像金字塔，在multi scale下进行检测，才能得到不错的结果。然而，使用CNN时，由于其本身具有的一定的尺度不变性，大家常常是只在单一scale下（也就是原始图像作为输入），就可以达到不错的结果。不过很多时候参加COCO等竞赛的队伍还是会在TEST的时候使用这项技术，能够取得更好的成绩。但是这样会造成计算时间的巨大开销，TRAIN和TEST的不一致。TRAIN中引入金字塔，内存就会吃紧。所以主流的Fast/Faster RCNN并没有使用金字塔。 换个角度，我们知道在CNN中，输入会逐层处理，经过Conv/Pooling的操作后，不同深度的layer产生的feature map的spatial dimension是不一样的，这就是作者在摘要中提到的“inherent multi-scale pyramidal hierarchy of deep CNN”。不过，还有一个问题，就是深层和浅层的feature map虽然构成了一个feature pyramid，但是它们的语义并不对等：深层layer的feature map有更抽象的语义信息，而浅层feature map有较高的resolution，但是语义信息还是too yong too simple。 SSD做过这方面的探索。但是它采用的方法是从浅层layer引出，又加了一些layer，导致无法reuse high resolution的feature map。我们发现，浅层的high resolution feature map对检测小目标很有用处。 那我们想要怎样呢？ 高层的low resolution，strong semantic info特征如何和浅层的high resolution，weak semantic info自然地结合？ 不引入过多的额外计算，最好也只需要用single scale的原始输入。 用一张图总结一下。下图中蓝色的轮廓线框起来的就是不同layer输出的feature map。蓝色线越粗，代表其语义信息越强。在（a）中，是将图像做成金字塔，分别跑一个NN来做，这样计算量极大。（b）中是目前Faster RCNN等采用的方法，只在single scale上做。（c）中是直接将各个layer输出的层级feature map自然地看做feature pyramid来做。（d）是本文的方法，不同层级的feature map做了merge，能够使得每个level的语义信息都比较强（注意看蓝色线的粗细）。 我们使用这种名为FPN的技术，不用什么工程上的小花招，就打败了目前COCO上的最好结果。不止detection，FPN也能用在图像分割上（当然，现在我们知道，MaskRCNN中的关键技术之一就是FPN）。 FPN有人可能会想，其实前面的网络有人也做过不同深度layer的merge啊，通过skip connection就可以了。作者指出，那种方法仍然是只能在最终的single scale的output feature map上做，而我们的方法是在all level上完成，如下图所示。 Bottom-up pathway Bottom-up pathway指的是网络的前向计算部分，会产生一系列scale相差2x的feature map。当然，在这些downsample中间，还会有layer的输出spatial dimension是一致的。那些连续的有着相同spatial dimension输出的layer是一个stage。这样，我们就完成了传统金字塔方法和CNN网络的名词的对应。 以ResNet为例，我们用每个stage中最后一个residual block的输出作为构建金字塔的feature map，也就是C2~C5。它们的步长分别是$4, 8, 16, 32$。我们没用conv1。 Top-down pathway和lateral connectionTop-down pathway是指将深层的有更强语义信息的feature经过upsampling变成higher resolution的过程。然后再与bottom-up得到的feature经过lateral connection（侧边连接）进行增强。 下面这张图展示了做lateral connection的过程。注意在图中，越深的layer位于图的上部。我们以框出来放大的那部分举例子。从更深的层输出的feature经过2x up处理（spatial dimension一致了），从左面来的浅层的feature经过1x1 conv处理（channel dimension一致了），再进行element-wise的相加，得到了该stage最后用于prediction的feature（其实还要经过一个3x3 conv的处理，见下引文）。 一些细节，直接引用： To start the iteration, we simply attach a 1x1 convolutional layer on C5 to produce the coarsest resolution map. Finally, we append a 3x3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling. 此外，由于金字塔上的所有feature共享classifier和regressor，要求它们的channel dimension必须一致。本文固定使用$256$。而且这些外的conv layer没有使用非线性激活。 这里给出一个基于PyTorch的FPN的第三方实现kuangliu/pytorch-fpn，可以对照论文捋一遍。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109## ResNet的blockclass Bottleneck(nn.Module): expansion = 4 def __init__(self, in_planes, planes, stride=1): super(Bottleneck, self).__init__() self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False) self.bn1 = nn.BatchNorm2d(planes) self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=stride, padding=1, bias=False) self.bn2 = nn.BatchNorm2d(planes) self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=1, bias=False) self.bn3 = nn.BatchNorm2d(self.expansion*planes) self.shortcut = nn.Sequential() if stride != 1 or in_planes != self.expansion*planes: self.shortcut = nn.Sequential( nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(self.expansion*planes) ) def forward(self, x): out = F.relu(self.bn1(self.conv1(x))) out = F.relu(self.bn2(self.conv2(out))) out = self.bn3(self.conv3(out)) out += self.shortcut(x) out = F.relu(out) return outclass FPN(nn.Module): def __init__(self, block, num_blocks): super(FPN, self).__init__() self.in_planes = 64 self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False) self.bn1 = nn.BatchNorm2d(64) # Bottom-up layers, backbone of the network self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1) self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2) self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2) self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2) # Top layer # 我们需要在C5后面接一个1x1, 256 conv，得到金字塔最顶端的feature self.toplayer = nn.Conv2d(2048, 256, kernel_size=1, stride=1, padding=0) # Reduce channels # Smooth layers # 这个是上面引文中提到的抗aliasing的3x3卷积 self.smooth1 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) self.smooth2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) self.smooth3 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1) # Lateral layers # 为了匹配channel dimension引入的1x1卷积 # 注意这些backbone之外的extra conv，输出都是256 channel self.latlayer1 = nn.Conv2d(1024, 256, kernel_size=1, stride=1, padding=0) self.latlayer2 = nn.Conv2d( 512, 256, kernel_size=1, stride=1, padding=0) self.latlayer3 = nn.Conv2d( 256, 256, kernel_size=1, stride=1, padding=0) def _make_layer(self, block, planes, num_blocks, stride): strides = [stride] + [1]*(num_blocks-1) layers = [] for stride in strides: layers.append(block(self.in_planes, planes, stride)) self.in_planes = planes * block.expansion return nn.Sequential(*layers) ## FPN的lateral connection部分: upsample以后，element-wise相加 def _upsample_add(self, x, y): '''Upsample and add two feature maps. Args: x: (Variable) top feature map to be upsampled. y: (Variable) lateral feature map. Returns: (Variable) added feature map. Note in PyTorch, when input size is odd, the upsampled feature map with `F.upsample(..., scale_factor=2, mode='nearest')` maybe not equal to the lateral feature map size. e.g. original input size: [N,_,15,15] -&gt; conv2d feature map size: [N,_,8,8] -&gt; upsampled feature map size: [N,_,16,16] So we choose bilinear upsample which supports arbitrary output sizes. ''' _,_,H,W = y.size() return F.upsample(x, size=(H,W), mode='bilinear') + y def forward(self, x): # Bottom-up c1 = F.relu(self.bn1(self.conv1(x))) c1 = F.max_pool2d(c1, kernel_size=3, stride=2, padding=1) c2 = self.layer1(c1) c3 = self.layer2(c2) c4 = self.layer3(c3) c5 = self.layer4(c4) # Top-down # P5: 金字塔最顶上的feature p5 = self.toplayer(c5) # P4: 上一层 p5 + 侧边来的 c4 # 其余同理 p4 = self._upsample_add(p5, self.latlayer1(c4)) p3 = self._upsample_add(p4, self.latlayer2(c3)) p2 = self._upsample_add(p3, self.latlayer3(c2)) # Smooth # 输出做一下smooth p4 = self.smooth1(p4) p3 = self.smooth2(p3) p2 = self.smooth3(p2) return p2, p3, p4, p5 应用下面作者会把FPN应用到FasterRCNN的两个重要步骤：RPN和Fast RCNN。 FPN加持的RPN在Faster RCNN中，RPN用来提供ROI的proposal。backbone网络输出的single feature map上接了$3\times 3$大小的卷积核来实现sliding window的功能，后面接两个$1\times 1$的卷积分别用来做objectness的分类和bounding box基于anchor box的回归。我们把最后的classifier和regressor部分叫做head。 使用FPN时，我们在金字塔每层的输出feature map上都接上这样的head结构（$3\times 3$的卷积 + two sibling $1\times 1$的卷积）。同时，我们不再使用多尺度的anchor box，而是在每个level上分别使用不同大小的anchor box。具体说，对应于特征金字塔的$5$个level的特征，P2 - P6，anchor box的大小分别是$32^2, 64^2, 128^2, 256^2, 512^2$。不过每层的anchor box仍然要照顾到不同的长宽比例，我们使用了$3$个不同的比例：$1:2, 1:1, 2:1$（和原来一样）。这样，我们一共有$5\times 3 = 15$个anchor box。 训练过程中，我们需要给anchor boxes赋上对应的正负标签。对于那些与ground truth有最大IoU或者与任意一个ground truth的IoU超过$0.7$的anchor boxes，是positive label；那些与所有ground truth的IoU都小于$0.3$的是negtive label。 有一个疑问是head的参数是否要在不同的level上共享。我们试验了共享与不共享两个方法，accuracy是相近的。这也说明不同level之间语义信息是相似的，只是resolution不同。 FPN加持的Fast RCNNFast RCNN的原始方法是只在single scale的feature map上做的，要想使用FPN，首先应该解决的问题是前端提供的ROI proposal应该对应到pyramid的哪一个label。由于我们的网络基本都是在ImageNet训练的网络上做transfer learning得到的，我们就以base model在ImageNet上训练时候的输入$224\times 224$作为参考，依据当前ROI和它的大小比例，确定该把这个ROI对应到哪个level。如下所示： k = \lfloor k_0 + \log_2(\sqrt{wh}/224)\rfloor后面接的predictor head我们这里直接连了两个$1024d$的fc layer，再接final classification和regression的部分。同样的，这些参数对于不同level来说是共享的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - YOLO v3]]></title>
      <url>%2F2018%2F04%2F01%2Fpaper-yolov3%2F</url>
      <content type="text"><![CDATA[YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：YOLOv3: An Incremental Improvement。下面这张图是YOLO V3与RetinaNet的比较。 可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。 YOLO v3比你们不知道高到哪里去了YOLO v3在保持其一贯的检测速度快的特点前提下，性能又有了提升：输入图像为$320\times 320$大小的图像，可以在$22$ms跑完，mAP达到了$28.2$，这个数据和SSD相同，但是快了$3$倍。在TitanX上，YOLO v3可以在$51$ms内完成，$AP_{50}$的值为$57.9$。而RetinaNet需要$198$ms，$AP_{50}$近似却略低，为$57.5$。 ps：啥是APAP就是average precision啦。在detection中，我们认为当预测的bounding box和ground truth的IoU大于某个阈值（如取为$0.5$）时，认为是一个True Positive。如果小于这个阈值，就是一个False Positive。 所谓precision，就是指检测出的框框中有多少是True Positive。另外，还有一个指标叫做recall，是指所有的ground truth里面，有多少被检测出来了。这两个概念都是来自于classification问题，通过设定上面IoU的阈值，就可以迁移到detection中了。 我们可以取不同的阈值，这样就可以绘出一条precisio vs recall的曲线，计算曲线下的面积，就是AP值。COCO中使用了0.5:0.05:0.95十个离散点近似计算（参考COCO的说明文档网页）。detection中常常需要同时检测图像中多个类别的物体，我们将不同类别的AP求平均，就是mAP。 如果我们只看某个固定的阈值，如$0.5$，计算所有类别的平均AP，那么就用$AP_{50}$来表示。所以YOLO v3单拿出来$AP_{50}$说事，是为了证明虽然我的bounding box不如你RetinaNet那么精准（IoU相对较小），但是如果你对框框的位置不是那么敏感（$0.5$的阈值很多时候够用了），那么我是可以做到比你更好更快的。 Bounding Box位置的回归这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的$p_w$和$p_h$）。网络预测四个值：$t_x$，$t_y$，$t_w$，$t_h$。我们知道，YOLO网络最后输出是一个$M\times M$的feature map，对应于$M \times M$个cell。如果某个cell距离image的top left corner距离为$(c_x, c_y)$（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为： \begin{aligned}b_x &= \sigma(t_x) + c_x\\ b_y &= \sigma(t_y) + c_y\\ b_w &= p_w e^{t_w}\\ b_h &= p_h e^{t_h}\end{aligned}PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$b_w = p_w t_w^\prime$），而是要做一个对数变换，实际预测的是$\log(\cdot)$。这里小小解释一下。 这是因为如果不做变换，直接预测相对形变$t_w^\prime$，那么要求$t_w^\prime &gt; 0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。 在训练的时候，使用平方误差损失。 另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给$1$；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取$0.5$），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有倍assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。 分类预测我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如Woman和Person这样的重叠标签。 FPN加持的多尺度预测之前YOLO的一个弱点就是缺少多尺度变换，使用FPN中的思路，v3在$3$个不同的尺度上做预测。在COCO上，我们每个尺度都预测$3$个框框，所以一共是$9$个。所以输出的feature map的大小是$N\times N\times [3\times (4+1+80)]$。 然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了$2$倍。 照上一段所说方法，再一次在final scale尺度下给出预测。 代码实现在v3中，作者新建了一个名为yolo的layer，其参数如下：12345678910[yolo]mask = 0,1,2## 9组anchor对应9个框框anchors = 10,13, 16,30, 33,23, 30,61, 62,45, 59,119, 116,90, 156,198, 373,326classes=20 ## VOC20类num=9jitter=.3ignore_thresh = .5truth_thresh = 1random=1 打开yolo_layer.c文件，找到forward部分代码。可以看到，首先，对输入进行activation。注意，如论文所说，对类别进行预测的时候，没有使用v2中的softmax或softmax tree，而是直接使用了logistic变换。12345678910for (b = 0; b &lt; l.batch; ++b)&#123; for(n = 0; n &lt; l.n; ++n)&#123; int index = entry_index(l, b, n*l.w*l.h, 0); // 对 tx, ty进行logistic变换 activate_array(l.output + index, 2*l.w*l.h, LOGISTIC); index = entry_index(l, b, n*l.w*l.h, 4); // 对confidence和C类进行logistic变换 activate_array(l.output + index, (1+l.classes)*l.w*l.h, LOGISTIC); &#125;&#125; 我们看一下如何计算梯度。12345678910111213141516171819202122232425262728293031323334353637383940414243444546for (j = 0; j &lt; l.h; ++j) &#123; for (i = 0; i &lt; l.w; ++i) &#123; for (n = 0; n &lt; l.n; ++n) &#123; // 对每个预测的bounding box // 找到与其IoU最大的ground truth int box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 0); box pred = get_yolo_box(l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.w*l.h); float best_iou = 0; int best_t = 0; for(t = 0; t &lt; l.max_boxes; ++t)&#123; box truth = float_to_box(net.truth + t*(4 + 1) + b*l.truths, 1); if(!truth.x) break; float iou = box_iou(pred, truth); if (iou &gt; best_iou) &#123; best_iou = iou; best_t = t; &#125; &#125; int obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 4); avg_anyobj += l.output[obj_index]; // 计算梯度 // 如果大于ignore_thresh, 那么忽略 // 如果小于ignore_thresh，target = 0 // diff = -gradient = target - output // 为什么是上式，见下面的数学分析 l.delta[obj_index] = 0 - l.output[obj_index]; if (best_iou &gt; l.ignore_thresh) &#123; l.delta[obj_index] = 0; &#125; // 这里仍然有疑问，为何使用truth_thresh?这个值是1 // 按道理，iou无论如何不可能大于1啊。。。 if (best_iou &gt; l.truth_thresh) &#123; // confidence target = 1 l.delta[obj_index] = 1 - l.output[obj_index]; int class = net.truth[best_t*(4 + 1) + b*l.truths + 4]; if (l.map) class = l.map[class]; int class_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, 4 + 1); // 对class进行求导 delta_yolo_class(l.output, l.delta, class_index, class, l.classes, l.w*l.h, 0); box truth = float_to_box(net.truth + best_t*(4 + 1) + b*l.truths, 1); // 对box位置参数进行求导 delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.delta, (2-truth.w*truth.h), l.w*l.h); &#125; &#125; &#125;&#125; 我们首先来说一下为何confidence（包括后面的classification）的diff计算为何是target - output的形式。对于logistic regression，假设logistic函数的输入是$o = f(x;\theta)$。其中，$\theta$是网络的参数。那么输出$y = h(o)$，其中$h$指logistic激活函数（或sigmoid函数）。那么，我们有： \begin{aligned}P(y=1|x) &= h(o)\\ P(y=0|x) &= 1-h(o)\end{aligned}写出对数极大似然函数，我们有： \log L = \sum y\log h+(1-y)\log(1-h)为了使用SGD，上式两边取相反数，我们有损失函数： J = -\log L = \sum -y\log h-(1-y)\log(1-h)对第$i$个输入$o_i$求导，我们有： \begin{aligned}\frac{\partial J}{\partial o_i} &= \frac{\partial J}{\partial h_i}\frac{\partial h_i}{\partial o_i}\\ &= [-y_i/h_i-(y_i-1)/(1-h_i)] \frac{\partial h_i}{\partial o_i} \\ &= \frac{h_i-y_i}{h_i(1-h_i)} \frac{\partial h_i}{\partial o_i}\end{aligned}根据logistic函数的求导性质，有： \frac{\partial h_i}{\partial o_i} = h_i(1-h_i)所以，有 \frac{\partial J}{\partial o_i} = h_i-y_i其中，$h_i$即为logistic激活后的输出，$y_i$为target。由于YOLO代码中均使用diff，也就是-gradient，所以有delta = target - output。 关于logistic回归，还可以参考我的博客：CS229 简单的监督学习方法。 下面，我们看下两个关键的子函数，delta_yolo_class和delta_yolo_box的实现。1234567891011121314151617181920212223242526272829303132333435// class是类别的ground truth// classes是类别总数// index是feature map一维数组里面class prediction的起始索引void delta_yolo_class(float *output, float *delta, int index, int class, int classes, int stride, float *avg_cat) &#123; int n; // 这里暂时不懂 if (delta[index])&#123; delta[index + stride*class] = 1 - output[index + stride*class]; if(avg_cat) *avg_cat += output[index + stride*class]; return; &#125; for(n = 0; n &lt; classes; ++n)&#123; // 见上，diff = target - prediction delta[index + stride*n] = ((n == class)?1 : 0) - output[index + stride*n]; if(n == class &amp;&amp; avg_cat) *avg_cat += output[index + stride*n]; &#125;&#125;// box delta这里没什么可说的，就是square error的求导float delta_yolo_box(box truth, float *x, float *biases, int n, int index, int i, int j, int lw, int lh, int w, int h, float *delta, float scale, int stride) &#123; box pred = get_yolo_box(x, biases, n, index, i, j, lw, lh, w, h, stride); float iou = box_iou(pred, truth); float tx = (truth.x*lw - i); float ty = (truth.y*lh - j); float tw = log(truth.w*w / biases[2*n]); float th = log(truth.h*h / biases[2*n + 1]); delta[index + 0*stride] = scale * (tx - x[index + 0*stride]); delta[index + 1*stride] = scale * (ty - x[index + 1*stride]); delta[index + 2*stride] = scale * (tw - x[index + 2*stride]); delta[index + 3*stride] = scale * (th - x[index + 3*stride]); return iou;&#125; 上面，我们遍历了每一个prediction的bounding box，下面我们还要遍历每个ground truth，根据IoU，为其分配一个最佳的匹配。12345678910111213141516171819202122232425262728293031323334353637383940414243// 遍历ground truthfor(t = 0; t &lt; l.max_boxes; ++t)&#123; box truth = float_to_box(net.truth + t*(4 + 1) + b*l.truths, 1); if(!truth.x) break; // 找到iou最大的那个bounding box float best_iou = 0; int best_n = 0; i = (truth.x * l.w); j = (truth.y * l.h); box truth_shift = truth; truth_shift.x = truth_shift.y = 0; for(n = 0; n &lt; l.total; ++n)&#123; box pred = &#123;0&#125;; pred.w = l.biases[2*n]/net.w; pred.h = l.biases[2*n+1]/net.h; float iou = box_iou(pred, truth_shift); if (iou &gt; best_iou)&#123; best_iou = iou; best_n = n; &#125; &#125; int mask_n = int_index(l.mask, best_n, l.n); if(mask_n &gt;= 0)&#123; int box_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 0); float iou = delta_yolo_box(truth, l.output, l.biases, best_n, box_index, i, j, l.w, l.h, net.w, net.h, l.delta, (2-truth.w*truth.h), l.w*l.h); int obj_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4); avg_obj += l.output[obj_index]; // 对应objectness target = 1 l.delta[obj_index] = 1 - l.output[obj_index]; int class = net.truth[t*(4 + 1) + b*l.truths + 4]; if (l.map) class = l.map[class]; int class_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, 4 + 1); delta_yolo_class(l.output, l.delta, class_index, class, l.classes, l.w*l.h, &amp;avg_cat); ++count; ++class_count; if(iou &gt; .5) recall += 1; if(iou &gt; .75) recall75 += 1; avg_iou += iou; &#125;&#125; Darknet网络架构引入了ResidualNet的思路（$3\times 3$和$1\times 1$的卷积核，shortcut连接），构建了Darknet-53网络。 YOLO的优势和劣势把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚： Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! “Training humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.” [16] If humans have a hard time telling the difference, how much does it matter? 使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。 下面是具体的数据比较。 我们是身经百战，见得多了作者还贴心地给出了什么方法没有奏效。 anchor box坐标$(x, y)$的预测。预测anchor box的offset，no stable，不好。 线性offset预测，而不是logistic。精度下降。 focal loss。精度下降。 双IoU阈值，像Faster RCNN那样。效果不好。 参考资料下面是一些可供利用的参考资料： YOLO的项目主页Darknet YOLO 作者主页上的paper链接 知乎专栏上的全文翻译 FPN论文Feature pyramid networks for object detection 知乎上的解答：AP是什么，怎么计算]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and]]></title>
      <url>%2F2018%2F03%2F24%2Fpaper-squeezenet%2F</url>
      <content type="text"><![CDATA[SqueezeNet由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。 模型压缩相关工作模型压缩的好处主要有以下几点： 更好的分布式训练。server之间的通信往往限制了分布式训练的提速比例，较少的网络参数能够降低对server间通信需求。 云端向终端的部署，需要更低的带宽，例如手机app更新或无人车的软件包更新。 更易于在FPGA等硬件上部署，因为它们往往都有着非常受限的片上RAM。 相关工作主要有两个方向，即模型压缩和模型结构自身探索。 模型压缩方面的工作主要有，使用SVD分解，Deep Compression等。模型结构方面比较有意义的工作是GoogLeNet的Inception module（可在博客内搜索Xception查看Xception的作者是如何受此启发发明Xception结构的）。 本文的作者从网络设计角度出发，提出了名为SqueezeNet的网络结构，使用比AlexNet少$50$倍的参数，在ImageNet上取得了comparable的结果。此外，还探究了CNN的arch是如何影响model size和最终的accuracy的。主要从两个方面进行了探索，分别是CNN microarch和CNN macroarch。前者意为在更小的粒度上，如每一层的layer怎么设计，来考察；后者是在更为宏观的角度，如一个CNN中的不同layer该如何组织来考察。 PS: 吐槽：看完之后觉得基本没探索出什么太有用的可以迁移到其他地方的规律。。。只是比较了自己的SqueezeNet在不同参数下的性能，有些标题党之嫌，题目很大，但是里面的内容并不完全是这样。CNN的设计还是实验实验再实验。 SqueezeNet为了简单，下文简称SNet。SNet的基本组成是叫做Fire的module。我们知道，对于一个CONV layer，它的参数数量计算应该是：$K \times K \times M \times N$。其中，$K$是filter的spatial size，$M$和$N$分别是输入feature map和输出activation的channel size。由此，设计SNet时，作者的依据主要是以下几点： 把$3\times 3$的卷积替换成$1\times 1$，相当于减小上式中的$K$。 减少$3\times 3$filter对应的输入feature map的channel，相当于减少上式的$M$。 delayed downsample。使得activation的feature map能够足够大，这样对提高accuracy有益。CNN中的downsample主要是通过CONV layer或pooling layer中stride设置大于$1$得到的，作者指出，应将这种操作尽量后移。 Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal. Fire ModuleFire Module是SNet的基本组成单元，如下图所示。可以分为两个部分，一个是上面的squeeze部分，是一组$1\times 1$的卷积，用来将输入的channel squeeze到一个较小的值。后面是expand部分，由$1\times 1$和$3\times 3$卷积mix起来。使用$s_{1 x 1}$，$e_{1x1}$和$e_{3x3}$表示squeeze和expand中两种不同卷积的channel数量，令$s_{1x1} &lt; e_{1x1} + e_{3x3}$，用来实现上述策略2. 下面，对照PyTorch实现的SNet代码看下Fire的实现，注意上面说的CONV后面都接了ReLU。1234567891011121314151617181920212223class Fire(nn.Module): def __init__(self, inplanes, squeeze_planes, expand1x1_planes, expand3x3_planes): super(Fire, self).__init__() self.inplanes = inplanes ## squeeze 部分 self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=1) self.squeeze_activation = nn.ReLU(inplace=True) ## expand 1x1 部分 self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes, kernel_size=1) self.expand1x1_activation = nn.ReLU(inplace=True) ## expand 3x3部分 self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes, kernel_size=3, padding=1) self.expand3x3_activation = nn.ReLU(inplace=True) def forward(self, x): x = self.squeeze_activation(self.squeeze(x)) ## 将expand 部分1x1和3x3的cat到一起 return torch.cat([ self.expand1x1_activation(self.expand1x1(x)), self.expand3x3_activation(self.expand3x3(x))], 1) SNet有了Fire Module这个基础材料，我们就可以搭建SNet了。一个单独的conv1 layer，后面接了$8$个连续的Fire Module，最后再接一个conv10 layer。此外，在conv1，fire4, fire8和conv10后面各有一个stride=2的MAX Pooling layer。这些pooling的位置相对靠后，是对上述策略$3$的实践。我们还可以在不同的Fire Module中加入ResNet中的bypass结构。这样，形成了下图三种不同的SNet结构。 一些细节： 为了使得$1\times 1$和$3\times 3$的卷积核能够有相同spatial size的输出，$3\times 3$的卷积输入加了padding=1。 在squeeze layer和expand layer中加入了ReLU。 在fire 9后加入了drop ratio为$0.5$的Dropout layer。 受NIN启发，SNet中没有fc层。 更多的细节和训练参数的设置可以参考GitHub上的官方repo。 同样的，我们可以参考PyTorch中的实现。注意下面实现了v1.0和v1.1版本，两者略有不同。v1.1版本参数更少，也能够达到v1.0的精度。 SqueezeNet v1.1 (in this repo), which requires 2.4x less computation than SqueezeNet v1.0 without diminshing accuracy. 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647class SqueezeNet(nn.Module): def __init__(self, version=1.0, num_classes=1000): super(SqueezeNet, self).__init__() if version not in [1.0, 1.1]: raise ValueError("Unsupported SqueezeNet version &#123;version&#125;:" "1.0 or 1.1 expected".format(version=version)) self.num_classes = num_classes if version == 1.0: self.features = nn.Sequential( nn.Conv2d(3, 96, kernel_size=7, stride=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(96, 16, 64, 64), Fire(128, 16, 64, 64), Fire(128, 32, 128, 128), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(256, 32, 128, 128), Fire(256, 48, 192, 192), Fire(384, 48, 192, 192), Fire(384, 64, 256, 256), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(512, 64, 256, 256), ) else: self.features = nn.Sequential( nn.Conv2d(3, 64, kernel_size=3, stride=2), nn.ReLU(inplace=True), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(64, 16, 64, 64), Fire(128, 16, 64, 64), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(128, 32, 128, 128), Fire(256, 32, 128, 128), nn.MaxPool2d(kernel_size=3, stride=2, ceil_mode=True), Fire(256, 48, 192, 192), Fire(384, 48, 192, 192), Fire(384, 64, 256, 256), Fire(512, 64, 256, 256), ) # Final convolution is initialized differently form the rest final_conv = nn.Conv2d(512, self.num_classes, kernel_size=1) self.classifier = nn.Sequential( nn.Dropout(p=0.5), final_conv, nn.ReLU(inplace=True), nn.AvgPool2d(13, stride=1)) 实验把SNet和AlexNet分别经过Deep Compression，在ImageNet上测试结果如下。可以看到，未被压缩时，SNet比AlexNet少了$50$倍，accuracy是差不多的。经过压缩，SNet更是可以进一步瘦身成不到$0.5$M，比原始的AlexNet瘦身了$500+$倍。 注意上述结果是使用HanSong的Deep Compression技术（聚类+codebook）得到的。这种方法得到的模型在通用计算平台（CPU/GPU）上的优势并不明显，需要在作者提出的EIE硬件上才能充分发挥其性能。对于线性的量化（直接用量化后的$8$位定点存储模型），Ristretto实现了SNet的量化，但是有一个点的损失。 Micro Arch探索所谓CNN的Micro Arch，是指如何确定各层的参数，如filter的个数，kernel size的大小等。在SNet中，主要是filter的个数，即上文提到的$s_{1x1}$，$e_{1x1}$和$e_{3x3}$。这样，$8$个Fire Module就有$24$个超参数，数量太多，我们需要加一些约束，暴露主要矛盾，把问题变简单一点。 我们设定$base_e$是第一个Fire Module的expand layer的filter个数，每隔$freq$个Fire Module，会加上$incr_e$这么多。那么任意一个Fire Module的expand layer filter的个数为$e_i = base_e + (incr_e \times \lfloor \frac{i}{freq}\rfloor)$。 在expand layer，我们有$e_i = e_{i,1x1} + e_{i,3x3}$，设定$pct_{3x3} = e_{i,3x3}/e_i$为$3\times 3$的conv占的比例。 设定$SR = s_{i,1x1} / e_i$，为squeeze和expand filter个数比例。 SR的影响$SR$于区间$[0.125, 1]$之间取，accuracy基本随着$SR$增大而提升，同时模型的size也在变大。但$SR$从$0.75$提升到$1.0$，accuracy无提升。publish的SNet使用了$SR=0.125$。 1X1和3x3的比例pct的影响为了减少参数，我们把部分$3\times 3$的卷积换成了$1\times 1$的，构成了expand layer。那么两者的比例对模型的影响？$pct$在$[0.01, 0.99]$之间变化。同样，accuracy和model size基本都随着$pct$增大而提升。当大于$0.5$时，模型的accuracy基本无提升。 Macro Arch探索这里主要讨论了是否使用ResNet中的bypass结构。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - MobileNets, Efficient Convolutional Neural Networks for Mobile Vision Applications]]></title>
      <url>%2F2018%2F03%2F23%2Fpaper-mobilenet%2F</url>
      <content type="text"><![CDATA[MobileNet是建立在Depthwise Separable Conv基础之上的一个轻量级网络。在本论文中，作者定量计算了使用这一技术带来的计算量节省，提出了MobileNet的结构，同时提出了两个简单的超参数，可以灵活地进行模型性能和inference时间的折中。后续改进的MobileNet v2以后讨论。 Depthwise Separable ConvDepthwise Separable Conv把卷积操作拆成两个部分。第一部分，depthwise conv时，每个filter只在一个channel上进行操作。第二部分，pointwise conv是使用$1\times 1$的卷积核做channel上的combination。在Caffe等DL框架中，一般是设定卷积层的group参数，使其等于input的channel数来实现depthwise conv的。而pointwise conv和使用标准卷积并无不同，只是需要设置kernel size = 1。如下，是使用PyTorch的一个例子。 1234567891011def conv_dw(inp, oup, stride): return nn.Sequential( ## 通过设置group=input channels来实现depthwise conv nn.Conv2d(inp, inp, 3, stride, 1, groups=inp, bias=False), nn.BatchNorm2d(inp), nn.ReLU(inplace=True), nn.Conv2d(inp, oup, 1, 1, 0, bias=False), nn.BatchNorm2d(oup), nn.ReLU(inplace=True), ) 这样做的好处就是能够大大减少计算量。假设原始conv的filter个数为$N$，kernel size大小为$D_k$，输入的维度为$D_F\times D_F\times M$，那么总的计算量是$D_K\times D_K\times M\times N\times D_F\times D_F$（设定stride=1，即输入输出的feature map在spatial两个维度上相同）。 改成上述Depthwise Separable Conv后，计算量变为两个独立操作之和，即$D_K\times D_K\times M\times D_F \times D_F + M\times N\times D_F\times D_F$，计算量是原来的$\frac{1}{N} + \frac{1}{D_K^2} &lt; 1$。 在实际使用时，我们在两个卷积操作之间加上BN和非线性变换层，如下图所示： MobileNet下图展示了如何使用Depthwise Separable Conv构建MobileNet。表中的dw表示depthwise conv，后面接的stride=1的conv即为pointwise conv。可以看到，网络就是这样的单元堆叠而成的。最后使用了一个全局的均值pooling，后面接上fc-1000来做分类。 此外，作者指出目前的深度学习框架大多使用GEMM实现卷积层的计算（如Caffe等先使用im2col，再使用GEMM）。但是pointwis= conv其实不需要reordering，说明目前的框架这里还有提升的空间。（不清楚目前PyTorch，TensorFlow等对pointwise conv和depthwise conv的支持如何） 在训练的时候，一个注意的地方是，对depthwise conv layer，weight decay的参数要小，因为这层本来就没多少个参数。 这里，给出PyTorch的一个第三方实现。123456789101112131415161718self.model = nn.Sequential( conv_bn( 3, 32, 2), conv_dw( 32, 64, 1), conv_dw( 64, 128, 2), conv_dw(128, 128, 1), conv_dw(128, 256, 2), conv_dw(256, 256, 1), conv_dw(256, 512, 2), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 512, 1), conv_dw(512, 1024, 2), conv_dw(1024, 1024, 1), nn.AvgPool2d(7),)self.fc = nn.Linear(1024, 1000) 网络设计超参数的影响wider？or thinner？描述网络，除了常见的深度，还有一个指标就是宽度。网络的宽度受filter个数的影响。更多的filter，说明网络更胖，提取feature的能力”看起来“就会越强。MobileNet使用一个超参数$\alpha$来实验。某个层的filter个数越多，带来的结果就是下一层filter的input channel会变多，$\alpha$就是前后input channel的数量比例。可以得到，计算量会大致变为原来的$\alpha^2$倍。 resolution如果输入的spatial dimension变成原来的$\rho$倍，也就是$D_F$变了，那么会对计算量带来影响。利用上面总结的计算公式不难发现，和$\alpha$一样，计算量会变成原来的$\rho^2$倍。 实际中，我们令$\alpha$和$\rho$都小于$1$，构建了更少参数的mobilenet。下面是一个具体参数设置下，网络计算量和参数数目的变化情况。 Depthwise Separable Conv真的可以？同样的网络结构，区别在于使用/不使用Depthwise Separable Conv技术，在ImageNet上的精度相差很少（使用这一技术，下降了$1$个点），但是参数和计算量却节省了很多。 更浅的网络还是更瘦的网络如果我们要缩减网络的参数，是更浅的网络更好，还是更瘦的网络更好呢？作者设计了参数和计算量相近的两个网络进行了比较，结论是相对而言，缩减网络深度不是个好主意。 alpha和rho的定量影响定量地比较了不同$\alpha$和$\rho$的设置下，网络的性能。$\alpha$越小，网络精度越低，而且下降速度是加快的。 输入图像的resolution越小，网络精度也越低。 和其他网络的对比这里只贴出结果。一个值得注意的地方是，SqueezeNet虽然参数很少，但是计算量却很大。而MobileNet可以达到参数也很少。这是通过depthwise separable conv带来的好处。 应用接下来，论文讨论了MobileNet在多种不同任务上的表现，证明了它的泛化能力和良好表现。可以在网上找到很多基于MobileNet的detection，classification等的开源项目代码，这里就不再多说了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Xception, Deep Learning with Depthwise separable Convolution]]></title>
      <url>%2F2018%2F03%2F22%2Fpaper-xception%2F</url>
      <content type="text"><![CDATA[在MobileNet, ShuffleMet等轻量级网络中，depthwise separable conv是一个很流行的设计。借助Xception: Deep Learning with Depthwise separable Convolution，对这种分解卷积的思路做一个总结。 起源自从AlexNet以来，DNN的网络设计经过了ZFNet-&gt;VGGNet-&gt;GoogLeNet-&gt;ResNet等几个发展阶段。本文作者的思路正是受GoogLeNet中Inception结构启发。Inception结构是最早有别于VGG等“直筒型”结构的网络module。以Inception V3为例，一个典型的Inception模块长下面这个样子： 对于一个CONV层来说，它要学习的是一个$3D$的filter，包括两个空间维度（spatial dimension），即width和height；以及一个channel dimension。这个filter和输入在$3$个维度上进行卷积操作，得到最终的输出。可以用伪代码表示如下：12345678// 对于第i个filter// 计算输入中心点(x, y)对应的卷积结果sum = 0for c in 1:C for h in 1:K for w in 1:K sum += in[c, y-K/2+h, x-K/2+w] * filter_i[c, h, w]out[i, y, x] = sum 可以看到，在$3D$卷积中，channel这个维度和spatial的两个维度并无不同。 在Inception中，卷积操作更加轻量级。输入首先被$1\times 1$的卷积核处理，得到了跨channel的组合(cross-channel correlation)，同时将输入的channel dimension减少了$3\sim 4$倍（一会$4$个支路要做concat操作）。这个结果被后续的$3\times 3$卷积和$5\times 5$卷积核处理，处理方法和普通的卷积一样，见上。 由此作者想到，Inception能够work证明后面的一条假设就是：卷积的channel相关性和spatial相关性是可以解耦的，我们没必要要把它们一起完成。 简化Inception，提取主要矛盾接着，为了更好地分析问题，作者将Inception结构做了简化，保留了主要结构，去掉了AVE Pooling操作，如下所示。 好的，我们现在将底层的$3$个$1\times 1$的卷积核组合起来，其实上面的图和下图是等价的。一个“大的”$1\times 1$的卷积核（channels数目变多），它的输出结果在channel上被分为若干组（group），每组分别和不同的$3\times 3$卷积核做卷积，再将这$3$份输出拼接起来，得到最后的输出。 那么，如果我们把分组数目继续调大呢？极限情况，我们可以使得group number = channel number，如下所示： Depthwise Separable Conv这种结构和一种名为depthwise separable conv的技术很相似，即首先使用group conv在spatial dimension上卷积，然后使用$1\times 1$的卷积核做cross channel的卷积（又叫做pointwise conv）。主要有两点不同： 操作的顺序。在TensorFlow等框架中，depthwise separable conv的实现是先使用channelwise的filter只在spatial dimension上做卷积，再使用$1\times 1$的卷积核做跨channel的融合。而Inception中先使用$1\times 1$的卷积核。 非线性变换的缺席。在Inception中，每个conv操作后面都有ReLU的非线性变换，而depthwise separable conv没有。 第一点不同不太重要，尤其是在深层网络中，这些block都是堆叠在一起的。第二点论文后面通过实验进行了比较。可以看出，去掉中间的非线性激活，能够取得更好的结果。 Xception网络架构基于上面的分析，作者认为这样的假设是合理的：cross channel的相关和spatial的相关可以完全解耦。 we make the following hypothesis: that the mapping of cross-channels correlations and spatial correlations in the feature maps of convolutional neural networks can be entirely decoupled. Xception的结构基于ResNet，但是将其中的卷积层换成了depthwise separable conv。如下图所示。整个网络被分为了三个部分：Entry，Middle和Exit。 The Xception architecture: the data first goes through the entry flow, then through the middle flow which is repeated eight times, and finally through the exit flow. Note that all Convolution and SeparableConvolution layers are followed by batch normalization [7] (not included in the diagram). All SeparableConvolution layers use a depth multiplier of 1 (no depth expansion).]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS229 简单的监督学习方法]]></title>
      <url>%2F2018%2F03%2F21%2Fcs229-supervised-learning%2F</url>
      <content type="text"><![CDATA[回过头去复习一下基础的监督学习算法，主要包括最小二乘法和logistic回归。 最小二乘法最小二乘法是一个线性模型，即： \hat{y} = h_\theta(x) = \sum_{i=1}^{m}\theta_i x_i = \theta^T x定义损失函数为Mean Square Error(MSE)，如下所示。其中，不戴帽子的$y$表示给定的ground truth。 J(\theta) = \frac{1}{2}(\hat{y}-y)^2那么，最小二乘就是要找到这样的参数$\theta^*$，使得： \theta^* = \arg\min J(\theta)梯度下降使用梯度下降方法求解上述优化问题，我们有： \theta_{i+1} = \theta_{i} - \alpha \nabla_\theta J(\theta)求导，有： \begin{aligned}\nabla_\theta J(\theta) &= \frac{1}{2}\nabla_\theta (\theta^T x - y)^2 \\ &= (\theta^T x - y) x\end{aligned}由于这里的损失函数是一个凸函数，所以梯度下降方法能够保证到达全局的极值点。 上面的梯度下降只是对单个样本来做的。实际上，我们可以取整个训练集或者训练集的一部分，计算平均损失函数$J(\theta) = \frac{1}{N}\sum_{i=1}^{N}J_i(\theta)$，做梯度下降，道理是一样的，只不过相差了常数因子$\frac{1}{N}$。 正则方程除了梯度下降方法之外，上述问题还存在着解析解。我们将所有的样本输入$x^{(i)}$作为行向量，构成矩阵$X \in \mathbb{R}^{N\times d}$。其中，$N$为样本总数，$d$为单个样本的特征个数。那么，对于参数$\theta\in\mathbb{R}^{d\times 1}$来说，$X\theta$的第$i$行就可以给出模型对第$i$个样本的预测结果。我们将ground truth排成一个$N\times 1$的矩阵，那么，损失函数可以写作： J(\theta) = \frac{1}{2N} \Vert X\theta-y \Vert_2^2将$\Vert x\Vert_2^2$写作$x^T x$，同时略去常数项，我们有： \begin{aligned}J &= (X\theta - y)^T (X\theta - y) \\ &= \theta^T X^T X\theta - 2\theta^T x^T y +y^T y\end{aligned}对其求导，有： \nabla_\theta J = X^T X\theta - X^T y 这其中，主要用到的矩阵求导性质如下：令导数为$0$，求得极值点处： \theta^* = (X^TX)^{-1}X^T y概率解释这里对上述做法给出一个概率论上的解释。首先我们要引入似然函数（likelihood function）的概念。 似然函数是一个关于模型参数$\theta$的函数，它描述了某个参数$\theta$下，给出输入$x$，得到输出$y$的概率。用具体的公式表示如下： L(\theta) = \prod_{i=1}^{N}P(y^{(i)}|x^{(i)};\theta)假设线性模型的预测结果和ground truth之间的误差服从Gaussian分布，也就是说， y - \theta^T x = \epsilon \sim \mathcal{N}(0, \sigma^2)那么上面的似然函数可以写作： L(\theta) = \prod_{i=1}^{N}\frac{1}{\sqrt{2\pi\sigma}}\exp(\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})如何估计参数$\theta$呢？我们可以认为，参数$\theta$使得出现样本点$(x^{(i)}, y^{(i)})$的概率变大，所以才能被我们观测到。自然，我们需要使得似然函数$L(\theta)$取得极大值，也就是说： \theta^* = \arg\max L(\theta)通过引入$\log(\cdot)$，可以将连乘变成连加，同时不改变函数的单调性。这样，实际上我们操作的是对数似然函数$\log L(\theta)$。有： \begin{aligned} \mathcal{l} &= \log L(\theta) \\ &= \sum_{i=1}^{N}\log \frac{1}{\sqrt{2\pi\sigma^2}} \exp (\frac{(y^{(i)}-\theta^T x^{(i)})^2}{2\sigma^2})\\ &= N\log\frac{1}{\sqrt{2\pi\sigma^2}} -\frac{1}{\sigma^2}\frac{1}{2}\sum_{i=1}^{N}(y^{(i)}-\theta^T x^{(i)})^2 \end{aligned}略去前面的常数项不管，后面一项正好是最小二乘法的损失函数。要想最大化对数似然函数，也就是要最小化上面的损失函数。 所以，最小二乘法的损失函数可以由数据集的噪声服从Gaussian分布自然地导出。 加权最小二乘法加权最小二乘法是指对数据集中的数据赋予不同的权重，一个重要的用途是使用权重$w^{(i)} = \exp (-\frac{(x^{(i)}-x)^2}{2\tau^2})$做局部最小二乘。不再多说。 logistic回归虽然叫回归，但是logistic回归解决的问题是分类问题。 logistic函数logistic函数$\sigma(x) = \frac{1}{1+e^{-x}}$，又叫sigmoid函数，将输入$(-\infty, +\infty)$压缩到$(0, 1)$之间。它的形状如下： 对其求导，发现导数值可以完全不依赖于输入$x$： \frac{d\sigma(x)} {dx} = \sigma(x)(1-\sigma(x))我们将logistic函数的输入取做$x$的feature的线性组合，就得到了假设函数$h_\theta(x) = \sigma(\theta^T x)$。 logistic回归logistic函数的输出既然是在$(0,1)$上，我们可以将其作为概率。也就是说，我们认为它的输出是样本点属于类别$1$的概率： \begin{aligned}P(y=1|x) &= h_\theta(x) \\ P(y=0|x) &= 1-h_\theta(x) \end{aligned}或者我们写的更紧凑些： P(y|x) = (h_\theta(x))^y (1-h_\theta(x))^{(1-y)}我们仍然使用上述极大似然的估计方法，求取参数$\theta$，为求简练，隐去了上标$(i)$。 \begin{aligned}L(\theta) &= \prod_{i=1}^{N}P(y|x;\theta) \\ &=\prod (h_\theta(x))^y (1-h_\theta(x))^{(1-y)} \end{aligned}取对数： \log L(\theta) = \sum_{i=1}^{N}y\log(h(x)) + (1-y)\log(1-h(x))所以，我们的损失函数为$J(\theta) = - [y\log(h(x)) + (1-y)\log(1-h(x))]$。把$h(x)$换成$P$，岂不就是深度学习中常用的交叉损失熵在二分类下的特殊情况？ 回到logistic回归，使用梯度下降，我们可以得到更新参数的策略： \theta_{i+1} = \theta_i - \alpha (h_\theta(x) - y)x啊哈！形式和最小二乘法完全一样。只不过要注意，现在的$h_\theta(x)$已经变成了一个非线性函数。 感知机在上述logistic回归基础上，我们强制将其输出映射到$\lbrace 1, -1\rbrace$。即将$\sigma(x)$换成$g(x)$： g(x) = \begin{cases} 1, \quad\text{if}\quad x \ge 0\\ 0, \quad\text{if}\quad x < 0\end{cases}使用同样的更新方法，我们就得到了感知机模型（perceptron machine）。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hack PyCaffe]]></title>
      <url>%2F2018%2F03%2F16%2Fcaffe-hack-python-interface%2F</url>
      <content type="text"><![CDATA[这篇文章主要是Github: PyCaffe Tutorial中Hack Pycaffe的翻译整理。后续可能会加上一些使用boost和C++为Python接口提供后端的解释。这里主要讨论如何为Pycaffe添加自己想要的功能。至于Pycaffe的使用，留待以后的文章整理。 PyCaffe的代码组织结构见Caffe的python目录。下面这张图是与PyCaffe相关的代码的分布。其中src和include是Caffe框架的后端C++实现，python目录中是与PyCaffe关系更密切的代码。可以看到，除了_caffe.cpp以外，其他都是纯python代码。_caffe.cpp使用boost提供了C++与python的绑定，而其他python脚本在此层的抽象隔离之上，继续完善了相关功能，提供了更加丰富的API、 添加纯Python功能首先，我们介绍如何在C++构建的PyCaffe隔离之上，用纯python实现想要的功能。 添加的功能和PyCaffe基本平行，不需要改变已有代码有的时候想加入的功能和PyCaffe的关系基本是平行的，比如想仿照PyTorch等框架，加入对数据进行预处理的Transformer功能（这个API其实已经在PyCaffe中实现了，这里只是举个例子）。为了实现这个功能，我们可能需要使用numpy和opencv等包装图像的预处理操作，但是和Caffe本身基本没什么关系。在这样的情况下，我们直接编写即可。要注意在python/caffe/__init__.py中import相关的子模块或函数。这个例子可以参考caffe.io的实现（见python/caffe/io.py文件）。 添加的功能需要Caffe的支持，向已有的类中添加函数如果添加的功能需要Caffe的支持，可以在pycaffe.py内添加，详见Net的例子。由于python的灵活性，我们可以参考Net的实现方式，待函数实现完成后，使用&lt;class&gt;.&lt;function&gt; = my_function动态地添加。如下所示，注意_Net_forward函数的第一个参数必须是self。 123def _Net_forward(self, blobs=None, start=None, end=None, **kwargs): # do somethingNet.forward = _Net_forward 与之相似，我们还可以为已经存在的类添加字段。注意，函数用@property装饰，且参数有且只有一个self， 12345678910111213# This function will be called when accessing net.blobs@propertydef _Net_blobs(self): """ An OrderedDict (bottom to top, i.e., input to output) of network blobs indexed by name """ if not hasattr(self, '_blobs_dict'): self._blobs_dict = OrderedDict(zip(self._blob_names, self._blobs)) return self._blobs_dict # Set the field `blobs` to call _Net_blobsNet.blobs = _Net_blobs PyCaffe中已经实现的类主要有：Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver。 使用C++添加功能当遇到如下情况时，可能需要修改C++代码： 为了获取更底层的权限控制，如一些私有字段。 性能考虑。 这时，你应该去修改python/caffe/_caffe.cpp文件。这个文件使用了boost实现了python与C++的绑定。 为了添加一个字段，可以在Blob部分添加如下的代码。这样，就会将python中Blob类的num字段绑定到C++的Blob&lt;Dtype&gt;::num()方法上。1.add_property("num", &amp;Blob&lt;Dtype&gt;::num) 使用.def可以为python相应的类绑定方法。在下面的代码中，首先实现了Net_Save方法，然后将其绑定到了python中Net类的save方法上。这样，通过python调用net.save(filename)即可。 注意，当你修改了_caffe,cpp后，记得使用make pycaffe重新生成动态链接库。 12345678910# Declare the functionvoid Net_Save(const Net&lt;Dtype&gt;&amp; net, string filename) &#123; // ...&#125;// ...bp::class_&lt;Net&lt;Dtype&gt;&gt;("Net", bp::no_init)# Now we can call net.save(file).def("save", &amp;Net_Save) 当然，上面介绍的这些还很基础，关于boost的python绑定，可以参考官方的文档：boost: python binding]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Learning both Weights and Connections for Efficient Neural Networks]]></title>
      <url>%2F2018%2F03%2F14%2Fpaper-network-prune-hansong%2F</url>
      <content type="text"><![CDATA[Han Song的Deep Compression是模型压缩方面很重要的论文。在Deep Compression中，作者提出了三个步骤来进行模型压缩：剪枝，量化和霍夫曼编码。其中，剪枝对应的方法就是基于本文要总结的这篇论文：Learning both Weights and Connections for Efficient Neural Networks。在这篇论文中，作者介绍了如何在不损失精度的前提下，对深度学习的网络模型进行剪枝，从而达到减小模型大小的目的。 概述DNN虽然能够解决很多以前很难解决的问题，但是一个应用方面的问题就是这些模型通常都太大了。尤其是当运行在手机等移动设备上时，对电源和网络带宽都是负担。对于电源来说，由于模型巨大，所以只能在外部内存DRAM中加载，造成能耗上升。具体数值见下表。所以模型压缩很有必要。本文就是使用剪枝的方法，将模型中不重要的权重设置为$0$，将原来的dense model转变为sparse model，达到压缩的目的。 解决什么问题？如何在不损失精度的前提下，对DNN进行剪枝（或者说稀疏化），从而压缩模型。 为什么剪枝是work的？为什么能够通过剪枝的方法来压缩模型呢？难道剪掉的那些连接真的不重要到可以去掉吗？论文中，作者指出，DNN模型广泛存在着参数过多的问题，具有很大的冗余（见参考文献NIPS 2013的一篇文章Predicting parameters in deep learning）。 Neural networks are typically over-parameterized, and there is significant redundancy for deep learning models 另外，作者也为自己的剪枝方法找到了生理学上的依据，生理学上发现，对于哺乳动物来说，婴儿期会产生许多的突触连接，在后续的成长过程中，不怎么用的那些突出会退化消失。 怎么做作者的方法分为三个步骤： Train Connectivity: 按照正常方法训练初始模型。作者认为该模型中权重的大小表征了其重要程度 Prune Connection: 将初始模型中那些低于某个阈值的的权重参数置成$0$（即所谓剪枝） Re-Train: 重新训练，以期其他未被剪枝的权重能够补偿pruning带来的精度下降 为了达到一个满意的压缩比例和精度要求，$2$和$3$要重复多次。 相关工作为了减少网络的冗余，减小模型的size，有以下相关工作： 定点化。将weight使用8bit定点表示，32bit浮点表示activation。 低秩近似。使用矩阵分解等方法。 网络设计上，NIN等使用Global Average Pooling取代FC层，可以大大减少参数量，这种结构已经得到了广泛使用。而FC也并非无用。在Pooling后面再接一个fc层，便于后续做迁移学习transfer learning。 从优化上下手，使用损失函数的Hessian矩阵，比直接用weight decay更好。 HashedNet等工作，这里不再详述。 如何Prune主要分为三步，上面 概述 中 怎么做 部分已经简单列出。下面的算法流程摘自作者的博士论文，可能更加详细清楚。 正则项的选择L1和L2都可以用来做正则，惩罚模型的复杂度。使用不同的正则方法会对pruning和retraining产生影响。实验发现，采用L2做正则项较好。见下图，可以看到详细的比较结果，分别是with/without retrain下L1和L2正则对精度的影响。还可以看到一个共性的地方，就是当pruning的比例大于某个阈值后，模型的精度会快速下降。 DropoutDropout是一项防止过拟合的技术。要注意的是，在retraining的时候，我们需要对Dropout ratio做出调整。因为网络中的很多连接都被剪枝剪下来了，所以dropout的比例要变小。下面给出定量的估计。 对于FC层来说，如果第$i$层的神经元个数是$N_i$，那么该层的连接数$C_i$用乘法原理可以很容易得到：$C_i = N_{i-1}N_i$。也就是说，连接数$C\sim N^2$。而dropout是作用于神经元的（dropout是将$N_i$个神经元输出按照概率dropout掉）。所以，比例$D^2 \sim C$，最后得到： D_r = D_o \sqrt{\frac{C_{ir}}{C_{io}}}其中，下标$r$表示retraining，$o$表示初始模型(original)。 Local Pruning在retraining部分，在初始模型基础上继续fine tune较好。为了能够更有效地训练，在训练FC层的时候，可以将CONV的参数固定住。反之亦然。 另外，不同深度和类型的layer对剪枝的敏感度是不一样的。作者指出，CONV比FC更敏感，第$1$个CONV比后面的要敏感。下图是AlexNet中各个layer剪枝比例和模型精度下降之间的关系。可以印证上面的结论。 多次迭代剪枝应该迭代地进行多次剪枝 + 重新训练这套组合拳。作者还尝试过根据参数的绝对值依概率进行剪枝，效果不好。 对神经元进行剪枝将神经元之间的connection剪枝后（或者说将权重稀疏化了），那些$0$输入$0$输出的神经元也应该被剪枝了。然后，我们又可以继续以这个神经元出发，剪掉与它相关的connection。这个步骤可以在训练的时候自动发生。因为如果某个神经元已经是dead状态，那么它的梯度也会是$0$。那么只有正则项推着它向$0$的方向。 实验使用Caffe实现，需要加入一个mask来表示剪枝。剪枝的阈值，是该layer的权重标准差乘上某个超参数。这里：Add pruning possibilities at inner_product_layer #4294 ，有人基于Caffe官方的repo给FC层加上了剪枝。这里：Github: DeepCompression,，有人实现了Deep Compression，可以参考他们的实现思路。 对于实验结果，论文中比对了LeNet和AlexNet。此外，作者的博士论文中给出了更加详细的实验结果，在更多的流行的模型上取得了不错的压缩比例。直接引用如下，做一个mark： On the ImageNet dataset, the pruning method reduced the number of parameters of AlexNet by a factor of 9× (61 to 6.7 million), without incurring accuracy loss. Similar experiments with VGG-16 found that the total number of parameters can be reduced by 13× (138 to 10.3 million), again with no loss of accuracy. We also experimented with the more efficient fully-convolutional neural networks: GoogleNet (Inception-V1), SqueezeNet, and ResNet-50, which have zero or very thin fully connected layers. From these experiments we find that they share very similar pruning ratios before the accuracy drops: 70% of the parameters in those fully-convolutional neural networks can be pruned. GoogleNet is pruned from 7 million to 2 million parameters, SqueezeNet from 1.2 million to 0.38 million, and ResNet-50 from 25.5 million to 7.47 million, all with no loss of Top-1 and Top-5 accuracy on Imagenet. 下面 参考资料 部分也给出了作者在GitHub上放出的Deep Compression的结果，可以前去参考。 学习率的设置跑模型跑实验，一个重要的超参数就是学习率$LR$。这里作者也给了一个经验规律。一般在训练初始模型的时候，学习率都是逐渐下降的。刚开始是一个较大的值$LR_1$，最后是一个较小的值$LR_2$。它们之间可能有数量级的差别。作者指出，retraining的学习率应该介于两者之间。可以取做比$LR_1$小$1 \sim 2$个数量级。 RNN和LSTM在博士论文中，作者还是用这一技术对RNN/LSTM在Neural Talk任务上做了剪枝，取得了不错的结果。 参考资料 HanSong的个人主页：Homepage HanSong的博士论文：Efficient Methods and Hardware for Deep Learning 后续的Deep Compression论文：DEEP COMPRESSION- COMPRESSING DEEP NEURAL NETWORKS WITH PRUNING, TRAINED QUANTIZATION AND HUFFMAN CODING Deep Compression AlexNet: Github: Deep-Compression-AlexNet]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe中的Net实现]]></title>
      <url>%2F2018%2F02%2F28%2Fcaffe-net%2F</url>
      <content type="text"><![CDATA[Caffe中使用Net实现神经网络，这篇文章对应Caffe代码总结Net的实现。 proto中定义的参数123456789101112131415161718192021222324252627282930313233343536message NetParameter &#123; // net的名字 optional string name = 1; // consider giving the network a name // 以下几个都是弃用的参数，为了定义输入的blob（大小） // 下面有使用推荐`InputParameter`进行输入设置的方法 // DEPRECATED. See InputParameter. The input blobs to the network. repeated string input = 3; // DEPRECATED. See InputParameter. The shape of the input blobs. repeated BlobShape input_shape = 8; // 4D input dimensions -- deprecated. Use &quot;input_shape&quot; instead. // If specified, for each input blob there should be four // values specifying the num, channels, height and width of the input blob. // Thus, there should be a total of (4 * #input) numbers. repeated int32 input_dim = 4; // Whether the network will force every layer to carry out backward operation. // If set False, then whether to carry out backward is determined // automatically according to the net structure and learning rates. optional bool force_backward = 5 [default = false]; // The current &quot;state&quot; of the network, including the phase, level, and stage. // Some layers may be included/excluded depending on this state and the states // specified in the layers&apos; include and exclude fields. optional NetState state = 6; // Print debugging information about results while running Net::Forward, // Net::Backward, and Net::Update. optional bool debug_info = 7 [default = false]; // The layers that make up the net. Each of their configurations, including // connectivity and behavior, is specified as a LayerParameter. repeated LayerParameter layer = 100; // ID 100 so layers are printed last. // DEPRECATED: use &apos;layer&apos; instead. repeated V1LayerParameter layers = 2;&#125; Input的定义在train和deploy的时候，输入的定义常常是不同的。在train时，我们需要提供数据$x$和真实值$y$，这样网络的输出$\hat{y} = \mathcal{F}_\theta (x)$与真实值$y$计算损失，bp，更新网络参数$\theta$。 在deploy时，推荐使用InputLayer定义网络的输入，下面是$CAFFE/models/bvlc_alexnet/deploy.prototxt中的输入定义：1234567891011layer &#123; name: &quot;data&quot; type: &quot;Input&quot; // 该层layer的输出blob名称为data，供后续layer使用 top: &quot;data&quot; // 定义输入blob的大小：10 x 3 x 227 x 227 // 说明batch size = 10 // 输入彩色图像，channel = 3, RGB // 输入image的大小：227 x 227 input_param &#123; shape: &#123; dim: 10 dim: 3 dim: 227 dim: 227 &#125; &#125;&#125; 头文件Net的描述头文件位于$CAFFE/include/caffe/net.hpp中。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[捉bug记 - JupyterNotebook中使用pycaffe加载多个模型一直等待的现象]]></title>
      <url>%2F2018%2F02%2F27%2Fbug-pycaffe-jupyternotebook-awaiting-for-data%2F</url>
      <content type="text"><![CDATA[JupyteNotebook是个很好的工具，但是在使用pycaffe试图在notebook中同时加载多个caffemodel模型的时候，却出现了无法加载的问题。 bug重现我想在notebook中比较两个使用不同方法训练出来的模型，它们使用了同样的LMDB文件进行训练。加载第一个模型没有问题，但当加载第二个模型时，却一直等待。在StackOverflow上我发现了类似的问题，可以见：Can’t load 2 models in pycaffe。 解决方法这是由于pycaffe（是否要加上jupyter-notebook？因为不用notebook，以前没有出现过类似问题）不能并发读取同样的LMDB所导致的。但是很遗憾，没有发现太好的解决办法。最后只能是将LMDB重新copy了一份，并修改prototxt文件，使得两个模型分别读取不同的LMDB。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe中卷积的大致实现思路]]></title>
      <url>%2F2018%2F02%2F26%2Fconv-in-caffe%2F</url>
      <content type="text"><![CDATA[参考资料：知乎：在Caffe中如何计算卷积。 使用im2col将输入的图像或特征图转换为矩阵，后续就可以使用现成的线代运算优化库，如BLAS中的GEMM，来快速计算。 im2col的工作原理如下：每个要和卷积核做卷积的patch被抻成了一个feature vector。不同位置的patch，顺序堆叠起来， 最后就变成了这样： 同样的，对卷积核也做类似的变换。将单一的卷积核抻成一个行向量，然后把c_out个卷积核顺序排列起来。 我们记图像那个矩阵是A，记卷积那个矩阵是F。那么，对于第i个卷积核来说，它现在实际上是F里面的第i个行向量。为了计算它在原来图像上的各个位置的卷积，现在我们需要它和矩阵A中的每行做点积。也就是 F_i * [A_1^T, A_2^T, … A_i^T] （也就是A的转置）。推广到其他的卷积核，就是说，最后的结果是F*A^T. 我们可以用矩阵维度验证。F的维度是Cout x (C x K x K). 输入的Feature map matrix的维度是(H x W) x (C x K x K)。那么上述矩阵乘法的结果就是 Cout x (H x W)。正好可以看做输出的三维blob的大小：Cout x H x W。 这里Convolution in Caffe: a memo还有贾扬清对于自己当时在caffe中实现conv的”心路历程“，题图出自此处。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Learning Structured Sparsity in Deep Neural Networks]]></title>
      <url>%2F2018%2F02%2F24%2Fpaper-ssl-dnn%2F</url>
      <content type="text"><![CDATA[DNN的稀疏化？用L1正则项不就好了？在很多场合，这种方法的确可行。但是当试图使用FPGA/AISC加速DNN的前向计算时，我们希望DNN的参数能有一些结构化的稀疏性质。这样才能减少不必要的cache missing等问题。在这篇文章中，作者提出了一种结构化稀疏的方法，能够在不损失精度的前提下，对深度神经网络进行稀疏化，达到加速的目的。本文作者温伟，目前是杜克大学Chen Yiran组的博士生，做了很多关于结构化稀疏和DNN加速相关的工作。本文发表在NIPS 2016上。本文的代码已经公开：GitHub 摘要为了满足DNN的计算速度要求，我们提出了Structure Sparisity Learning (SSL)技术来正则化DNN的“结构”（例如CNN filter的参数，filter的形状，包括channel和网络层深）。它可以带来： 大的DNN —&gt; 紧凑的模型 —&gt; 计算开销节省 硬件友好的结构化稀疏 —&gt; 便于在专用硬件上加速 提供了正则化，提高网络泛化能力 —&gt; 提高了精度 实验结果显示，这种方法可以在CPU/GPU上对AlexNet分别达到平均$5.1$和$3,1$倍的加速。在CIFAR10上训练ResNet，从$20$层减少到$18$层，并提高了精度。 LASSOSSL是基于Group LASSO的，所以正式介绍文章之前，首先简单介绍LASSO和Group LASSO，LASSO)(least absolute shrinkage and selection operator)是指统计学习中的特征选择方法。以最小二乘法求解线性模型为例，可以加上L1 norm作为正则化约束，见下式，其中$\beta$是模型的参数。具体推导过程可以参见wiki页面。 \min_{\beta \in R^p}\frac{1}{N} \Vert(y-X\beta)\Vert_2^2 + \lambda \Vert \beta \Vert_1而Group LASSO就是将参数分组，进行LASSO操作。 这里只是简单介绍一下LASSO。SSL下面还会详细介绍，不必过多执着于LASSO。 结构化稀疏DNN通常参数很多，计算量很大。为了减少计算开销，目前的研究包括：稀疏化，connection pruning, low rank approximation等。然而前两种方法只能得到随机的稀疏，无规律的内存读取仍然制约了速度。下面这种图是一个例子。我们使用了L1正则进行稀疏。和原模型相比，精度损失了$2$个点。虽然稀疏度比较高，但是实际加速效果却很差。可以看到，在conv3，conv4和conv5中，有的情况下反而加速比是大于$1$的。 low rank approx利用矩阵分解，将预训练好的模型的参数分解为小矩阵的乘积。这种方法需要较多的迭代次数，同时，网络结构是不能更改的。 基于下面实验中观察的事实，我们提出了SSL来直接学习结构化稀疏。 网络中的filter和channel存在冗余 filter稀疏化为别的形状能够去除不必要的计算 网络的深度虽然重要，但是并不意味着深层的layer对网络性能一定是好的 假设第$l$个卷积层的参数是一个$4D$的Tensor，$W^{(l)}\in R^{N_l \times C_l \times M_l \times N_l}$，那么SSL方法可以表示为优化下面这个损失函数： E(W)=E_D{W} + \lambda R(W) + \lambda_g \sum_{l=1}^{L}R_g(W^{(l)})这里，$W$代表DNN中所有权重的集合。$E_D(W)$代表在训练集上的loss。$R$是非结构化的正则项，例如L2 norm。$R_g$是指结构化稀疏的正则项，注意是逐层计算的。对于每一层来说（也就是上述最后一项求和的每一项），group LASSO可以表示为： R_g(w) = \sum_{g=1}^{G}\Vert w^{(g)} \Vert_g其中，$w^{(g)}$是该层权重$W^{(l)}$的一部分，不同的分组可以重叠。$G$是分组的组数。$\Vert \cdot \Vert_g$指的是group LASSO，这里使用的是$\Vert w^{(g)}\Vert_g = \sqrt{\sum_{i=1}^{|w^{(g)}|}(w_i^{(g)})^2}$，也就是$2$范数。 SSL有了上面的损失函数，SSL就取决于如何对weight进行分组。对不同的分组情况分类讨论如下。图示见博客开头的题图。 惩罚不重要的filter和channel假设$W^{(l)}_{n_l,:,:,:}$是第$n$个filter，$W^{(l)}_{:, c_l, :,:}$是所有weight的第$c$个channel。可以通过下面的约束来去除相对不重要的filter和channel。注意，如果第$l$层的weight中某个filter变成了$0$，那么输出的feature map中就有一个全$0$，所以filter和channel的结构化稀疏要放到一起。下面是这种形式下的损失函数。为了简单，后面的讨论中都略去了正常的正则化项$R(W)$。 E(W) = E_D(W) + \lambda_n \sum_{l=1}^{L}(\sum_{n_l=1}^{N_l}\Vert W^{(l)}_{n_l,:,:,:}\Vert_g) + \lambda_c\sum_{l=1}^{L}(\sum_{cl=1}^{C_l}\Vert W^{(l)}_{:,c_l,:,:}\Vert_g)任意形状的filter所谓任意形状的filter，就是将filter中的一些权重置为$0$。可以使用下面的分组方法： E(W) = E_D(W) + \lambda_s \sum_{l=1}^{L}(\sum_{c_l=1}^{C_l}\sum_{m_l=1}^{M_l}\sum_{k_l=1}^{K_l})\Vert W^{(l)}_{:,c_l,m_l,k_l} \Vert_g网络深度损失函数如下： E(W) = E_D(W) + \lambda_d \sum_{l=1}^{L}\Vert W^{(l)}\Vert_g不过要注意的是，某个layer被稀疏掉了，会切断信息的流通。所以受ResNet启发，加上了short-cut结构。即使SSL移去了该layer所有的filter，上层的feature map仍然可以传导到后面。 两类特殊的稀疏规则特意提出下面两种稀疏规则，下面的实验即是基于这两种特殊的稀疏结构。 2D filter sparsity卷积层中的3D卷积可以看做是2D卷积的组合（做卷积的时候spatial和channel是不相交的）。这种结构化稀疏是将该卷积层中的每个2D的filter，$W^{(l)}_{n_l,c_l,:,:}$，看做一个group，做group LASSO。这相当于是上述filter-wise和channel-wise的组合。 filter-wise和shape-wise的组合加速GEMM在Caffe中，3D的权重tensor是reshape成了一个行向量，然后$Nl$个filter的行向量堆叠在一起，就成了一个2D的矩阵。这个矩阵的每一列对应的是$W^{(l)}{:,c_l,m_l,k_l}$，称为shape sparsity。两者组合，矩阵的零行和零列可以被抽去，相当于GEMM的矩阵行列数少了，起到了加速的效果。 实验分别在MNIST，CIFAR10和ImageNet上做了实验，使用公开的模型做baseline，并以此为基础使用SSL训练。 LeNet&amp;MLP@MNIST分别使用Caffe中实现的LeNet和MLP做实验。 LeNet限制SSL为filter-wise和channel-wise稀疏化，来惩罚不重要的filter。下表中，LeNet-1是baseline，2和3是使用不同强度得到的稀疏化结果。可以看到，精度基本没有损失($0.1%$)，但是filter和channel数量都有了较大减少，FLOP大大减少，加速效果比较明显。 将网络conv1的filter可视化如下。可以看到，对于LeNet2来说，大多数filter都被稀疏掉了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[捉bug记 - Cannot create Cublas handle. Cublas won't be available.]]></title>
      <url>%2F2018%2F02%2F08%2Fbug-pycaffe-using-cublas%2F</url>
      <content type="text"><![CDATA[这两天在使用Caffe的时候出现了一个奇怪的bug。当使用C++接口时，完全没有问题；但是当使用python接口时，会出现错误提示如下：12common.cpp:114] Cannot create Cublas handle. Cublas won&apos;t be available.common.cpp:121] Cannot create Curand generator. Curand won&apos;t be available. 令人疑惑的是，这个python脚本我前段时间已经用过几次了，却没有这样的问题。 如果在Google上搜索这个问题，很多讨论都是把锅推给了驱动，不过我使用的这台服务器并没有更新过驱动或系统。本来想要试试重启大法，但是上面还有其他人在跑的任务，所以重启不太现实。 最后找到了这个issue: Cannot use Caffe on another GPU when GPU 0 has full memory。联想到我目前使用的服务器上GPU０也正是在跑着一项很吃显存的任务（如下所示），所以赶紧试了一下里面@longjon的方法。 使用CUDA_VISIBLE_DEVICES变量，指定Caffe能看到的显卡设备。1CUDA_VISIBLE_DEVICES=2 python my_script.py --gpu_id=0 果然就可以了！ 这个问题应该出在pycaffe的初始化上。这里不再深究。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[论文 - Visualizing and Understanding ConvNet]]></title>
      <url>%2F2018%2F02%2F08%2Fpaper-visualize-convnet%2F</url>
      <content type="text"><![CDATA[Visualizing &amp; Understanding ConvNet这篇文章是比较早期关于CNN调试的文章，作者利用可视化方法，设计了一个超过AlexNet性能的网络结构。 引言继AlexNet之后，CNN在ImageNet竞赛中得到了广泛应用。AlextNet成功的原因包括以下三点： Large data。 硬件GPU性能。 一些技巧提升了模型的泛化能力，如Dropout技术。 不过CNN仍然像一只黑盒子，缺少可解释性。这使得对CNN的调试变得比较困难。我们提出了一种思路，可以找出究竟input中的什么东西对应了激活后的Feature map。 (对于神经网络的可解释性，可以从基础理论入手，也可以从实践中的经验入手。本文作者实际上就是在探索如何能够更好得使用经验对CNN进行调试。这种方法仍然没有触及到CNN本质的可解释性的东西，不过仍然在工程实践中有很大的意义，相当于将黑盒子变成了灰盒子。从人工取火到炼金术到现代化学，也不是这么一个过程吗？) 在AlexNet中，每个卷积单元常常由以下几个部分组成： 卷积层，使用一组学习到的$3D$滤波器与输入（上一层的输出或网络输入的数据）做卷积操作。 非线性激活层，通常使用ReLU(x) = max(0, x)。 可选的，池化层，缩小Feature map的尺寸。 可选的，LRN层（现在已经基本不使用）。 DeconvNet我们使用DeconvNet这项技术，寻找与输出的激活对应的输入模式。这样，我们可以看到，输入中的哪个部分被神经元捕获，产生了较强的激活。 如图所示，展示了DeconvNet是如何构造的。 首先，图像被送入卷积网络中，得到输出的feature map。对于输出的某个激活，我们可以将其他激活值置成全$0$，然后顺着deconvNet计算，得到与之对应的输入。具体来说，我们需要对三种不同的layer进行反向操作。 Uppooling在CNN中，max pooling操作是不可逆的（信息丢掉了）。我们可以使用近似操作：记录最大值的位置；在deconvNet中，保留该标记位置处的激活值。如下图所示。右侧为CNN中的max pooling操作。中间switches显示的是最大值的位置（用灰色标出）。在左侧的deconvNet中，激活值对应给到相应的灰色位置。这个操作被称为Uppooing。 Rectification在CNN中，一般使用relu作为非线性激活。deconvNet中也做同样的处理。 Filtering在CNN中，一组待学习的filter用来与输入的feature map做卷积。得到输出。在deconvNet中，使用deconv操作，输入是整流之后的feature map。 对于最终输出的activation中的每个值，经过deconv的作用，最终会对应到输入pixel space上的一小块区域，显示了它们对最终输出的贡献。 CNN的可视化要想可视化，先要有训练好的CNN模型。这里用作可视化的模型基于AlexNet，但是去掉了group。另外，为了可视化效果，将layer $1$的filter size从$11\times 11$变成$7\times 7$，步长变成$2$。具体训练过程不再详述。 训练完之后，我们将ImageNet的validation数据集送入到网络中进行前向计算， 如下所示，是layer $1$的可视化结果。可以看到，右下方的可视化结果被分成了$9\times 9$的方格，每个方格内又细分成了$9\times 9$的小格子。其中，大格子对应的是$9$个filter，小格子对应的是top 9的激活利用deconvNet反算回去对应的image patch、因为layer 1的filter个数正好也是$9$，所以可能稍显迷惑。 附录这里是关于CNN可视化的一些额外资料： Zeiler关于本文的talk：Visualizing and Understanding Deep Neural Networks by Matt Zeiler 斯坦福CS231课程的讲义：Visualizing what ConvNets learn ICML 2015上的另一篇CNN可视化的paper：Understanding Neural Networks Through Deep Visualization以及他们的开源工具：deep-visualization-toolbox 一篇知乎专栏的文章：Deep Visualization:可视化并理解CNN]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Incremental Network Quantization 论文阅读]]></title>
      <url>%2F2018%2F01%2F25%2Finq-paper%2F</url>
      <content type="text"><![CDATA[卷积神经网络虽然已经在很多任务上取得了很棒的效果，但是模型大小和运算量限制了它们在移动设备和嵌入式设备上的使用。模型量化压缩等问题自然引起了大家的关注。Incremental Network Quantization这篇文章关注的是如何使用更少的比特数进行模型参数量化以达到压缩模型，减少模型大小同时使得模型精度无损。如下图所示，使用5bit位数，INQ在多个模型上都取得了不逊于原始FP32模型的精度。实验结果还是很有说服力的。作者将其开源在了GitHub上，见Incremental-Network-Quantization。 量化方法INQ论文中，作者采用的量化方法是将权重量化为$2$的幂次或$0$。具体来说，是将权重$W_l$（表示第$l$层的参数权重）舍入到下面这个有限集合中的元素（在下面的讨论中，我们认为$n_1 &gt; n_2$）： 假设用$b$bit表示权重，我们分出$1$位单独表示$0$。 PS：这里插一句。关于为什么要单独分出$1$位表示$0$，毕竟这样浪费了($2^b$ vs $2^{b-1}+1$)。GitHub上有人发issue问，作者也没有正面回复这样做的原因。以我的理解，是方便判定$0$和移位。因为作者将权重都舍入到了$2$的幂次，那肯定是为了后续将乘法变成移位操作。而使用剩下的$b-1$表示，可以方便地读出移位的位数，进行操作。 这样，剩下的$b-1$位用来表示$2$的幂次。我们需要决定$n_1$和$n_2$。因为它俩决定了表示范围。它们之间的关系为： (n_1-n_2 + 1) \times 2 = 2^{b-1}其中，乘以$2$是考虑到正负对称的表示范围。 如何确定$n_1$呢（由上式可知，有了$b$和$n_1$，$n_2$就确定了）。作者考虑了待量化权重中的最大值，我们需要设置$n_1$，使其刚好不溢出。所以有： n_1 = \lfloor \log_2(4s/3) \rfloor其中，$s$是权重当中绝对值最大的那个，即$s = \max \vert W_l\vert$。 之后做最近舍入就可以了。对于小于最小分辨力$2^{n_2}$的那些权重，将其直接截断为$0$。 训练方法量化完成后，网络的精度必然会下降。我们需要对其进行调整，使其精度能够恢复原始模型的水平。为此，作者提出了三个主要步骤，迭代地进行。即 weight partition（权重划分）, group-wise quantization（分组量化） 和re-training（训练）。 re-training好理解，就是量化之后要继续做finetuning。前面两个名词解释如下：weight partition是指我们不是对整个权重一股脑地做量化，而是将其划分为两个不相交的集合。group-wise quantization是指对其中一个集合中的权重做量化，另一组集合中的权重不变，仍然为FP32。注意，在re-training中，我们只对没有量化的那组参数做参数更新。下面是论文中的表述。 Weight partition is to divide the weights in each layer of a pre-trained full-precision CNN model into two disjoint groups which play comple- mentary roles in our INQ. The weights in the first group are responsible for forming a low-precision base for the original model, thus they are quantized by using Equation (4). The weights in the second group adapt to compensate for the loss in model accuracy, thus they are the ones to be re-trained. 训练步骤可以用下图来表示。在第一个迭代中，将所有的权重划分为黑色和白色两个部分（图$1$）。黑色部分的权重进行量化，白色部分不变（图$2$）。然后，使用SGD更新那些白色部分的权重（图$3$）。在第二次迭代中，我们扩大量化权重的范围，重复进行迭代$1$中的操作。在后面的迭代中，以此类推，只不过要不断调大量化权重的比例，最终使得所有权重都量化为止。 pruning-inspired strategy在权重划分步骤，作者指出，随机地将权重量化，不如根据权重的幅值，优先量化那些绝对值比较大的权重。比较结果见下图。 在代码部分，INQ基于Caffe框架，主要修改的地方集中于blob.cpp和sgd_solver.cpp中。量化部分的代码如下，首先根据要划分的比例计算出两个集合分界点处的权重大小。然后将大于该值的权重进行量化，小于该值的权重保持不变。下面的代码其实有点小问题，data_copy使用完之后没有释放。关于代码中mask的作用，下文介绍。 12345678910111213141516171819202122232425// blob.cpp// INQ if(is_quantization)&#123; Dtype* data_copy=(Dtype*) malloc(count_*sizeof(Dtype)); caffe_copy(count_,data_vec,data_copy); caffe_abs(count_,data_copy,data_copy); std::sort(data_copy,data_copy+count_); //data_copy order from small to large //caculate the n1 Dtype max_data=data_copy[count_-1]; int n1=(int)floor(log2(max_data*4.0/3.0)); //quantizate the top 30% of each layer, change the "partition" until partition=0 int partition=int(count_*0.7)-1; for (int i = 0; i &lt; (count_); ++i) &#123; if(std::abs(data_vec[i])&gt;=data_copy[partition]) &#123; data_vec[i] = weightCluster_zero(data_vec[i],n1); mask_vec[i]=0; &#125; &#125; 参数更新在re-training中，我们只对未量化的那些参数进行更新。待更新的参数，mask中的值都是$1$，这样和diff相乘仍然不变；不更新的参数，mask中的值都是$0$，和diff乘起来，相当于强制把梯度变成了$0$。 12// sgd_solver.cppcaffe_gpu_mul(net_params[param_id]-&gt;count(),net_params[param_id]-&gt;gpu_mask(),net_params[param_id]-&gt;mutable_gpu_diff(),net_params[param_id]-&gt;mutable_gpu_diff()); 结语论文中还有一些其他的小细节，这里不再多说。本文的作者还维护了一个关于模型量化压缩相关的repo，也可以作为参考。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe 中的 SyncedMem介绍]]></title>
      <url>%2F2018%2F01%2F12%2Fcaffe-syncedmem%2F</url>
      <content type="text"><![CDATA[Blob是Caffe中的基本数据结构，类似于TensorFlow和PyTorch中的Tensor。图像读入后作为Blob，开始在各个Layer之间传递，最终得到输出。下面这张图展示了Blob和Layer之间的关系： Caffe中的Blob在实现的时候，使用了SyncedMem管理内存，并在内存（Host）和显存（device）之间同步。这篇博客对Caffe中SyncedMem的实现做一总结。 SyncedMem的作用Blob是一个多维的数组，可以位于内存，也可以位于显存（当使用GPU时）。一方面，我们需要对底层的内存进行管理，包括何何时开辟内存空间。另一方面，我们的训练数据常常是首先由硬盘读取到内存中，而训练又经常使用GPU，最终结果的保存或可视化又要求数据重新传回内存，所以涉及到Host和Device内存的同步问题。 同步的实现思路在SyncedMem的实现代码中，作者使用一个枚举量head_来标记当前的状态。如下所示： 12345// in SyncedMemenum SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;;// 使用过Git吗？ 在Git中那个标志着repo最新版本状态的变量就叫 HEAD// 这里也是一样，标志着最新的数据位于哪里SyncedHead head_; 这样，利用head_变量，就可以构建一个状态转移图，在不同状态切换时进行必要的同步操作等。 具体实现SyncedMem的类声明如下： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950/** * @brief Manages memory allocation and synchronization between the host (CPU) * and device (GPU). * * TODO(dox): more thorough description. */class SyncedMemory &#123; public: SyncedMemory(); explicit SyncedMemory(size_t size); ~SyncedMemory(); // 获取CPU data指针 const void* cpu_data(); // 设置CPU data指针 void set_cpu_data(void* data); // 获取GPU data指针 const void* gpu_data(); // 设置GPU data指针 void set_gpu_data(void* data); // 获取CPU data指针，并在后续将改变指针所指向内存的值 void* mutable_cpu_data(); // 获取GPU data指针，并在后续将改变指针所指向内存的值 void* mutable_gpu_data(); // CPU 和 GPU的同步状态：未初始化，在CPU（未同步），在GPU（未同步），已同步 enum SyncedHead &#123; UNINITIALIZED, HEAD_AT_CPU, HEAD_AT_GPU, SYNCED &#125;; SyncedHead head() &#123; return head_; &#125; // 内存大小 size_t size() &#123; return size_; &#125;#ifndef CPU_ONLY void async_gpu_push(const cudaStream_t&amp; stream);#endif private: void check_device(); void to_cpu(); void to_gpu(); void* cpu_ptr_; void* gpu_ptr_; size_t size_; SyncedHead head_; bool own_cpu_data_; bool cpu_malloc_use_cuda_; bool own_gpu_data_; // GPU设备编号 int device_; DISABLE_COPY_AND_ASSIGN(SyncedMemory);&#125;; // class SyncedMemory 我们以to_cpu()为例，看一下如何在不同状态之间切换。 123456789101112131415161718192021222324252627282930313233343536inline void SyncedMemory::to_gpu() &#123; // 检查设备状态（使用条件编译，只在DEBUG中使能） check_device();#ifndef CPU_ONLY switch (head_) &#123; case UNINITIALIZED: // 还没有初始化呢~所以内存啥的还没开 // 先在GPU上开块显存吧~ CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); caffe_gpu_memset(size_, 0, gpu_ptr_); // 接着，改变状态标志 head_ = HEAD_AT_GPU; own_gpu_data_ = true; break; case HEAD_AT_CPU: // 数据在CPU上~如果需要，先在显存上开内存 if (gpu_ptr_ == NULL) &#123; CUDA_CHECK(cudaMalloc(&amp;gpu_ptr_, size_)); own_gpu_data_ = true; &#125; // 数据拷贝 caffe_gpu_memcpy(size_, cpu_ptr_, gpu_ptr_); // 改变状态变量 head_ = SYNCED; break; // 已经在GPU或者已经同步了，什么都不做 case HEAD_AT_GPU: case SYNCED: break; &#125;#else // NO_GPU 是一个宏，打印FATAL ERROR日志信息 // 编译选项没有开GPU支持，只能说 无可奉告 NO_GPU;#endif&#125; 注意到，除了head_以外，SyncedMemory中还有own_gpu_data_（同样，也有own_cpu_data_）的成员。这个变量是用来标志当前CPU或GPU上有没有分配内存，从而当我们使用set_c/gpu_data或析构函数被调用的时候，能够正确释放内存/显存的。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe中的BatchNorm实现]]></title>
      <url>%2F2018%2F01%2F08%2Fcaffe-batch-norm%2F</url>
      <content type="text"><![CDATA[这篇博客总结了Caffe中BN的实现。 BN简介由于BN技术已经有很广泛的应用，所以这里只对BN做一个简单的介绍。 BN是Batch Normalization的简称，来源于Google研究人员的论文：Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift。对于网络的输入层，我们可以采用减去均值除以方差的方法进行归一化，对于网络中间层，BN可以实现类似的功能。 在BN层中，训练时，会对输入blob各个channel的均值和方差做一统计。在做inference的时候，我们就可以利用均值和方法，对输入$x$做如下的归一化操作。其中，$\epsilon$是为了防止除数是$0$，$i$是channel的index。 \hat{x_i} = \frac{x_i-\mu_i}{\sqrt{Var(x_i)+\epsilon}}不过如果只是做如上的操作，会影响模型的表达能力。例如，Identity Map($y = x$)就不能表示了。所以，作者提出还需要在后面添加一个线性变换，如下所示。其中，$\gamma$和$\beta$都是待学习的参数，使用梯度下降进行更新。BN的最终输出就是$y$。 y_i = \gamma \hat{x_i} + \beta如下图所示，展示了BN变换的过程。 上面，我们讲的还是inference时候BN变换是什么样子的。那么，训练时候，BN是如何估计样本均值和方差的呢？下面，结合Caffe的代码进行梳理。 BN in Caffe在BVLC的Caffe实现中，BN层需要和Scale层配合使用。在这里，BN层专门用来做“Normalization”操作（确实是人如其名了），而后续的线性变换层，交给Scale层去做。 下面的这段代码取自He Kaiming的Residual Net50的模型定义文件。在这里，设置batch_norm_param中use_global_stats为true，是指在inference阶段，我们只使用已经得到的均值和方差统计量，进行归一化处理，而不再更新这两个统计量。后面Scale层设置的bias_term: true是不可省略的。这个选项将其配置为线性变换层。 12345678910111213141516171819layer &#123; bottom: &quot;conv1&quot; top: &quot;conv1&quot; name: &quot;bn_conv1&quot; type: &quot;BatchNorm&quot; batch_norm_param &#123; use_global_stats: true &#125;&#125;layer &#123; bottom: &quot;conv1&quot; top: &quot;conv1&quot; name: &quot;scale_conv1&quot; type: &quot;Scale&quot; scale_param &#123; bias_term: true &#125;&#125; 这就是Caffe中BN层的固定搭配方法。这里只是简单提到，具体参数的意义待我们深入代码可以分析。 BatchNorm 层的实现上面说过，Caffe中的BN层与原始论文稍有不同，只是做了输入的归一化，而后续的线性变换是交由后续的Scale层实现的。 proto定义的相关参数我们首先看一下caffe.proto中关于BN层参数的描述。保留了原始的英文注释，并添加了中文解释。123456789101112131415161718192021222324252627282930313233message BatchNormParameter &#123; // If false, normalization is performed over the current mini-batch // and global statistics are accumulated (but not yet used) by a moving // average. // If true, those accumulated mean and variance values are used for the // normalization. // By default, it is set to false when the network is in the training // phase and true when the network is in the testing phase. // 设置为False的话，更新全局统计量，对当前的mini-batch进行规范化时，不使用全局统计量，而是 // 当前batch的均值和方差。 // 设置为True，使用全局统计量做规范化。 // 后面在BN的实现代码我们会看到，这个变量默认随着当前网络在train或test phase而变化。 // 当train时为false，当test时为true。 optional bool use_global_stats = 1; // What fraction of the moving average remains each iteration? // Smaller values make the moving average decay faster, giving more // weight to the recent values. // Each iteration updates the moving average @f$S_&#123;t-1&#125;@f$ with the // current mean @f$ Y_t @f$ by // @f$ S_t = (1-\beta)Y_t + \beta \cdot S_&#123;t-1&#125; @f$, where @f$ \beta @f$ // is the moving_average_fraction parameter. // BN在统计全局均值和方差信息时，使用的是滑动平均法，也就是 // St = (1-beta)*Yt + beta*S_&#123;t-1&#125; // 其中St为当前估计出来的全局统计量（均值或方差），Yt为当前batch的均值或方差 // beta是滑动因子。其实这是一种很常见的平滑滤波的方法。 optional float moving_average_fraction = 2 [default = .999]; // Small value to add to the variance estimate so that we don&apos;t divide by // zero. // 防止除数为0加上去的eps optional float eps = 3 [default = 1e-5];&#125; OK。现在可以进入BN的代码实现了。阅读大部分代码都没有什么难度，下面主要结合代码讲解use_global_stats变量的作用和均值（方差同理）的计算。由于均值和方差的计算原理相近，所以下面只会详细介绍均值的计算。 SetUpBN层的SetUp代码如下。首先，会根据当前处于train还是test决定是否使用全局的统计量。如果prototxt文件中设置了use_global_stats标志，则会使用用户给定的配置。所以一般在使用BN时，无需对use_global_stats进行配置。 这里有一个地方容易迷惑。BN中要对样本的均值和方差进行统计，即我们需要两个blob来存储。但是从下面的代码可以看到，BN一共有3个blob作为参数。这里做一解释，主要参考了wiki的moving average条目。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152template &lt;typename Dtype&gt;void BatchNormLayer&lt;Dtype&gt;::LayerSetUp(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; BatchNormParameter param = this-&gt;layer_param_.batch_norm_param(); moving_average_fraction_ = param.moving_average_fraction(); // 默认根据当前是否处在TEST模式而决定是否使用全局mean和var use_global_stats_ = this-&gt;phase_ == TEST; if (param.has_use_global_stats()) use_global_stats_ = param.use_global_stats(); // 得到channels数量 // 为了防止越界，首先检查输入是否为1D if (bottom[0]-&gt;num_axes() == 1) channels_ = 1; else channels_ = bottom[0]-&gt;shape(1); eps_ = param.eps(); if (this-&gt;blobs_.size() &gt; 0) &#123; LOG(INFO) &lt;&lt; "Skipping parameter initialization"; &#125; else &#123; // 参数共3个 this-&gt;blobs_.resize(3); vector&lt;int&gt; sz; sz.push_back(channels_); // mean 和var都是1D的长度为channels的向量 // 因为在规范化过程中，要逐channel进行，即： // for c in range(channels): // x_hat[c] = (x[c] - mean[c]) / std[c] this-&gt;blobs_[0].reset(new Blob&lt;Dtype&gt;(sz)); this-&gt;blobs_[1].reset(new Blob&lt;Dtype&gt;(sz)); // 这里的解释见下 sz[0] = 1; this-&gt;blobs_[2].reset(new Blob&lt;Dtype&gt;(sz)); for (int i = 0; i &lt; 3; ++i) &#123; caffe_set(this-&gt;blobs_[i]-&gt;count(), Dtype(0), this-&gt;blobs_[i]-&gt;mutable_cpu_data()); &#125; &#125; // Mask statistics from optimization by setting local learning rates // for mean, variance, and the bias correction to zero. // mean 和 std在训练的时候是不需要梯度下降来更新的，这里强制把其learning rate // 设置为0 for (int i = 0; i &lt; this-&gt;blobs_.size(); ++i) &#123; if (this-&gt;layer_param_.param_size() == i) &#123; ParamSpec* fixed_param_spec = this-&gt;layer_param_.add_param(); fixed_param_spec-&gt;set_lr_mult(0.f); &#125; else &#123; CHECK_EQ(this-&gt;layer_param_.param(i).lr_mult(), 0.f) &lt;&lt; "Cannot configure batch normalization statistics as layer " &lt;&lt; "parameters."; &#125; &#125;&#125; 在求取某个流数据（stream）的平均值的时候，常用的一种方法是滑动平均法，也就是使用系数$\alpha$来做平滑滤波，如下所示： S_t = \alpha Y_t + (1-\alpha) S_{t-1}上面的式子等价于： S_t = \frac{\text{WeightedSum}_n}{\text{WeightedCount}_n}其中，\text{WeightedSum}_n = Y_t + (1-\alpha) \text{WeightedSum}_{n-1} \text{WeightedCount}_n = 1 + (1-\alpha) \text{WeightedCount}_{n-1}而Caffe中BN的实现中，blobs_[0]和blobs_[1]中存储的实际是$\text{WeightedSum}_n$，而blos_[2]中存储的是$\text{WeightedCount}_n$。所以，真正的mean和var是两者相除的结果。即：12mu = blobs_[0] / blobs_[2]var = blobs_[1] / blobs_[2] Forward下面是Forward CPU的代码。主要应该注意当前batch的mean和var的求法。123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104template &lt;typename Dtype&gt;void BatchNormLayer&lt;Dtype&gt;::Forward_cpu(const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;Dtype&gt;*&gt;&amp; top) &#123; const Dtype* bottom_data = bottom[0]-&gt;cpu_data(); Dtype* top_data = top[0]-&gt;mutable_cpu_data(); int num = bottom[0]-&gt;shape(0); int spatial_dim = bottom[0]-&gt;count()/(bottom[0]-&gt;shape(0)*channels_); // 如果不是就地操作，首先将bottom的数据复制到top if (bottom[0] != top[0]) &#123; caffe_copy(bottom[0]-&gt;count(), bottom_data, top_data); &#125; // 如果使用全局统计量，我们需要先计算出真正的mean和var if (use_global_stats_) &#123; // use the stored mean/variance estimates. const Dtype scale_factor = this-&gt;blobs_[2]-&gt;cpu_data()[0] == 0 ? 0 : 1 / this-&gt;blobs_[2]-&gt;cpu_data()[0]; // mean = blobs[0] / blobs[2] caffe_cpu_scale(variance_.count(), scale_factor, this-&gt;blobs_[0]-&gt;cpu_data(), mean_.mutable_cpu_data()); // var = blobs[1] / blobs[2] caffe_cpu_scale(variance_.count(), scale_factor, this-&gt;blobs_[1]-&gt;cpu_data(), variance_.mutable_cpu_data()); &#125; else &#123; // 不使用全局统计量时，我们要根据当前batch的mean和var做规范化 // compute mean // spatial_sum_multiplier_是全1向量 // batch_sum_multiplier_也是全1向量 // gemv做矩阵与向量相乘 y = alpha*A*x + beta*y。 // 下面式子是将bottom_data这个矩阵与一个全1向量相乘， // 相当于是在统计行和。 // 注意第二个参数channels_ * num指矩阵的行数，第三个参数是矩阵的列数 // 所以这是在计算每个channel的feature map的和 // 结果out[n][c]是指输入第n个sample的第c个channel的和 // 同时，传入了 1. / (num * spatial_dim) 作为因子乘到结果上面，作用见下面 caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim, 1. / (num * spatial_dim), bottom_data, spatial_sum_multiplier_.cpu_data(), 0., num_by_chans_.mutable_cpu_data()); // 道理和上面相同，注意下面通过传入CblasTrans，指定了矩阵要转置。所以是在求列和 // 这样，就求出了各个channel的和。 // 上面不是已经除了 num * spatial_dim 吗？这就是求和元素的总数量 // 到此，我们就完成了对当前batch的平均值的求解 caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, 1., num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0., mean_.mutable_cpu_data()); &#125; // subtract mean // gemm是在做矩阵与矩阵相乘 C = alpha*A*B + beta*C // 下面这个是在做broadcasting subtraction caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1, batch_sum_multiplier_.cpu_data(), mean_.cpu_data(), 0., num_by_chans_.mutable_cpu_data()); caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num, spatial_dim, 1, -1, num_by_chans_.cpu_data(), spatial_sum_multiplier_.cpu_data(), 1., top_data); // 计算当前的var if (!use_global_stats_) &#123; // compute variance using var(X) = E((X-EX)^2) caffe_sqr&lt;Dtype&gt;(top[0]-&gt;count(), top_data, temp_.mutable_cpu_data()); // (X-EX)^2 caffe_cpu_gemv&lt;Dtype&gt;(CblasNoTrans, channels_ * num, spatial_dim, 1. / (num * spatial_dim), temp_.cpu_data(), spatial_sum_multiplier_.cpu_data(), 0., num_by_chans_.mutable_cpu_data()); caffe_cpu_gemv&lt;Dtype&gt;(CblasTrans, num, channels_, 1., num_by_chans_.cpu_data(), batch_sum_multiplier_.cpu_data(), 0., variance_.mutable_cpu_data()); // E((X_EX)^2) // compute and save moving average // 做滑动平均，更新全局统计量，这里可以参见上面的式子 this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] *= moving_average_fraction_; this-&gt;blobs_[2]-&gt;mutable_cpu_data()[0] += 1; caffe_cpu_axpby(mean_.count(), Dtype(1), mean_.cpu_data(), moving_average_fraction_, this-&gt;blobs_[0]-&gt;mutable_cpu_data()); int m = bottom[0]-&gt;count()/channels_; Dtype bias_correction_factor = m &gt; 1 ? Dtype(m)/(m-1) : 1; caffe_cpu_axpby(variance_.count(), bias_correction_factor, variance_.cpu_data(), moving_average_fraction_, this-&gt;blobs_[1]-&gt;mutable_cpu_data()); &#125; // normalize variance caffe_add_scalar(variance_.count(), eps_, variance_.mutable_cpu_data()); caffe_sqrt(variance_.count(), variance_.cpu_data(), variance_.mutable_cpu_data()); // replicate variance to input size // 同样是在做broadcasting caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, num, channels_, 1, 1, batch_sum_multiplier_.cpu_data(), variance_.cpu_data(), 0., num_by_chans_.mutable_cpu_data()); caffe_cpu_gemm&lt;Dtype&gt;(CblasNoTrans, CblasNoTrans, channels_ * num, spatial_dim, 1, 1., num_by_chans_.cpu_data(), spatial_sum_multiplier_.cpu_data(), 0., temp_.mutable_cpu_data()); caffe_div(temp_.count(), top_data, temp_.cpu_data(), top_data); // TODO(cdoersch): The caching is only needed because later in-place layers // might clobber the data. Can we skip this if they won't? caffe_copy(x_norm_.count(), top_data, x_norm_.mutable_cpu_data());&#125; 由上面的计算过程不难得出，当经过很多轮迭代之后，blobs_[2]的值会趋于稳定。下面我们使用$m_t$来表示第$t$轮迭代后的blobs_[2]的值，也就是$\text{WeightedCount}_n$，使用$\alpha$表示moving_average_fraction_，那么我们有： m_t = 1 + \alpha m_{t-1}可以求取$m_t$的通项后令$t=\infty$，可以得到，$m_{\infty}=\frac{1}{1-\alpha}$。 Backward在做BP的时候，我们需要分情况讨论。 当use_global_stats == true的时候，BN所做的操作是一个线性变换BN(x) = \frac{x-\mu}{\sqrt{Var}}所以\frac{\partial L}{\partial x} = \frac{1}{\sqrt{Var}}\frac{\partial L}{\partial y} 对应的代码如下。其中，temp_是broadcasting之后的输入x的标准差（见上面Forward部分的代码最后），做逐元素的除法即可。1234if (use_global_stats_) &#123; caffe_div(temp_.count(), top_diff, temp_.cpu_data(), bottom_diff); return;&#125; 当use_global_stats == false的时候，BN所做操作虽然也是上述线性变换。但是注意，现在式子里面的$\mu$和$Var(x)$都是当前batch计算出来的，也就是它们都是输入x的函数。所以就麻烦了不少。这里我并没有推导，而是看了这篇博客，里面有详细的推导过程，写的很易懂。我将最后的结果贴在下面，对计算过程感兴趣的可以去原文章查看。 我们使用$y$来代替上面的$\hat{x_i}$，并且上下同时除以$m$，就可以得到Caffe BN代码中所给的BP式子： \frac{\partial f}{\partial x_i} = \frac{\frac{\partial f}{\partial y}-E[\frac{\partial f}{\partial y}]-yE[\frac{\partial f}{\partial y}y]}{\sqrt{\sigma^2+\epsilon}}1234567891011// if Y = (X-mean(X))/(sqrt(var(X)+eps)), then//// dE(Y)/dX =// (dE/dY - mean(dE/dY) - mean(dE/dY \cdot Y) \cdot Y)// ./ sqrt(var(X) + eps)//// where \cdot and ./ are hadamard product and elementwise division,// respectively, dE/dY is the top diff, and mean/var/sum are all computed// along all dimensions except the channels dimension. In the above// equation, the operations allow for expansion (i.e. broadcast) along all// dimensions except the channels dimension where required. 下面的代码部分就是实现上面这个式子的内容，注释很详细，要解决的一个比较棘手的问题就是broadcasting，这个有兴趣可以看一下。对Caffe中BN的介绍就到这里。下面介绍与BN经常成对出现的Scale层。 Scale层的实现Caffe中将后续的线性变换使用单独的Scale层实现。Caffe中的Scale可以根据需要配置成不同的模式： 当输入blob为两个时，计算输入blob的逐元素乘的结果（维度不相同时，第二个blob可以做broadcasting）。 当输入blob为一个时，计算输入blob与一个可学习参数gamma的按元素相乘结果。 当设置bias_term: true时，添加一个偏置项。 用于BN的线性变换的计算方法很直接，这里不再多说了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[shell编程]]></title>
      <url>%2F2017%2F11%2F10%2Fshell-programming%2F</url>
      <content type="text"><![CDATA[介绍基本的shell编程方法，参考的教程是Linux Shell Scripting Tutorial, A Beginner’s handbook。 变量变量是代码的基本组成元素。可以认为shell中的变量类型都是字符串。 shell中的变量可以分为两类：系统变量和用户自定义变量。下面分别进行介绍。 在代码中使用变量值的时候，需要在前面加上$。echo命令可以在控制台打印相应输出。所以使用echo $var就可以输出变量var的值。 系统变量系统变量是指Linux中自带的一些变量。例如HOME,PATH等。其中PATH又叫环境变量。更多的系统变量见下表： 用户定义的变量用户自定义变量是用户命名并赋值的变量。使用下面的方法定义： 123# 注意不要在等号两边插入空格name=value# 如 n=10 局部变量和全局变量局部变量是指在当前代码块内可见的变量，使用local声明。例如下面的代码，将依次输出：111, 222, 111.1234567891011#! /bin/shnum=111 # 全局变量func1()&#123; local num=222 # 局部变量 echo $num&#125;echo "before---$num"func1echo "after---$num" 变量之间的运算使用expr可以进行变量之间的运算，如下所示： 1234# 注意要在操作符两边空余空格expr 1 + 3# 由于*是特殊字符，所以乘法要使用转义expr 10 \* 2 ``和””使用``（也就是TAB键上面的那个）包起来的部分，是可执行的命令。而使用””（引号）包起来的部分，是字符串。 12345678a=`expr 10 \* 3`# output: 3echo $a# output: aecho a# output: expr 10 \* 3a="expr 10 \* 3"echo $a 另外，使用””（双引号）括起来的字符串会发生变量替换，而用’’（单引号）括起来的字符串则不会。 123a=1echo "$a" # 输出 1echo '$a' # 输出 $a 读取输入使用read var1, var2, ...的方式从键盘的输入读取变量的值。 1234# input a=1read a# ouptut: 2echo `expr $a + 1` 基本概念命令的返回值当bash命令成功执行后，返回给系统的返回值为0；否则为非零。可以据此判断上步操作的状态。使用$?可以取出上一步执行的返回值。 123456# 将echo 错输为ecohecoh "hello"# output: 非零(127)echo $?# output: 0echo $? 通配符通配符是指*,?和[...]这三类。 *可以匹配任意多的字符，?用来匹配一个字符。[...]用来匹配括号内的字符。见下表。 [...]表示法还有如下变形： 使用-用来指示范围。如[a-z]，表示a到z间任意一个字符。 使用^或!表示取反。如[!a-p]表示除了a到p间字符的其他字符。 输入输出重定向重定向是指改变命令的输出位置。使用&gt;进行输出重定向。使用&lt;进行输入重定向。例如，ls -l &gt; a.txt，将本目录下的文件信息输出到文本文件a.txt中，而不再输出到终端。 此外，&gt;&gt;同样是输出重定向。但是它会在文件末尾追加写入，不会覆盖文件的原有内容。 搭配使用&lt;和&gt;可以做文件处理。例如，tr group1 group2命令可以将group1中的字符变换为group2中对应位置的字符。使用如下命令： 1tr "[a-z]" "A-Z" &lt; ori.txt &gt; out.txt 可以将ori.txt中的小写字母转换为大写字母输出到out.txt中。 管道（pipeline）管道|可以将第一个程序的输出作为第二个程序的输入。例如： 1cat ori.txt | tr "[a-z]" "A-Z" 会将ori.txt中的小写字母转换为大写，并在终端输出。 过滤器（Filter）Filter是指那些输入和输出都是控制台的命令。通过Filter和输入输出重定向，可以很方便地对文件内容进行整理。例如： 1sort &lt; names.txt | uniq &gt; u_names.txt uniq命令可以实现去重，但是需要首先对输入数据进行排序。上面的Filter可以将输入文件names.txt中的行文本去重后输出到u_names.txt中去。 控制流if 条件控制在bash中使用if条件控制的语法和MATLAB等很像，要在末尾加上类似end的指示符，如下： 1234if conditionthen XXXfi 或者加上else，使用如下的形式：123456789if conditionthen do somethingelif conditionthen do somethingelse do somethingfi 那么，如何做逻辑运算呢？需要借助test关键字。 对于整数来说，我们可以使用if test op1 oprator op2的方式，判断操作数op1和op2的大小关系。其中，operator可以是-gt，-eq等。 或者另一种写法：if [ op1 operator op2 ]，但是注意后者[]与操作数之间有空格。如下表所示（点击可放大）： 对于字符串，支持的逻辑判断如下： 举个例子，我们想判断输入的值是否为1或2，可以使用如下的脚本。注意[]的两边一定要加空格。1234567891011#! /bin/basha=1if [ $1=$a ]then echo "you input 1"elif [ $1=2 ]then echo "you input 2"else echo "you input $1"fi]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[过秦论]]></title>
      <url>%2F2017%2F10%2F27%2Ffuck-gfw%2F</url>
      <content type="text"><![CDATA[秦以耕战立国，起于西北，从边陲弱小诸侯，借以天时地利，用商鞅、李斯，举法家大旗。而后逐鹿中原，坑赵括，灌大梁，掳六国贵族，结束战乱，建立大一统帝国。然而，强秦却二世而亡。“始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。”封建王朝的皇帝总觉得自己的国祚能够绵延不绝，历史却一次次地打他们的脸。 秦孝公据崤函之固，拥雍州之地，君臣固守以窥周室，有席卷天下，包举宇内，囊括四海之意，并吞八荒之心。当是时也，商君佐之，内立法度，务耕织，修守战之具；外连衡而斗诸侯。于是秦人拱手而取西河之外。 孝公既没，惠文、武、昭襄蒙故业，因遗策，南取汉中，西举巴、蜀，东割膏腴之地，北收要害之郡。诸侯恐惧，会盟而谋弱秦，不爱珍器重宝肥饶之地，以致天下之士，合从缔交，相与为一。当此之时，齐有孟尝，赵有平原，楚有春申，魏有信陵。此四君者，皆明智而忠信，宽厚而爱人，尊贤而重士，约从离衡，兼韩、魏、燕、楚、齐、赵、宋、卫、中山之众。于是六国之士，有宁越、徐尚、苏秦、杜赫之属为之谋，齐明、周最、陈轸、召滑、楼缓、翟景、苏厉、乐毅之徒通其意，吴起、孙膑、带佗、倪良、王廖、田忌、廉颇、赵奢之伦制其兵。尝以十倍之地，百万之众，叩关而攻秦。秦人开关延敌，九国之师，逡巡而不敢进。秦无亡矢遗镞之费，而天下诸侯已困矣。于是从散约败，争割地而赂秦。秦有余力而制其弊，追亡逐北，伏尸百万，流血漂橹。因利乘便，宰割天下，分裂山河。强国请服，弱国入朝。 延及孝文王、庄襄王，享国之日浅，国家无事。及至始皇，奋六世之余烈，振长策而御宇内，吞二周而亡诸侯，履至尊而制六合，执敲扑而鞭笞天下，威振四海。南取百越之地，以为桂林、象郡；百越之君，俯首系颈，委命下吏。乃使蒙恬北筑长城而守藩篱，却匈奴七百余里。胡人不敢南下而牧马，士不敢弯弓而报怨。于是废先王之道，焚百家之言，以愚黔首；隳名城，杀豪杰，收天下之兵，聚之咸阳，销锋镝，铸以为金人十二，以弱天下之民。然后践华为城，因河为池，据亿丈之城，临不测之渊，以为固。良将劲弩守要害之处，信臣精卒陈利兵而谁何。天下已定，始皇之心，自以为关中之固，金城千里，子孙帝王万世之业也。 始皇既没，余威震于殊俗。然陈涉瓮牖绳枢之子，氓隶之人，而迁徙之徒也；才能不及中人，非有仲尼、墨翟之贤，陶朱、猗顿之富；蹑足行伍之间，而倔起阡陌之中，率疲弊之卒，将数百之众，转而攻秦，斩木为兵，揭竿为旗，天下云集响应，赢粮而景从。山东豪俊遂并起而亡秦族矣。 且夫天下非小弱也，雍州之地，崤函之固，自若也。陈涉之位，非尊于齐、楚、燕、赵、韩、魏、宋、卫、中山之君也；锄櫌棘矜，非铦于钩戟长铩也；谪戍之众，非抗于九国之师也；深谋远虑，行军用兵之道，非及向时之士也。然而成败异变，功业相反，何也？试使山东之国与陈涉度长絜大，比权量力，则不可同年而语矣。然秦以区区之地，致万乘之势，序八州而朝同列，百有余年矣；然后以六合为家，崤函为宫；一夫作难而七庙隳，身死人手，为天下笑者，何也？仁义不施而攻守之势异也。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu/Mac 工具软件列表]]></title>
      <url>%2F2017%2F10%2F22%2Fuseful-tools-list%2F</url>
      <content type="text"><![CDATA[工作环境大部分在Ubuntu和MacOS上进行，这里是一个我觉得在这两个平台上很有用的工具的整理列表，大部分都可以在两个系统上找到。这里并不会给出具体安装方式，因为最好的文档总是软件的官方Document或者GitHub的README。 zsh和Oh-my-zsh如果经常在终端敲命令而且还在用系统自带的Bash？可以考虑试一下zsh替代bash，并使用oh-my-zsh武装zsh。 关于oh-my-zsh的帖子网上已经有很多，不过我还并没有用到太多的功能。oh-my-zsh中可以配置插件，不过我只是使用了colored-man-pages。顾名思义，它可以将使用man查询时的页面彩色输出。如下所示。 autojump使用autojump，可以很方便地在已经访问过的文件夹间快速跳转。甚至都不需要输入目标文件夹的全名，支持自动联想。 除了自动跳转功能，我还将其作为终端到文件资源管理器(Mac: Finder)的跳转功能。12# 跳转到path并使用文件资源管理器打开jo path tldrtldr (too long don’t read)是一款能够给出bash命令常用功能的工具。在Linux系统中，很多命令都有一长串参数。这其中很多都是不常用的。而我们使用时，常常是使用某几个常见的功能选项。tldr就能够给出命令的简要描述和例子。 例如，使用其查询tar的常用方法： 123456789101112131415161718192021222324252627tldr tar# outputtarArchiving utility.Often combined with a compression method, such as gzip or bzip.- Create an archive from files: tar cf target.tar file1 file2 file3- Create a gzipped archive: tar czf target.tar.gz file1 file2 file3- Extract an archive in a target folder: tar xf source.tar -C folder- Extract a gzipped archive in the current directory: tar xzf source.tar.gz- Extract a bzipped archive in the current directory: tar xjf source.tar.bz2- Create a compressed archive, using archive suffix to determine the compression program: tar caf target.tar.xz file1 file2 file3- List the contents of a tar file: tar tvf source.tar tldr支持多种语言，我使用了python包安装。但是不知为何，tldr在我这里总显示奇怪的背景颜色，看上去很别扭。所以我实际使用的是tldr-py。 tmux用SSH登录到服务器上时，如果网络连接不稳定或是自己的主机意外断电，会造成正在跑的代码死掉。因为进程是依附于SSH的会话Session的。tmux是一个终端的“分线器”，可以很方便地将正在进行的终端会话detach掉，使其转入后台运行。正是有这一特点，所以我们可以在SSH会话时，新建tmux会话，在其中跑一些耗时很长的代码，而不必担心SSH掉线。当然，也可以将tmux作为一款终端多任务的管理软件，方便地在多个任务中进行跳转。不过这个功能，我更加常用的是下面的guake。 虽然Ubuntu14.04可以通过apt-get的方式安装tmux，不过为了能够使用一款好用的配置oh-my-tmux（要求tmux&gt;=2.1），还是推荐去GitHub上自己编译安装tmux。如果使用Ubuntu16.04及以上版本，那么官方源中的tmux已经足够新，无需自己编译安装。 guakeguake是一款Ubuntu上可以方便呼出终端的应用（按下F12，终端将以全屏的方式铺满桌面，F11可以切换全屏或半屏）。 Dash/ZealDash是Mac上一款用于查询API文档的软件。在Ubuntu或Windows上，我们可以使用Zeal这个替代软件。Zeal和Dash基本上无缝衔接，但是却是免费的（Mac上的软件真是好贵。。。）。之前我已经写过一篇博客，介绍如何自己制作文档导入Zeal中。 sshfs使用sshfs可以在本地机器上挂载远程服务器某个文件夹。这样，操作本地的该文件夹就相当于操作远程服务器上的该文件夹（小心使用rm）。 Alfred/MutateAlfred是Mac上一款非常好用的软件，就像蝙蝠侠身边的老管家一样，可以帮你自动化处理很多事情。除了原生功能，还可以自己编写脚本实现扩展。例如查询豆瓣电影，查询ip，计算器等。鉴于这款软件的大名，这里不再多说。 Mutate是Ubuntu上的一款替代软件。同时，它也提供了方便的扩展接口，只需要按照模板编写python/shell代码，可以很方便地将自己的自动化处理功能加入软件中。 MosMac笔记本的触摸板默认的竖直滚动方向是和触摸屏一样的，也就是手指向上移动，页面向下滚动。习惯了这种方式的话，如果插上外接鼠标就会显得很不方便，因为系统默认鼠标也是按照这种逻辑。但是实际上，很多人更习惯于向下滚动滚轮，页面向下滚动。另外，鼠标在Mac上的滚动不是很自然。Mos是一款国内开发者开源的调节鼠标滚动方向的工具软件。 一个用于在 MacOS 上平滑你的鼠标滚动效果或单独设置滚动方向的小工具, 让你的滚轮爽如触控板 | A lightweight tool used to smooth scrolling and set scroll direction independently for your mouse on MacOS http://mos.caldis.me 可以使用下面的命令进行安装，或自行前往Release页面下载安装。1brew cask install mos]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[学一点PyQT]]></title>
      <url>%2F2017%2F08%2F29%2Flearn-pyqt%2F</url>
      <content type="text"><![CDATA[Qt是一个流行的GUI框架，支持C++/Python。这篇文章是我在这两天通过PyQT制作一个串口通信并画图的小程序的时候，阅读PyQT5的一篇教程时候的记录。 主要模块PyQt5中的主要三个模块如下： QtCore: 和GUI无关的核心功能：文件，时间，多线程等 QtGui：和GUI相关的的东西：事件处理，2D图形，字体和文本等 QtWidget：GUI中的相关组件，例如按钮，窗口等。 其他模块还有QtBluetooth，QtNetwork等，都是比较专用的模块，用到再说。 HelloWorld这里首先给出一段简单的程序，可以在桌面上显示一个窗口。123456789101112import sysfrom PyQt5.QtWidgets import QApplication, QWidgetif __name__ == '__main__': app = QApplication(sys.argv) w = QWidget() w.resize(250, 150) w.move(300, 300) w.setWindowTitle('Simple') w.show() sys.exit(app.exec_()) 下面介绍上面代码的含义：1app = QApplication(sys.argv) 每个Qt5应用必须首先创建一个application，后面会用到。12345w = QWidget()w.resize(250, 150)w.move(300, 300)w.setWindowTitle('Simple')w.show() QtWidget是所有组件的父类，我们创建了一个Widget。没有任何parent widget的Widget会作为窗口出现。接下来，调用其成员函数实现调整大小等功能。最后使用show()将其显示出来。 1sys.exit(app.exec_()) 进入application的主循环，等待事件的触发。当退出程序（也许是通过Ctrl+C实现的）或者关闭窗口（点击关闭）后，主循环退出。 添加一个按钮下面，我们为窗口添加按钮，并为其添加事件响应动作。 参考文档可知，按钮QPushButton存在这样的构造函数：1__init__ (self, QWidget parent = None) 下面的代码在初始化QPushButton实例btn时，将self作为参数传入，指定了其parent。另外，在指定按钮大小的时候，使用了sizeHint()方法自适应调节其大小。 同时，为按钮关联了点击动作。Qt中的事件响应机制通过信号和槽实现。点击事件一旦发生，信号clicked会被释放。然后槽相对的处理函数被调用。所谓的槽可以使PyQt提供的slot，或者是任何Python的可调用对象（函数或者实现了__call__()方法的对象）。 我们调用了现成的处理函数，来达到关闭窗口的目的。使用instance()可以得到当前application实例，调用其quit()方法即是退出当前应用，自然窗口就被关闭了。123456789101112131415161718192021222324import sysfrom PyQt5.QtWidgets import QApplication, QWidget, QPushButton, QToolTipfrom PyQt5.QtCore import QCoreApplicationclass MyWindow(QWidget): def __init__(self): super(MyWindow, self).__init__() self._init_ui() def _init_ui(self): btn = QPushButton('quit', self) btn.clicked.connect(QCoreApplication.instance().quit) btn.setToolTip('This is a &lt;b&gt;QPushButton&lt;/b&gt; widget') btn.move(50, 50) btn.resize(btn.sizeHint()) self.setGeometry(300, 300, 300, 200) self.setWindowTitle('Window with Button') self.show()if __name__ == '__main__': app = QApplication(sys.argv) window = MyWindow() sys.exit(app.exec_()) 使用Event处理事件除了上述的信号和槽的处理方式，也可以使用Event相关的类进行处理。下面的代码在关闭窗口时弹出对话框确认是否关闭。根据用户做出的选择，调用event.accept()或ignore()完成对事件的处理。 12345678910111213141516171819202122import sysfrom PyQt5.QtWidgets import QWidget, QMessageBox, QApplicationclass MyWindow(QWidget): def __init__(self): super(MyWindow, self).__init__() self._init_ui() def _init_ui(self): self.setGeometry(300, 300, 300, 200) self.show() def closeEvent(self, ev): reply = QMessageBox.question(self, 'Message', 'Are you sure?', QMessageBox.Yes | QMessageBox.No, QMessageBox.No) if reply == QMessageBox.Yes: ev.accept() else: ev.ignore()if __name__ == '__main__': app = QApplication(sys.argv) win = MyWindow() sys.exit(app.exec_()) 使用Layout组织Widget组织Widget的方式可以通过绝对位置调整，但是更推荐使用Layout组织。 绝对位置是通过指定像素多少来确定widget的大小和位置。这样的话，有以下几个缺点： 不同平台可能显示效果不统一； 当parent resize的时候，widget大小和位置并不会自动调整 编码太麻烦，牵一发而动全身 下面介绍几种常见的Layout类。 Box Layout有QVBoxLayout和QHBoxLayout，用来将widget水平或者竖直排列起来。下面的代码通过这两个layout将按钮放置在窗口的右下角。关键的地方在于使用addSkretch()方法将一个QSpacerItem实例对象插入到了layout中，占据了相应位置。 1234567891011121314151617181920212223242526272829303132import sysfrom PyQt5.QtWidgets import (QWidget, QPushButton, QHBoxLayout, QVBoxLayout, QApplication)class MyWindow(QWidget): def __init__(self): super(MyWindow, self).__init__() self._init_ui() def _init_ui(self): okButton = QPushButton("OK") cancelButton = QPushButton("Cancel") hbox = QHBoxLayout() hbox.addStretch(1) hbox.addWidget(okButton) hbox.addWidget(cancelButton) vbox = QVBoxLayout() vbox.addStretch(1) vbox.addLayout(hbox) self.setLayout(vbox) self.setGeometry(300, 300, 300, 150) self.setWindowTitle('Buttons') self.show()if __name__ == '__main__': app = QApplication(sys.argv) win = MyWindow() sys.exit(app.exec_()) Grid LayoutQGridLayout将空间划分为行列的grid。在向其中添加item的时候，要指定位置。如下，将5行4列的grid设置为计算器的面板模式。123456789101112131415161718192021222324252627282930313233import sysfrom PyQt5.QtWidgets import (QWidget, QGridLayout, QPushButton, QApplication)class MyWindow(QWidget): def __init__(self): super(MyWindow, self).__init__() self._init_ui() def _init_ui(self): grid = QGridLayout() self.setLayout(grid) names = ['Cls', 'Bck', '', 'Close', '7', '8', '9', '/', '4', '5', '6', '*', '1', '2', '3', '-', '0', '.', '=', '+'] positions = [(i,j) for i in range(5) for j in range(4)] for position, name in zip(positions, names): if name == '': continue button = QPushButton(name) grid.addWidget(button, *position) self.move(300, 150) self.setWindowTitle('Calculator') self.show()if __name__ == '__main__': app = QApplication(sys.argv) win = MyWindow() sys.exit(app.exec_()) 另外，我们还可以通过setSpacing()方法设置每个单元格之间的间隔。如果某个widget需要占据多个单元格，可以在addWidget()方法中指定要扩展的行列数。 事件驱动PyQt提供了两种事件驱动的处理方式： 使用event句柄。事件可能是由于UI交互或者定时器等引起，由接收对象进行处理。 信号和槽。某个widge交互时，释放相应信号，被槽对应的函数捕获进行处理。 信号和槽可以见上面使用按钮关闭窗口的例子，关键在于调用信号的connect()函数将其绑定到某个槽上。Python中的可调用对象都可以作为槽。 而使用event句柄处理时，需要重写override原来的处理函数，见上面使用其在关闭窗口时进行弹窗确认的例子。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[doc2dash——制作自己的dash文档]]></title>
      <url>%2F2017%2F08%2F26%2Fdoc2dash-usage%2F</url>
      <content type="text"><![CDATA[Dash是Mac上一款超棒的应用，提供了诸如C/C++/Python甚至是OpenCV/Vim等软件包或工具软件的参考文档。只要使用App的“Download Docsets”功能，就能轻松下载相应文档。使用的时候只需在Dash的搜索框内输入相应关键词，Dash会在所有文档中进行搜索，给出相关内容的列表。点击我们要寻找的条款，就能够直接在本地阅读文档。在Ubuntu/Windows平台上，Dash也有对应的替代品，例如zeal就是一款Windows/Linux平台通用的Dash替代软件。 这样强大的软件，如果只能使用官方提供的文档岂不是有些大材小用？doc2dash就是一款能够自动生成Dash兼容docset文件的工具。例如，可以使用它为PyTorch生成本地化的docset文件，并导入Dash/zeal中，在本地进行搜索阅读。这不是美滋滋？ 本文章是基于doc2dash的官方介绍，对其使用进行的总结。 安装doc2dashdoc2dash是基于Python开发的。按照官方网站介绍，为了避免Python包的冲突，最好使用虚拟环境进行安装。我的机器上安装有Anaconda环境，所以首先使用conda create命令新建用于doc2dash的虚拟环境。1conda create -n doc2dash 接下来，激活虚拟环境，并使用pip install命令安装。12source activate doc2dashpip install doc2dash doc2dash支持的输出格式可以通过sphinx或者pydoctor。其中前者更加常用。下面以PyTorch项目的文档生成为例，介绍doc2dash的具体用法。 生成PyTorch文档doc2dash使用sphinx生成相应的文档。在上述安装doc2dash的过程中，应该已经安装了sphinx包。不过我们还需要手动安装，以便处理rst文档。 1pip install sphinx_rtd_theme 进入PyTorch的文档目录docs/，PyTorch已经为我们提供了Makefile，调用sphinx包进行文档处理，可以选择make html命令生成相应的HTML文档，生成的位置为build/html。 12# in directory $PYTORCH/docs, runmake html 接下来，就可以使用doc2dash来继续sphinx的工作，生成Dash可用的文档文件了~使用-n指定生成的文件名称，后面跟source文件夹路径即可。 12# $PYTORCH/docs/build/html即为生成的HTML目录doc2dash -n pytorch $PYTORCH/docs/build/html 之后，把生成的pytorch.docset导入到Dash中即可。如下图所示，点击“+”找到文件添加即可。 在Ubuntu上安装zealzeal是Dash在非Mac平台上的替代软件。在Ubuntu上可以使用如下方式轻松安装（见官方网站介绍）。 123sudo add-apt-repository ppa:zeal-developers/ppasudo apt-get updatesudo apt-get install zeal 安装后，可以使用Tool/Docsets下载相应的公开文档。如果想要添加自己生成的文档，只需要将生成的docset文件放到软件的文档库中即可，默认位置应在$HOME/.local/share/Zeal/Zeal/docsets。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用IPDB调试Python代码]]></title>
      <url>%2F2017%2F08%2F21%2Fdebugging-with-ipdb%2F</url>
      <content type="text"><![CDATA[IPDB是什么？IPDB（Ipython Debugger），和GDB类似，是一款集成了Ipython的Python代码命令行调试工具，可以看做PDB的升级版。这篇文章总结IPDB的使用方法，主要是若干命令的使用。更多详细的教程或文档还请参考Google。 安装与使用IPDB以Python第三方库的形式给出，使用pip install ipdb即可轻松安装。 在使用时，有两种常见方式。 集成到源代码中通过在代码开头导入包，可以直接在代码指定位置插入断点。如下所示：123456import ipdb# some codex = 10ipdb.set_trace()y = 20# other code 则程序会在执行完x = 10这条语句之后停止，展开Ipython环境，就可以自由地调试了。 命令式上面的方法很方便，但是也有不灵活的缺点。对于一段比较棘手的代码，我们可能需要按步执行，边运行边跟踪代码流并进行调试，这时候使用交互式的命令式调试方法更加有效。启动IPDB调试环境的方法也很简单：1python -m ipdb your_code.py 常用命令IPDB调试环境提供的常见命令有： 帮助帮助文档就是这样一个东西：当你写的时候觉得这TM也要写？当你看别人的东西的时候觉得这TM都没写？ 使用h即可调出IPDB的帮助。可以使用help command的方法查询特定命令的具体用法。 下一条语句使用n(next)执行下一条语句。注意一个函数调用也是一个语句。如何能够实现类似“进入函数内部”的功能呢？ 进入函数内部使用s(step into)进入函数调用的内部。 打断点使用b line_number(break)的方式给指定的行号位置加上断点。使用b file_name:line_number的方法给指定的文件（还没执行到的代码可能在外部文件中）中指定行号位置打上断点。 另外，打断点还支持指定条件下进入，可以查询帮助文档。 一直执行直到遇到下一个断点使用c(continue)执行代码直到遇到某个断点或程序执行完毕。 一直执行直到返回使用r(return)执行代码直到当前所在的这个函数返回。 跳过某段代码使用j line_number(jump)可以跳过某段代码，直接执行指定行号所在的代码。 更多上下文在IPDB调试环境中，默认只显示当前执行的代码行，以及其上下各一行的代码。如果想要看到更多的上下文代码，可以使用l first[, second](list)命令。 其中first指示向上最多显示的行号，second指示向下最多显示的行号（可以省略）。当second小于first时，second指的是从first开始的向下的行数（相对值vs绝对值）。 根据SO上的这个问题，你还可以修改IPDB的源码，一劳永逸地改变上下文的行数。 我在哪里调试兴起，可能你会忘了自己目前所在的行号。例如在打印了若干变量值后，屏幕完全被这些值占据。使用w或者where可以打印出目前所在的行号位置以及上下文信息。 这是啥我们可以使用whatis variable_name的方法，查看变量的类别（感觉有点鸡肋，用type也可以办到）。 列出当前函数的全部参数当你身处一个函数内部的时候，可以使用a(argument)打印出传入函数的所有参数的值。 打印使用p(print)和pp(pretty print)可以打印表达式的值。 清除断点使用cl或者clear file:line_number清除断点。如果没有参数，则清除所有断点。 再来一次使用restart重新启动调试器，断点等信息都会保留。restart实际是run的别名，使用run args的方式传入参数。 退出使用q退出调试，并清除所有信息。 当然，这并不是IPDB的全部。其他的命令还请参照帮助文档。文档在手，天下我有！]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Focal Loss论文阅读 - Focal Loss for Dense Object Detection]]></title>
      <url>%2F2017%2F08%2F14%2Ffocal-loss-paper%2F</url>
      <content type="text"><![CDATA[Focal Loss这篇文章是He Kaiming和RBG发表在ICCV2017上的文章。关于这篇文章在知乎上有相关的讨论。最近一直在做强化学习相关的东西，目标检测方面很长时间不看新的东西了，把自己阅读论文的要点记录如下，也是一次对这方面进展的回顾。 下图来自于论文，是各种主流模型的比较。其中横轴是前向推断的时间，纵轴是检测器的精度。作者提出的RetinaNet在单独某个维度上都可以吊打其他模型。不过图上没有加入YOLO的对比。YOLO的速度仍然是其一大优势，但是精度和其他方法相比，仍然不高。 Update@2018.03.26 YOLO更新了v3版本，见项目主页，并“点名”与有Focal Loss加持的Retina Net相比较，见下图。 为什么要有Focal Loss？目前主流的检测算法可以分为两类：one-state和two-stage。前者以YOLO和SSD为代表，后者以RCNN系列为代表。后者的特点是分类器是在一个稀疏的候选目标中进行分类（背景和对应类别），而这是通过前面的proposal过程实现的。例如Seletive Search或者RPN。相对于后者，这种方法是在一个稀疏集合内做分类。与之相反，前者是输出一个稠密的proposal，然后丢进分类器中，直接进行类别分类。后者使用的方法结构一般较为简单，速度较快，但是目前存在的问题是精度不高，普遍不如前者的方法。 论文作者指出，之所以做稠密分类的后者精度不够高，核心问题（central issus）是稠密proposal中前景和背景的极度不平衡。以我更熟悉的YOLO举例子，比如在PASCAL VOC数据集中，每张图片上标注的目标可能也就几个。但是YOLO V2最后一层的输出是$13 \times 13 \times 5$，也就是$845$个候选目标！大量（简单易区分）的负样本在loss中占据了很大比重，使得有用的loss不能回传回来。 基于此，作者将经典的交叉熵损失做了变形（见下），给那些易于被分类的简单例子小的权重，给不易区分的难例更大的权重。同时，作者提出了一个新的one-stage的检测器RetinaNet，达到了速度和精度很好地trade-off。 \text{FL}(p_t) = -(1-p_t)^\gamma \log(p_t)物体检测的两种主流方法在深度学习之前，经典的物体检测方法为滑动窗，并使用人工设计的特征。HoG和DPM等方法是其中比较有名的。 R-CNN系的方法是目前最为流行的物体检测方法之一，同时也是目前精度最高的方法。在R-CNN系方法中，正负类别不平衡这个问题通过前面的proposal解决了。通过EdgeBoxes，Selective Search，DeepMask，RPN等方法，过滤掉了大多数的背景，实际传给后续网络的proposal的数量是比较少的（1-2K）。 在YOLO，SSD等方法中，需要直接对feature map的大量proposal（100K）进行检测，而且这些proposal很多都在feature map上重叠。大量的负样本带来两个问题： 过于简单，有效信息过少，使得训练效率低； 简单的负样本在训练过程中压倒性优势，使得模型发生退化。 在Faster-RCNN方法中，Huber Loss被用来降低outlier的影响（较大error的样本，也就是难例，传回来的梯度做了clipping，也只能是$1$）。而FocalLoss是对inner中简单的那些样本对loss的贡献进行限制。即使这些简单样本数量很多，也不让它们在训练中占到优势。 Focal LossFocal Loss从交叉熵损失而来。二分类的交叉熵损失如下： \text{CE}(p, y) = \begin{cases}-\log(p) \quad &\text{if}\quad y = 1\\ -\log(1-p) &\text{otherwise}\end{cases}对应的，多分类的交叉熵损失是这样的： \text{CE}(p, y) = -\log(p_y)如下图所示，蓝色线为交叉熵损失函数随着$p_t$变化的曲线($p_t$意为ground truth，是标注类别所对应的概率)。可以看到，当概率大于$.5$，即认为是易分类的简单样本时，值仍然较大。这样，很多简单样本累加起来，就很可能盖住那些稀少的不易正确分类的类别。 为了改善类别样本分布不均衡的问题，已经有人提出了使用加上权重的交叉熵损失，如下（即用参数$\alpha_t$来平衡，这组参数可以是超参数，也可以由类别的比例倒数决定）。作者将其作为比较的baseline。 \text{CE}(p) = -\alpha_t\log(p_t)作者提出的则是一个自适应调节的权重，即Focal Loss，定义如下。由上图可以看到$\gamma$取不同值的时候的函数值变化。作者发现，$\gamma=2$时能够获得最佳的效果提升。 \text{FL}(p_t) = -(1-p_t)^\gamma\log(p_t)在实际实验中，作者使用的是加权之后的Focal Loss，作者发现这样能够带来些微的性能提升。 实现这里给出PyTorch中第三方给出的Focal Loss的实现。在下面的代码中，首先实现了one-hot编码，给定类别总数classes和当前类别index，生成one-hot向量。那么，Focal Loss可以用下面的式子计算（可以对照交叉损失熵使用onehot编码的计算）。其中，$\odot$表示element-wise乘法。 L = -\sum_{i}^{C}\text{onehot}\odot (1-P_i)^\gamma \log P_i123456789101112131415161718192021222324252627282930313233343536import torchimport torch.nn as nnimport torch.nn.functional as Ffrom torch.autograd import Variabledef one_hot(index, classes): size = index.size() + (classes,) view = index.size() + (1,) mask = torch.Tensor(*size).fill_(0) index = index.view(*view) ones = 1. if isinstance(index, Variable): ones = Variable(torch.Tensor(index.size()).fill_(1)) mask = Variable(mask, volatile=index.volatile) return mask.scatter_(1, index, ones)class FocalLoss(nn.Module): def __init__(self, gamma=0, eps=1e-7): super(FocalLoss, self).__init__() self.gamma = gamma self.eps = eps def forward(self, input, target): y = one_hot(target, input.size(-1)) logit = F.softmax(input) logit = logit.clamp(self.eps, 1. - self.eps) loss = -1 * y * torch.log(logit) # cross entropy loss = loss * (1 - logit) ** self.gamma # focal loss return loss.sum() 模型初始化对于一般的分类网络，初始化之后，往往其输出的预测结果是均等的（随机猜测）。然而作者认为，这种初始化方式在类别极度不均衡的时候是有害的。作者提出，应该初始化模型参数，使得初始化之后，模型输出稀有类别的概率变小（如$0.01$）。作者发现这种初始化方法对于交叉熵损失和Focal Loss的性能提升都有帮助。 在后续的模型介绍部分，作者较为详细地说明了模型初始化方法。首先，从imagenet预训练得到的base net不做调整，新加入的卷积层权重均初始化为$\sigma=0.01$的高斯分布，偏置项为$0$。对于分类网络的最后一个卷积层，将偏置项置为$b=-\log((1-\pi)/\pi)$。这里的$\pi$参数是一个超参数，其意义是在训练的初始阶段，每个anchor被分类为前景的概率。在实验中，作者实际使用的大小是$0.01$。 这样进行模型初始化造成的结果就是，在初始阶段，不会产生大量的False Positive，使得训练更加稳定。 RetinaNet作者利用前面介绍的发现和结论，基于ResNet和Feature Pyramid Net（FPN）设计了一种新的one-stage检测框架，命名为RetinaNet。（待续）]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[DELL 游匣7559安装Ubuntu和CUDA记录]]></title>
      <url>%2F2017%2F08%2F10%2Finstall-ubuntu-in-dell%2F</url>
      <content type="text"><![CDATA[虽说开源大法好，但是在我的DELL 游匣7559笔记本上安装Ubuntu+Windows双系统可是耗费了我不少精力。这篇博客是我参考这篇文章成功安装Ubuntu16.04和CUDA的记录。感谢上文作者的记录，我才能够最终解决这个问题。基本流程和上文作者相同，只不过没有安装后续的bumblee等工具，所以本文并不是原创，而更多是翻译和备份。 蛋疼的过往之前我安装过Ubuntu14.04，但是却不支持笔记本的无线网卡，所以一直很不方便。搜索之后才发现，笔记本使用的无线网卡要到Ubuntu15.10以上才有支持，所以想要安装16.04.结果却发现安装界面都进不去。。。 安装Ubuntu我使用的版本号为Ubuntu16.04.3，使用Windows中的UltraISO制作U盘启动盘。在Windows系统中，通过电池计划关闭快速启动功能，之后重启。在开机出现DELL徽标的时候，按下F12进入BIOS，关闭Security Boot选项。按F10保存并重启，选择U盘启动。 选择“Install Ubuntu”选项，按e，找到包含有quiet splash的那行脚本，将quiet splash替换为以下内容： 1nomodeset i915.modeset=1 quiet splash 之后按F10重启，会进入Ubuntu的安装界面。如何安装Ubuntu这里不再详述。安装完毕之后，重启。出现Ubuntu GRUB引导界面之后，高亮Ubuntu选项（一般来说就是第一个备选项），按e，按照上述方法替换quiet splash。确定可以进入Ubuntu系统并登陆。 GRUB设置下面，修改GRUB设置，避免每次都手动替换。编辑相应配置文件：sudo vi /etc/default/grub，找到包含GRUB_CMDLINE_LINUX_DEFAULT的那一行，将其修改如下（就是将我们上面每次手动输入的内容直接写到了配置里面）： 1GRUB_CMDLINE_LINUX_DEFAULT="nomodeset i915.modeset=1 quiet splash" 更新系统软件配置更新源（清华的很好用，非教育网也能轻轻松松上700K），使用如下命令更新， 1234sudo apt-get updatesudo apt-get upgradesudo apt-get dist-upgradesudo apt-get autoremove 参考博客中指出，如果这个过程中让你选GRUB文件，要选择保持原有文件。但是我并没有遇到这个问题。可能是由于我的Ubuntu版本已经是16.04中目前最新的了？ 由于后续有较多的终端文件编辑操作，建议这时候顺便安装Vim。1sudo apt-get install vim 更新完之后，重启，确认可以正常登陆系统。 移除原有的Nvidia和Nouveau驱动按下ALT+CTL+F1，进入虚拟终端，首先关闭lightdm服务。这项操作之后会比较经常用到。1sudo service lightdm stop 之后，执行卸载操作：123sudo apt-get remove --purge nvidia*sudo apt-get remove --purge bumblebee*sudo apt-get --purge remove xserver-xorg-video-nouveau* 编辑配置文件，/etc/modprobe.d/blacklist.conf，将Nouveau加入到黑名单中：12345blacklist nouveaublacklist lbm-nouveaualias nouveau offalias lbm-nouveau offoptions nouveau modeset=0 编辑/etc/init/gpu-manager.conf文件，将其前面几行注释掉，改成下面的样子，停止gpu-manager服务：1234567# Comment these start on settings ; GPU Manager ruins our work#start on (starting lightdm# or starting kdm# or starting xdm# or starting lxdm)taskexec gpu-manager --log /var/log/gpu-manager.log 之后，更新initramfs并重启。1sudo update-initramfs -u -k all 重启后，确定可以正常登陆系统。并使用下面的命令确定Nouveau被卸载掉了：12# 正常情况下，下面的命令应该不产生任何输出lsmod | grep nouveau 并确定关闭了gpu-manager服务：1sudo service gpu-manager stop 至此，Ubuntu系统算是安装完毕了。如果没有使用CUDA的需求，可以从这里开始，安安静静地做一个使用Ubuntu的美男子/小仙女了。 安装CUDA鉴于国内坑爹的连接资本主义世界的网络环境，建议还是先去Nvidia的官网把CUDA离线安装包下载下来再安装。我使用的是CUDA-8.0-linux.deb安装包。 按ALT+CTL+F1进入虚拟终端，停止lightdm服务，并安装一些可能要用到的包。123sudo service lightdm stopsudo apt-get install linux-headers-$(uname -r)sudo apt-get install mesa-utils 安装CUDA包：1234sudo dpkg -i YOUR_CUDA_DEB_PATHsudo apt-get updatesudo apt-get install cuda-8-0sudo apt-get autoremove 安装完毕之后使用sudo reboot重启，确定能够正常登陆系统。 在这个过程中，作者提到登录界面会出现两次，再次重启之后没有这个问题了。我也遇到了相同的情况。所以，不要慌！ 测试CUDA我们来测试一下CUDA。首先，依照你使用shell的不同，将环境变量加入到~/.bashrc或者~/.zshrc（如果使用zsh）中去。12export PATH="$PATH:/usr/local/cuda-8.0/bin"export LD_LIBRARY_PATH="$LD_LIBRARY_PATH:/usr/local/cuda-8.0/lib" 接下来，我们将使用CUDA自带的example进行测试：123456789# 导入我们刚加入的环境变量source ~/.bashrccd /usr/local/cuda-8.0/bin# 将CUDA example拷贝到$HOME下./cuda-install-samples-8.0.sh ~# 进入拷贝到的那个目录 buildcd ~/NVIDIA_CUDA-8.0_Samplesmake -j12# 自己挑选几个目录进去运行编译生成的可执行文件测试吧~ Last But Not Least安装完CUDA之后，不要随便更新系统！！！否则可能会损坏你的Kernel和Xserver。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 8 定制new和delete]]></title>
      <url>%2F2017%2F07%2F03%2Feffective-cpp-08%2F</url>
      <content type="text"><![CDATA[手动管理内存，这既是C++的优点，也是C++中很容易出问题的地方。本章主要给出分配内存和归还时候的注意事项，主角是operator new和operator delete，配角是new_handler，它在当operator new无法满足客户内存需求时候被调用。 另外，operator new和operator delete只用于分配单一对象内存。对于数组，应使用operator new[]，并通过operator delete[]归还。除非特别指定，本章中的各项既适用于单一operator new，也适用于operator new[]。 最后，STL中容器使用的堆内存是由容器拥有的分配器对象（allocator objects）来管理的。本章不讨论。 49 了解new-handler的行为什么是new-handler？当operator new无法满足内存分配需求时，会抛出异常。在抛出异常之前，会先调用一个客户指定的错误处理函数，这就是所谓的new-handler，也就是一个擦屁股的角色。 为了指定new-handler，必须调用位于标准库&lt;new&gt;的函数set_new_handler。其声明如下：1234namespace std &#123; typedef void (*new_handler) (); new_handler set_new_handler(new_handler p) throw();&#125; 其中，传入参数p是你要指定的那个擦屁股函数的指针，返回参数是被取代的那个原始处理函数。throw()表示该函数不抛出任何异常。 当operator new无法满足内存需求时，会不断调用set_new_handler()，直到找到足够的内存。更加具体的介绍见条款51. 一个设计良好的new_handler函数可以是以下的设计策略： 设法找到更多的内存可供使用，以便使得下一次的operator new成功。 安装另一个new_handler函数。即在其中再次调用set_new_handler，找到其他的擦屁股函数接盘。 卸载new_handler函数。即将NULL指针传进set_new_handler()中去。这样，operator new会抛出异常。 抛出bad_alloc（或其派生类）异常。 不返回（放弃治疗），直接告诉程序exit或abort。 有的时候想为不同的类定制不同的擦屁股函数。这时候，需要为每个类提供自己的set_new_handler()函数和operator new。如下所示，由于对类的不同对象而言，擦屁股机制都是相同的，所以我们将擦屁股函数声明为类内的静态成员。 123456789101112131415class A &#123;public: static std::new_handler set_new_handler(std::new_handler p) throw(); static void* operator new(std::size_t size) throw(std::bad_alloc);private: static std::new_handler current_handler;&#125;;// 实现文件std::new_handler A::set_new_handler(std::new_handler p) throw() &#123; std::new_hanlder old = current_handler; current_handler = p; return old;&#125; 静态成员变量必须在类外进行定义（除非是const且为整数型），所以需要在类外定义：12// 实现文件std::new_handler A::current_handler = 0; 在实现自定义的operator new的时候，首先调用set_new_handler()将自己的擦屁股函数安装为默认，然后调用global的operator new进行内存分配，最后恢复，把原来的擦屁股函数复原回去。书中，作者使用了一个类进行包装，利用类在scope的自动构造与析构，实现了自动化处理： 1234567891011121314151617// 这个类实现了自动安装与恢复new_handlerclass Helper &#123;public: explicit Helper(std::new_handler p): handler(p) &#123;&#125; ~Helper() &#123;std::set_new_handler(handler); &#125;private: std::new_handler handler; // 禁止拷贝构造与赋值 Helper(const Helper&amp;); Helper&amp; operator= (const Helper&amp;);&#125;;// 实现类A自定义的operator newvoid* A::operator new(std::size_t size) throw(std::bad_alloc) &#123; // 存储了函数返回值，也就是原始的 new_handler Helper h(std::set_new_handler(current_handler)); return ::operator new(size);&#125; 新的问题随之而来。如果我们想方便地复用上述代码呢？一个简单的方法是建立一个mixin风格的基类，这种基类用来让派生类继承某个唯一的能力（本例中是设定类的专属new_handler的能力）。而为了让不同的类获得不同的current_handler变量，我们把这个基类做成模板。 12345678910template &lt;typename T&gt;class HandlerHelper &#123;public: static std::new_handler set_new_handler(std::new_handler p) throw(); static void* operator new(std::size_t size) throw(std::bad_alloc); ... // 其他的new版本，见条款52private: static std::new_handler current_handler;&#125;;// 实现部分的代码不写了，和上面的Helper和A中的对应内容基本完全一样 这样，我们只要让类A继承自HandlerHelper&lt;A&gt;即可（看上去很怪异。。。）：123class A: public HandlerHelper&lt;A&gt; &#123; ...&#125;; 50 了解替换new和delete的合适时机最常见的理由（替换之后你能得到什么好处）： 检测运用上的错误。比如缓冲区越界，我们可以在delete的时候进行检查。 强化效能。编译器实现的operator new是为了普适性的功能，改成自定义版本可能提升效能。 收集使用上的统计数据。为了优化程序性能，理当先收集你的软件如何使用动态内存。自定义的operator new和delete能够收集到这些信息。 但是，写出能正常工作的new却不一定获得很好的性能。（各种细节上的问题，例如内存的对齐。也正因为如此，这里不再重复书上的一个具体实现）例如Boost库中的Pool对分配大量小型对象很有帮助。 51 编写new和delete时候需要遵守常规自定义的operator new需要满足以下几点： 如果有足够的内存，则返回其指针；否则，遵循条款49的约定。 具体地，如果内存不足，那么应该循环调用new_handling函数（里面可能会清理出一些内存以供使用）。只有当指向new_handling的指针为NULL时，才抛出异常bad_alloc。 C++规定，即使用户申请的内存大小为0，也要返回一个合法指针。这个看似诡异的行为是为了简化语言的其他部分。 还要避免掩盖正常的operator new。 下面就是一个自定义operator new的例子：123456789101112131415161718192021222324void* operator new(size_t size) throw(bad_alloc) &#123; // 你的operator new也可能接受额外参数 using namespace std; if(size == 0) &#123; size = 1; // 处理0byte申请 &#125; while(true) &#123; // ... try to alloc memory if(success) &#123; return the pointer; &#125; // 处理分配失败，找出当前的handler // 我们没有诸如get_new_handler()的方法来获取new_handler函数句柄 // 所以只能用下面这种方法，利用set_new_handler的返回值获取当前处理函数 new_handler globalHandler = set_new_handler(0); set_new_handler(globalHandler); if(globalHandler) &#123; (*globalHandler)(); &#125; else &#123; throw bad_alloc(); &#125; &#125;&#125; 在自定义operator delete时候，注意处理空指针的情况。C++确保delete NULL pointer是永远安全的。1234void operator delete(void* memory) throw() &#123; if(memory == 0) return; // ...&#125; 52 写了placement new也要写placement delete如果operator new接受的参数除了一定会有的那个size_t之外还有其他参数，那么它就叫做placement new。一个特别有用的placement new的用法是接受一个指针指向对象该被构造之处。声明如下所示：1void* operator new(size_t size, void* memory) throw(); 上述placement new已经被纳入C++规范（可以在头文件&lt;new&gt;中找到它。）这个函数常用来在vector的未使用空间上构造对象。实际上这是placement的得来：特定位置上的new。有的时候，人们谈论placement new时，实际是在专指这个函数。 本条款主要探讨与placement new使用不当相关的内存泄漏问题。当你写一个new表达式时，共有两个函数被调用： 分配内存的operator new 该类的构造函数 假设第一个函数调用成功，第二个函数却抛出异常。这时候我们需要将第一步申请得到的内存返还并恢复旧观，否则就会造成内存泄漏。具体来说，系统会调用和刚才申请内存的operator new对应的delete版本。 如果目前面对的是正常签名的operator new delete，不会有问题。不过若是当时调用的是修改过签名形式的placement new时，就可能出现问题。例如，我们有下面的placement new，它的功能是在分配内存的时候做一些logging工作。1234// 某个类Wedget内部有自定义的placement new如下static void* operator new(size_t size, ostream&amp; logStream) throw (bad_alloc);Widget* pw = new (std::cerr) Widget; 如果系统找不到相应的placement delete版本，就会什么都不做。这样，就无法归还已经申请的内存，造成内存泄漏。所以有必要声明一个placement delete，对应那个有logging功能的placement new。123static void operator delete(void* memory, ostream&amp; logStream) throw();// 这样，即使下式抛出异常，也能正确处理Widget* pw = new (std::cerr) Widget; 然而，如果什么异常都没有抛出，而客户又使用了下面的表达式返还内存：1delete pw; 那么它调用的是正常版本的delete。所以，除了相对应的placement delete，还有必要同时提供正常版本的delete。前者为了解决构造过程中有异常抛出的情况，后者处理无异常抛出。 一个比较简单的做法是，建立一个基类，其中有所有正常形式的new和delete。12345678910111213141516171819202122232425class StdNewDeleteForms &#123;public: // 正常的new和delete static void* operator new(std::size_t size) throw std::bad_alloc) &#123; return ::operator new(size); &#125; static void operator delete(void* memory) throw() &#123; ::operator delete(memory); &#125; // placement new 和 delete static void* operator new(std::size_t size, void* p) throw() &#123; ::operator new(size, p); &#125; static void operator delete(void* memory, void* p) throw() &#123; ::operator delete(memory, p); &#125; // nothrow new 和 delete static void* operator new(std::size_t size, const std::nothrow_t&amp; nt) throw() &#123; return ::operator new(size, nt); &#125; static void operator delete(void* memory, const std::nothrow_t&amp;) throw() &#123; ::operator delete(mempry); &#125;&#125;; 上面这个类中包含了C++标准中已经规定好的三种形式的new和delete。那么，凡是想以自定义方式扩充标准形式，可利用继承机制和using声明（见条款39），取得标准形式。12345678910class Widget: public StdNewDeleteForms &#123;public: // 使用标准new 和 delete using StdNewDeleteForms::operator new; using StdNetDeleteForms::operator delete; // 添加自定义的placement new 和 delete static void* operator new(std::size_t size, std::ostream&amp; logStream) throw(std::bad_alloc); static void operator delete(void* memory, std::ostream&amp; logStream) throw();&#125;;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法]]></title>
      <url>%2F2017%2F06%2F25%2Fhinton-nnml-06%2F</url>
      <content type="text"><![CDATA[第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的博客和对应的论文以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。 下图即来自上面的这篇博客。 Momentum我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。 具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\epsilon$为学习率，$\alpha$为momentum系数。 \Delta w_t = v_t = \alpha v_{t-1} - \epsilon g_t = \Delta w_t - \epsilon g_t为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）： v_t = \alpha(v_{t-1} + \frac{\epsilon g}{1-\alpha}) - \frac{\epsilon g}{1-\alpha}由于$\alpha &lt; 0$，所以当$t = \infty$时，只剩下了后面的常数项，即： v_\infty = -\frac{\epsilon}{1-\alpha}g也就是说，权重更新的幅度变成了原来的$\frac{1}{1-\alpha}$倍。若取$\alpha=0.99$，则加速$100$倍。 Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。 一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。 Adaptive Learning Rate这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。 例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。 下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。 RMSProprprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。 我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。 \text{MeanSquare}(w, t) = 0.9 \text{MeanSquare}(w, t-1) + 0.1g_t^2然后，将梯度除以上面的得到的Mean Square值。 RMSProp还有一些变种，列举如下： 课程总结 对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。 对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。 如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。 “Modern” SGD从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。 学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。 学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。 学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。 高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面） AdagradAdagrad对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即： g_{i} = \bigtriangledown_{\theta_i} J(\theta)Adagrad使用一个系数来为不同的参数修正学习率，如下： \hat{g_i} = \frac{1}{\sqrt{G_i+\epsilon}}g_i其中，$G_i$是截止到当前时间步$t$时，参数$\theta_i$对应梯度$g_i$的平方和。 我们可以把上面的式子写成矩阵形式。其中，$\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \odot g_t$。 \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t我们再来看PyTorch中的相关实现： 123456# for each gradient of parameters:# addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t# let epsilon = 1E-10state['sum'].addcmul_(1, grad, grad) # 计算 Gstd = state['sum'].sqrt().add_(1e-10) # 计算 \sqrt(G)p.data.addcdiv_(-clr, grad, std) # 更新 由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。 Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。 AdadeltaAdadelta给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。 E[g_t^2] = \gamma E[g_{t-1}^2] + (1-\gamma)g_t^2由于$\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。 \hat{g}_t = \frac{1}{\text{RMS}[g]}g_t作者还观察到，这样更新的话，其实$\theta$和$\Delta \theta$的单位是不一样的（此时$\Delta \theta$是无量纲数）。所以，作者提出再乘上一个$\text{RMS}[\Delta \theta]$来平衡（同时去掉了学习率$\eta$），所以，最终的参数更新如下： \theta_{t+1} = \theta_t - \frac{\text{RMS}[\Delta \theta]}{\text{RMS}[g]}g_t这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率lr这一参数设定，默认值为$1.0$。代码注释中，我使用MS来指代$E[x^2]$。即，$\text{RMS}[x] = \sqrt{\text{MS}[x]+\epsilon}$。12345678910# update: MS[g] = MS[g]*\rho + g*g*(1-\rho)square_avg.mul_(rho).addcmul_(1 - rho, grad, grad)# current RMS[g] = sqrt(MS[g] + \epsilon)std = square_avg.add(eps).sqrt_()# \Delta \theta = RMS[\Delta \theta] / RMS[g]) * gdelta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)# update parameter: \theta -= lr * \Delta \thetap.data.add_(-group['lr'], delta)# update MS[\Delta \theta] = MS[\Delta \theta] * \rho + \Delta \theta^2 * (1-\rho)acc_delta.mul_(rho).addcmul_(1 - rho, delta, delta) AdamAdaptive momen Estimation（Adam，自适应矩估计），是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。 \begin{aligned}m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t\\v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2\end{aligned}作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正： \begin{aligned}\hat{m} &= \frac{m}{1-\beta_1}\\ \hat{v}&=\frac{v}{1-\beta_2}\end{aligned}参数的更新如下： \theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t} + \epsilon}}\hat{m_t}作者给出$\beta_1 = 0.9$，$\beta_2=0.999$，$\epsilon=10^{-8}$。 为了更好地理解PyTorch中的实现方式，需要对上式进行变形： \Delta \theta = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta \frac{m_t}{\sqrt{v_t}}代码中令$\text{step_size} = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta$。同时，$\beta$也要以指数规律衰减，即：$\beta_t = \beta_0^t$。 12345678910111213141516# exp_avg is `m`: expected average of gexp_avg.mul_(beta1).add_(1 - beta1, grad)# exp_avg_sq is `v`: expected average of g's squareexp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)# \sqrt&#123;v_t + \epsilon&#125;denom = exp_avg_sq.sqrt().add_(group['eps'])# 1 - \beta_1^tbias_correction1 = 1 - beta1 ** state['step']# 1 - \beta_2^tbias_correction2 = 1 - beta2 ** state['step']# get step_sizestep_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1# delta = -step_size * m / sqrt(v)p.data.addcdiv_(-step_size, exp_avg, denom) AdaMax上面Adam中，实际上我们是用梯度$g$的$2$范数（$\sqrt{\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。 由于无穷范数就是求绝对值最大的分量，所以这种方法叫做AdaMax。其对应的$\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）： u_t = \beta_2^\infty u_{t-1} + (1-\beta_2^\infty) g_t^\infty我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。 相应的更新权重操作为： \theta_{t+1} = \theta_t -\frac{\eta}{u_t}\hat{m}_t在PyTorch中的实现如下：12345678910111213141516# Update biased first moment estimate, which is \hat&#123;m&#125;_texp_avg.mul_(beta1).add_(1 - beta1, grad)# 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个# Update the exponentially weighted infinity norm.norm_buf = torch.cat([ exp_inf.mul_(beta2).unsqueeze(0), grad.abs().add_(eps).unsqueeze_(0)], 0)## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）torch.max(norm_buf, 0, keepdim=False, out=(exp_inf, exp_inf.new().long()))## beta1 correctionbias_correction = 1 - beta1 ** state['step']clr = group['lr'] / bias_correctionp.data.addcdiv_(-clr, exp_avg, exp_inf)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 7 模板与泛型编程]]></title>
      <url>%2F2017%2F06%2F23%2Feffective-cpp-07%2F</url>
      <content type="text"><![CDATA[模板是C++联邦中的重要成员。要想用好STL，必须了解模板。同时，模板元编程也是C++中的黑科技。 41 了解隐式接口和编译器多态OOP总是以显式接口和运行期多态解决问题。通过虚函数，运行期将根据变量（指针或引用）的动态类型决定究竟调用哪一个函数。 而在模板与泛型世界中，例如下面的代码。没有明确指出，但是要求类型T支持操作符&lt;。隐式接口不基于函数的声明，而是由有效表达式组成。 同时，模板的具现化（instantiated）是在编译期发生的。通过模板具现化和函数重载，实现了多态。12template &lt;typename T&gt;bool fun(const T&amp; a, const T&amp; b) &#123;return a &lt; b;&#125; 42 了解typename的双重意义typename和class一样，都可以用来表明模板函数或者模板类。这时候两者完全相同，如下：12template &lt;typename/class T&gt;void fun(T&amp; a) &#123;...&#125; 但是有一个地方只能用typename，即标识嵌套从属类型名称。所谓从属类型名称，是指依赖于某个模板参数的名称。如果该类型名称呈嵌套状态，则称为嵌套从属名称。如：1234567// 打印容器内的第二个元素template &lt;typename C&gt;void print_2nd_element(const C&amp; container) &#123; // 嵌套类型 typename C::const_iterator it(container.begin()); cout &lt;&lt; *++it;&#125; 不过不要在基类列或者成员初始化列表中以其作为基类的修饰符。如： 12345678template &lt;typename T&gt;class Derived: public Base&lt;T&gt;::Nested &#123; //即使Nest是嵌套类型，这里也不用typenamepublic: explicit Derived(int x) :Base&lt;T&gt;::Nested(x) &#123; // 成员初始化列表也不用 typename Base&lt;T&gt;::Nested tmp; //这时候要使用 &#125;&#125;; 43 学习处理模板化基类内的名称这个问题的根源在于模板特化，造成特化版本与一般版本接口不同。因为编译器不能够在模板化的基类中寻找继承而来的名称。例如下面的离子： 123456789101112131415161718192021222324252627class TypeA &#123;public: void fun();&#125;;class TypeB &#123;public: void fun();&#125;;template &lt;typename Type&gt;class Base &#123;public: void do_something() &#123; Type x; x.fun(); &#125;&#125;;template &lt;typename Type&gt;class Derived: public Base&lt;Type&gt; &#123;public: void do_something_too() &#123; // ... do_something(); // 调用基类的函数，这里无法编译 &#125;&#125;; 这是因为存在如下可能，Base&lt;Tyep&gt;对某种Type进行了特化。12345template &lt;&gt;class Base&lt;TypeC&gt; &#123;public: // 这里没有实现 dom_somthing 函数&#125;; 所以存在这样的可能：class Derived&lt;TypeC&gt;: public Base&lt;TypeC&gt;，而这里面是没有do_something函数的。 为了解决这个问题，有三种办法： 在基类调用方法前面加上this-&gt;。 1234void do_something_too() &#123; // ... this-&gt;do_something(); // 调用基类的函数，这里无法编译&#125; 使用using声明式。虽然这里和条款33一样都是用了这一技术，但是目的是不一样的。条款33中是因为派生类名称掩盖了基类，而这里是因为编译器本身就不进入基类中进行查找。 12345using Base&lt;Type&gt;::do_something;void do_something_too() &#123; // ... this-&gt;do_something(); // 调用基类的函数，这里无法编译&#125; 明白指出被调用的函数位于基类中。这种方法最不推荐，因为如果被调用的是虚函数，上述的明确资格修饰符会关闭虚函数的运行时绑定行为。 1234void do_something_too() &#123; // ... Base&lt;Type&gt;::do_something(); // 调用基类的函数，这里无法编译&#125; 44 将与参数无关的代码抽离模板由于模板会具象化生成多个类或者多个函数，所以最好将与模板参数无关的代码抽离出去，防止代码膨胀造成程序体积变大和效率下降。 如下所示是一个$N$阶方阵，其中n是阶数。如果我们对每个不同阶数的矩阵都写一遍矩阵求逆操作，会造成代码膨胀。 12345template &lt;typename T, size_t n&gt;class Matrix &#123;public: void invert();&#125;; 一种可行的解决方案是提取出一个公共的基类用于实现矩阵转置。 1234567891011template &lt;typename T&gt;class MatrixBase &#123;protected: MatrixBase(size_t n, T* pMem) // 存储矩阵大小和指针 :size(n), pData(pMem) &#123;&#125; void setDataPtr(T* ptr) &#123;pData = ptr;&#125; // 设置指针 void invert(); // 实现求逆private: size_t size; T* pData;&#125;; 而矩阵类继承自刚才这个没有设定非类型参数的基类。我们这里使用private继承来显示新的矩阵派生类只是根据旧的基类实现，而不是想表示Is-a的关系。123456789template &lt;typename T, size_t n&gt;class Matrix: private MatrixBase&lt;T&gt; &#123;public: Matrix(): MatrixBase&lt;T&gt;(n, 0), pData(new T[n*n]) &#123; this-&gt;setDataPtr(pData.get()); // 将指针副本传给基类 &#125;private: boost::scoped_array&lt;T&gt; pData;&#125;; 然而这样改动并不一定比原来的效率更高。因为按原来的写法，常量n是个编译器常量，编译器可以通过常量的广传做优化。所以，实际使用时，还是要以profile为准。 上述的例子是由于非类型参数造成的代码膨胀，而类型参数有时也会出现这种问题。如有的平台上int和long有相同的二进制表述。那么vector&lt;int&gt;和vector&lt;long&gt;的成员函数可能完全相同，也会造成代码膨胀。 在很多平台上，不同类型的指针二进制表述是一样的，所以凡是模板中含有指针，如vector&lt;int*&gt;, list&lt;const int*&gt;等，往往应该对成员函数使用唯一的底层实现。例如，当你在操作某个成员函数而它操作的是一个强类型指针（即T*）时，你应该让它调用另一个无类型指针void*的函数，由后者完成实际工作。 45 运用成员函数模板接受所有兼容类型使用场景一，我们可以将某个类的拷贝构造函数写成模板函数，使其能够接受兼容类型。比如对于智能指针，我们希望能够实现原始指针那种向上转型的能力。如下所示，基类指针能够指向基类和派生类。12Base* p = new Base;Base* p = new Derived; 12345678910111213// 一个通用的智能指针模板template &lt;typename T&gt;class SmartPointer &#123;public: // 为了兼容类型，需要再引入一个模板参数 U template &lt;typename U&gt; SmartPointer(const SmartPointer&lt;U&gt;&amp; other) // 这里可能会发生指针之间的隐式类型转换 :ptr(other.get()) &#123;...&#125; T* get() const &#123; return ptr; &#125;private: T* ptr;&#125;; 成员函数模板还可以用来作为赋值操作。1234567891011template &lt;typename T&gt;class shared_ptr &#123;public: ... // 接受任意兼容的shared_ptr赋值 template &lt;typename Y&gt; shared_ptr&amp; operator = (shared_ptr&lt;Y&gt; const&amp; r); // 接受任意兼容的auto_ptr赋值 template &lt;typename Y&gt; shared_ptr&amp; operator = (auto_ptr&lt;Y&gt; const&amp; r);&#125;; 不过声明泛化版本的拷贝构造函数和赋值运算符，并不会阻止编译器为你生成默认的版本。所以如果你想控制拷贝或赋值的方方面面，必须同时声明泛化版本和普通版本。即：1shared_ptr&amp; operator = (shared_ptr const&amp; r); 46 需要类型转换时请为模板定义（friend）非成员函数回顾条款24，在其中指出，只有非成员函数才有能力在所有实参身上实施隐式类型转换。当这一规则延伸到模板世界中时，情况又有不同。如下所示，我们将实数类Rational声明为模板。 12345678910111213template &lt;typename T&gt;class Rational &#123;public: Rational(const T&amp; numerator=0, const T&amp; denominator=1);&#125;;template &lt;typename T&gt;const Rational&lt;T&gt; operator*(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs)&#123;...&#125;Rational&lt;int&gt; onehalf(1, 2);Rational&lt;int&gt; res = onehalf * 2; // 改成模板后便会编译错误！ 这是因为在进行模板类型推导时，并未将2进行隐式类型转换（否则，就是一个鸡生蛋蛋生鸡的问题了）。所以编译器没法找到这样的一个函数。 解决方法是将这个运算符重载函数声明为Rational&lt;T&gt;的友元函数。这样，在onehalf被声明时，Rational&lt;int&gt;类被具现化，则该友元函数也被声明出来了。 然而这时也只能通过编译而链接出错。因为无法找到函数的定义。解决方法是将函数体移动到类内部（即声明时即定义）。对于更复杂的函数，我们可以定义一个在模板类外部的辅助函数，而由这个友元函数去调用。123456789101112131415template &lt;typename T&gt; class Rational; // 前向声明template &lt;typename T&gt;const Rational&lt;T&gt; doMultiply(const Rational&lt;T&gt;&amp; lhs, const Rational&lt;T&gt;&amp; rhs) &#123;&#125;;template &lt;typename T&gt;class Rational &#123;public: Rational(const T&amp; numerator=0, const T&amp; denominator=1) &#123; &#125; // ... friend const Rational&lt;T&gt; operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123;return doMultiply(lhs, rhs); &#125;&#125;; 47 使用trait表现类型信息STL中的advance函数可以将某个迭代器移动给定的距离。但是对于不同的迭代器，我们需要采用不同的策略。 输入迭代器。输入迭代器只能前向移动，每次一步，而且是只读一次，模仿的是输入文件的指针。例如istream_iterator。 输出迭代器。输出迭代器只能向前移动，每次一步，而且是只写一次，模仿的是输出文件的指针。例如ostream_iterator。 前向迭代器。只能向前移动，每次一步，可以读或写所指物一次以上。例如单向链表。 双向迭代器。可以向前向后移动，每次一步，可以读或写所指物一次以上，例如双向链表。 随机迭代器。可以随意跳转任意距离，例如vector或原始指针。 为了对它们进行分类，C++有对应的tag标签。12345struct input_iterator_tag &#123;&#125;;struct output_iterator_tag &#123;&#125;;struct forward_iterator_tag: public input_iterator_tag &#123;&#125;;struct bidirectional_iterator_tag: public forward_iterator_tag &#123;&#125;;struct random_access_iterator_tag: public bidirectional_iterator_tag &#123;&#125;; 所以我们可以在advance的代码中，对迭代器的类型进行判断，从而采取不同的操作。trait就是能够让你在编译器获得类型信息。 我们希望trait也能够应用于内建类型，所以直接类型内的嵌套信息这种方案被排除了。因为我们无法对内建类型，如原始指针塞进去这个类型信息（对用户自定义的类型倒是很简单）。STL采用的方案是将其放入模板及其特化版本中。STL中有好几个这样的trait（而且C++11加入了更多），其中针对迭代器的是iterator_traits。 为了实现这一功能，我们要在定义相应迭代器的时候，指明其类型（通常通过typedef来实现）。如队列的迭代器支持随机访问，则：123456789template &lt;typename T&gt;class deque &#123;public: class iterator &#123; public: typedef random_access_iterator_tag iterator_category; // ... &#125;&#125;; 这样，我们就能在iterator_traits内部通过访问迭代器的iterator_category来获得其类型信息啦~如下所示，iterator_traits只是鹦鹉学舌般地表现IterT说自己是什么。 1234template &lt;typename IterT&gt;struct iterator_traits &#123; typedef typename IterT::iterator_category iterator_category;&#125;; 如何支持原始指针呢？用模板特化就好了~ 1234template &lt;typename T&gt;struct iterator_traits&lt;T*&gt; &#123; typedef random_access_iterator_tag iterator_category;&#125;; 总结起来，如何设计并实现一个traits呢？ 确认若干你想要获取到的类型相关信息，例如本例中我们想要获得迭代器的分类（category）。 为该信息取一个名称，如iterator_category 提供一个模板和相关的特化版本，内含你想要提供的类型相关信息。 好了，下面我们可以实现advance了。12345678template &lt;typename IterT, typename DistT&gt;void advance(IterT&amp; iter, DistT d) &#123; if(typeid(typename std::iterator_traits&lt;IterT&gt;::iterator_category == typeid(std::random_access_iterator_tag) &#123; // ... &#125; // ...&#125; 然而，为什么要将在编译期能确定的事情搞到运行时再确定呢？我们可以通过函数重载的方法实现编译期的if-else功能。 我们为不同类型的迭代器实现不同的移动方法。1234567891011121314template &lt;typename IterT, typename DistT&gt;void doAdvance(IterT&amp; iter, Dist d, std::random_access_iterator_tag) &#123; iter += d;&#125;// ...其他类型的迭代器对应的 doadvance// 用advance函数包装这些重载函数template &lt;typename Iter, typename DistT&gt;void advance(IterT&amp; iter, Dist d) &#123; doAdvance(iter, d, typename std::iterator_traits&lt;IterT&gt;::iterator_category()); // 注意 typename // 注意传入的是对象实例，所以要 iterator_category()&#125; 也就是说 首先建立一组重载函数或函数模板（真正干活的劳工），彼此之间的差异只在trait参数。 建立包装函数（包工头），调用上述劳工函数并传递trait信息。 48 认识模板元编程模板元编程（Template Metaprogram， TMP）能够实现将计算前移到编译器，能够实现早期错误侦测（如科学计算上的量度单位是否正确）和更高的执行效率（MXNet利用模板实现懒惰求值，消除中间临时量）。 条款47介绍了选择分支结构如何借由trait实现。这里介绍循环由递归模板具现化实现的方法。 为了生成斐波那契数列，我们首先定义一个模板参数为n的模板类。然后指出其值可以递归地由模板具现化实现。并通过模板特化给出递归基。 123456789template &lt;unsigned n&gt;struct F &#123; enum &#123;value = n * F&lt;n-1&gt;::value &#125;;&#125;;template &lt;&gt;struct F&lt;0&gt; &#123; enum &#123;value = 1 &#125;;&#125;; TMP博大精深，想要深入学习，还是要参考相关书籍。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 6 继承与面向对象设计]]></title>
      <url>%2F2017%2F06%2F17%2Feffective-cpp-06%2F</url>
      <content type="text"><![CDATA[C++中允许多重继承，并且可以指定继承是否是public or private等。成员函数也可以是虚函数或者非虚函数。如何在OOP这一C++联邦中的重要一员的规则下，写出易于拓展，易于维护且高效的代码？ 真•面向对象编程！ 32 确定public继承塑模出Is-a的关系请把这条规则记在心中：public继承意味着Is-a（XX是X的一种）的关系。适用于base class上的东西也一定能够用在derived class身上。因为每一个derived class对象也是一个base class的对象。 不过，在实际使用时，可能并不是那么简单。举个例子，在鸟类这个基类中定义了fly()这一虚函数，而企鹅很显然是一种鸟，但是却没有飞翔的能力。类似的情况需要在编程实践中灵活处理。 33 避免遮掩继承而来的名称这个题材实际和作用域有关。当C++遇到某个名称时，会首先在local域中寻找，如果找到，就不再继续寻找。这样，derived class中的名称可能会遮盖base class中的名称。 一种解决办法是使用using声明。如下所示：1234567891011121314151617181920class Base &#123;public: virtual void f1() = 0; void f3(); void f3(double);&#125;;class Derived: public Base &#123;public: using Base::f3; virtual void f1(); void f3();&#125;;Derived d;d.f1(); // 没问题，调用了Derived中的f1d.f3(); // 没问题，调用了Derived中的f3double x;d.f3(x); // 没问题，调用了Base中的f3。// 但是如果没有using声明的话，Base::f3会被冲掉。 34 区分接口继承和实现继承表面上直截了当的public继承，可以细分为函数接口继承和函数实现继承。以下面的这个例子来说明： 12345678class Shape &#123;public: virtual void draw() const = 0; virtual void error(const std::string&amp; msg); int getID() const;&#125;;class Rect: public Shape &#123;...&#125;;class Circle: public Shape &#123;...&#125;; 纯虚函数声明纯虚函数（如draw()函数）是为了让derived class只继承函数接口。乃是一种约定：“你一定要实现某某，但是我不管你如何实现”。不过，你仍然可以给纯虚函数提供函数定义。 虚函数非纯虚函数（如error()函数）的目的是，让derived class继承该函数的接口和缺省实现。乃是约定“你必须支持XX，但是如果你不想自己实现，可以用我提供的这个”。然而可能会出现这样一种局面：derived class的表现与base class不同，但是又忘记了重写这个虚函数。为了避免这种情况，可以使用下面的技术来达到“除非你明确要求，否则我才不给你提供那个缺省定义”的目的。 12345678910111213class Base &#123;public: virtual void fun() = 0; // 注意，我们改写成了纯虚函数protected: void default_fun() &#123;...&#125;; // 缺省实现&#125;;// 此时，若想使用缺省实现，就必须显式地调用class Derived: public Base &#123;public: virtual void fun() &#123; default_fun(); &#125;&#125;; 不过这样导致一个多余的default_fun()函数。如果不想添加额外的函数，我们可以使用上述提到的拥有定义的纯虚函数来实现。 123456789101112131415161718192021class Base &#123;public: virtual void fun() = 0;&#125;;// 为纯虚函数提供定义void Base::fun() &#123; // 缺省行为&#125;class Derived: public Base &#123;public: // 显式调用基类的纯虚函数，实现缺省行为 virtual void fun() &#123;Base::fun(); &#125;&#125;;class Derived2: public Base &#123;public: // 实现自定义的行为 virtual void fun() &#123; // ... &#125;&#125;; 非虚函数这意味着你不应该在derived class中定义不同的行为（老老实实用我给你的！），使得其继承了一份接口和强制实现。 35 考虑virtual函数之外的其他选择虚函数使得多态成为可能。不过在一些情况下，为了实现多态，不一定非要使用虚函数。本条款介绍了一些相关技术。 在某游戏中，需要设计一个计算角色剩余血量的函数。下面是一种惯常的设计。1234class GameCharacter &#123;public: virtual int healthValue() const;&#125;; 使用non-virtual interface实现template method模式这种流派主张virtual函数应该几乎总是私有的。较好的设计时将healthValue()函数设为非虚函数，并调用虚函数进行实现。这个调用函数中，可以做一些预先准备（互斥锁，日志等），后续可以做一些打扫工作。 1234567891011class GameCharacter &#123;public: int healthValue() const &#123; // ... 前期准备 int ret_val = doHealthValue(); // ... 后续清理 return ret_val &#125;private: virtual int doHealthValue() const &#123;...&#125;&#125;; 这样做的好处是基类明确定义了该如何实现求血量这个行为，同时又给了一定的自由，派生类可以重写doHealthValue()函数，针对自身的特点计算血量。 使用函数指针实现策略模式上述方案实际上是对虚函数的调用进行了一次包装。我们还可以借由函数指针实现策略模式，为不同的派生类甚至不同的对象实例做出不同的实现。 123456789101112131415class GameCharacter; // 前置声明// 计算血量的缺省方法int defaultHealthValue(const GameCharacter&amp;);class GameCharacter &#123;public: typedef int (*HealthCalcFun) (const GameCharacter&amp;); explicit GameCharacter(HealthCalcFun f=defaultHealthValue :healthFunc(f)&#123; // ... &#125;private: HealthCalcFun healthFunc;&#125;; 这样，我们通过在构造时候传入相应的函数指针，就可以实现计算血量的个性化设置。比如两个同样的boss，血量下降方式就可以不一样。或者我们可以在运行时候，通过设定healthFunc，来实现动态血量计算方法的变化。 借由std::function实现策略模式作为上面的改进，我们可以使用std::function（C++11），这样，不止函数指针可以使用，函数对象等也都可以了。（关于std::function的大致介绍，可以看这里）。 我们只需将上面的typedef改掉即可。不再使用函数指针，而是更加高级更加通用的std::function。 1typedef std::function&lt;int(const GameCharacter&amp;)&gt; HealthCalcFun; 使用古典的策略模式如下图所示。对于血量计算，我们单独抻出来一个基类，并有不同的实现。GameCharacter类中则含有一个指向HealthCalcFun类实例的指针。 12345678910111213141516171819//我们首先定义HealthCalcFunc基类class GameCharacter; // 前向声明class HealthCalcFunc &#123;public: virtual int calc(const GameCharacter&amp; gc) const &#123;...&#125;&#125;;HealthCalcFunc defaultCalcFunc;class GameCharacter &#123;private: HealthCalcFunc* pfun;public: explicit GameCharacter(HealthCalcFunc* p=&amp;defaultCalcFunc): pfun(p) &#123;&#125; int healthValue() const &#123; return pfun-&gt;calc(*this); &#125;&#125;; 该条款给出了虚函数的若干替代方案。 36 绝不重新定义继承而来的非虚函数在条款34中已经指出，非虚函数是一种实现继承的约定。派生类不应该重新定义非虚函数。这破坏了约定。 如下所示。123456789101112131415class B &#123;public: void mf() &#123;...&#125;&#125;;class D: public B &#123;public: void mf() &#123;...&#125;&#125;;D d;B* pb = &amp;d;D* pd = &amp;d;pb-&gt;mf(); // 调用的是B::mf()pd-&gt;mf(); // 调用的是D::mf() 这是因为非虚函数的绑定是编译期行为（和虚函数的动态绑定相对，其发生在运行时）。由于pb被声明为一个指向B的指针，所以其调用的是B的成员函数mf()。 为了不至于让自己陷入精神分裂与背信弃义的境地，请不要重新定义继承而来的非虚函数。 37 绝不重新定义继承而来的缺省参数值由于条款36的分析，所以我们只讨论继承而来的是带有缺省参数的虚函数。这样一来，本条款背后的逻辑就很清晰了：因为缺省参数同样是静态绑定的，而虚函数却是动态绑定。让我们再解释一下。 静态类型是指在程序中被声明时的类型（不论其真实指向是什么）。123// Circle是Shape的派生类Shape* ps;Shape* pc = new Circle; // 静态类型都是Shape 动态类型是指当前所指对象的类型。就上例来说，pc的动态类型是Circle*，而ps没有动态类型，因为它并没有指向任何对象实例。动态类型常常可以通过赋值改变。1ps = new Circle; // 现在ps的动态类型是Circle* 虚函数是运行时决定的，取决于发出调用的那个对象的动态类型。 不过遵守此项条款，有时又会造成不便。看下例： 12345678910class Shape &#123;public: enum ShapeColor &#123;RED, GREEN&#125;; virtual void draw(ShapeColor c=RED) const=0;&#125;;class Circle: public Shape &#123;public: virtual void draw(ShapeColor c=RED) const;&#125;; 第一个问题，代码重复，我写了两遍缺省参数。第二造成了代码依存。比如我想换成GREEN为默认参数，需要在基类和派生类中同时修改。 一种解决方法是采用条款35中的替代设计，如NVI方法。令基类中的一个public的非虚函数调用私有的虚函数，而后者可以被派生类重新定义。我们只需要在public的非虚函数中定义缺省参数即可。 1234567891011121314class Shape &#123;public: void draw(ShapeColor c=RED) const &#123; doDraw(c); // 调用私有的虚函数 &#125;private: //真正的工作在此完成 virtual void doDraw(ShapeColor c) const = 0; &#125;;class Circle: public Shape &#123;private: virtual void doDraw(ShapeColor c) const; // 派生类重写这个真正的实现&#125;; 38 通过复合塑模has-a或“根据某物实现出”复合是指某种对象内含其他对象。复合实际有两层意义，一种较好理解，即has-a，如人有名字、性别等他类，一种是指根据某物实现（is-implemented-in-terms-of）。例如实现消息管理的某个类中含有队列作为实现。 39 明智而审慎地使用private继承私有继承意味着条款38中的“根据某物实现出”。例如D私有继承自B，不是说D是某种B，私有继承完全是一种技术上的实现（和对现实的抽象没有半毛钱关系）。B的每样东西在D中都是不可见的，也就是成了黑箱，因为它们本身就是实现细节，你只是考虑用B来实现D的功能而已。 但是复合也能达到相同的效果啊~我在D中加入一个B的对象实例不就好了？很多情况下的确是这样，如果没有必要，不建议使用私有继承。 40 明智而审慎地使用多重继承使用多重继承有可能造成歧义。例如，C继承自A和B，而两个基类中都含有成员函数mf()。那么当d.mf()的时候，究竟是在调用哪个呢？你必须明确地指出,d.A::mf()。 使用多重继承还可能会造成“钻石型”继承。任何时候继承体系中某个基类和派生类之间有一条以上的相通路线，就面临一个问题，是否要让基类中的每个成员变量经由每一条路线被复制？如果只想保留一份，那么需要将File定为虚基类，所有直接继承自它的类采用虚继承。 1234class File &#123;...&#125;;class InputFile: virtual public File &#123;...&#125;;class OutputFile: virtual public File &#123;...&#125;;class IOFile: public InputFile, public OutputFile &#123;...&#125;; 从正确的角度看，public的继承总应该是virtual的。不过这样会造成代码体积的膨胀和执行效率的下降。 所以，如无必要，不要使用虚继承。即使使用，尽可能避免在其中放置数据（类似Java或C#中的接口Interface） 附注 std::function的基本使用std::function的作用类似于函数指针，但是能力更加强大。我们可以将函数指针，函数对象，lambda表达式或者类中的成员函数作为std::function。如下所示： 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162#include &lt;functional&gt;#include &lt;iostream&gt;struct Foo &#123; Foo(int num) : num_(num) &#123;&#125; void print_add(int i) const &#123; std::cout &lt;&lt; num_+i &lt;&lt; '\n'; &#125; int num_;&#125;;void print_num(int i)&#123; std::cout &lt;&lt; i &lt;&lt; '\n';&#125;struct PrintNum &#123; void operator()(int i) const &#123; std::cout &lt;&lt; i &lt;&lt; '\n'; &#125;&#125;;int main()&#123; // store a free function // 函数指针 std::function&lt;void(int)&gt; f_display = print_num; f_display(-9); // lambda表达式 // store a lambda std::function&lt;void()&gt; f_display_42 = []() &#123; print_num(42); &#125;; f_display_42(); // store the result of a call to std::bind // 绑定之后的函数对象 std::function&lt;void()&gt; f_display_31337 = std::bind(print_num, 31337); f_display_31337(); // store a call to a member function // 类中的成员函数，第一个参数为类实例的const reference std::function&lt;void(const Foo&amp;, int)&gt; f_add_display = &amp;Foo::print_add; const Foo foo(314159); f_add_display(foo, 1); f_add_display(314159, 1); // store a call to a data member accessor std::function&lt;int(Foo const&amp;)&gt; f_num = &amp;Foo::num_; std::cout &lt;&lt; "num_: " &lt;&lt; f_num(foo) &lt;&lt; '\n'; // store a call to a member function and object using std::placeholders::_1; std::function&lt;void(int)&gt; f_add_display2 = std::bind( &amp;Foo::print_add, foo, _1 ); f_add_display2(2); // store a call to a member function and object ptr std::function&lt;void(int)&gt; f_add_display3 = std::bind( &amp;Foo::print_add, &amp;foo, _1 ); f_add_display3(3); // store a call to a function object std::function&lt;void(int)&gt; f_display_obj = PrintNum(); f_display_obj(18);&#125;]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Silver RL课程 - DP Planning]]></title>
      <url>%2F2017%2F06%2F06%2Fsilver-rl-dp%2F</url>
      <content type="text"><![CDATA[上讲中介绍了MDP这一基本概念。之后的lecture以此出发，介绍不同情况下的最优策略求解方法。本节假设我们对MDP过程的所有参数都是已知的，这时候问题较为简单，可以直接得到确定的解。这种问题叫做planning问题，求解方法是动态规划。 动态规划动态规划是计算机科学中常用的思想方法。对于一个复杂的问题，我们可以将它划分成若干的子问题，然后再将子问题的解答合并为原问题的解。要想用动态规划解决问题，该问题必须满足以下两个条件： 最优子结构。能够分解为若干子问题。 子问题重叠。分解后的子问题存在重叠，我们可以通过记忆化的方法进行缓存和重用。 MDP问题的求解符合上述要求。贝尔曼方程给出了原问题递归的分解（考虑状态$S$时，我们可以考虑从状态$s$出发的下一个状态$s^\prime$，而在考虑状态$s^\prime$的时候，问题和原问题是一样的，只不过问题规模变小了）；而使用值函数我们相当于记录了中间结果。值函数充当了缓存与记事簿的作用。 迭代策略估计给定一个策略，我们想要知道该策略的期望回报是多少，也就是其对应的值函数$v_\pi(s)$。首先回顾一下上讲中得到的值函数的贝尔曼方程如下（全概率公式）： v_{k+1}(s) = \sum_{a\in \mathcal{A}}\pi(a|s)(R_s^a+\gamma\sum_{s^\prime \in \mathcal{S}} P_{ss^\prime}^av_k(s^\prime))我们有如下的迭代估计方法：在每一轮迭代中，对于所有状态$s\in \mathcal{S}$，使用上式利用上轮中的$v(s^\prime)$更新$v_{k+1}(s)$，直到收敛。 给出下面的算例。$4\times 4$的格子中，$0$和$15$是出口。在状态$0$和$15$向自身转移时，奖赏为$0$。其他状态来回转换时，奖赏均为$-1$。如果当前移动使得更新后的位置超过格子的边界，则状态仍然保持原状。求采取随机策略$\pi$，即每个状态下，上下左右四个方向移动的概率均为$0.25$时候各个状态的值函数$v_\pi(s)$。 这里直接将Python实现的计算过程贴在下面，注意在每一轮迭代开始前，暂存当前值函数的副本。 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061import numpy as npv = [0 for _ in xrange(16)]line1 = range(1, 4)line4 = range(12, 15)col1 = [4, 8, 12]col4 = [3, 7, 11]# the environment simulatordef get_new_loc(idx, action): if idx == 0 or idx == 15: ret = idx reward = 0 return ret, reward if action == 0: # up if idx in line1: ret = idx else: ret = idx-4 elif action == 1: # down if idx in line4: ret = idx else: ret = idx+4 elif action == 2: # left if idx in col1: ret = idx else: ret = idx-1 elif action == 3: # right if idx in col4: ret = idx else: ret = idx+1 reward = -1 return ret, rewardgamma = 1.K = [1, 2, 3, 10, 100]for k in xrange(1, 101): # in each iteration, update v(s) via: # v(s) = \sum_a \pi(a|s) + \gamma \sum_s^\prime P_&#123;ss^\prime&#125;^a v(s^\prime) v_aux = v[:] for i in xrange(16): v_aux[i] = 0. for action in range(4): j, r = get_new_loc(i, action) v_aux[i] += 0.25*(r+gamma*v[j]) v = v_aux if k in K: print 'k = &#123;&#125; '.format(k), print ', '.join(map(lambda x: '&#123;:.1f&#125;'.format(x), v)) 策略的改进评估过某个策略的值函数后，我们可以改进该策略，使用的方法为贪心法。具体来说，在某个状态$s$时，我们更新此时的动作为能够使得$Q(s,a)$取得最大，接下来继续执行原策略的那个动作（也就是我们只看一步）。如下所示： \pi^\prime = arg\max_{a\in \mathcal{A}}q_\pi(s,a)以上小节中给出的算例为例，最终值函数结果为： 那么对于位置$1$，由于其左方的状态值函数最大，为$0$。所以，我们认为从位置$1$出发的最优策略应该是向左移动。其他同理。这样，对于任何一个状态，它都可以通过选取$q(s,a)$最大的那个动作达到下一个状态，再递推地走下去（如右侧图中的箭头所示）。 为什么这种贪心方法有效呢？这里直接把证明过程粘贴如下。 当上述单步提升不再满足时，上图中的不等号就变成了等号，算法收敛到了最优解。 值迭代首先介绍最优化定理（也可以解释上述贪心方法为什么work，类比图中最短路径的分析）。这条定理是说某个策略对于状态$s$是最优的，当且仅当，对于每个由$s$出发可达的状态$s^\prime$，都有，该策略对$s^\prime$也是最优的。这提示我们，可以通过下面的式子更新$s$处的最优值函数的值。 v^\ast(s) = \max_{a\in \mathcal{A}}R_s^a+\gamma\sum_{s^\prime\in \mathcal{S}}P_{ss^\prime}^av^\ast(s^\prime)通过迭代地进行这个步骤，就能够收敛到最优值函数。每次迭代中，都首先计算最后一个状态的值函数，然后逐渐回滚，更新前面的。如下图所示（求取最短路径）： 每轮迭代，都从$1$号开始。考虑$1$号，第一轮时候，大家都是$0$。当选取动作为向左移动时候，上式取得最大值。所以$s^\prime=0$。更新之后，其值变为了$-1$（因为把reward加上去了），接下来更新其他。并开始新的迭代轮次，最终收敛。 注意到，这里和上面策略迭代-改进来求取最优策略不同，这里并不存在一个显式的策略。或者说，在策略迭代的时候，我们是要选取某个动作$a$，使得值-动作函数$q(s,a)$取值最大。而在值迭代的过程中，我们只关心下个状态的值函数和在这个转换过程中得到的奖励。 异步DP上面我们讨论的是同步迭代更新。也就是说，在更新前，我们要先备份各个状态的值函数，更新时是使用状态$s^\prime$的旧值来计算$s$的新值。如下图所示：上面讨论了同步迭代的三个主要问题： 我们也可以使用异步方法。主要包括以下三种： 就地（in-place） DP就地DP只存储一份值函数，在更新时，有可能在使用新的状态值函数$v_{\text{new}}(s^\orime)$来更新$v(s)$。 带有优先级的状态扫描（Prioritized Sweeping）根据贝尔曼方程的误差来指示更新先更新哪个状态的值函数，即 |\max\_{a\in A}(R\_s^a+\gamma\sum\_{s^\prime\in S}P\_{ss^\prime}^a v(s^\prime)-v(s)|实现的具体细节如下： 实时DP使用智能体与环境交互的经验（experience）来挑选状态。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Silver RL课程 - MDP]]></title>
      <url>%2F2017%2F05%2F31%2Fsilver-rl-mdp%2F</url>
      <content type="text"><![CDATA[Silver在英国UCL讲授强化学习的slide总结。背景介绍部分略去不表，第一篇首先介绍强化学习中重要的数学基础-马尔科夫决策过程（MDP）。 马尔科夫性质不严谨地来说，马尔科夫性质是指未来与过去无关，只与当前的状态有关。我们说某个State是Markov的，等价于下面的等式成立： P[S_{t+1}|S_t] = P[S_{t+1}|S_1, \dots, S_t]定义状态转移概率（State Transition Probability）如下： P_{ss^\prime} = P[S_{t+1}=s^\prime|S_t=s]前后两个时刻的状态不同取值的状态转移概率可以写成一个矩阵的形式。矩阵中的任意元素$P_{i,j}$表示$t$时刻状态$i$在$t+1$时刻转移到状态$j$的概率。矩阵满足行和为$1$的约束。 下面，我们从马尔科夫性质展开，逐步地加入一些额外的参量，一步步引出强化学习中的马尔科夫决策过程。 马尔科夫过程马尔科夫过程（或者叫做马尔科夫链）是指随机过程中的状态满足马尔科夫性质。我们可以使用二元组$(S, P)$来描述马氏过程。其中， $S$是一个有限状态集合。 $P$是状态转移矩阵，定义如上。 马尔科夫奖赏过程马尔科夫奖赏过程（不知道如何翻译，Markov reward process）在马氏过程基础上加上了状态转移过程中的奖赏reward。可以使用四元组$(S, P, R, \gamma)$来表示。其中， $R$代表奖励函数，$R_s = E[R_{t+1}|S_t=s]$，是指当前状态为$s$时，下一步状态转移过程中的期望奖励。 $\gamma$是折旧率（discount），$\gamma \in [0,1]$ 定义回报（Return）为当前时刻往后得到的折旧总奖励，即： G_t = R_{t+1}+\gamma R_{t+2}+... = \sum_{k=0}^{\infty}\gamma^kR_{t+k+1}折旧率的引入，有以下几点考虑： 在有环存在的马氏过程中，避免了无穷大回报的出现。 未来的不确定性对当前的影响较小。 事实上的考虑，例如投资市场上，即时的奖励比迟滞的奖励能够有更多的利息。 人类行为倾向于即时奖励。 如果马氏过程是存在终止的，有的时候也可以使用$\gamma=1$，也就是不打折。 值函数值函数（Value function）的意义是以期望的形式（条件期望）给出了状态$s$的长期回报，如下： v(s) = E[G_t|S_t=s]值函数可以分为两个部分，即时奖励$R_{t+1}$和后续状态的折旧值函数。如下所示： 最后一步推导时，第二项的变形从直觉上推断还是比较容易的，但是还是把比较严格的推导过程写在下面： \begin{aligned} E[G_{t+1}|S_t = s] &= \sum_{s^\prime\in S}E[G_{t+1}|S_{t+1}=s^\prime]P(S_{t+1}=s^\prime|S_t=s)\\ &=\sum_{s^\prime}v(S_{t+1}=s^\prime)P(S_{t+1}=s^\prime|S_t=s)\\ &=E[v(S_{t+1})|S_t=s] \end{aligned}上面的结论就是贝尔曼方程，它给出了计算值函数的递归公式。如下图所示，状态节点$s$处的值函数可以分为两个部分，分别是转换到状态$s^\prime$过程中收到的奖励$r$，和从新的状态$s^\prime$出发，得到的值函数。我们想要知道$t$时刻某个状态$s$的值函数，只需要从后向前遍历，递归地去计算。 把上面形式求期望的过程展开，可以得到下面的等价形式（更像上面补充的证明过程的思路）。其中，后面一项就是状态转移构成的树结构中以当前状态节点$s$为父节点的所有子节点的值函数，用转移概率进行加权。这个比上式更为直观。 v(s) = R_s + \gamma\sum_{s^\prime \in S}P_{ss^\prime}v(s^\prime)或者写成下面的矩阵形式，更加紧凑： v = R+\gamma Pv 当我们对系统模型（包括奖励函数和概率转换矩阵）全部知道时，可以直接求解贝尔曼方程如下： 对于含有$n$个状态的系统，求解复杂度是$\mathcal{O}(n^3)$。当$n$较大时，常用的替代的迭代求解方法有： 动态规划 DP 蒙特卡洛仿真（Monte-Carlo evaluation） 时间差分学习（Temporal Difference Learning） 马尔科夫决策过程马尔科夫决策过程（MDP）是带有决策的马尔科夫奖励过程。其中有一个env（环境），其状态量满足马尔科夫性质。MDP可以用五元组$(S,A,P,R,\gamma)$描述。其中， $A$是一个有限决策集合。 $P_{ss^\prime}^a = P(S_{t+1}=s^\prime|S_t=s, A_t=a)$是状态转移概率矩阵。 $R_{s}^a = E[R_{t+1}|S_t=s, A_t=a]$是奖励函数（与动作也挂钩） 策略策略（Policy）$\pi$是指在给定状态情况下，采取动作的概率分布，如下： \pi(a|s)=P(A_t=a|S_t=s)对于一个智能体，如果策略确定了，那么它对环境的表现也就决定了。MDP的策略与历史无关，只与当前的状态有关。同时，策略是平稳过程，与时间无关。例如，无论在开局，还是终局，只要棋盘上的落子一样（也就是状态一样），那么围棋程序应该给出相同的落子动作决策。 当我们给定一个MDP和相应的策略$\pi$时，状态转移过程$S_1,S_2,\dots$是一个马氏过程$(S, P^\pi)$（上标$\pi$表示$P$由$\pi$决定）。而状态和奖励构成的过程$S_1,R_1,\dots$是一个马氏奖赏过程$(S, P^\pi, R^\pi, \gamma)$。具体来说，如下（就是全概率公式）： \begin{aligned} P_{ss^\prime}^\pi &= \sum_{a\in A}\pi(a|s)P_{ss^\prime}^a\\ R_s^\pi &=\sum_{a\in A}\pi(a|s)R_s^a \end{aligned}值函数MDP的值函数$v_\pi(s)$是指在当前状态$s$出发，使用策略$\pi$得到的回报期望，即， v_\pi(s) = E_\pi[G_t|S_t=s]引入“动作-值”函数（action-value function）$q_\pi(s,a)$，意义是从当前状态$s$出发，执行动作$a$，再使用策略$\pi$得到的回报期望，即， q_\pi(s,a) = E_\pi[G_t|S_t=s,A_t=a]贝尔曼方程两者的关系如下如所示（通过全概率公式联系）： 注意上图描述的是$t$时刻的状态$s$下，$v(s)$和$q(s,a)$的关系。我们继续顺着状态链往前，可以得到下图所示$q(s,a)$和$t+1$时刻的状态$s^\prime$的值函数$v(s^\prime)$之间的关系。同样是一个全概率公式： 综合上面两幅图中给出的关系，我们有相邻时刻值函数$v(s)$和$v(s^\prime)$的关系： 同样，相邻时刻Q函数的关系： 写成紧凑的矩阵形式： v_\pi = R^\pi + \gamma P^\pi v_\pi这个方程的解是： v_\pi = (1-\gamma P^\pi)^{-1}R^\pi和上面对策略函数的分解类似，我们有下面两式成立： v_\pi(s) = E_\pi[R_{t+1} + \gamma v_\pi(sS_{t+1})]最优值函数最优的值函数是指在所有的策略中，使得$v_\pi(s)$取得最大值的那个，即： v_\ast(s) = \max_\pi v_\pi(s)最优Q函数的定义同理： q_\ast(s,a) = \max_\pi q_\pi(s,a)定义策略$\pi$集合上的一个偏序为 \pi > \pi^\prime \quad \text{if} \quad v_\pi(s) > v_{\pi^\prime}(s), \forall s如果我们已经知道了最优的值函数，那么我们可以在每一步选取动作的时候，选取那个使得当前Q函数取得最大值的动作即可。这很straight forward，用数学语言表达就是： 同样地，对于最优值函数，也有贝尔曼递归方程成立。下面是一个形象化的推导，和上面导出贝尔曼方程的思路是一样的。 常用的求解方法包括： 值迭代（Value Iteration） 策略迭代（Policy Iteration） Q Learning Sarsa]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 02]]></title>
      <url>%2F2017%2F05%2F25%2Fhinton-nnml-02%2F</url>
      <content type="text"><![CDATA[这是第二周的课程内容，主要介绍了几种神经网络的分类，详细地介绍了感知机这一最简单的模型。 Different neural network archsFeed-forward neural network 前馈神经网络前馈神经网络可能是最常见的网络，主要由输入层，若干隐含层和输出层组成。一般，当隐含层数目超过$1$时，我们可以说网络是deep的。 Recurrent networkRNN内部的节点之间存在有向的环，这使得它能够使用内部状态来对动态过程建模。RNN能力强大，但是不易训练。 RNN常用来对序列进行建模（modeling squence）。这里有一篇不错的介绍。 Symmetrically connected network这种网络结构上很像RNN，但是它的节点之间的连接是对称的，意思是说由此到彼和由彼到此的权重相同。 Percetron训练最早的神经网络应用是感知机。使用感知机等统计学习方法进行模式分类的一般思路是： 将原始输入向量转化为feature activation。 寻找合适的权重对feature进行加权，得到某个标量。 如果这个标量大于某一给定的阈值，则分类为正；否则分类为负。 对于决策节点，常常使用Binary threshold neuron，将输入映射为${0,1}$。而这相当于给输入加上一个偏置项，然后和$0$比较。 所以，感知机的数学模型可以描述如下： y=\begin{cases}1, \quad \text{if} \quad \sum w_ix_i+b>0 \\ 0 \quad \text{otherwise}\end{cases}训练时，遍历样本集中的样本点，根据真实值与预测值是否相同，有如下的更新方法： 若预测值与真实值相同，则不作调整； 否则，若错输出为$0$，则将输入的$x$加到权重$w$上去； 若错输出为1，则从权重$w$上将输入的$x$减去。 当训练集确实是线性可分的时候，这种方法能够保证找到那样的一组参数，使得样本集完全正确分类。 原理下面从几何角度分析一下感知机。 权重空间（Weight Space的概念），对于权重向量的每个分量，都有一个维度对应。空间中的某个点就代表一个权重的实例。 让我们忽略偏置项，那么每个训练样本都可以看作是权重空间的分类超平面。这个超平面的方程可以写作$x^\dagger w = 0$，平面的法向量就是输入样本$x$。下图中假设输入样本为正样本，黑线即为超平面。平面上方的权重都是正确的（例如绿色的那个），下方的则都会使得该样本分类错误（红色的那个）。因为黑线上方的那些权重和输入$x$的夹角小于直角，也就是说内积是大于$0$的，自然就会给出正样本的预测。反之同理。 当输入样本为负样本时，分析同理。只不过正确的权重此时应该位于平面下方，与输入向量夹角大于直角。 所以，感知机的参数调整就是要在权重空间内找到某个权重点，使其在所有的训练样本构成的这么多超平面都位于正确的位置。 下面用刚才的这种思考方式证明上述训练方法的正确性。 首先考虑当前的权重和最终要找的那个权重之间的距离平方为$d_a^2+d_b^2$，如图所示。 我们希望，对于每个错分的样例，学习算法能够将当前的参数向正确的参数推进。对照上图，似乎我们只要加上蓝色的那个输入向量就可以了。然而如果输入向量长度较长，而我们离正确的权值又比较近了，有可能出现更加远离的情况。 我们取一个margin，认为我们要找的那个权重可行域不仅要满足分类正确，还要保证分类面和可行域的距离大于margin。（这里不是很懂，这样来看可行域的条件更加苛刻了，如果有正确分类面却没有这样的可行域呢？有可能出现吗？感知机这里还是看李航的统计学习更清楚点。。。我还是喜欢解析而不是几何。。。） 每次做出一次错误分类，权重根据输入向量做更新，向可行域前进至少input vector的长度这么多。这样不停迭代，就能收敛。（前面还提到了这是一个凸优化，也是一脸懵逼。。。） 感知机的局限感知机不能解决线性不可问题，如异或运算。通过做特征变换，选取不同的特征，可能可以解决。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Ubuntu Cannot Mount exfat格式硬盘的解决办法]]></title>
      <url>%2F2017%2F05%2F04%2Fubuntu-cannot-mount-exfat-disk%2F</url>
      <content type="text"><![CDATA[我的移动硬盘为东芝1TB容量，为了能够在Windows和MacOS下使用，我将其格式化为exfat格式。然而我发现这样一来，在Ubuntu14.04下不能挂载。虽然可见盘符，但是却提示unable to mount。这篇文章是对解决办法的记录。 解决方法参见页面，运行以下命令： 123456$ sudo -i # 获取root权限# apt-get update# apt-get install --reinstall exfat-fuse exfat-utils# mkdir -p /media/user/exfat# chmod -Rf 777 /media/user/exfat# fdisk -l 之后我发现直接点击盘符的挂载即可，而无需使用他的后续命令。 在弹出驱动器的时候，会出现虽然顺利弹出，但是马上（大概3s），移动硬盘又被读取的情况。所以只能利用间隙，很快地将硬盘取下。不知道会不会有什么损害。所以如果方便的话，还是格式为NTFS格式，再花一些大洋去买Mac上读写NTFS格式硬盘的软件工具吧。。。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Neural Network for Machine Learning - Lecture 01]]></title>
      <url>%2F2017%2F05%2F03%2Fhinton-nnml-01%2F</url>
      <content type="text"><![CDATA[Hinton 在 Coursera 课程“Neural Network for Machine Learning”新开了一个班次。对课程内容做一总结。课程内容也许已经跟不上最近DL的发展，不过还是有很多的好东西。 Why do we need ML?从数据中学习，无需手写大量的逻辑代码。ML算法从数据中学习，给出完成任务（如人脸识别）的程序。这里的程序可能有大量的参数组成。得益于硬件的发展和大数据时代的来临，机器学习的应用越来越广泛。如下图所示，MNIST数据集中的手写数字2变化多端，很难人工设计代码逻辑找到判别一个手写数字是不是2的方法。 What are neural network?为什么要研究神经网络？ 理解人脑机理的一个途径； 受到神经系统启发的并行计算 新的学习算法来解决现实问题（本课所关心的） （这里总结的不是很科学，勉强概括了讲义的内容）神经元结构如下所示。树突（dendritic tree）和其他神经元相连作为输入，轴突（axon）发散出很多分支和其他神经元相连。轴突和树突之间通过突触（synapse）连接。轴突有足够的电荷产生兴奋。这样完成神经元到神经元的communication。 神经元之间互相连接。对不同的神经元输入，有不同的权重。这些权重可以变化，使得神经元之间的连接或变得更加紧密或疏离。人类大脑的神经元多达$10^{11}$个，每一个都有多达$10^4$个连接权重。不同神经元分布式计算，带宽很大。 大脑中不同的神经元分工不同（局部损坏造成相应的身体功能受损），但是这些神经元长得都差不多，它们也可以在一定的环境下发育成特定功能的神经元。 而人工神经网络就是根据神经元的兴奋传导机理，人工模拟的神经网络。 Simple models of different neurons我们简化神经元模型，用数学函数去近似描述它们的功能。这也是科学研究的通用思路，忽略次要矛盾，抓住主要矛盾。之后逐步向上加复杂度，更好地描述实验现象。下面介绍几种神经元的简化模型。 Linear neuron顾名思义，这种神经元用来进行线性组合的变换，不过要注意加上偏置项。如下所示： y = b+\sum_{i=1}^{n}w_ix_iBinary threshold neuron这种神经元用来将输入信号加权后做二元阈值化，我们可以通过两种方法来描述： y = \begin{cases}1 \quad \text{if} \quad z\ge \theta\\ 0\quad \text{otherwise}\end{cases}其中，$z$是输入信号的线性组合，$z=\sum_{i}w_ix_i$ 或者， y = \begin{cases}1 \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}其中，$z$是输入信号的线性组合并加上偏置项，$z = b+\sum_{i}w_ix_i$ Rectified linear neuron和上面的二值化神经元对比，有： y = \begin{cases}z \quad \text{if} \quad z\ge 0\\ 0\quad \text{otherwise}\end{cases}Sigmoid neuron这种神经元通过logistic函数将输入shrink到区间$(0, 1)$，如下所示： y = \frac{1}{1+\exp(-z)} 由于Logistic函数将$(-\infty, +\infty)$的值压缩为S型，所以得名Sigmoid。 Stochastic binary neuron这种函数的输出仍是二值化的，而且是将Logistic函数的输入作为输出$1$的概率。也就是： P(y=1) = \frac{1}{1+\exp(-z)}对于上面的Rectified linear neuron，也可以做类似的变形，将输出看作是泊松分布的系数。 Three types of learning即有监督学习，无监督学习和强化学习。 有监督学习：给定输入向量，预测输出。 无监督学习：学习一个对于输出来说的好的表示（good internal representation of input）。 强化学习：学习如何决策达到最大期望奖赏。 有监督学习有监督学习可以细分为分类和回归问题。有监督学习中，我们需要寻找一个model(由一个函数$f$和决定这个函数的参数$W$决定)，将输入$x$应映射为实数（回归问题）或者离散值（分类问题）。 所谓的训练，就是指不断调整参数$W$，使得训练集合中的$x$在当前映射下得到的预测值与真实值之间的差异尽可能小。 在回归问题中，常常使用欧氏距离的平方作为差异的衡量。 强化学习在强化学习中，算法要给出动作或者动作序列。与有监督学习不同，强化学习中没有真实值，只有不定时（occasional）出现的奖赏。 强化学习的难点如下： 奖赏通常是delayed的。以AlphaGo来说，你很难追究中间某一步棋的决策对最后输赢的影响。 奖赏通常只是一个标量，提供不了太多的信息。我只能知道这局最后的输赢，但是对于其他信息基本都不知道。 无监督学习无监督学习以前受到的关注不多，这可能和它的目的不明确有关系。其中一个目的是能够提供输入的更好的表示，以用于强化学习和有监督学习。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-光流估计]]></title>
      <url>%2F2017%2F05%2F03%2Fcs131-opticalflow%2F</url>
      <content type="text"><![CDATA[光流法是通过检测图像像素点的强度随时间的变化进而推断出物体移动速度及方向的方法。由于成像物体与相机之间存在相对运动，导致其成像后的像素强度值不同。通过连续观测运动物体图像序列帧间的像素强度变化，就可以估计物体的运动信息。 是你让我的世界从那刻变成粉红色 划掉。。。 光流的计算光流（Optical Flow），是指图像中像素强度的“表象”运动。这里的表象运动，是指图像中的像素变化并不一定是由于运动造成的，还有可能是由于外界光照的变化引起的。 光流估计就是指利用时间上相邻的两帧图像，得到点的运动。满足以下几点假设： 前后两帧点的位移不大（泰勒展开） 外界光强保持恒定。 空间相关性，每个点的运动和他们的邻居相似（连续函数，泰勒展开） 其中第二条外界光强保持恒定，可以从下面的等式来理解。 在相邻的两帧图像中，点$(x,y)$发生了位移$(u,v)$，那么移动前后两点的亮度应该是相等的。如下： I(x,y,t-1) = I(x+u, y+v, t)从这个式子出发，我们将其利用Taylor展开做一阶线性近似。其中$I_x$, $I_y$, $I_t$分别是Image对这几个变量的偏导数。 I(x+u,y+v,t) = I(x,y,t-1)+I_xu+I_yv+I_t上面两式联立，可以得到， I_xu+I_yv+I_t=0上式中，$I_x$, $I_y$可以通过图像沿$x$方向和$y$方向的导数计算，$I_t$可以通过$I(x,y,t)-I(x,y,t-1)$计算。未知数是$(u,v)$， 正是我们想要求解的每个像素在前后相邻两帧的位移。 这里只有一个方程，却有两个未知数（实际是$N$个方程，$2N$个未知数，$N$是图像中待估计的像素点的个数，但是我们通过矩阵表示，将它们写成了如上式所述的紧凑形式），所以是一个不定方程。我们需要找出其它的约束求解方程。 上面就是光流估计的基本思想。下面一节介绍估计光流的一种具体方法：Lucas-Kanade方法 L-K方法上述式子虽然给出了光流估计的思路，但是还是没有办法解出位移量。L-K方法依据相邻像素之间的位移相似的假设，通过一个观察窗口，将窗口内的像素点的位移看做是相同的，建立了一个超定方程，使用最小二乘法进行求解。下面是观察窗口为$5\times 5$的时候，建立的方程。 使用最小二乘法求解，可以得到如下的式子，求和号代表是对窗口内的每一个像素点求和。 上式即是L-K方法求解光流估计问题的方程。通过求解这个方程，就可以得到光流的估计$(u,v)$。但是上式什么时候有解呢？ $\mathbf{A}^\dagger \mathbf{A}$是可逆的。 $\mathbf{A}^\dagger \mathbf{A}$不应该太小（噪声）。这意味着它的特征值$\lambda_1$, $\lambda_2$不应该太小。 $\mathbf{A}^\dagger \mathbf{A}$不应该是病态的（稳定性）。这意味着它的特征值$\lambda_1/\lambda_2$不应该太大。 而我们在Harris角点检测的时候已经讨论过$\mathbf{A}^\dagger \mathbf{A}$这个矩阵的特征值情况了！也许，写成下面的形式更好看出来。 下面这张图就是当时的讨论结果。 上面就是使用L-K方法估计光流的一般思路。 金字塔方法在最开始的假设中，第一条指出点的位移应该是较小的。从上面的分析可以看出，当位移较大时，Taylor展开式一阶近似误差较大。其修正方法就是这里要介绍的金字塔方法。我们通过将图像降采样，就能够使得较大的位移在高层金字塔图像中变小，满足假设条件1.如下所示。 作业：基于光流法的帧间插值问题描述假设视频流中的相邻两帧$I_0$和$I_1$，分别标记其时刻为$t=0$和$t=1$。我们希望能够在这两帧之间生成新的插值帧$I_t, 0&lt;t&lt;1$。比如说你手头的视频是24帧的帧率，想在一台刷新频率为60Hz的显示器上播放，那么这项技术可以带来更流畅的观看体验。 简单粗暴法我们可以简单粗暴地使用线性插值方法，简单的认为插值帧是第一帧和最后一帧的线性组合，也就是说： I_t = (1-t)I_0+tI_1这种方法称为”cross-fading”。效果如下。可以看到有较多的模糊抖动。 基于光流法使用光流可以知道像素点在图像平面的运动信息，从而在帧间建立点的对应关系。我们记像素点在水平方向和竖直方向的速度分别为$u_t(x,y)$和$v_t(x,y)$。我们可以根据$t=0$和$t=1$的两帧图像解出光流信息，即$u_0(x,y)$和$v_0(x,y)$。那么我们认为光流保持不变，就可以计算插值帧的某一点在$t=0$时候的对应点坐标。接下来，赋值就可以了。如下式所示： I_t(x+tu_0(x,y), y+tv_0(x,y)) = I_0(x,y)用MATLAB实现如下：1234567for y =1:height for x = 1:width dy = min(max(round(y+v0(y,x)*t), 1), height); dx = min(max(round(x+u0(y,x)*t), 1), width); img(dy,dx,:) = img0(y,x,:); endend 这种方法叫做”ForwardWarpping”。效果如下。可以看到，与上一种方法对比，画面有了明显的提升。 改进上面的方法我们假设光流一直保持不变，用$t=1$时刻的光流去代替之间所有时刻的光流。但实际上光流一定是在实时变化的。使用backward warpping改进，使用$t$时刻的光流反推。 I_t(x,y) = I_0(x-tu_t(x,y), y-tv_t(x,y))然而，我们并不能得到$t$时刻光流$u_t$和$v_t$的准确值，只能近似计算。方法如下： u_t(\hat{x},\hat{y}) = u_0(x,y)v_t(\hat{x},\hat{y}) = v_0(x,y)其中，$x^\prime = x+u_0t$，$y^\prime = y+v_0t$，$\hat{x}\in\lbrace\text{floor}(x^\prime), \text{ceil}(x^\prime)\rbrace$，$\hat{y}\in\lbrace\text{floor}(y^\prime), \text{ceil}(y^\prime)\rbrace$。 这在一定程度上补偿了光流计算的误差。 如果某个点$(\hat{x}, \hat{y})$被多个初始点$(x, y)$对应，那么我们选取使得下面的式子取得最小值的那个点$(x, y)$，也就是选取那个亮度变化最小的点。 \vert I_0(x, y) − I_1(x + u_0(x, y), y + v_0(x, y))\vert如果某个点没有找到相对应的初始点，那么我们使用线性插值方法为其填充光流。 下面是这种方法的效果示意。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 5 实现]]></title>
      <url>%2F2017%2F05%2F01%2Feffective-cpp-05%2F</url>
      <content type="text"><![CDATA[第四章中探讨了如何更好地提出类的定义和函数声明，精心设计的接口能让后续工作轻松不少。然而如何能够正确高效地实现，也是一件重要的事情。行百里者半九十。 26 尽可能延后变量定义式的出现时间变量，尤其是自定义对象，一旦被定义，就会调用构造函数；一旦生命周期结束，又要调用析构函数。所以，延后变量定义的时间，并且最好能够给定恰当的初始值，这样能够提高代码执行效率。 27 尽量少做转型动作第一，安全考虑。C++的类型系统保证类型错误不会发生。理论上如果你的代码“很干净”地通过编译，就表明它不意图在任何对象上执行不安全和无意义的操作，这是一个很有价值的保险。不要轻易打破。 第二，效率问题。这里展开说。 C++中的转型动作有以下三种： (T)expression，C时代的风格 T(expression)，函数风格 新式风格，包括static_cast,const_cast,dynamic_cast和reinterpret_cast四种。 作者不提倡使用两种旧风格，而使用下面的四种。一种理由是它们容易被自动化代码检查工具匹配检查。 对于 const_cast，用于脱掉某对象的const属性。 对于static_cast，用于进行强制类型转换。例如将non-const类型转换为const类型，或者将double类型转换为int等。 对于dynamic_cast，用于执行安全向下转换，也就是用于决定某对象是否归属继承体系中的某个类型。注意，dynamic_cast的很多实现都很慢，尤其是继承深度较深时，也是这几个里面唯一可能造成重大（注意，并非其他三者不会带来执行时间开销）运行成本的动作。 很多时候，之所以使用dynamic_cast是因为你想在一个（你认为是）某个派生类对象中执行派生类中（并非从基类继承）的成员函数，但你的手上只有指向这个派生类对象的base指针或引用。这种情况下，也许将派生类的这个函数在基类中也定义一个空函数体的函数，再由派生类重写可能更好。 对于reinterpret_cast，它用来实现低级转型，实际动作和结果依赖于编译器，表示它不可移植。例如将指针转换为int，此转换是在bit层面的转换，更详细的信息可以参见cpp reference的介绍。 在它们之中，reinterpret_cast和const_cast完全是编译器层面的东西。static_cast会导致编译器生成对应CPU指令，但是是在编译器就能决定的。dynamic_cast是在运行时多态的一种手段。 28 避免返回指向对象内部成分的句柄（handle）当成员函数返回类内部私有成员变量（或者私有成员函数，较少见）的句柄（如指针，引用或迭代器），而且成员变量的资源又存储于对象之外，这时候，虽然可以将此成员函数声明为const，但是实际上并不能避免通过此句柄修改资源的情况发生。 例如在自定义的string类中，使用堆上的数组储存字符串。如果某个成员函数能够返回字符串数组，那么可以使用这个指针修改数组内的值，而这也是符合const约定的。 所以，若无十分必要，不要返回对象内部的句柄。有此需要时，首先考虑是否应返回const handle&amp;。 即使这样，还有可能造成返回的句柄比变量本身生命周期更长，也就是句柄所指之物已经被析构，句柄此时成为空悬状态，造成问题。 29 为“异常安全”努力是值得的异常安全函数是指即使发生异常，也不会泄露资源或者允许任何数据结构被破坏。由于在代码执行过程中，可能发生内存申请失败等等异常，导致我们逻辑上已经设想好的程序控制流被中断，造成内存泄漏（后续的delete操作没有执行）。此外，我们希望如果异常发生，变量的值（程序的状态）能够恢复到异常发生之前。 让我们先看一个不满足异常安全的函数例子：12345678// 自定义`Menu`类中修改背景图片的成员函数void Menu::changeBg(std::istream&amp; imgSrc) &#123; lock(&amp;this-&gt;mutex); // 互斥锁 1 delete this-&gt;bg; // 释放本已有的bg2 ++this-&gt;imgChangeCnt; // 计数器 3 bg = new Image(imgSrc); // 新图片 4 unlock(&amp;this-&gt;mutex); // 释放锁 5&#125; 上面的代码存在以下问题，使得它不满足异常安全性。 资源泄漏。一旦第4行内存申请失败，那么第五行无法执行，互斥锁永远把持住了。 数据被破坏。还是上面的情况，则bg此时的资源已经被析构，而且计数器的值也增加了。· 解决第一个问题，可以考虑使用智能指针，即条款13中的使用对象管理资源。 我们将异常安全分为以下三类： 基本型。异常被抛出后，程序内的数据不会被破坏。但是并不保证程序的现实状态（究竟bg是何值） 强烈保证。异常抛出后，程序恢复到该函数调用前的状态。copy-and-swap策略是达成这一目标的常见方法。首先为待修改的对象原件做出一份副本，然后在副本上做一切修改。若有任何修改抛出异常，则原件不受影响。待所有修改完成后，再将修改过后的副本和原件在一个不抛出异常的swap操作中交换。 在这里，常常采用pImpl技术，也就是在对象中仅存储资源的（智能）指针，在swap中只操作该指针即可。 绝不抛出异常。作用于内置类型（int或指针等）身上的所有操作都提供了nothrow保证。 30 透彻了解内联的方方面面inline函数在编译期实现函数本体的替换，避免了函数调用的开销，还可能使得编译器基于上下文进行优化，鼓励使用inline替换函数宏定义。 然而，inline不要乱用。首先，inline会使得目标码体积变大。可能造成额外的换页行为，降低高速缓存的命中概率，反而造成性能的损失。 另一方面，inline只是对编译器的申请，不是真的一定内联。 inline函数通常定义在头文件中（或者直接定义在类的内部，这样无需加入inline关键字），这是因为在编译中编译器需要知道这个函数具体长什么样子，才能够实现内联。 有时候，虽然编译器有意愿内敛某函数，但是还是会为它产生一个函数本体。这常常发生在取某个内联函数地址时。与此并提，编译器通常不对通过函数指针调用的内联函数进行内联。也就是说，是否真的内联，还与函数的调用方式有关。 作者给出的建议是，一开始不要将任何函数内联，之后使用profile工具进行优化。不要忘记28法则，80%的程序执行时间花在了20%的代码上。除非找对了目标，否则优化都是无用功。将内联函数应用于调用频繁且小型的函数身上。 31 将文件间的编译依存关系降至最低C++的头文件包含机制饱受批评。连串的编译依存关系常常使得项目的编译时间大大加长。 首先，程序库头文件应该“完全且仅有声明式”的存在，将实现代码放入cpp文件中。 另外，之所以C++编译时容易出现“牵一发而动全身”的情况，是因为C++与Java等语言不同。在Java中编译器只分配一个指针指向实际对象，也就不需要知道对象的实际大小。而C++编译器却需要知道对象中每个成员变量的明确定义，才能知道对象的实际大小，从而在内存中分配空间。 从这里出发，我们可以参考Java等语言中的思路，建立一个handle类，在其中包含原来那个类的完全数据，而在新的类中定义一个指向该handle类的指针，这也就是前面所提到的pImpl方法。 使用这种思虑，定义的包含有Date类型对象（指明这个人的生日）的Person类如下： 1234567891011121314151617181920212223#include &lt;string&gt; // for string#include &lt;memory&gt; // for shared_ptrclass PersonImpl; // Person实现类的前置声明class Date; // Person接口用到的类的前置声明class Person &#123;public: Person(const std::string&amp; name, const Date&amp; birthday); std::string name() const;private: std::shared_ptr&lt;PersonImpl&gt; pImpl; // 指向实现类&#125;;/*****************实现文件****************/#include "Person.h"#include "PersonImpl.h"Person::Person(const std::string&amp; name, const Date&amp; birthday): pImpl(new PersonImpl(name, birthday)) &#123;&#125;std::string Person::name() const &#123; return pImpl-&gt;name();&#125; 在上面的代码中，通过构造handle类PersonImpl，在Person中我们只需要前置声明Date，而无需包含头文件date.hpp。这样，即使Date或者Person有修改，影响也仅限于Date的实现文件和PersonImpl而已，不会传导到Person和使用了Person的其他代码文件。通过这种做法，实际上Person成为了一个单纯的接口，具体的实现在PersonImpl中完成，实现了“接口与实现的分离”。 综上： 如果使用object pointer或者object reference可以完成任务，就不要使用object。只要前置声明就可以定义出指向该类型的pointer或者reference，但是需要完整地定义式才能定义object。 如果能够，尽量用类的声明式替换定义式 。注意，当声明某个函数而它用到某个类时，你并不需要这个类的定义式。即使函数以pass-by-value方式传递参数（通常情况下这也不是一个好主意）或返回值。 为声明式和定义式提供不同的头文件（Person本身和PersonImpl）。这两个文件应该保持一致。声明式改变了，需要修改定义式头文件。程序库客户应该包含声明文件。 除了上面的方法，也可以将Person定义为抽象基类（Caffe中的Layer就是类似的模式）。为了达成这一目标，Person需要一个虚构造函数（见条款7）和一系列的纯虚函数（作为接口，等待派生类重写实现）。如下所示： 12345class Person &#123;public: virtual ~Person(); virtual string name() const = 0;&#125;; 客户必须能够为这种类创建对象。通常的做法是调用一个工厂函数，返回派生类的（智能）指针。这样的函数常常在抽象基类中声明为static。 12345class Person &#123;public: static shared_ptr&lt;Person&gt; create(const string&amp; name, const Date&amp; birthday);// ... 刚才的其他代码&#125;; 当然，要想使用，我们还必须定义派生类实现相应的接口。 123456789class RealPerson: public Person &#123;public: RealPerson(const string&amp; name, const Date&amp; birthday): name(name), birthDate(birthday) &#123;&#125; virtual ~RealPerson() &#123;&#125; string name() const &#123; return this-&gt;name; &#125;private: string name; Date birthDate;&#125;; 上面的工厂函数create()的实现： 123shared_ptr&lt;Person&gt; Person::create(const string&amp; name, const Date&amp; date) &#123; return shared_ptr&lt;Person&gt;(new RealPerson(name, date));&#125; 实际应用中的工厂函数会像工厂一样，根据客户需要，产出不同的派生类对象。 当然，使用上述技术增大了程序运行时间开销和内存空间。这需要在工程中分情况讨论。是否这部分的开销大到了需要无视接口实现分离原则的地步？如果是的，那就用具象的类代替他们。但是，不要因噎废食。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 4 设计与声明]]></title>
      <url>%2F2017%2F04%2F29%2Feffective-cpp-04%2F</url>
      <content type="text"><![CDATA[良好的代码架构能够使得后续编码工作变的简单。尤其在OOP的世界中，如何能够设计良好的C++接口？我们的目标是高效，易用，易拓展。 18 让接口容易被使用，不易被误用首先，考虑客户可能犯什么错误。书中提到可以构建类型系统防范客户输入不合理的数据。同时限制什么可以做，什么不可以做。例如加入const限定修饰符。 其次，尽量使接口与内建类型等保持一致。例如STL中统一使用size()方法获取容器的大小。 任何接口如果强制客户记得某件事情，那么就会有犯错的危险。较佳的方法是先发制人，例如预定函数的返回值为智能指针，防止客户接触裸指针。 19 设计class犹如设计type设计自定义的class要慎重，就好比语言设计者小心翼翼地设计语言的内置类型。一般有如下考虑： 新的对象如何创建和销毁？这关系到构造和析构函数。 对象初始化和赋值有何区别？这关系到构造函数和赋值运算符。 新的对象如何以pass-by-value方式传递，意味着什么？这关系到copying函数的实现。 什么是新类型的合法值？可能需要对setter()函数进行参数检查。 新类型在继承图中的位置？这关系到虚函数，以及析构函数是否为虚函数。 新类型需要什么样的转换？只能显式构造还是允许隐式转换（意味着你需要自己实现隐式转换函数）。 什么样的操作符和函数对此类型是合理的？这涉及到访问权限，以及新类型与外界的交互。 什么样的标准函数应该驳回？是否要禁止编译器生成默认函数。 谁该取用成员？这决定了成员的访问权限，以及某些类或函数是否为friend。 什么事新类型的未声明接口？ 新类型有多一般化？是否建立template class更好？ 新的类型真的必要吗？如果是要定义新的派生类来为已经存在的类添加功能，也许使用non-member函数或者模板技术是更好的选择。 20 宁以pass-by-reference-to-const替换pass-by-value对于较大对象，pass-by-value有可能成为费时的操作。而且，如果以派生类对象实参传入一个以基类为形参的函数，会导致切片发生，也就是函数内部可见的仍然是基类对象，无法实现多态。 总而言之，当按照值传递方法传入参数时，请再三考虑是否传入常值引用是更好的选择。但该条款不适用于内建类型和STL中的迭代器和函数对象。对它们而言，值传递通常更为恰当。 21 必须返回对象时，不要妄想返回其reference函数返回时，存在局部对象析构和返回值的构造，不要妄图对此优化，返回局部non-static对象的引用几乎必然导致失败！ C++11中引入的移动构造也许是解决这个问题的可行之道，以后总结。 22 将成员变量声明为private封装，封装，还是封装！ 而且，请记住，其实只有两种访问权限：private（提供了封装）和其他（包括protected，不提供封装）。 23 宁以non-member和non-friend函数替换member函数对于类中的数据进行操作时，常常可以使用成员函数的方法，也可以编写一个non-member函数，通过调用类的公开方法实现目的。作者认为应偏向后者。原因有三： 封装性。我们以能够获取类私有成员变量的代码多少进行封装性的量度。如果引入类的成员函数，这个函数可以肆无忌惮地访问类内的所有成员，这使得封装被破坏。 代码设计的弹性。使用成员函数需要对类进行修改，而使用后者，我们可以借助C++中的名字空间，将相似功能的函数组织在不同的hpp和cpp文件中。需要的时候可以随时添加（因为C++的名字空间支持跨文件，而类声明并不是）。 编译开销。每次都要修改类的话，还要重新编译。而使用non-member函数，可以不断做加法，编译时完全可以只处理新文件。 24 若所有参数均需要类型转换，请为此采用non-member函数作者举出自定义的有理数类与整型数做乘法的例子。首先，我们不将构造函数声明为explicit，可以完成整形到有理数类的隐式类型转换。 重载乘法的运算符可以被声明为有理数类的成员函数，如下所示：123456class Rational &#123;// ...public: Rational(int numerator=0, int denominator=1); const Rational operator*(const Rational&amp; rhs) const;&#125;; 然而，这样做的话，auto res = 2*Rational(4,5)就无法通过编译，因为int并没有实现operator*(const Rational&amp;)操作。 更好的方法是将其作为non-member函数，123const Rational operator*(const Rational&amp; lhs, const Rational&amp; rhs) &#123; //...&#125; 25 考虑写出一个不抛出异常的swap()函数这一条款更像是模板特化规则的大杂烩。 STL中的swap()函数是交换两个对象内容的不错选择。它的实现大致如下（平淡无奇）：12345678namespace std &#123;template &lt;typename T&gt;void swap(T&amp; a, T&amp;b) &#123; T tmp(a); a = b; b = tmp;&#125;&#125; 但是对于某些pImpl（pointer to implementation）手法的类（指类的数据成员实际死一个指针，而不是数据成员的实在值），标准库的这一实现未免效率较低，因为我们实际上一般只需要交换两个对象的指针即可。 如何对我们的对象Widget实现特化？ 如果Widget不是模板类，那么我们需要进行全特化。加入以下：123456namespace std &#123;template &lt;&gt;void swap&lt;Widget&gt;(Widget&amp; a, Widget&amp; b) &#123; swap(pImpl, b.pImpl);&#125;&#125; 更好的解决方法是先将swap()定义为Widget类的公共成员函数，然后再全特化标准库的swap()方法时调用。这样与STL的约定保持一致。STL中vector等容器即是这样的。一方面提供了公开方法进行交换，另一方面特化了std名字空间的swap()方法。 当Widget是模板类时，需要进行偏特化。也许看上去是这样：123456namespace std &#123;template &lt;typename T&gt;void swap(Widget&lt;T&gt;&amp; a, Widget&lt;T&gt;&amp; b) &#123; a.swap(b);&#125;&#125; 但是程序员可以全特化std中的模板，却不能加入新的类或函数进入std中。在实际中，这样写出的程序一般仍然能够编译运行，但是这种行为确实是未定义的。所以最好不要这样做。 所以，可以在Widget存在的名字空间内定义swap()（而不是加入std），这里涉及到C++中的模板实例化查找规则，不再多说了。作者在条款末尾总结了一般规则： 一般使用标准库中的swap()即可。 如果自己实现，首先提供一个public的swap()成员函数，注意这儿函数决不能抛出异常。 在类或者模板在的名字空间中提供一个non-member的swap()函数，并令它调用上述的swap()成员函数。 如果是类，而不是模板，那么特化std::swap()，并令它调用上述swap()成员函数。 在客户端代码调用swap()时，确定包含一个using声明式，以便让std::swap()在你的函数内可见，然后不加任何名字空间修饰符，赤裸裸调用swap()。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP阅读 - Chapter 3 资源管理]]></title>
      <url>%2F2017%2F04%2F25%2Feffective-cpp-03%2F</url>
      <content type="text"><![CDATA[C++相信程序员，将内存等底层资源毫无保留地献给程序员使用。然而，做到正确处理资源，写出健壮的代码并不容易，内存泄漏的幽灵始终徘徊在C++程序员身边。遵守本章给出的建议能够使你尽可能地陷入资源泄漏的泥沼，避免奇怪而又毫无头绪的调试。 13 以对象管理资源书中对“资源”的解释为：一旦使用，将来必须还给系统。最常见的资源是动态分配的内存，此外还有文件描述器，互斥锁，图像界面的笔刷和字型，数据库连接和网络套接字等。 本条款可以（较浅显地）归纳为： 不要使用裸指针！要使用智能指针！ 我们不应该指望程序员有多么富有责任心。当获得资源时，不能寄希望于程序员会“良心发现”，在使用完后将其释放。解决这一问题的方法是使用对象管理资源。这样，当离开对象的作用域之后，对象自动析构，资源就会被返回给系统。 许多资源动态分配在堆中。这种情况下，智能指针是一个很好的选择（在C++11中引入了weak_ptr和shared_ptr，请使用它们。如果没有C++11，请使用boost）。 以对象管理资源的两个关键想法： 获得资源后立即放进管理对象内。也就是所谓的RAII(Resource Acquisition Is Initialization)。暴露裸指针是危险的！ 管理对象利用析构机制确保资源被释放。不论控制流如何离开区块，一旦对象被销毁，其析构函数自然调用，资源被释放。 14 在资源管理类中小心coping行为有时候，资源并非位于堆中（书中所给例子为互斥锁），这时可能需要我们自己建立资源管理类。 如下面的例子，我们将对不同的底层资源使用情景给出不同的解决方案。 12345678910// Lock是互斥锁的资源管理类class Lock &#123;public: explicit Lock(Mutex* pm):mutexPtr(pm) &#123; // 获得资源 lock(mutexPtr); &#125; ~Lock() &#123;unlock(mutexPtr); &#125; //释放资源private: Mutex* mutexPtr;&#125;; 这样，我们期望能够使用Lock对象实现对互斥锁的自动管理。 123456Mutex m; // 互斥锁// ...&#123;Lock ml(&amp;m);// ...&#125; // 在区块外，ml自动析构，实现解锁 然而，如何处理Lock对象的拷贝？ 情景一，禁止复制。就像上例，很多时候对互斥锁的复制毫无道理。我们可以使用条款6中的trick禁止类的copying行为发生。 情景二，对底层资源进行引用计数。也许我们可以利用shared_ptr，但是需要为其传入参数，指定其析构时并不是要返还资源，而是要解锁。具体请参看shared_ptr部分文档。 情景三，复制底层资源。这时候要注意深度拷贝，例如字符串数组。 情景四，移除底层资源所有权。将所有权移至新的对象。 15 在资源管理类中提供对原始资源的访问许多API（尤其是和遗留下来的C代码API交互）时，需要获取底层资源的指涉。 对于这种情况，智能指针提供了get()函数用来获取其原始指针的拷贝。同时，它们也重载了-&gt;和*操作符，允许隐式转换为原始指针。 我们的自定义资源管理类也可以参考它们的实现。其中，隐式转换到类型T可以通过定义operator T()实现。隐式类型转换可能使得代码量更少，客户更方便。但是！请慎用隐式类型转换。 16 成对使用new和delete时采取相同的形式这项条款是说如果动态分配内存时候使用了new T()得到了单个对象的内存空间，那么销毁时应该使用delete销毁；如果当初使用了new T[]得到了对象数组空间，那么销毁时应该使用delete []。两者不能混用，否则会导致未定义行为。 另外，除非必要，不要使用原始数组。STL中的vector和string是替代数组的不错选择。 17 以独立语句将newed对象置于智能指针以独立语句将newed对象存储于智能指针，否则一旦发生异常，有可能导致难以察觉的内存泄露。 书中给出了一个例子，是由于逗号表达式的执行顺序不定造成的。 如下面的函数声明： 12int priority() &#123; /*some code*/&#125;void process(shared_ptr&lt;Widget&gt; pw, int priority) &#123; /*some code*/&#125; 在使用时，也许你会这样调用process函数。 1process(new Widget(), priority()); 首先，这样是不能通过编译器的。因为shared_ptr的构造函数是explicit的，不能够隐式将原始指针转换为shared_ptr对象。但是改为下面的代码就没问题了吗？ 1process(shared_ptr&lt;Widget&gt;(new Widget()), priority()); 由于C++中函数参数的核算顺序是不确定的，所以可能发生： new出来一个Widget资源 调用priority()函数，注意此时可能引发异常，使得Widget资源无法回收 构造shared_ptr对象 问题已经很明确了。所以我们应该首先确保资源确实被智能指针获取到了，使用下面的独立语句更好。 12auto pw = shared_ptr&lt;Widget&gt;(new Widget());process(pw, priority());]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 2 构造拷贝和赋值函数]]></title>
      <url>%2F2017%2F04%2F24%2Feffective-cpp-02%2F</url>
      <content type="text"><![CDATA[在第二章中，作者主要关注了在C++的OOP“联邦”中行事的注意事项。主要包括有虚函数的情况下的继承以及copying function（拷贝构造函数和拷贝赋值函数）的处理。 05 了解C++默默编写并调用了哪些函数C++编译器会自动为类添加默认构造函数和拷贝构造函数和析构函数，以及拷贝赋值函数。 在默认构造函数中，会调用基类的构造函数以及各个成员变量的构造函数。 在拷贝构造函数和拷贝赋值函数中，将单纯地将源对象中的non-static的成员变量拷贝到目标对象（浅拷贝）。 06 若不想使用编译器自动生成的函数，那就明确拒绝有的时候，我们的类故意设计为不能拷贝构造或赋值。这时候，可以将拷贝构造函数和拷贝赋值函数声明为private，并且不提供函数的实现。 07 为多态基类声明虚析构函数多态是OOP的基本概念之一。在C++中，可能遇到这样的情景：使用base class的指针或引用指向derived派生类对象，以此实现运行时的不同逻辑。这种情况下，要为基类声明虚析构函数。这是因为，当派生类对象经由一个base class指针被删除时，如果该base class带有非虚的析构函数，其结果是未定义的。常常会造成派生类自己的成员变量不能被销毁，造成内存泄露。 而那些意图并非是用来当做base class的类来说，随意将其虚构函数声明为virtual也是不恰当的。因为这会额外引入虚函数表，造成对象体积的无谓增大，给性能造成影响，而且丧失了对C语言的移植性。 由于那些被用来作为base class等待派生类继承的类通常情况下都有虚函数存在（派生类正是对虚函数重写，实现了多态），所以这一条款可以归纳如下： 那些有虚函数的类，几乎确定都应该有一个虚析构函数。 对于STL而言，记住其中的容器都不是为了继承而设计的，不要继承STL中的容器，包括vector，string等。 有的时候，我们可能会想声明一个抽象基类作为接口。当手上没有纯虚函数时候，可以将析构函数声明为纯虚的。然而这时候问题来了，我们需要为这个纯虚析构函数提供了一个空定义。 12345class ABC &#123;public: virtual ~ABC() = 0; &#125;;ABC::~ABC() &#123;&#125; 这看上去很违背常理，因为一般情况下纯虚函数不需要实现。这里是因为当派生类被销毁时，其析构函数中会调用~ABC()，所以必须为这个函数提供一份定义。 08 别让异常逃离析构函数如果在析构函数中抛出异常，会导致未定义的行为。 析构函数不应该抛出异常。如果析构函数调用的函数可能会抛出异常，要在析构函数中捕获异常，然后结束程序（这不是未定义行为）或者吞掉它们。 09 绝不在构造函数和析构函数中调用虚函数你不应该在构造函数和析构函数中调用虚函数，这样的调用通常不会导致你想要的结果。 为什么呢？ 如果我们在派生类的构造函数中使用了从基类中继承而来的虚函数。在派生类的构造函数之前，基类的构造函数被调用，这时候，所调用的虚函数实际是基类中的那个版本！析构函数同理。 直接在构造函数中调用虚函数看上去很容易避免。然而在构造函数中你可能会调用其他的初始化函数，你应该确保这些初始化函数中没有调用虚函数。否则，你可能会陷入到苦涩头疼的调试中去。编译器通常不会发现此类问题，但是在程序运行中，如果基类的虚函数是纯虚函数，程序很可能中止（和后面的相比，也许这还算好的）。如果基类中的虚函数有自己的实现，那么你可能就会头疼于程序的表现为何出乎意料（期望调用派生类的重写版本，实际仍是基类的原始版本）。 10 令operator=返回一个*this的引用为了实现连锁赋值，赋值操作符必须返回一个引用，指向操作符左侧的赋值实参。这条规范被大多数人遵守。除非有确实好的理由，否则最好按规范办事。 所谓连锁赋值，是指下面这样的情况： 1a = b = c; 这一条款不仅适用于赋值运算符，也适用于+=等。 1234567class A &#123;public: A&amp; operator=(const A&amp; rhs) &#123; // ... return *this; &#125;&#125;; 11 在operator=中处理自我赋值所谓自我赋值，是指： 123Widget w;// ...w = w; 自我赋值常常发生在同一个对象的不同别名之间。在实现赋值运算符时，应注意处理这一现象。 一种方法是进行“证同测试”，如下： 1234A&amp; operator=(const A&amp; rhs) &#123; if(&amp;rhs == this) return *this; //证同测试 // ...&#125; 不过作者指出，这种方法不具备异常安全性。另一种方法是在函数内部，合理安排语句顺序，防止提前释放该对象本身的资源。 还有一种方法，使用copy-swap方法，首先拷贝构造一个rhs的拷贝，然后交换该拷贝和*this， 毫无疑问，使用证同测试方法和作者后文的方法都会造成性能的些许下降，这需要根据具体情况具体分析，合理采用。 12 复制对象时勿忘每一个成分这一条款是指存在继承时，实现copying函数（指拷贝构造函数和拷贝赋值函数）不要忘记base class部分成员。看下面的例子： 12345678910class Derived: public Base&#123;private: int a;public: Derived(const Derived&amp; rhs):a(rhs.a) &#123;&#125; Derived&amp; operator=(const Derived&amp; rhs) &#123; a = rhs.a; return *this; &#125;&#125;; 上面的代码只是拷贝了派生类新加入的成员a，而对基类中已有的成员未作处理。要记住， 任何时候自己实现copying函数时，要担起重责大任，小心地复制其基类的成员。由于基类成员往往声明为private，所以，一般调用基类的成员函数进行拷贝。将上面的代码修改为： 123456Derived(const Derived&amp; rhs):Base(rhs), a(rhs.a) &#123;&#125;Derived&amp; operator=(const Derived&amp; rhs) &#123; Base::operator=(rhs); a = rhs.a; return *this;&#125; 此外，两种copying函数的实现往往是相似的。然而，不要试图在一个函数中调用另一个函数。把相似代码提取出来，写成一个独立的init()函数是一个更好的选择。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python中的迭代器和生成器]]></title>
      <url>%2F2017%2F04%2F21%2Fpython-iter-generator%2F</url>
      <content type="text"><![CDATA[在STL中，迭代器可以剥离算法和具体数据类型之间的耦合，使得库的维护者只需要为特定的迭代器（如前向迭代器，反向迭代器和随机迭代器）等实现算法即可，而不用关心具体的数据结构。在Python中，迭代器更是无处不在。这篇博客简要介绍Python中的迭代器和生成器，它们背后的原理以及如何实现一个自定义的迭代器/生成器，主要参考了教程Iterators &amp; Generators。 迭代器使用for循环时，常常遇到迭代器。如下所示，可能是最常用的一种方式。 12for i in range(100): # do something 100 times 在Python中，凡是可迭代的对象（Iterable Object），都可以用上面的方式进行迭代循环。例如，当被迭代对象是字符串时，每次得到的是字符串中的单个字符；当被迭代对象是文本文件时，每次得到的是文件中每一行的字符串；当被迭代对象是字典时，每次得到的是字典的key。 同样，也有很多函数接受的参数为可迭代对象。例如list()和tuple()，当传入的参数为刻碟带对象时，返回的是由迭代返回值组成的列表或者元组。例如 1list(&#123;'x':1, 'y':2&#125;) # =&gt; ['x', 'y'] 为什么list或者str这样的可迭代对象能够被迭代呢？或者，自定义的类满足什么条件，就可以用for x in XXX这种方法来遍历了呢？ 在Python中，有内建的函数iter()和next()。一般用法时，iter()方法接受一个可迭代对象，会调用这个对象的__iter__()方法，返回作用在这个可迭代对象的迭代器。而作为一个迭代器，必须有“迭代器的自我修养”，也就是实现next()方法（Python3中改为了__next__()方法）。 如下面的例子，yrange_iter是yrange的一个迭代器。yrange实现了__iter__()方法，是一个可迭代对象。调用iter(yrange object)的结果就是返回一个yrange_iter的对象实例。 1234567891011121314151617# Version 1.0 使用迭代器类class yrange_iter(object): def __init__(self, yrange): self.n = yrange.n self.i = 0 def next(self): v = self.i self.i += 1 return vclass yrange(object): def __init__(self, n): self.n = n def __iter__(self): return yrange_iter(self)print type(iter(yrange(5))) # &lt;class '__main__.yrange_iter'&gt; 而不停地调用迭代器的next()方法，就能够不断输出迭代序列。如下所示： 12345678910In [3]: yiter = iter(yrange(5))In [4]: yiter.next()Out[4]: 0In [5]: yiter.next()Out[5]: 1In [6]: yiter.next()Out[6]: 2 其实，上面的代码略显复杂。在代码量很小，不是很在意代码可复用性时，我们完全可以去掉yrange_iter，直接让yrange.__iter__()方法返回其自身实例。这样，我们只需要在yrange类中实现__iter__()方法和next()方法即可。如下所示： 12345678910111213141516171819202122# Version2.0 简化版，迭代器是本身class yrange(object): def __init__(self, n): self.n = n def __iter__(self): self.i = 0 return self def next(self): v = self.i self.i += 1 return vIn [8]: yiter = iter(yrange(5))In [9]: yiter.next()Out[9]: 0In [10]: yiter.next()Out[10]: 1In [11]: yiter.next()Out[11]: 2 然而，上述的代码仍然存在问题，我们无法指定迭代器生成序列的长度，也就是self.n实际上并没有用到。如果我只想产生0到10以内的序列呢？ 我们只需要加入判断条件，当超出序列边界时，抛出Python内建的StopIteration异常即可。 12345678910111213141516# Version3.0 加入边界判断，生成有限长度序列class yrange(object): def __init__(self, n): self.n = n def __iter__(self): self.i = 0 return self def next(self): if self.i == self.n: raise StopIteration v = self.i self.i += 1 return vfor i in yrange(5): print i Problem 1Write an iterator class reverse_iter, that takes a list and iterates it from the reverse direction. 1234567891011class reverse_iter(object): def __init__(self, alist): self.container = alist self.i = len(alist) def next(self): if self.i == 0: raise StopIteration self.i -= 1 return self.container[self.i]it = reverse_iter([1, 2, 3, 4]) 生成器生成器是一种方法，他指定了如何生成序列中的元素，生成器内部包含特殊的yield语句。此外，生成器函数是懒惰求值，只有当调用next()方法时，生成器才开始顺序执行，直到遇到yield语句。yield语句就像return，但是并未退出，而是打上断电，等待下一次next()方法的调用，再从上一次的断点处开始执行。我直接贴出教程中的代码示例。 12345678910111213141516171819202122232425262728&gt;&gt;&gt; def foo(): print "begin" for i in range(3): print "before yield", i yield i print "after yield", i print "end"&gt;&gt;&gt; f = foo()&gt;&gt;&gt; f.next()beginbefore yield 00&gt;&gt;&gt; f.next()after yield 0before yield 11&gt;&gt;&gt; f.next()after yield 1before yield 22&gt;&gt;&gt; f.next()after yield 2endTraceback (most recent call last): File "&lt;stdin&gt;", line 1, in &lt;module&gt;StopIteration&gt;&gt;&gt; 生成器表达式生成器表达式和列表相似，将[]换为()即可。如下所示： 123for i in (x**2 for x in [1,2,3,4]): print i# print 1 4 9 16 生成器的好处在于惰性求值，这样一来，我们还可以生成无限长的序列。因为生成器本来就是说明了序列的生成方式，而并没有真的生成那个序列。 下面的代码使用生成器得到前10组勾股数。通过在调用take()方法时修改传入实参n的大小，该代码可以很方便地转换为求取任意多得勾股数。生成器的重要作用体现在斜边x的取值为$[0, \infty]$。如果不使用生成器，恐怕就需要写出好几行的循环语句加上break配合才可以达到相同的效果。 1234567891011121314151617181920212223def integer(start, end=None): """Generate integer sequence [start, end) If `end` is not given, then [start, \infty] """ i = start while True: if end is not None and i == end: raise StopIteration yield i i += 1def take(n, g): i = 0 while True: if i &lt; n: yield g.next() i += 1 else: raise StopIteration# 假定 x&gt;y&gt;z，以消除两直角边互换的情况，如10, 6, 8和10, 8, 6tup = ((x,y,z) for x in integer(0) for y in integer(0, x) for z in integer(0, y) if x*x==y*y+z*z)list(take(10, tup)) Problem 2Write a program that takes one or more filenames as arguments and prints all the lines which are longer than 40 characters. 12345678910111213141516def readfiles(filenames): for f in filenames: for line in open(f): yield linedef grep(lines): return (line for line in lines if len(line)&gt;40)def printlines(lines): for line in lines: print line,def main(filenames): lines = readfiles(filenames) lines = grep(lines) printlines(lines) Problem 3Write a function findfiles that recursively descends the directory tree for the specified directory and generates paths of all the files in the tree. 注意get_all_file()方法中递归中生成器的写法，见SO的这个帖子。 1234567891011121314import osdef generate_all_file(root): for item in os.listdir(root): item = os.path.join(root, item) if os.path.isfile(item): yield os.path.abspath(item) else: for item in generate_all_file(item): yield itemdef findfiles(root): for item in generate_all_file(root): print item Problem 4Write a function to compute the number of python files (.py extension) in a specified directory recursively. 1234def generate_all_py_file(root): return (file for file in generate_all_file(root) if os.path.splitext(file)[-1] == '.py')print len(list(generate_all_py_file('./'))) Problem 5Write a function to compute the total number of lines of code in all python files in the specified directory recursively. 123def generate_all_line(root): return (line for f in generate_all_py_file(root) for line in open(f))print len(list(generate_all_line('./'))) Problem 6Write a function to compute the total number of lines of code, ignoring empty and comment lines, in all python files in the specified directory recursively. 1234def generate_all_no_empty_and_comment_line(root): return (line for line in generate_all_line(root) if not (line=='' or line.startswith('#')))print len(list(generate_all_no_empty_and_comment_line('./'))) Problem 7Write a program split.py, that takes an integer n and a filename as command line arguments and splits the file into multiple small files with each having n lines. 1234567891011121314151617def get_numbered_line(filename): i = 0 for line in open(filename): yield i, line i += 1def split(file_name, n): i = 0 f = open('output-%d.txt' %i, 'w') for idx, line in get_numbered_line(file_name): f.write(line) if (idx+1) % n == 0: f.close() i += 1 f = open('output-%d.txt' %i, 'w') f.close() Problem 9The built-in function enumerate takes an iteratable and returns an iterator over pairs (index, value) for each value in the source. Write a function my_enumerate that works like enumerate. 1234567def my_enumerate(iterable): i = 0 seq = iter(iterable) while True: val = seq.next() yield i, val i += 1]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Effective CPP 阅读 - Chapter 1 让自己习惯C++]]></title>
      <url>%2F2017%2F04%2F20%2Feffective-cpp-01%2F</url>
      <content type="text"><![CDATA[本系列是《Effective C++》一书的阅读摘记，分章整理各个条款。 01 将C++视作语言联邦C++在发明之初，只是一个带类的C。现在的C++已经变成了同时支持面向过程，面向对象，支持泛型和模板元编程的巨兽。这部分内容可以参见“C++的设计与演化”一书。 本条款中，将C++概括为四个次语言组成的联邦： 传统C：面向过程，也规定了C++基本的语法。 OOP：面向对象，带类的C，加入了继承，虚函数等概念。 Template：很多针对模板需要特殊注意的条款，甚至催生了模板元编程。 STL：标准模板库。使用STL要遵守它的约定。 想要高效地使用C++，必须根据不同的情况遵守不同的编程规范。 02 尽量使用const, enum, inline替换 #define（以编译器替换预处理器）#define是C时代遗留下来的预编译指令。 当#define用来定义某个常量时，通常const是一个更好的选择。 12#define PI 3.14const double PI = 3.14; 当此常量为整数类型（int, char, bool）等时，也可以使用enum定义常量。这种做法常常用在模板元编程中。 1enum &#123;K = 1&#125;; 对于const常量，你可以获取变量的地址，但是对于enum来说，无法获取变量的地址。对于这一点来说，enum和#define相类似。 另一种可能使用#define的场景是宏定义。这种情形可以使用inline声明内联函数解决。 总之，尽可能相信编译器的力量。使用#define将遮蔽编译器的视野，带来奇怪的问题。 03 尽可能使用constconst不止是给程序员看的，而且为编译器指定了一个语义约束，即这个对象是不该被改变的。所以任何试图修改这个对象的操作，都会被编译器检查出来，并给出error。 所以，如果某一变量满足const的要求，那么请加上const，和编译器签订一份契约，保护你的行为。 这里不再讨论const的寻常用法。提示一下：当修饰指针变量时，const在星号左边，是指指针所指物是常量；当const在星号右边，是指指针本身是常量。如下所示： 123456const int* p = &amp;a;*p = 5; // 非法p = &amp;b; // 合法int* const p = &amp;a;p = &amp;b; // 非法*p = 5; // 合法 STL中，如果声明某个迭代器为const，是指该迭代器本身是常量；如果你的意思是迭代器指向的元素为常量，那么使用const_iterator。 const更丰富的用法是用于函数声明中， 当修饰返回值时，意思是返回值不能修改。这可以让你避免无意义的赋值，尤其是以下的错误： 1if (fun(a, b) = c) // 这里错把 == 打成了 = 当修饰参数时，常常用做 pass-by-const-reference 的形式，不再多说了。 当修饰函数本身时，常常用在类中的成员函数上，意思是这个函数将不改变对象的成员。 这种情况下，可能会有const重载现象。 12345678class my_string&#123; const char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125; char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125;&#125;; 实际调用时，根据调用该函数的对象是否是const的来决定究竟调用哪个版本。 上面的实现未免过于复杂，我们还可以改成下面的形式： 123456789class my_string&#123; const char&amp; operator[](size_t pos) &#123; return this-&gt;ptr[pos]; &#125; char&amp; operator[](size_t pos) &#123; return const_cast&lt;char&amp;&gt;( static_cast&lt;const my_string&amp;&gt;(*this)[pos]); &#125;&#125;; 注意上面的代码进行了两次类型转换。由non-const reference转为const reference是类型安全的，使用static_cast进行。最后我们要脱掉const char&amp;的const属性，使用了const_cast。 对于const成员函数，有时不得不修改类中的某些成员变量，可以将这些变量声明为mutable。 04 确保对象在使用前已经被初始化使用未被初始化的变量有可能导致未定义的行为，导致奇怪的bug。所以推荐为所有变量进行初始化。 对于内建类型，需要手动初始化。 对于用户自定义类型，一般需要调用构造函数初始化。推荐在构造函数中使用初始化列表进行初始化，这样可以避免不必要的性能损失。原因见下： 1234public A(name, age) &#123; this-&gt;name = name; // 这是赋值，不是初始化！ this-&gt;age = age;&#125; 如果在类A的构造函数中使用初始化列表，就可以避免上面的赋值，而是使用copy-construct实现。 需要注意，成员初始化的顺序与其在类中声明的顺序相同，与初始化列表中的顺序无关。所以推荐将两者统一。 讨论完上述情况，再来看一种特殊变量：不同编译单元non-local static变量，是指不在某个函数scope下的static变量。这种变量的初始化顺序是未定义的，所以作者推荐使用单例模式，将它们移动到某个函数中去，明确初始化顺序。这里不再多说了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Caffe中的底层数学计算函数]]></title>
      <url>%2F2017%2F03%2F08%2Fmathfunctions-in-caffe%2F</url>
      <content type="text"><![CDATA[Caffe中使用了BLAS库作为底层矩阵运算的实现，这篇文章对mathfunction.hpp 文件中的相关函数做一总结。我们在自己实现layer运算的时候，也要注意是否Caffe中已经支持了类似运算，不要从scratch开始编码，自己累点不算啥，CPU/GPU的运算能力发挥不出来，更别说自己写的代码也许还是错的，那就尴尬了。。。 BLAS介绍以下内容参考BLAS wiki页面整理。这里不涉及BLAS的过多内容，只为介绍Caffe中的相关函数做一过渡。 BLAS的全称是基础线性代数子程序库（Basic Linear Algebra Subprograms），提供了一些低层次的通用线性代数运算的实现函数，如向量的相加，数乘，点积和矩阵相乘等。BLAS的实现根绝硬件平台的不同而不同，常常利用了特定处理器的硬件特点进行加速计算（例如处理器上的向量寄存器和SIMD指令集），提供了C和Fortran语言支持。 不同的厂商根据自己硬件的特点，在BLAS的统一框架下，开发了自己的加速库，比如AMD的ACML（已经不再支持），Intel的MKL，ATLAS和OpenBLAS。其中后面的三个均可以在Caffe中配置使用。 在BLAS中，实现了矩阵与矩阵相乘的函数gemm（GEMM: General Matrix to Matrix Multiplication）和矩阵和向量相乘的函数gemv，这两个数学运算的高效实现，关系到整个DL 框架的运算速度。下面这张图来源于Jia Yangqing的博士论文。 可以看到，在前向计算过程中，无论是CPU还是GPU，大量时间都花在了卷积层和全连接层上。全连接层不必多说，就是一个输入feature和权重的矩阵乘法。卷积运算也是通过矩阵相乘实现的。因为我们可以把卷积核变成一列，和相应的feature区域做相乘（如下图，这部分可以看一下Caffe中im2col部分的介绍和代码）。 对于BLAS和GEMM等对DL的作用意义，可以参见这篇文章Why GEMM is at the heart of deep learning的分析。上面的图也都来源于这篇博客。 矩阵运算函数矩阵运算函数在文件math_functions.hpp中可以找到。其中的函数多是对BLAS相应API的包装。这部分内容主要参考了参考资料[1]中的内容。谢谢原作者的整理。 矩阵与矩阵，矩阵与向量的乘法函数caffe_cpu_gemm()是对BLAS中矩阵与矩阵相乘函数gemm的包装。与之对应的caffe_cpu_gemv()是对矩阵与向量相乘gemv函数的包装。以前者为例，其实现代码如下： 12345678910template&lt;&gt;void caffe_cpu_gemm&lt;float&gt;(const CBLAS_TRANSPOSE TransA, const CBLAS_TRANSPOSE TransB, const int M, const int N, const int K, const float alpha, const float* A, const float* B, const float beta, float* C) &#123; int lda = (TransA == CblasNoTrans) ? K : M; int ldb = (TransB == CblasNoTrans) ? N : K; cblas_sgemm(CblasRowMajor, TransA, TransB, M, N, K, alpha, A, lda, B, ldb, beta, C, N);&#125; 可以看到，这个函数是对单精度浮点数（Single Float）的模板特化，在函数内部调用了BLAS包中的cblas_sgemm()函数。其功能是计算C = alpha * A * B + beta * C。参数的具体含义可以查看BLAS的相关文档。 矩阵/向量的加减下面的函数都是将X指针所指的数据作为src，将Y指针所指的数据为dst。同时，第一个参数统一是向量的长度。 caffe_axpy(N, alpha, x, mutable y)实现向量加法Y = alpha * X + Y。 caffe_axpby(N, alpha, x, beta, mutable y)实现向量加法Y = alpha * X + beta * Y 这两个函数的用法可以参见欧氏距离loss函数中的梯度计算： 1234567caffe_cpu_axpby( bottom[i]-&gt;count(), // count alpha, // alpha diff_.cpu_data(), // a Dtype(0), // beta bottom[i]-&gt;mutable_cpu_diff()); // b&#125; 其中，bottom[i]-&gt;count()给定了blob的大小，也就是向量的长度。alpha实际是由顶层top_blob传来的loss_weight，也即是*top_blob-&gt;cpu_diff()/batch_size。由于是直接将加权后的diff直接赋给bottom_blob的cpu_diff，所以，将beta赋值为0。 内存相关和C语言中的memset()和memcpy()类似，Caffe内也提供了对内存的拷贝与置位。使用方法也和两者相似： caffe_copy(N, x, mutable y)实现向量拷贝。源地址和目标地址服从上小节的约定。 caffe_set(N, alpha, mutable x)实现向量的置位，将向量分量填充为值alpha。 查看其实现可以知道，这里Caffe中直接调用了memset()完成任务。123456789101112131415template &lt;typename Dtype&gt;void caffe_set(const int N, const Dtype alpha, Dtype* Y) &#123; if (alpha == 0) &#123; memset(Y, 0, sizeof(Dtype) * N); // NOLINT(caffe/alt_fn) return; &#125; for (int i = 0; i &lt; N; ++i) &#123; Y[i] = alpha; &#125;&#125;// 模板的特化template void caffe_set&lt;int&gt;(const int N, const int alpha, int* Y);template void caffe_set&lt;float&gt;(const int N, const float alpha, float* Y);template void caffe_set&lt;double&gt;(const int N, const double alpha, double* Y); 而caffe_copy()中则是直接实现了CPU和GPU的功能。注意到下面代码中调用cudaMemcpy()的时候，使用了参数cudaMemcpyDefault。通过查阅文档，这个变量的含义是cudaMemcpyDefault: Default based unified virtual address space。通过它，我们可以无需知道源地址和目标地址是否在CPU内存或者GPU内存上而分别处理，减少了代码负担。 123456789101112131415template &lt;typename Dtype&gt;void caffe_copy(const int N, const Dtype* X, Dtype* Y) &#123; if (X != Y) &#123; if (Caffe::mode() == Caffe::GPU) &#123;#ifndef CPU_ONLY // NOLINT_NEXT_LINE(caffe/alt_fn) CUDA_CHECK(cudaMemcpy(Y, X, sizeof(Dtype) * N, cudaMemcpyDefault));#else NO_GPU;#endif &#125; else &#123; memcpy(Y, X, sizeof(Dtype) * N); // NOLINT(caffe/alt_fn) &#125; &#125;&#125; 所谓的unified virtual address（UVA）就是下图这个意思（见P2P&amp;UVA）。 有了这个东西，可以将内存和GPU显存看做一个统一的内存空间。CUDA运行时会根据指针的值自动判断数据的实际位置。这样一来，简化了编程者的工作量，如下所示： 其使用条件如下： 向量逐元素运算 caffe_add(N, a, b, y)函数实现Y[i] = a[i] + b[i]。 caffe_sub, caffe_div, caffe_mul同理。 caffe_exp, caffe_powx, caffe_abs, caffe_sqr, caffe_log相似，这里只将caffe_exp()的实现复制如下： 12345// 又是模板特化template &lt;&gt;void caffe_exp&lt;float&gt;(const int n, const float* a, float* y) &#123; vsExp(n, a, y); // 返回 y[i] = exp(a[i])&#125; caffe_scal实现向量的数乘。这个函数常常用在loss_layer中计算反传的梯度，常常要乘上一个标量loss_weight。 caffe_add_scalar实现向量每个分量与标量相加。 GPU版本相应地，和基于BLAS的CPU数学计算函数相似，各GPU版本的函数声明也放在了math_functions.hpp中，而相应的实现代码在math_functions.cu中。 随机数产生器Caffe中还提供了若干随机数产生器，可以用来做数据（如权重矩阵）的初始化等。 这里，Caffe提供饿了均匀分布（uniform），高斯分布（gaussian），伯努利分布（bernoulli）的实现。这里就不再详述，使用函数caffe_rng_distribution_name即可。 参考资料【1】seven-first 的博客]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Residual Net论文阅读 - Identity Mapping in Deep Residual Networks]]></title>
      <url>%2F2017%2F03%2F07%2Fresidualnet-paper2-identitymapping%2F</url>
      <content type="text"><![CDATA[这篇文章是He Kaiming继最初的那篇ResidualNet的论文后又发的一篇。这篇论文较为详细地探讨了由第一篇文章所引入的Identity Mapping，结合具体实验，测试了很多不同的组合结构，从实践方面验证了Identity Mapping的重要性。同时，也试图对Identity Mapping如此好的作用做一解释。尤其是本文在上篇文章的基础上，提出了新的残差单元结构，并指出这种新的结构具有更优秀的性能。 从残差到残差在第一篇文章中，作者创造性地提出了残差网络结构。简洁的网络结构，优异的性能，实在是一篇佳作，得到CVPR best paper实至名归。在这篇文章的开头，作者回顾了残差网络的一般结构，如下所示。其中，$x_l$和$x_{l+1}$表示第$l$个残差单元的输入和输出，$\mathcal{F}$为残差函数。 y_l = h(x_l) + \mathcal{F}(x_l, W_l)x_{l+1} = f(y_l)在上篇文章中，作者将$f(x)$取作ReLU函数，将$h(x)$取作Identity Mapping，即， h(x_l) = x_l在这篇文章中，作者提出了新的网络结构，和原有结构比较如下。 作者提出，将BN层和ReLU看做是后面带参数的卷积层的”前激活”（pre-activation），取代原先的“后激活”（post-activation）。这样就得到了上图右侧的新结构。 从右图可以看到，本单元的输入$x_l$首先经过了BN层和ReLU层的处理，然后才通过卷积层，之后又是BN-ReLU-conv的结构。这些处理过后得到的$y_l$，直接和$x_l$相加，得到该层的输出$x_{l+1}$。 利用这种新的残差单元结构，作者构建了1001层的残差网络，在CIFAR-10/100上进行了测试，验证了新结构能够更容易地训练（收敛速度快），并且拥有更好的泛化能力（测试集合error低）。 Identity Mapping: Why Always me?作者在后面的文章中对这种新结构进行了理论分析和实验测试，试图解释Identity Mapping为何这么重要。实验部分不再多介绍了，无非就是把作者的实验配置和结果贴出来，好麻烦。。。这里只把作者的理论分析整理如下。如果想要复现坐着的工作，还是要结合论文去好好看一下实验部分。 在使用了这种新结构之后，我们有， x_{l+1} = x_l + \mathcal{F}(x_l, W_l)如果我们的网络都是由这种残差网络组成的，递归地去倒到前面较浅的某一层，则： x_L = x_l +\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i)从上面的式子可以看出， 深层单元$L$的特征$x_L$可以被表示为浅层单元的特征$x_l$j加上它们之间各层的残差函数$\mathcal{F}$。 我们有，$x_L=x_0+\sum_{i=0}^{L-1}\mathcal{F}(x_i, W_i)$，而普通网络$x_L$和$x_l$的关系比较复杂，$x_L = \prod_{i=0}^{L-1}W_ix_0$。看上去，前者的优化应该更加简单。 计算bp的时候，有， \frac{\partial \epsilon}{\partial x_l} = \frac{\partial \epsilon} {\partial x_L}\frac{\partial x_L}{\partial x_l} = \frac{\partial \epsilon}{\partial x_L}(1+\frac{\partial}{\partial x_l}\sum_{i=l}^{L-1}\mathcal{F}(x_i, W_i))上式表明，由于残差单元的短路连接（shortcut），$x_l$处的梯度基本不会出现消失的情况（除非后面一项正好等于-1）。 如果不做Identity Mapping，而是乘上一个系数$\lambda$呢？作者发现这会在上面的式子上出现$\lambda^k$的形式，造成梯度以指数规律vanish或者爆炸。同样的，如果乘上一个权重，也会有类似的效应。 所以，Identity Mapping是坠吼的！ 花式跑实验论文的后半部分，作者开始花式做实验，调研了很多不同的结构，具体实验方案和对比结果可以参看原论文。这里不再罗列了。附上自己用PyTorch实现的164层ResNet在CIFAR10上的训练代码：Gist Code: ResNet-164 training experiment on CIFAR10 using PyTorch。 下面的可视化结果由DMLC/tensorboard实现。图上在$30K$次迭代附近有明显的性能提升，对应于学习率的调整，变为原来的$0.1$。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO网络参数的解析与存储]]></title>
      <url>%2F2017%2F03%2F06%2Fyolo-cfg-parser%2F</url>
      <content type="text"><![CDATA[YOLO的原作者使用了自己开发的Darknet框架，而没有选取当前流行的其他深度学习框架实现算法，所以有必要对其网络模型的参数解析与存储方式做一了解，方便阅读源码和在其他流行的框架下的算法移植。 YOLO网络结构定义的CFG文件YOLO中的网络定义采用和Caffe类似的方式，都是通过一个顺序堆叠layer来对神经网络结构进行定义的文件来描述。不同的地方在于，Caffe中使用了Google家出品的protobuf，省时省力，无需自己实现解析文件的功能，但是也使得Caffe对第三方库的依赖更加严重。相信很多人在编译Caffe的时候都出现过无法链接等蛋疼无比的问题。而YOLO的作者则是使用了自己定义的一种CFG文件格式，需要自己实现解析功能。 CFG文件的格式可以归纳如下（可以打开某个CFG文件进行对照）：123456789[net]# 这里会对net的参数进行配置# 同时YOLO将对net的求解器的参数也放在了这里[conv]# 一些conv层的参数描述[maxpool]# 一些池化层的参数描述# 顺序堆叠的其他layer描述 在Darknet的代码中，将每个[]符号导引的参数列表叫做section。 网络结构解析器 Parser具体的解析实现参见parser.c文件。我们先以convolutional_layer parse_convolutional(list *options, size_params params)函数为例，看一下Darknet是如何完成对卷积层参数的解析的。 从函数签名可以看出，这个函数接受一个list的变量（Darknet中将堆叠起来的这些层描述抽象成链表），而size_params类型的变量params指示了该层上一层的参数情况，其具体定义如下：12345678910typedef struct size_params&#123; int batch; int inputs; int h; int w; int c; int index; int time_steps; network net;&#125; size_params; 这样，在构建该层卷积层的时候，我们就能够知道上一层的输入维度等信息，方便做一些参数检查和layer初始化等的工作。 进入函数内部，会发现频繁出现option_find_int这个函数。从函数名字面意义看，应该是要解析字符串中的整型数。 我们首先来看一下这个函数的定义吧~这个函数并不在parser.c中，而是在option_list.c 文件中。 12345678910111213// l: data pointer to the list// key: the key to find, example: "filters", "padding"// def: default valueint option_find_int(list *l, char *key, int def)&#123; // 去找到该key对应的数值，使用atoi转换为整型数 char *v = option_find(l, key); if(v) return atoi(v); // 使用XXX_quiet版本可以不打印此信息 fprintf(stderr, "%s: Using default '%d'\n", key, def); // 没有找到key，返回默认值 return def;&#125; 而其中的option_find函数则是逐项顺序查找，匹配字符串来实现的。 12345678910111213char *option_find(list *l, char *key)&#123; node *n = l-&gt;front; while(n)&#123; kvp *p = (kvp *)n-&gt;val; if(strcmp(p-&gt;key, key) == 0)&#123; p-&gt;used = 1; return p-&gt;val; &#125; n = n-&gt;next; &#125; return 0;&#125; 构建conv层由此，我们可以通过CFG文件得到卷积层的参数了。接下来需要调用其初始化函数，进行构建。 123456789101112131415161718192021222324 // 首先得到参数 int n = option_find_int(options, "filters",1); int size = option_find_int(options, "size",1); int stride = option_find_int(options, "stride",1); int pad = option_find_int_quiet(options, "pad",0); int padding = option_find_int_quiet(options, "padding",0); if(pad) padding = size/2;// 激活函数是通过匹配其名称的方法得到的 char *activation_s = option_find_str(options, "activation", "logistic"); ACTIVATION activation = get_activation(activation_s); // 通过上层的信息得到batch size，做参数检查 int batch,h,w,c; h = params.h; w = params.w; c = params.c; batch=params.batch; if(!(h &amp;&amp; w &amp;&amp; c)) error("Layer before convolutional layer must output image."); int batch_normalize = option_find_int_quiet(options, "batch_normalize", 0); int binary = option_find_int_quiet(options, "binary", 0); int xnor = option_find_int_quiet(options, "xnor", 0); // 调用初始化函数 convolutional_layer layer = make_convolutional_layer(batch,h,w,c,n,size,stride,padding,activation, batch_normalize, binary, xnor, params.net.adam); layer.flipped = option_find_int_quiet(options, "flipped", 0); layer.dot = option_find_float_quiet(options, "dot", 0); 所以，如果在阅读源码时候，对layer的某个成员变量不知道什么意思的话，可以参考此文件，看一下原始解析对应的字符串是什么，一般这个字符串描述是比较具体的。 构建网络有了各个layer的解析方法，接下来就可以逐层读取参数信息并构建网络了。 Darknet中对应的函数为network parse_network_cfg(char *filename)，这个函数接受文件名为参数，进行网络结构的解析。 首先，调用read_cfg(filename)得到CFG文件的一个层次链表，接着只要对这个链表进行解析就好了。不过对第一个section，也就是[net] section，要特殊对待。这里不再多说了。 保存参数信息Darknet中保存带参数的layer的信息是直接写入二进制文件。仍然以卷积层为例，其保存代码如下所示： 12345678910111213141516171819202122232425void save_convolutional_weights(layer l, FILE *fp)&#123; if(l.binary)&#123; //save_convolutional_weights_binary(l, fp); //return; &#125;#ifdef GPU if(gpu_index &gt;= 0)&#123; pull_convolutional_layer(l); &#125;#endif int num = l.n*l.c*l.size*l.size; fwrite(l.biases, sizeof(float), l.n, fp); // 由于darknet设计时，没有单独设计BN层，所以BN的参数也是和其所在的层一起保存的，如果读取时候要注意分别讨论 if (l.batch_normalize)&#123; fwrite(l.scales, sizeof(float), l.n, fp); fwrite(l.rolling_mean, sizeof(float), l.n, fp); fwrite(l.rolling_variance, sizeof(float), l.n, fp); &#125; fwrite(l.weights, sizeof(float), num, fp); if(l.adam)&#123; fwrite(l.m, sizeof(float), num, fp); fwrite(l.v, sizeof(float), num, fp); &#125;&#125; 在保存整个网络的参数信息的时候，同样逐层保存到同一个二进制文件中就好了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Residual Net论文阅读 - Deep Residual Learning for Image Recongnition]]></title>
      <url>%2F2017%2F03%2F05%2Fresidualnet-paper%2F</url>
      <content type="text"><![CDATA[Residuel Net是MSRA HeKaiming组的作品，斩获了ImageNet挑战赛的所有项目的第一，并荣获CVPR的best paper，成为state of the ar的网络结构。这篇文章记录了阅读最初论文“Deep Residual Learning for Image Recongnition”的重点。 更深的网络 -&gt; 更好的性能在ImageNet等比赛上，大家已经发现了一个现象，就是更深的网络往往能够获得更好的成绩。从LeNet到AlexNet再到VGG Net和GoogLeNet，网络层次越来越深，然而增加网络深度在实际中遇到了很多的问题。 在序言部分，这篇论文也是首先提出了一个问题：我们只需要不断在现有结构基础上堆叠更多的layer就可以获得更好的网络吗？ Is learning better networks as easy as stacking more layers? 很明显，答案是否定的。一个问题就是梯度的消失（或爆炸）。在bp过程中，过深的网络结构会导致传导到底层的梯度变的很小（或飞升），导致训练失败。这一问题在BN层提出之后得到了一定的解决。 另一个问题是在实验过程中观测到的。通过实验，作者发现，当不断增加网络深度的时候，网络的性能会不再提升。如果再继续添加深度，网络的性能甚至会下降！下面是作者在CIFAR-10上做的实验，使用56层的网络比20层的网络，无论在训练集还是测试集上都落于下风。 个人觉得，这个现象看上去意料之外，情理之中。并不是说56层的网络的学习能力不如20层，而是训练不同深度的神经网络的难度是不同的。作者联想到（这里的想法很好！），如果我们已经有了一个较浅的网络（shadow net），然后我们在其后面接上若干的等同映射（Identity Mapping），那么新得到的更深的网络应该是和前者有相同的表现的。这个思想实验，巧妙地说明了并不是更深的网络变坏了，而是我们现有的方法不能很好地训练更深的网络。 残差单元也是受上面这个思想实验中的Identity Mapping的启发，作者设计了一种残差网络结构，以它为基本单元构建更深的网络，以期解决第二个问题。 使用残差单元时，我们不再让网络去直接学映射$\mathcal{H}(x)$，而是学习映射$\mathcal{F}(x) = \mathcal{H}(x) - x$。作者也简单说了为何使用这种残差结构。这种方法给要学的映射加上了一个等同映射作为参考。同时考虑极端情况，如果最优的结构真的是等同映射的话，那么学习到的$\mathcal{F}(x)$为$0$就好了。这个网络的性能起码是不输于那个浅层网络的。 应用这种残差结构就可以很容易地搭建深层网络了。使用这种技术，作者构建了多达$1000$多层的网络，同时在多项比赛中狂揽桂冠，在实践中证明了它的威力。 残差学习从上面的介绍看出，使用残差结构后，网络不再直接学习最终的映射$\mathcal{H}$，而是这个映射和输入的残差$\mathcal{F}$。这里叫做残差学习（Residual Learning）。 神经网络可以近似任意复杂的函数（作者指出此处存疑，还是作为假设）所以，它不仅可以逼近$\mathcal{H}$，当然也可以逼近残差。这两者虽然都可以通过网络近似，但是训练难度是不同的。 上面图中的残差单元结构可以写成下面的式子，其中的$W_i$就是决定残差映射$\mathcal{F}$的参数，也是训练中要优化的东西。这里为了书写简单，省略了偏置项。 y = \mathcal{F(x,\lbrace W_i\rbrace)}+x上面的式子要求$\mathcal{F}(x)$与$x$有相同的维度，如果维度不同的话，可以给输入$x$乘上一个权重参数矩阵$W_s$，做一下维度匹配。当然，即使维度相同，我们也可以乘上一个方阵$W_s$，但是这样一来，一是给网络引入了更多的参数（这样，我们的残差结构打脸效果不就打折扣了？），同时在论文中的实验部分也证明了加入这个矩阵对提升性能没用（Identity Mapping已经够用了）。 同时，在设计残差结构的时候，也不必非要像上面的图那样设计两层，完全可以设计更多（比如下面的bottleneck结构，只是别减少得只剩一层了，那样的话$\mathcal{F}$只剩一个线性映射可以学了。。。）。 ImageNet实验和比较由此，我们可以构建残差网络。这里开始，作者通过一系列实验，来证明残差结构的优越性：更少的参数，更深的层数，更优秀的性能。 这里着重介绍作者在ImageNet上的实验结果。 作者首先对比了18层和34层plain网络和残差网络的表现，进一步验证了序言中的结论。采用普通的结构，更深的网络（34层）表现反而不如较浅的网络，而使用残差结构则没有这个问题。从下图左右的对比可以很清楚地看出这个现象。作者同时指出这一现象不大可能是由于梯度消失造成的。 另外一个从实验中观察到的现象指出，对于18层这种较浅的网络，使用残差结构能够加快收敛速度，使得训练更加容易。 同时，对于上面提到的维度不匹配的问题，作者提出了三个解决方案并进行了对比。 方案A使用zero-padding的方法 方案B使用乘上权重矩阵的方法 方案C不止在维度不匹配时乘权重矩阵，而且所有的Identity Mapping都换成这种形式 实验结果表明，模型表现A&lt;B&lt;C。但是性能差距较小。由于C引入了很多额外的参数，所以并不使用这种方法（聚焦主要矛盾）。 Bottleneck结构为了节省训练时间，作者提出了一种新的变形——Bottleneck结构。见下图右侧。首先将两层结构扩展为三层，最前面和最后面都是$1\times 1$的卷积核，来进行channel的变形。通过前面的$1\times 1$卷积核，将channel降下来。和$3\times 3$卷积核作用后，再用最后的$1\times 1$卷积核升上去。 使用这一单元结构，作者构建了50层，101层和152层的深层网络，并最终取得了很好的成绩。 附录在附录中，作者描述了在Pascal VOC和COCO目标检测和定位任务中使用Residual Net的情况。对于目标检测这个任务，后续可以参见MSRA的R-FCN那篇文章。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[toy demo - PyTorch + MNIST]]></title>
      <url>%2F2017%2F03%2F04%2Fpytorch-mnist-example%2F</url>
      <content type="text"><![CDATA[本篇文章介绍了使用PyTorch在MNIST数据集上训练MLP和CNN，并记录自己实现过程中的若干问题。 加载MNIST数据集PyTorch中提供了MNIST，CIFAR，COCO等常用数据集的加载方法。MNIST是torchvision.datasets包中的一个类，负责根据传入的参数加载数据集。如果自己之前没有下载过该数据集，可以将download参数设置为True，会自动下载数据集并解包。如果之前已经下载好了，只需将其路径通过root传入即可。 在加载图像后，我们常常需要对图像进行若干预处理。比如减去RGB通道的均值，或者裁剪或翻转图像实现augmentation等，这些操作可以在torchvision.transforms包中找到对应的操作。在下面的代码中，通过使用transforms.Compose()，我们构造了对数据进行预处理的复合操作序列，ToTensor负责将PIL图像转换为Tensor数据（RGB通道从[0, 255]范围变为[0, 1]）， Normalize负责对图像进行规范化。这里需要注意，虽然MNIST中图像都是灰度图像，通道数均为1，但是仍要传入tuple。 之后，我们通过DataLoader返回一个数据集上的可迭代对象。一会我们通过for循环，就可以遍历数据集了。 1234567891011121314151617181920212223242526import torchimport torch.nn as nnfrom torch.autograd import Variableimport torchvision.datasets as dsetimport torchvision.transforms as transformsimport torch.nn.functional as Fimport torch.optim as optim# use cuda or notuse_cuda = torch.cuda.is_available()trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])train_set = dset.MNIST(root=root, train=True, transform=trans, download=download)test_set = dset.MNIST(root=root, train=False, transform=trans)batch_size = 128train_loader = torch.utils.data.DataLoader( dataset=train_set, batch_size=batch_size, shuffle=True)test_loader = torch.utils.data.DataLoader( dataset=test_set, batch_size=batch_size, shuffle=False) 网络构建在进行网络构建时，主要通过torch.nn包中的已经实现好的卷积层、池化层等进行搭建。例如下面的代码展示了一个具有一个隐含层的MLP网络。nn.Linear负责构建全连接层，需要提供输入和输出的通道数，也就是y = wx+b中x和y的维度。 123456789101112class MLPNet(nn.Module): def __init__(self): super(MLPNet, self).__init__() self.fc1 = nn.Linear(28*28, 500) self.fc2 = nn.Linear(500, 256) self.fc3 = nn.Linear(256, 10) def forward(self, x): x = x.view(-1, 28*28) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x 由于PyTorch可以实现自动求导，所以我们只需实现forward过程即可。这里由于池化层和非线性变换都没有参数，所以使用了nn.functionals中的对应操作实现。通过看文档，可以发现，一般nn里面的各种层，都会在nn.functionals里面有其对应。例如卷积层的对应实现，如下所示，需要传入卷积核的权重。 1234# With square kernels and equal stridefilters = autograd.Variable(torch.randn(8,4,3,3))inputs = autograd.Variable(torch.randn(1,4,5,5))F.conv2d(inputs, filters, padding=1) 同样地，我们可以实现LeNet的结构如下。 1234567891011121314151617class LeNet(nn.Module): def __init__(self): super(LeNet, self).__init__() self.conv1 = nn.Conv2d(1, 20, 5, 1) self.conv2 = nn.Conv2d(20, 50, 5, 1) self.fc1 = nn.Linear(4*4*50, 500) self.fc2 = nn.Linear(500, 10) def forward(self, x): x = F.relu(self.conv1(x)) x = F.max_pool2d(x, 2, 2) x = F.relu(self.conv2(x)) x = F.max_pool2d(x, 2, 2) x = x.view(-1, 4*4*50) x = F.relu(self.fc1(x)) x = self.fc2(x) return x 训练与测试在训练时，我们首先应确定优化方法。这里我们使用带动量的SGD方法。下面代码中的optim.SGD初始化需要接受网络中待优化的Parameter列表（或是迭代器），以及学习率lr，动量momentum。 1optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9) 接下来，我们只需要遍历数据集，同时在每次迭代中清空待优化参数的梯度，前向计算，反向传播以及优化器的迭代求解即可。 1234567891011121314151617181920212223242526272829303132333435363738394041424344## trainingmodel = LeNet()if use_cuda: model = model.cuda()optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)ceriation = nn.CrossEntropyLoss()for epoch in xrange(10): # trainning ave_loss = 0 for batch_idx, (x, target) in enumerate(train_loader): optimizer.zero_grad() if use_cuda: x, target = x.cuda(), target.cuda() x, target = Variable(x), Variable(target) out = model(x) loss = ceriation(out, target) ave_loss = ave_loss * 0.9 + loss.data[0] * 0.1 loss.backward() optimizer.step() if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader): print '==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, train loss: &#123;:.6f&#125;'.format( epoch, batch_idx+1, ave_loss) # testing correct_cnt, ave_loss = 0, 0 total_cnt = 0 for batch_idx, (x, target) in enumerate(test_loader): if use_cuda: x, targe = x.cuda(), target.cuda() x, target = Variable(x, volatile=True), Variable(target, volatile=True) out = model(x) loss = ceriation(out, target) _, pred_label = torch.max(out.data, 1) total_cnt += x.data.size()[0] correct_cnt += (pred_label == target.data).sum() # smooth average ave_loss = ave_loss * 0.9 + loss.data[0] * 0.1 if(batch_idx+1) % 100 == 0 or (batch_idx+1) == len(test_loader): print '==&gt;&gt;&gt; epoch: &#123;&#125;, batch index: &#123;&#125;, test loss: &#123;:.6f&#125;, acc: &#123;:.3f&#125;'.format( epoch, batch_idx+1, ave_loss, correct_cnt * 1.0 / total_cnt) 当优化完毕后，需要保存模型。这里官方文档给出了推荐的方法，如下所示：123torch.save(model.state_dict(), PATH) #保存网络参数the_model = TheModelClass(*args, **kwargs)the_model.load_state_dict(torch.load(PATH)) #读取网络参数 该博客的完整代码可以见：PyTorch MNIST demo。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[远程登录Jupyter笔记本]]></title>
      <url>%2F2017%2F02%2F26%2Fjupyternotebook-remote-useage%2F</url>
      <content type="text"><![CDATA[Jupyter Notebook可以很方便地记录代码和内容，很适合边写笔记边写示例代码进行学习总结。在本机使用时，只需在相应文件夹下使用jupyter notebook命令即可在浏览器中打开笔记页面，进行编辑。而本篇文章记述了如何在远端登录并使用Jupyter笔记本。这样，就可以利用服务器较强的运算能力来搞事情了。 配置jupter notebook登录远程服务器后，使用如下命令生成配置文件。 1jupyter notebook --generate-config 并对其内容进行修改。打开配置文件： 1vim ~/.jupyter/jupyter_notebook_config.py 主要修改两处地方： c.NotebookApp.ip=&#39;*&#39;，即不限制ip访问 c.NotebookApp.password = u&#39;hash_value&#39; 上面的hash_value是由用户给定的密码生成的。可以使用ipython中的命令轻松搞定。 12345from notebook.auth import passwdpasswd()"""这里会要求用户输入密码并确认，生成的hash值要填写到上面""" 当在服务器上运行jupyter-notebook时，我们常常不需要服务器上额外启动浏览器窗口。可以修改上述的配置文件，禁用服务器端的浏览器。找到下面这一行，改成False即可。 1c.NotebookApp.open_browser = True 启动notebook之后，在远程服务器上启动笔记本jupyter notebook。接着，在本地机器上访问远程服务器ip:8888（默认端口为8888，也可以在配置文件中修改），输入密码即可访问远程笔记本。 Jupyter Notebook的常用快捷键类似Vim编辑器，Jupyter Notebook有两种键盘输入模式： 编辑模式，在单元中键入代码或文本，这时的单元框线是绿色的。通过Esc可以切换至命令模式。 命令模式，键盘输入运行命令，这时的单元框线是蓝色的。通过Enter可以切换至编辑模式。 常用的快捷键有： h：弹出快捷键列表 m：将该Cell转换为Markdown输入状态 y：将该Cell转换为代码输入状态 shift+Enter：执行该Cell并将焦点置于下一个Cell（如果没有，将新建） Ctrl+Enter：执行该Cell 本篇内容参考自博客远程访问jupyter notebook以及Jupyter Notebook快捷键 。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[PyTorch简介]]></title>
      <url>%2F2017%2F02%2F25%2Fpytorch-tutor-01%2F</url>
      <content type="text"><![CDATA[这是一份阅读PyTorch教程的笔记，记录jupyter notebook的关键点。原地址位于GitHub repo。 PyTorch简介PyTorch是一个较新的深度学习框架。从名字可以看出，其和Torch不同之处在于PyTorch使用了Python作为开发语言，所谓“Python first”。一方面，使用者可以将其作为加入了GPU支持的numpy，另一方面，PyTorch也是强大的深度学习框架。 目前有很多深度学习框架，PyTorch主推的功能是动态网络模型。例如在Caffe中，使用者通过编写网络结构的prototxt进行定义，网络结构是不能变化的。而PyTorch中的网络结构不再只能是一成不变的。同时PyTorch实现了多达上百种op的自动求导（AutoGrad）。 TensorsTensor，即numpy中的多维数组。上面已经提到过，PyTorch对其加入了GPU支持。同时，PyTorch中的Tensor可以与numpy中的array很方便地进行互相转换。 通过Tensor(shape)便可以创建所需要大小的tensor。如下所示。 12345x = torch.Tensor(5, 3) # construct a 5x3 matrix, uninitialized# 或者随机填充y = torch.rand(5, 3) # construct a randomly initialized matrix# 使用size方法可以获得tensor的shape信息，torch.Size 可以看做 tuplex.size() # out: torch.Size([5, 3]) PyTorch中已经实现了很多常用的op，如下所示。 1234567891011121314151617# addition: syntax 1x + y # out: [torch.FloatTensor of size 5x3]# addition: syntax 2torch.add(x, y) # 或者使用torch包中的显式的op名称# addition: giving an output tensorresult = torch.Tensor(5, 3) # 预先定义sizetorch.add(x, y, out=result) # 结果被填充到变量result# 对于加法运算，其实没必要这么复杂out = x + y # 无需预先定义size# torch包中带有下划线的op说明是就地进行的，如下所示# addition: in-placey.add_(x) # 将x加到y上# 其他的例子: x.copy_(y), x.t_(). PyTorch中的元素索引方式和numpy相同。 12# standard numpy-like indexing with all bells and whistlesx[:,1] # out: [torch.FloatTensor of size 5] 对于更多的op，可以参见PyTorch的文档页面。 Tensor可以和numpy中的数组进行很方便地转换。并且转换前后并没有发生内存的复制（这里文档里面没有明说？），所以修改其中某一方的值，也会引起另一方的改变。如下所示。 1234567891011121314151617# Tensor 转为 np.arraya = torch.ones(5) # out: [torch.FloatTensor of size 5]# 使用 numpy方法即可实现转换b = a.numpy() # out: array([ 1., 1., 1., 1., 1.], dtype=float32)# 注意！a的值的变化同样引起b的变化a.add_(1)print(a)print(b) # a b的值都变成2# np.array 转为Tensorimport numpy as npa = np.ones(5)# 使用torch.from_numpy即可实现转换b = torch.from_numpy(a) # out: [torch.DoubleTensor of size 5]np.add(a, 1, out=a)print(a)print(b) # a b的值都变为2 PyTorch中使用GPU计算很简单，通过调用.cuda()方法，很容易实现GPU支持。 123456# let us run this cell only if CUDA is availableif torch.cuda.is_available(): print('cuda is avaliable') x = x.cuda() y = y.cuda() x + y # 在GPU上进行计算 Neural Network说完了数据类型Tensor，下一步便是如何实现一个神经网络。首先，对自动求导做一说明。 我们需要关注的是autograd.Variable。这个东西包装了Tensor。一旦你完成了计算，就可以使用.backward()方法自动得到（以该Variable为叶子节点的那个）网络中参数的梯度。Variable有一个名叫data的字段，可以通过它获得被包装起来的那个原始的Tensor数据。同时，使用grad字段，可以获取梯度（也是一个Variable）。 Variable是计算图的节点，同时Function实现了变量之间的变换。它们互相联系，构成了用于计算的无环图。每个Variable有一个creator的字段，表明了它是由哪个Function创建的（除了用户自己显式创建的那些，这时候creator是None）。 当进行反向传播计算梯度时，如果Variable是标量（比如最终的loss是欧氏距离或者交叉熵），那么backward()函数不需要参数。然而如果Variable有不止一个元素的时候，需要为其中的每个元素指明其（由上层传导来的）梯度（也就是一个和Variableshape匹配的Tensor）。看下面的说明代码。 1234567891011121314151617181920212223242526272829from torch.autograd import Variablex = Variable(torch.ones(2, 2), requires_grad = True)x # x 包装了一个2x2的Tensor"""Variable containing: 1 1 1 1[torch.FloatTensor of size 2x2]"""# Variable进行计算# y was created as a result of an operation,# so it has a creatory = x + 2y.creator # out: &lt;torch.autograd._functions.basic_ops.AddConstant at 0x7fa1cc158c08&gt;z = y * y * 3 out = z.mean() # out: Variable containing: 27 [torch.FloatTensor of size 1]# let's backprop nowout.backward() # 其实相当于 out.backward(torch.Tensor([1.0]))# print gradients d(out)/dxx.grad"""Variable containing: 4.5000 4.5000 4.5000 4.5000[torch.FloatTensor of size 2x2]""" 下面的代码就是结果不是标量，而是普通的Tensor的例子。12345678910111213141516171819# 也可以通过Tensor显式地创建Variablex = torch.randn(3)x = Variable(x, requires_grad = True)# 一个更复杂的 op例子y = x * 2while y.data.norm() &lt; 1000: y = y * 2# 计算 dy/dxgradients = torch.FloatTensor([0.1, 1.0, 0.0001])y.backward(gradients)x.grad"""Variable containing: 204.8000 2048.0000 0.2048[torch.FloatTensor of size 3]""" 说完了NN的构成元素Variable，下面可以介绍如何使用PyTorch构建网络了。这部分主要使用了torch.nn包。我们自定义的网络结构是由若干的layer组成的，我们将其设置为 nn.Module的子类，只要使用方法forward(input)就可以返回网络的output。下面的代码展示了如何建立一个包含有conv和max-pooling和fc层的简单CNN网络。 1234567891011121314151617181920212223242526272829303132333435363738394041424344import torch.nn as nn # 以我的理解，貌似有参数的都在nn里面import torch.nn.functional as F # 没有参数的（如pooling和relu）都在functional里面？class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel self.conv2 = nn.Conv2d(6, 16, 5) # 一会可以看到，input是1x32x32大小的。经过计算，conv-pooling-conv-pooling后大小为16x5x5。 # 所以fc层的第一个参数是 16x5x5 self.fc1 = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): # 你可以发现，构建计算图的过程是在前向计算中完成的，也许这可以让你体会到所谓的动态图结构 # 同时，我们无需实现 backward，这是被自动求导实现的 x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number x = x.view(-1, self.num_flat_features(x)) # 把它拉直 x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return x def num_flat_features(self, x): size = x.size()[1:] # all dimensions except the batch dimension num_features = 1 for s in size: num_features *= s return num_features# 实例化Net对象net = Net()net # 给出了网络结构"""Net ( (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1)) (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1)) (fc1): Linear (400 -&gt; 120) (fc2): Linear (120 -&gt; 84) (fc3): Linear (84 -&gt; 10))""" 我们可以列出网络中的所有参数。 1234params = list(net.parameters())print(len(params)) # out: 10, 5个权重，5个biasprint(params[0].size()) # conv1's weight out: torch.Size([6, 1, 5, 5])print(params[1].size()) # conv1's bias, out: torch.Size([6]) 给出网络的输入，得到网络的输出。并进行反向传播梯度。 1234input = Variable(torch.randn(1, 1, 32, 32))out = net(input) # 重载了()运算符？net.zero_grad() # bp前，把所有参数的grad buffer清零out.backward(torch.randn(1, 10)) 注意一点，torch.nn只支持mini-batch。所以如果你的输入只有一个样例的时候，使用input.unsqueeze(0)人为给它加上一个维度，让它变成一个4-D的Tensor。 网络训练给定target和网络的output，就可以计算loss函数了。在torch.nn中已经实现好了一些loss函数。 1234567891011output = net(input)target = Variable(torch.range(1, 10)) # a dummy target, for example# 使用平均平方误差，即欧几里得距离criterion = nn.MSELoss()loss = criterion(output, target)loss"""Variable containing: 38.6049[torch.FloatTensor of size 1]""" 网络的整体结构如下所示。 1234input -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; conv2d -&gt; relu -&gt; maxpool2d -&gt; view -&gt; linear -&gt; relu -&gt; linear -&gt; relu -&gt; linear -&gt; MSELoss -&gt; loss 我们可以使用previous_functions来获得该节点前面Function的信息。 123456789# For illustration, let us follow a few steps backwardprint(loss.creator) # MSELossprint(loss.creator.previous_functions[0][0]) # Linearprint(loss.creator.previous_functions[0][0].previous_functions[0][0]) # ReLU&quot;&quot;&quot;&lt;torch.nn._functions.thnn.auto.MSELoss object at 0x7fa18011db40&gt;&lt;torch.nn._functions.linear.Linear object at 0x7fa18011da78&gt;&lt;torch.nn._functions.thnn.auto.Threshold object at 0x7fa18011d9b0&gt;&quot;&quot;&quot; 进行反向传播后，让我们查看一下参数的变化。 1234567# now we shall call loss.backward(), and have a look at conv1's bias gradients before and after the backward.net.zero_grad() # zeroes the gradient buffers of all parametersprint('conv1.bias.grad before backward')print(net.conv1.bias.grad)loss.backward()print('conv1.bias.grad after backward')print(net.conv1.bias.grad) 计算梯度后，自然需要更新参数了。简单的方法可以自己手写： 123learning_rate = 0.01for f in net.parameters(): f.data.sub_(f.grad.data * learning_rate) 不过，torch.optim中已经提供了若干优化方法（SGD, Nesterov-SGD, Adam, RMSProp, etc）。如下所示。 123456789import torch.optim as optim# create your optimizeroptimizer = optim.SGD(net.parameters(), lr = 0.01)# in your training loop:optimizer.zero_grad() # zero the gradient buffersoutput = net(input)loss = criterion(output, target)loss.backward()optimizer.step() # Does the update 数据载入由于PyTorch的Python接口和np.array之间的方便转换，所以可以使用其他任何数据读入的方法（例如OpenCV等）。特别地，对于vision的数据，PyTorch提供了torchvision包，可以方便地载入常用的数据集（Imagenet, CIFAR10, MNIST, etc），同时提供了图像的各种变换方法。下面以CIFAR为例子。 123456789101112131415161718192021import torchvisionimport torchvision.transforms as transforms# The output of torchvision datasets are PILImage images of range [0, 1].# We transform them to Tensors of normalized range [-1, 1]# Compose: Composes several transforms together.# see http://pytorch.org/docs/torchvision/transforms.html?highlight=transformstransform=transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)), ]) # torchvision.transforms.Normalize(mean, std)# 读取CIFAR10数据集 trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)# 使用DataLoadertrainloader = torch.utils.data.DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)# Test集，设置train = Falsetestset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)testloader = torch.utils.data.DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck') 接下来，我们对上面部分的CNN网络进行小修，设置第一个conv层接受3通道的输入。并使用交叉熵定义loss。 1234567891011121314151617181920212223class Net(nn.Module): def __init__(self): super(Net, self).__init__() self.conv1 = nn.Conv2d(3, 6, 5) self.pool = nn.MaxPool2d(2,2) self.conv2 = nn.Conv2d(6, 16, 5) self.fc1 = nn.Linear(16*5*5, 120) self.fc2 = nn.Linear(120, 84) self.fc3 = nn.Linear(84, 10) def forward(self, x): x = self.pool(F.relu(self.conv1(x))) x = self.pool(F.relu(self.conv2(x))) x = x.view(-1, 16*5*5) x = F.relu(self.fc1(x)) x = F.relu(self.fc2(x)) x = self.fc3(x) return xnet = Net()# use a Classification Cross-Entropy losscriterion = nn.CrossEntropyLoss()optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9) 接下来，我们进行模型的训练。我们loop over整个dataset两次，对每个mini-batch进行参数的更新。并且设置每隔2000个mini-batch打印一次loss。 12345678910111213141516171819202122232425for epoch in range(2): # loop over the dataset multiple times running_loss = 0.0 for i, data in enumerate(trainloader, 0): # get the inputs inputs, labels = data # wrap them in Variable inputs, labels = Variable(inputs), Variable(labels) # zero the parameter gradients optimizer.zero_grad() # forward + backward + optimize outputs = net(inputs) loss = criterion(outputs, labels) loss.backward() optimizer.step() # print statistics running_loss += loss.data[0] if i % 2000 == 1999: # print every 2000 mini-batches print('[%d, %5d] loss: %.3f' % (epoch+1, i+1, running_loss / 2000)) running_loss = 0.0print('Finished Training') 我们在测试集上选取一个mini-batch（也就是4张，见上面testloader的定义），进行测试。 1234567891011dataiter = iter(testloader)images, labels = dataiter.next() # 得到image和对应的labeloutputs = net(Variable(images))# the outputs are energies for the 10 classes.# Higher the energy for a class, the more the network# thinks that the image is of the particular class# So, let's get the index of the highest energy_, predicted = torch.max(outputs.data, 1) # 找出分数最高的对应的channel，即为top-1类别print('Predicted: ', ' '.join('%5s'% classes[predicted[j][0]] for j in range(4))) 测试一下整个测试集合上的表现。 12345678910correct = 0total = 0for data in testloader: # 每一个test mini-batch images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) total += labels.size(0) correct += (predicted == labels).sum()print('Accuracy of the network on the 10000 test images: %d %%' % (100 * correct / total)) 对哪一类的预测精度更高呢？ 1234567891011class_correct = list(0. for i in range(10))class_total = list(0. for i in range(10))for data in testloader: images, labels = data outputs = net(Variable(images)) _, predicted = torch.max(outputs.data, 1) c = (predicted == labels).squeeze() for i in range(4): label = labels[i] class_correct[label] += c[i] class_total[label] += 1 上面这些训练和测试都是在CPU上进行的，如何迁移到GPU？很简单，同样用.cuda()方法就行了。 1net.cuda() 不过记得在每次训练测试的迭代中，images和label也要传送到GPU上才可以。 1inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda()) 更多的例子和教程更多的例子更多的教程]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Caffe中使用Baidu warpctc实现CTC Loss的计算]]></title>
      <url>%2F2017%2F02%2F22%2Fwarpctc-caffe%2F</url>
      <content type="text"><![CDATA[CTC(Connectionist Temporal Classification) Loss 函数多用于序列有监督学习，优点是不需要对齐输入数据及标签。本文内容并不涉及CTC Loss的原理介绍，而是关于如何在Caffe中移植Baidu美研院实现的warp-ctc，并利用其实现一个LSTM + CTC Loss的验证码识别demo。下面这张图引用自warp-ctc的项目页面。本文介绍内容的相关代码可以参见我的GitHub项目warpctc-caffe 移植warp-ctc本节介绍了如何将warp-ctc的源码在Caffe中进行编译。 首先，我们将warp-ctc的项目代码从GitHub上clone下来。在Caffe的include/caffe和src/caffe下分别创建名为3rdparty的文件夹，将warp-ctc中的头文件和实现文件分别放到对应的文件夹下。之后，我们需要对其代码和配置进行修改，才能在Caffe中顺利编译。 由于warp-ctc中使用了C++11的相关技术，所以需要修改Caffe的Makefile文件，添加C++11支持，可以参见Makefile。 对Caffe的修改就是这么简单，之后我们需要修改warp-ctc中的代码文件。这里的修改多且乱，边改边试着编译，所以可能其中也有不必要的修改。最后的目的就是能够使用GPU进行CTC Loss的计算。 warp-ctc提供了CPU多线程的计算，这里我直接将相应的openmp并行化语句删掉了。 另外，需要将warp-ctc中包含CUDA代码的相关头文件后缀名改为cuh，这样才能够通过编译。否则编译器会给出找不到__host__和__device__等等关键字的错误。 对于详细的修改配置，还请参见GitHub相应的代码文件。 实现CTC Loss计算编译没有问题后，我们可以编写ctc_loss_layer实现CTC Loss的计算。在实现时，注意参考文件ctc.h。这个文件中给出了使用warp-ctc进行CTC Loss计算的全部API接口。 ctc_loss_layer继承自loss_layer，主要是前向和反向计算的实现。由于warp-ctc中只对单精度浮点数float进行支持，所以，对于双精度网络参数，直接将其设置为NOT_IMPLEMENTED，如下所示。 1234567891011template &lt;&gt;void CtcLossLayer&lt;double&gt;::Forward_cpu( const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom, const vector&lt;Blob&lt;double&gt;*&gt;&amp; top) &#123; NOT_IMPLEMENTED;&#125;template &lt;&gt;void CtcLossLayer&lt;double&gt;::Backward_cpu(const vector&lt;Blob&lt;double&gt;*&gt;&amp; top, const vector&lt;bool&gt;&amp; propagate_down, const vector&lt;Blob&lt;double&gt;*&gt;&amp; bottom) &#123; NOT_IMPLEMENTED;&#125; 使用warp-ctc相关接口进行CTC Loss计算的步骤如下： 设置ctcOptions，指定使用CPU或GPU进行计算，并指定CPU计算的线程数和GPU计算的CUDA stream。 调用get_workspace_size()函数，预先为计算分配空间（分配的内存根据计算平台不同位于CPU或GPU）。 调用compute_ctc_loss()函数，计算loss和gradient。 其中，在第三步中计算gradient时，可以直接将对应blob的cpu/gpu_diff指针传入，作为gradient。 这部分的实现代码分别位于include/caffe/layers和src/caffe/layers/下。 验证码数字识别本部分相关代码位于examples/warpctc文件夹下。实验方案如下。 使用Python中的capycha进行包含0-9数字的验证码图片的产生，图片中数字个数从1到MAX_LEN不等。 使用10作为blank_label，将所有的标签序列在后面补blank_label以达到同样的长度MAX_LEN。 将图像的每一列看做一个time step，网络模型使用image data-&gt;2LSTM-&gt;fc-&gt;CTC Loss，简单粗暴。 模型训练过程中，数据输入使用HDF5格式。 数据产生使用captcha生成验证码图片。这里是一个简单的API demo。默认生成的图片大小为160x60。我们将其长宽缩小一半，使用80x30的彩色图片作为输入。 使用python中的h5py模块生成HDF5格式的数据文件。将全部图片分为两部分，80%作为训练集，20%作为验证集。 LSTM的输入在Caffe中已经有了lstm_layer的实现。lstm_layer要求输入的序列blob为TxNx...，也就是说我们需要将输入的image进行转置。 Caffe中Batch的内存布局顺序为NxCxHxW。我们将图像中的每一列作为一个time step输入的$x$向量。所以，在代码中使用了liuwei的SSD工作中实现的permute_layer进行转置，将W维度放到最前方。与之对应的参数定义如下： 123456789101112layer &#123; name: &quot;permuted_data&quot; type: &quot;Permute&quot; bottom: &quot;data&quot; top: &quot;permuted_data&quot; permute_param &#123; order: 3 # W order: 0 # N order: 1 # C order: 2 # H &#125;&#125; 另外，LSTM需要第二个输入，用于指示时序信号的起始位置。在代码中，我新加入了一个名为ContinuationIndicator的layer，产生对应的time indicator序列。 训练在某次试验中，迭代50,000次，实验过程中的损失函数变化如下： 在验证集上的精度变化如下： 最终模型的精度在98%左右。考虑到本实验只是简单堆叠了两层的LSTM，并使用CTC Loss进行训练，能够轻易达到这一精度，可以在一定程度上说明CTC Loss的强大。 至于该实验的具体细节，可以参考repo的相关具体代码实现。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-MeanShift]]></title>
      <url>%2F2017%2F02%2F12%2Fcs131-mean-shift%2F</url>
      <content type="text"><![CDATA[MeanShift最初由Fukunaga和Hostetler在1975年提出，但是一直到2000左右这篇PAMI的论文Mean Shift: A Robust Approach Toward Feature Space Analysis，将它的原理和收敛性等重新整理阐述，并应用于计算机视觉和图像处理领域之后，才逐渐为人熟知。 MeanShift是一种用来寻找特征空间内模态的方法。所谓模态（Mode），就是指数据集中最经常出现的数据。例如，连续随机变量概率密度函数的模态就是指函数的极大值。从概率的角度看，我们可以认为数据集（或者特征空间）内的数据点都是从某个概率分布中随机抽取出来的。这样，数据点越密集的地方就说明这里越有可能是密度函数的极大值。MeanShift就是一种能够从离散的抽样点中估计密度函数局部极大值的方法。 核密度估计上面提到的PAMI论文篇幅较长，且数学名词较多，我不是很理解。下面的说明过程主要参考了这篇博客和这篇讲义。 注意下面的推导中$x$是一个$d$维的向量。为了书写简单，没有写成向量的加粗形式。首先，先介绍核函数（Kernel Function）的概念。如果某个$\mathbb{R}^n\rightarrow \mathbb{R}$的函数满足以下条件，就能将其作为核函数。 比如高斯核函数： K(x) = \frac{1}{\sqrt{2\pi\sigma^2}}\exp(-\frac{x^2}{2\sigma^2})核密度估计是一种用来估计随机变量密度函数的非参数化方法。给定核函数$K$，带宽（bandwidth）参数$h$（指观察窗口的大小）。那么密度函数可以使用核函数进行估计，如下面的形式所示。其中，$n$是窗口内的数据点的数量。 f(x) = \frac{1}{nh^d}\sum_{i=1}^{n}K(\frac{x-x_i}{h})如果核函数是放射形状且对称的（例如高斯核函数），那么$K(x)$可以写成如下的形式，其中$c_{k,d}$是为了使得核函数满足积分为$1$的正则系数。 K(x) = c_{k,d}k(\Arrowvert x\Arrowvert ^2)mean shift向量那么，原来密度函数的极值点就是使得$f(x)$梯度为$0$的点。求取$f(x)$的梯度，可以得到下式。其中$g(s) = -k^\prime(s)$。 观察后可以发现，乘积第一项连带前面的系数，相当于是使用核函数$G(x) = c_{k,d}g(\Arrowvert x\Arrowvert^2)$得到的点$x$处的密度函数估计值（差了常数倍因子）（所以对于给定的某一点$x = x_0$，这一项是定值），后面一项拿出来，定义为$m_h(x)$，称为mean shift向量。 m_h(x) = \frac{\sum_{i=1}^{n}x_i g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}{\sum_{i=1}^{n}g(\Arrowvert \frac{x-x_i}{h} \Arrowvert^2)}-x所以说，$m_h(x)$的指向和$f(x)$的梯度是相同的。另外，从感性的角度出发。$m_h(x)$中的第一项（分式的那一大坨），相当于是在计算$x$周围窗口大小为$h$的这个领域的均值（$g(s)$是加权的系数）。所以$m_h(x)$实际上指示了局部均值和当前点$x$之间的位移。我们想要找到密度函数的局部极大值，不就应该让局部均值向着点密集的方向移动吗？ 算法流程所以，在给定初始位置$x_0$后，我们首先计算此点处的$m_h$，之后，将$x$沿着$m_h$移动即可。下面是一个简单的例子。数据点服从二维高斯分布，均值为$[1, 2]$。其中，红色菱形指示了迭代过程中mean shift的移动。 123456789101112131415161718192021222324252627282930313233343536373839%% generate datamu = [1 2];Sigma = [1 0; 0 2]; R = chol(Sigma);N = 250;data = repmat(mu, N, 1) + randn(N, 2)*R;figurehold onscatter(data(:, 1), data(:, 2), 50, 'filled');%% meanshiftmu0 = rand(1,2) * 5;mu = mean_shift(mu0, 10, data);function out = gaussian_kernel(x, sigma)% gauss kernel, g(x) = \exp(-x^2/2\sigma^2)out = exp(-x.*x/(2*sigma*sigma));endfunction mu = mean_shift(mu0, h, data)% implementation of meanshift algorithm% mu_&#123;k+1&#125; = meanshift(mu_&#123;k&#125;) + mu_&#123;k&#125; = \frac&#123;\sum_i=1^n xg&#125;&#123;\sum_i=1^n g&#125;mu = mu0;sigma = 1; % parameter for gaussian kernel functionfor iter = 1:20 fprintf('iter = %d, mu = [%f, %f]\n', iter, mu(1), mu(2)); scatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled'); offset = bsxfun(@minus, mu, data); % offset = x-x_i dis = sum(offset.^2, 2); % dis = ||x-x_i||^2 x = data(dis &lt; h, :); % neighborhood with bandwidth = h g = gaussian_kernel(offset(dis &lt; h), sigma); xg = x.*g; mu_prev = mu; mu = sum(xg, 1) / sum(g, 1); if norm(mu_prev - mu, 2) &lt; 1E-2 break; end plot([mu_prev(1) mu(1)], [mu_prev(2), mu(2)], 'b-.', 'linewidth', 2);endscatter(mu(1), mu(2), 50, [1,0,0], 'd', 'filled');end 同时，我也试验了其他的kernel函数，如神似logistaic形式，效果也是相似的。 K(x) = \frac{1}{e^x+e^{-x}+2}123function out = logistic_kernel(x)out = 1./(exp(x) + exp(-x) + 2);end]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在Ubuntu14.04构建Caffe]]></title>
      <url>%2F2017%2F02%2F09%2Fbuild-caffe-ubuntu%2F</url>
      <content type="text"><![CDATA[Caffe作为较早的一款深度学习框架，很是流行。然而，由于依赖项众多，而且Jia Yangqing已经毕业，所以留下了不少的坑。这篇博客记录了我在一台操作系统为Ubuntu14.04.3的DELL游匣7559笔记本上编译Caffe的过程，主要是在编译python接口时遇到的import error问题的解决和找不到HDF5链接库的问题。 修改Makefile.config当从github上clone下来Caffe的源码之后，我们首先需要修改Makefile.config文件，自定义配置。下面是我的配置文件，主要修改了CUDNN部分和Anaconda Python部分。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117## Refer to http://caffe.berkeleyvision.org/installation.html# Contributions simplifying and improving our build system are welcome!# cuDNN acceleration switch (uncomment to build with cuDNN).USE_CUDNN := 1 # 这里我们使用cudnn加速# CPU-only switch (uncomment to build without GPU support).# CPU_ONLY := 1# uncomment to disable IO dependencies and corresponding data layers# USE_OPENCV := 0# USE_LEVELDB := 0# USE_LMDB := 0# uncomment to allow MDB_NOLOCK when reading LMDB files (only if necessary)# You should not set this flag if you will be reading LMDBs with any# possibility of simultaneous read and write# ALLOW_LMDB_NOLOCK := 1# Uncomment if you're using OpenCV 3# OPENCV_VERSION := 3# To customize your choice of compiler, uncomment and set the following.# N.B. the default for Linux is g++ and the default for OSX is clang++# CUSTOM_CXX := g++# CUDA directory contains bin/ and lib/ directories that we need.CUDA_DIR := /usr/local/cuda# On Ubuntu 14.04, if cuda tools are installed via# "sudo apt-get install nvidia-cuda-toolkit" then use this instead:# CUDA_DIR := /usr# CUDA architecture setting: going with all of them.# For CUDA &lt; 6.0, comment the *_50 lines for compatibility.# 这里可以去掉sm_20和21，因为实在是已经太老了# 如果保留的话，编译时nvcc会给出警告CUDA_ARCH := -gencode arch=compute_30,code=sm_30 \ -gencode arch=compute_35,code=sm_35 \ -gencode arch=compute_50,code=sm_50 \ -gencode arch=compute_50,code=compute_50# BLAS choice:# atlas for ATLAS (default)# mkl for MKL# open for OpenBlasBLAS := atlas# Custom (MKL/ATLAS/OpenBLAS) include and lib directories.# Leave commented to accept the defaults for your choice of BLAS# (which should work)!# BLAS_INCLUDE := /path/to/your/blas# BLAS_LIB := /path/to/your/blas# Homebrew puts openblas in a directory that is not on the standard search path# BLAS_INCLUDE := $(shell brew --prefix openblas)/include# BLAS_LIB := $(shell brew --prefix openblas)/lib# This is required only if you will compile the matlab interface.# MATLAB directory should contain the mex binary in /bin.# MATLAB_DIR := /usr/local# MATLAB_DIR := /Applications/MATLAB_R2012b.app# NOTE: this is required only if you will compile the python interface.# We need to be able to find Python.h and numpy/arrayobject.h.PYTHON_INCLUDE := /usr/include/python2.7 \ /usr/lib/python2.7/dist-packages/numpy/core/include# Anaconda Python distribution is quite popular. Include path:# Verify anaconda location, sometimes it's in root.# 这里我们使用AnacondaANACONDA_HOME := $(HOME)/anaconda2 PYTHON_INCLUDE := $(ANACONDA_HOME)/include \ $(ANACONDA_HOME)/include/python2.7 \ $(ANACONDA_HOME)/lib/python2.7/site-packages/numpy/core/include# Uncomment to use Python 3 (default is Python 2)# PYTHON_LIBRARIES := boost_python3 python3.5m# PYTHON_INCLUDE := /usr/include/python3.5m \# /usr/lib/python3.5/dist-packages/numpy/core/include# We need to be able to find libpythonX.X.so or .dylib.#PYTHON_LIB := /usr/lib PYTHON_LIB := $(ANACONDA_HOME)/lib# Homebrew installs numpy in a non standard path (keg only)# PYTHON_INCLUDE += $(dir $(shell python -c 'import numpy.core; print(numpy.core.__file__)'))/include# PYTHON_LIB += $(shell brew --prefix numpy)/lib# Uncomment to support layers written in Python (will link against Python libs)WITH_PYTHON_LAYER := 1# Whatever else you find you need goes here.INCLUDE_DIRS := $(PYTHON_INCLUDE) /usr/local/includeLIBRARY_DIRS := $(PYTHON_LIB) /usr/local/lib /usr/lib# If Homebrew is installed at a non standard location (for example your home directory) and you use it for general dependencies# INCLUDE_DIRS += $(shell brew --prefix)/include# LIBRARY_DIRS += $(shell brew --prefix)/lib# NCCL acceleration switch (uncomment to build with NCCL)# https://github.com/NVIDIA/nccl (last tested version: v1.2.3-1+cuda8.0)# USE_NCCL := 1# Uncomment to use `pkg-config` to specify OpenCV library paths.# (Usually not necessary -- OpenCV libraries are normally installed in one of the above $LIBRARY_DIRS.)# USE_PKG_CONFIG := 1# N.B. both build and distribute dirs are cleared on `make clean`BUILD_DIR := buildDISTRIBUTE_DIR := distribute# Uncomment for debugging. Does not work on OSX due to https://github.com/BVLC/caffe/issues/171# DEBUG := 1# The ID of the GPU that 'make runtest' will use to run unit tests.TEST_GPUID := 0# enable pretty build (comment to see full commands)Q ?= @ 对于CUDNN，从Nvidia官方网站上下载后，可不按照官方给定的方法安装。直接将include中的头文件放于/usr/local/cuda-8.0/include下，将lib中的库文件放于/usr/loca/cuda-8.0/lib64文件夹下即可。 构建使用make -j8进行编译，并使用make pycaffe生成python接口。并在.bashrc中添加内容：1export PYTHONPATH=/path_to_caffe/python:$PYTHONPATH 结果在import caffe时出现问题如下：1ImportError: libcudnn.so.5: cannot open shared object file: No such file or directory 解决方法如下，详见GitHub issue讨论。1sudo ldconfig /usr/local/cuda/lib64 然而仍有问题，如下：1ImportError: No module named google.protobuf.internal 解决方法如下，详见G+ caffe-user group的帖子。1pip install protobuf 不过仍然存在的问题是远程SSH登录时，不能在ipython环境下导入caffe，不知为何。 使用make test; make runtest进行测试，结果提示HDF5动态链接库出现问题，怀疑与Anaconda中的HDF5冲突有关。错误信息如下： 1error while loading shared libraries: libhdf5_hl.so.10: cannot open shared object file: No such file or directory 解决方法为手动添加符号链接，详见GitHub讨论帖。 123cd /usr/lib/x86_64-linux-gnusudo ln -s libhdf5.so.7 libhdf5.so.10sudo ln -s libhdf5_hl.so.7 libhdf5_hl.so.10 另外，在另一台机器上使用MKL库时，发现会提示找不到相关动态链接库的问题。找到MKL的安装位置，默认应该在目录/opt/intel/mkl下。使用sudo权限，在目录/etc/ld.so.conf.d/下建立一个名为intel_mkl_setttings.conf的文件，将MKL安装位置下的链接库目录添加进去，如下所示：1/opt/intel/mkl/lib/intel64_lin/ 接着，运行sudo ldconfig命令，就可以了。 测试首先，通过make runtest看是否全部test可以通过。其次，可以试运行example下的LeNet训练。1234cd $CAFFE_ROOT./data/mnist/get_mnist.sh./examples/mnist/create_mnist.sh./examples/mnist/train_lenet.sh]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[在DigitalOcean上配置Shadowsocks实现IPV4/IPV6翻墙]]></title>
      <url>%2F2017%2F02%2F08%2Fdigitalocean-shadowsocks%2F</url>
      <content type="text"><![CDATA[之前我使用GoAgent来FQ，后来又使用了一段时间的免费SS服务，后来机缘巧合从一个印度佬那里挣了一些美元，所以搬到了DigitalOcean上。DO的机器，对我一个学生来说，说实话并不算便宜了，不过好在手里还有一些美元，也在GitHub那里进行了学生认证，算是也可以应付了。 之前我已经在DO的机器上配置过shadowsocks，还顺手给iPad解决了FQ的问题。然而当时没有记录，这次我换了一台机器，机房位于NY，把ss重新配置了一遍，再不做些记录，下次恐怕又要东翻西找。 申请机器在进行GitHub学生认证后，可以使用它发给的一个优惠码注册DO，不过仍然要绑定自己的PayPal账户，先交上五美金。之后，DO会赠送50刀。 申请机器时，直接选择Ubuntu 16.04操作系统，并勾选IPV6 Enable，省去后面的麻烦。 安装ss远程登录后，我们需要安装ss。安装命令很简单。12apt-get install python-pippip install shadowsocks 然而，在安装时，我遇到了一个奇怪的问题，提示我unsupported locale setting，后来搜索得知，是语言配置的问题，见这篇博文，解决办法如下：1export LC_ALL=C 编辑配置文件之后，进入/etc目录，建立一个名叫shadowsocks.json的文件（文件名任意，一会对应即可），文件配置内容如下：123456789&#123;&quot;server&quot;:&quot;::&quot;, &quot;server_port&quot;:8388,&quot;local_address&quot;: &quot;127.0.0.1&quot;,&quot;local_port&quot;: 1080,&quot;password&quot;:&quot;your_password（任写）&quot;,&quot;timeout&quot;:600,&quot;method&quot;:&quot;aes-256-cfb&quot;&#125; 其中第一行写成::即是为了IPV6连接。 编辑启动项，设置自动启动之后，我们编辑启动项配置文件，使得ss服务能够开机自动启动。 编辑/etc/rc.local文件，在exit 0之前添加如下命令。1ssserver -c /etc/shadowsocks.json -d start # 这里的json文件名要相对应 之后，使用reboot命令重启即可。 客户端配置客户端使用ss，编辑服务器，按照json文件中的内容填写即可。注意密码相对应。 IOS平台设置终于把Pad上的翻墙搞定了。。。参考资料为GitHub的相关页面，基本为傻瓜式操作。 IPsec VPN 服务器一键安装脚本 配置 IPsec/L2TP VPN 客户端 首先，使用如下命令在VPN服务器上搭建IPsec服务： 1wget https://git.io/vpnsetup -O vpnsetup.sh &amp;&amp; sudo sh vpnsetup.sh 然后按照下面的步骤在IOS平台上进行设置。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-KMeans聚类]]></title>
      <url>%2F2017%2F02%2F05%2Fcs131-kmeans%2F</url>
      <content type="text"><![CDATA[K-Means聚类是把n个点（可以是样本的一次观察或一个实例）划分到$k$个聚类中，使得每个点都属于离他最近的均值（此即聚类中心）对应的聚类，以之作为聚类的标准，满足最小化聚类中心和属于该中心的数据点之间的平方距离和（Sum of Square Distance，SSD）。 \text{SSD} = \sum_{i=1}^{k}\sum_{x\in c_i}(x-c_i)^2 目标函数K-Means方法实际上需要确定两个参数，$c^\ast$和$\delta^\ast$。其中$c_{i}^\ast$代表各个聚类中心的位置，$\delta_{ij}^\ast$的取值为$\lbrace 0,1\rbrace$，代表点$x_j$是否被分到了第$i$个聚类中心。 那么，目标函数可以写成如下的形式。 c^\ast, \delta^\ast = \arg\min_{c,\delta} \frac{1}{N}\sum_{j=1}^{N}\sum_{i=1}^{k}\delta_{i,j}(c_i-x_j)^2然而这样一来，造成了一种很尴尬的局面。一方面，要想优化$c_i$，需要我们给定每个点所属的类；另一方面，优化$\delta_{i,j}$，需要我们给定聚类中心。由于这两个优化变量之间是纠缠在一起的，K-Means算法如何能够达到收敛就变成了一个讨论“先有鸡还是先有蛋”的问题了。 实际使用中，K-Means的迭代过程，实际是EM算法的一个特例。 算法流程K-Means算法的流程如下所示。 假设我们有$N$个样本点，$\lbrace x_1, \dots, x_N\rbrace, x_i\in\mathbb{R}^D$，并给出聚类数目$k$。 首先，随机选取一系列的聚类中心点$\mu_i, i = 1,\dots, k$。接着，我们按照距离最近的原则为每个数据点指定相应的聚类中心并计算新的数据点均值更新聚类中心。如此这般，直到收敛。 算法细节初始化上面的K-Means方法对初始值很敏感。朴素的初始化方法是直接在数据点中随机抽取$k$个作为聚类初始中心点。不够好的初始值可能造成收敛速度很慢或者聚类失败。下面介绍两种改进方法。 kmeans++方法，尽可能地使初始聚类中心分散开（spread-out）。这种方法首先随机产生一个初始聚类中心点，然后按照概率$P = \omega(x-c_i)^2$选取其他的聚类中心点。其中$\omega$是归一化系数。 多次初始化，保留最好的结果。 K值的选取在上面的讨论中，我们一直把$k$当做已经给定的参数。但是在实际操作中，聚类类别数通常都是未知的。如何确定超参数$k$呢？ 我们可以使用不同的$k$值进行试验，并作出不同试验收敛后目标函数随$k$的变化。如下图所示，分别使用$1$到$4$之间的几个数字作为$k$值，曲线在$k=2$处有一个较为明显的弯曲点（elbow point）。故确定$k$取值为2。 距离的度量目标函数是各个cluster中的点与其中心点的距离均值。其中就涉及到了“距离”的量度。下面是几种较为常用的距离度量方法。 欧几里得距离（最为常用） 余弦距离（向量的夹角） 核函数（Kernel K-Means） 迭代终止条件当判断算法已经收敛时，退出迭代。常用的迭代终止条件如下： 达到了预先给定的最大迭代次数 在将数据点assign到不同聚类中心时，和上轮结果没有变化（说明已经收敛） 目标函数（平均的距离）下降小于阈值 基于K-Means的图像分割图像分割中可以使用K-Means算法。我们首先将一副图像转换为特征空间（Feature Space）。例如使用灰度或者RGB的值作为特征向量，就得到了1D或者3D的特征空间。之后，可以基于Feature Space上各点的相似度，将它们聚类。这样，就完成了对原始图像的分割操作。如下所示。 然而，上述方法有一个缺点。那就是在真实图像中，属于同一个物体的像素点常常在空间上是相关的（Spatial Coherent）。而上述方法没有考虑到这一点。我们可以根据颜色亮度和空间上的距离构建Feature Space，获得更加合理的分割效果。 在2012年PAMI上有一篇文章SLIC Superpixels Compared to State-of-the-art Superpixel Methods介绍了使用K-Means进行超像素分割的相关内容，可以参考。这里不再赘述了。 优点和不足作为非监督学习中的经典算法，K-Means的一大优点便是实现简单，速度较快，最小化条件方差（conditional variance，good represention of data，这里没有具体解释，不太懂）。 它的缺点主要有： 对outlier比较敏感。如下图所示。由于outlier的存在，右边的cluster硬是被拉长了。这方面，K-Medians更有鲁棒性。 每个点只能确定地属于或不属于某一个cluster。可以参考高斯混合模型和Fuzzy K-Means的soft assignment。 在round shape情况下性能比较好（想想圆形和方形各点的欧几里得距离）。而且每个cluster最好数据点数目和分布的密度相近。 如果数据点有非凸形状分布，算法表现糟糕。可以参考谱聚类（Spectral Clustering）或者kernel K-Means。 针对K-Means，也有不少相关改进工作，参考下面这幅图吧。 MATLAB实验下面是我自己写的简单的K-Means demo。首先，在数据产生环节，确定$K = 3$个cluster的中心位置，并随机产生$N$个数据点，并使用scatter函数做出散点图。 代码中的主要部分为my_kmeans函数的实现（为了不与内建的kmeans函数重名，故加上了my前缀）。在此函数内部，首先随机产生$K$个聚类中心，并对数据点进行初始的指定。接着，在迭代过程中，不断计算均值来更新聚类中心和assignment，当达到最大迭代次数或者相邻两次迭代中assignment的值不再变化为止，并计算对应的目标函数$J$的值。 注意到该函数初始确定聚类中心时有一段注释的代码，该段代码用于debug时，指定聚类中心，去除随机性，以验证后续K-Means迭代的正确性。 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119%% generate dataK = 3; % number of clusterspos = [-5, 5; 0, 1; 3, 6]; % position of cluster centersN = 20; % number of data pointsR = 3; % radius of clustersdata = zeros(N, 2); % dataclass = zeros(N, 1); % index of clusterfor i = 1:N idx = randi(3, 1); dr = R*rand(); data(i, :) = pos(idx, :) + [dr*cos(rand()*2*pi), dr*sin(rand()*2*pi)]; class(i) = idx;end%% visualization data pointsfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];for i = 1:K x = data(class == i, 1); y = data(class == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');end%% K-Meansbest_J = 1E100;best_idx = 0;for times = 1:5 % 5 times experiments to choose the best result [mu, assignment, J] = my_kmeans(data, K); if best_J &gt; J best_idx = times; best_J = J; end fprintf('%d experiment: J = %f\n', times, J); disp(mu);endfprintf('best: %d experiment: J = %f\n', best_idx, best_J);%% basic functionsfunction J = ssd(X, mu, assignment)% sum of square distance% X -- data, N*D matrix% mu -- centers of clusters, K*D matrix% assignment -- current assignment of data to clustersJ = 0;K = size(mu, 1);for k = 1:K x_k = X(assignment == k, :); mu_k = mu(k, :); err2 = bsxfun(@minus, x_k, mu_k).^2; J = J + sum(err2(:));endJ = J / size(X, 1);endfunction mu = compute_mu(X, assignment, K)mu = zeros(K, size(X, 2));for k = 1:K x_k = X(assignment == k, :); mu(k, :) = mean(x_k, 1);endendfunction assignment = assign(X, mu)% assign data points to clustersN = size(X, 1);assignment = zeros(N, 1);for i = 1:N x = X(i, :); err2 = bsxfun(@minus, x, mu).^2; dis = sum(err2, 2); [~, idx] = min(dis); assignment(i) = idx;endendfunction [mu, assignment, J] = my_kmeans(X, K)N = size(X, 1);assignment = zeros(N, 1);idx = randsample(N, K);mu = X(idx, :);% for i = 1:K% for j = 1:N% if assignment_gt(j) == i% mu(i,:) = X(j,:);% break;% end% end% endfigurehold oncolor = [1,0,0; 0,1,0; 0,0,1];scatter(mu(:,1), mu(:,2), 200, color, 'd');for iter = 1:20 assignment_prev = assignment; assignment = assign(X, mu); if assignment == assignment_prev break; end mu_prev = mu; mu = compute_mu(X, assignment, K); scatter(mu(:, 1), mu(:, 2), 200, color, 'd'); MU = zeros(2*K, 2); MU(1:2:end, :) = mu_prev; MU(2:2:end, :) = mu; mu_x = reshape(MU(:, 1), [], K); mu_y = reshape(MU(:, 2), [], K); plot(mu_x, mu_y, 'k-.');endfor i = 1:K x = X(assignment == i, 1); y = X(assignment == i, 2); scatter(x, y, 150, repmat(color(i,:), [length(x), 1]), 'filled');endJ = ssd(X, mu, assignment);end 在demo中，随机选取初始化的聚类中心，重复进行$5$次实验。下图是产生的原始数据的散点图。用不同的颜色标明了cluster。 下图是某次的实验结果，其中菱形和它们之间的连线指示了聚类中心的变化。注意，颜色只是用来区别不同的cluster，和上图并没有对应关系。可以看到，初始化时绿色标注的cluster的中心（也即是上图中的红色cluster）虽然偏离了很远，但是在迭代过程中也能实现纠正。 再换个大点的数据集来做，效果貌似还不错~ PS这里插一句与本文内容完全无关的一个tip：在Markdown中使用LaTex时，如果在内联形式（也就是行内使用两个美元符号插入公式）时，如果有多个下划线，那么Markdown有时会将下划线之间的部分认为是自己的斜体标记，如下所示：1$c_&#123;i&#125;^\ast$ XXX $\delta_&#123;ij&#125;^\ast$ 它的显示效果为$c{i}^\ast$ XXX $\delta{ij}^\ast$。 这时候，只需在下划线前加入反斜线进行转义即可（虽然略显麻烦）。如下所示：1$c\_&#123;i&#125;^\ast$ XXX $\delta\_&#123;ij&#125;^\ast$ 它的显示效果为$c_{i}^\ast$ XXX $\delta_{ij}^\ast$。 具体分析可以参见博客。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[B站视频“线性代数的本质”观后感]]></title>
      <url>%2F2017%2F02%2F05%2Fvideo-linear-alg-essential-property%2F</url>
      <content type="text"><![CDATA[线性代数对于现代科技的重要性不言而喻，在机器学习领域也是重要的工具。最近，在B站上发现了这样的一个视频：线性代数的本质。这个视频共有十集左右，每集大约10分钟，从线性空间入手，介绍线性变换，并由此介绍矩阵的相关性质，动画做的很棒，翻译得也很好，用了几天时间零散地把几集视频看完，记录一些心得。 从线性空间和线性变换讲起BIT的教学水平，不同的老师参差不齐。所幸的是，大一教授线性代数一课的吴惠彬老师，是一位既有深厚学问又善于教课的老师。我们所用的教材，正是吴老师主编的。和这个视频不同，教材是以线性方程组入手来介绍矩阵，而后引入到线性空间和线性变换。吴老师还曾经告诫大家，线性空间和线性变换一章较为抽象晦涩，不易理解。当然，很多年过去了，我也在研究生阶段继续修了线性代数的深入课程，如今想来，其实线性变换和矩阵的联系更为紧密一些。但是我们的教材之所以不用线性变换为引子，可能正是由于对于大一新生来说，过去十二年所受的基础教育都是操作一些看得见摸得着的数和几何体，一时难以接受线性空间这种东西吧~而也正是从这门课和高数开始，思考问题的方式和初等数学不一样了。我们更多地考察运动和极限，将具有共性的东西抽象出来，研究它们的等价性质。 而抽象这个问题，在本视频中，通过将问题聚焦在二维（偶有提及三维）平面并辅之以动画的形式避开了（所以这个视频也只能算是有趣的不严肃的科普性质）。视频从线性空间入手，首先介绍了向量和基的概念，而后引入出贯穿视频系列始终的线性变换。什么是线性变换呢？从“数”的角度来看，定义在线性空间上，满足叠加性和齐次性就是线性变换。而本视频则从“形”的角度出发（注意，这也是本视频贯穿始终的指导思想：用“形”来思考），给出了下面两个条件： 变换前后原点不动 变换前后平行等距线仍然保持平行等距（但是距离值可能会变）。 线性变换与矩阵的关系视频在阐述线性变换和矩阵关系的时候一带而过，不是很清楚（所以还是要去看严肃的教材啊）。下面是我写的一个补充说明。这也是整个视频系列的基础。 在由一组基向量$\alpha_i, i = 1,2,\dots,n$张成的线性空间$\mathcal{V}$上，任何一个向量$v$都可以表示为这组基的线性组合，也就是 v = \sum_{i=1}^{n}k_i\alpha_i则线性变换$\mathcal{T}$对$v$作用之后，有， u = \mathcal{T}(v) = \mathcal{T}(\sum_{i=1}^{n}k_i\alpha_i)根据线性变换的叠加性，有， u = \sum_{i=1}^{n}k_i\mathcal{T}(\alpha_i)设$\alpha_i$经过线性变换$\mathcal{T}$作用后，变换为$\beta_i$，那么， u = \sum_{i=1}^{n}k_i\beta_i也就是说， u = \begin{bmatrix}\mathcal{T}(\alpha_1), \mathcal{T}(\alpha_2), \cdots, \mathcal{T}(\alpha_n)\end{bmatrix} \begin{bmatrix}k_1\\\\ k_2\\\\ \vdots\\\\ k_n\end{bmatrix}上式说明，一个抽象的线性变换可以使用一个具体的矩阵来代表。这个矩阵的列，就是线性变换之后的那组基。 举个例子，旋转变换。如果旋转$\frac{\pi}{4}$，那么原来坐标轴方向的单位向量的$i$和$j$分别转到了$(\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$和$(-\frac{\sqrt{2}}{2},\frac{\sqrt{2}}{2})$。所以描述这个线性变换的矩阵为 A = \begin{bmatrix}\frac{\sqrt{2}}{2} & -\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}& \frac{\sqrt{2}}{2}\end{bmatrix}矩阵$A$的两列分别为变换后的基向量坐标。 矩阵乘法那么矩阵乘法的意义也就有了，那就是对向量做线性变换。例如上面的矩阵$A$与$x = \begin{bmatrix}-1\\ 0\end{bmatrix}$相乘，就是求取向量$-1i+0j$在旋转变换之后的新坐标。这时候，$-1$和$0$就是上面推导过程中的$k_1$和$k_2$，所以， Ax = -1\begin{bmatrix}\frac{\sqrt{2}}{2}\\\\ \frac{\sqrt{2}}{2}\end{bmatrix} + 0\begin{bmatrix}-\frac{\sqrt{2}}{2}\\\\\frac{\sqrt{2}}{2}\end{bmatrix}而矩阵与矩阵相乘呢？只要把右边的矩阵按列分块即可。其实这是矩阵与多个向量相乘的简写形式。 所以，对于任意向量$x$，$Ax$的结果就是组成矩阵$A$的各列向量的线性组合。这个由矩阵$A$的各列张成的空间叫做矩阵的列空间。 而那些使得方程$Ax=0$成立的$x$，张成的空间为矩阵的核空间。 矩阵的秩的意义就是矩阵列空间的维数。 同时，从这个角度看来，解决多元线性方程组的过程就变成了这样一个问题：即给定代表线性变换的矩阵以及变换后的向量，求解变换前向量。这个转换如下所示： 既然矩阵代表了某种线性变换，那么很自然的，可以想到，我们可以求取这个线性变换的逆变换，这个逆变换作用到$v$上，就可以得到原始的向量$x$了（而且这样的向量只有一个）！自然，这个逆变换也是有矩阵与其对应的，这个矩阵就是原矩阵的逆矩阵。那么是不是所有的矩阵都有逆矩阵呢？我们可以通过行列式来分析。 行列式仍然考虑二维矩阵，我们已经知道它表示了二维平面上的线性变换。而矩阵行列式的意义，就表明了变换前后，单位面积的正方形被放缩的比例。如果变换之后，$i$向量跑到了$j$向量的左手边，那么需要加上负号，表明在这个变换过程中，二维平面其实进过了一次翻转。 我们已经知道，行列式为$0$的矩阵实际对线性空间进行了“降维打击”。以二维平面举例，变换之后变成了一条直线（甚至变成了一个点），也就是说对于任意给定的一个变换后向量，有这样两种情况： 当这个向量不在这条直线上的时候，说明没有原始向量与其对应（否则矛盾），此时原方程组无解。 当这个向量在这条直线上的时候，说明很多的原始向量（而且必然是无穷多）与其对应，此时原方程组有无穷多组解。 你不能将一条直线“解压缩”为原始平面，所以行列式为$0$的矩阵，不存在逆矩阵。 点积叉积和对偶性这部分的对偶性这个概念比较玄一些，这里就不讲了。。。对于向量的点积，我们已经知道， \langle v, u \rangle = \sum_{i=1}^{n}v_iu_i从几何的角度看，向量的点积是将向量$u$向向量$v$的方向投影（有正负），投影长度乘上$v$的长度得到的。 按照上面矩阵乘法的思路，$u$可以看做代表一个从2维到1维的线性变换（也就是从$\mathbb{R}^2$到$\mathbb{R}$）。而根据上述矩阵变换和矩阵乘法的对应关系可知，变换后的结果就等于$v$的两个分量分别乘上两个基向量（也就是$u$的两个分量），这样我们就得到了向量点积的数值计算方法。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[YOLO 论文阅读]]></title>
      <url>%2F2017%2F02%2F04%2Fyolo-paper%2F</url>
      <content type="text"><![CDATA[YOLO(You Only Look Once)是一个流行的目标检测方法，和Faster RCNN等state of the art方法比起来，主打检测速度快。截止到目前为止（2017年2月初），YOLO已经发布了两个版本，在下文中分别称为YOLO V1和YOLO V2。YOLO V2的代码目前作为Darknet的一部分开源在GitHub。在这篇博客中，记录了阅读YOLO两个版本论文中的重点内容，并着重总结V2版本的改进。 Update@2018/04: YOLO v3已经发布！可以参考我的博客论文 - YOLO v3。 YOLO V1这里不妨把YOLO V1论文“You Only Look Once: Unitied, Real-Time Object Detection”的摘要部分意译如下： 我们提出了一种新的物体检测方法YOLO。之前的目标检测方法大多把分类器重新调整用于检测（这里应该是在说以前的方法大多将检测问题看做分类问题，使用滑动窗提取特征并使用分类器进行分类）。我们将检测问题看做回归，分别给出bounding box的位置和对应的类别概率。对于给定输入图像，只需使用CNN网络计算一次，就同时给出bounding box位置和类别概率。由于整个pipeline都是同一个网络，所以很容易进行端到端的训练。YOLO相当快。base model可以达到45fps，一个更小的model（Fast YOLO），可以达到155fps，同时mAP是其他可以实现实时检测方法的两倍。和目前的State of the art方法相比，YOLO的localization误差较大，但是对于背景有更低的误检（False Positives）。同时，YOLO能够学习到更加泛化的特征，例如艺术品中物体的检测等任务，表现很好。 和以往较为常见的方法，如HoG+SVM的行人检测，DPM，RCNN系方法等不同，YOLO接受image作为输入，直接输出bounding box的位置和对应位置为某一类的概率。我们可以把它和目前的State of the art的Faster RCNN方法对比。Faster RCNN方法需要两个网络RPN和Fast RCNN，其中前者负责接受图像输入，并提出proposal。后续Fast RCNN再给出这些proposal是某一类的概率。也正是因为这种“直来直去”的方法，YOLO才能达到这么快的速度。也真是令人感叹，网络参数够多，数据够多，什么映射关系都能学出来。。。下图就是YOLO的检测系统示意图。在test阶段，经过单个CNN网络前向计算后，再经过非极大值抑制，就可以给出检测结果。 基本思路 网格划分：将输入image划分为$S \times S$个grid cell，如果image中某个object box的中心落在某个grid cell内部，那么这个cell就对检测该object负责（responsible for detection that object）。同时，每个grid cell同时预测$B$个bounding box的位置和一个置信度。这个置信度并不只是该bounding box是待检测目标的概率，而是该bounding box是待检测目标的概率乘上该bounding box和真实位置的IoU的积。通过乘上这个交并比，反映出该bounding box预测位置的精度。如下式所示： \text{confidence} = P(\text{Object})\times \text{IoU}_{\text{pred}}^{\text{truth}} 网络输出：每个bounding box对应于5个输出，分别是$x,y,w,h$和上述提到的置信度。其中，$x,y$代表bounding box的中心离开其所在grid cell边界的偏移。$w,h$代表bounding box真实宽高相对于整幅图像的比例。$x,y,w,h$这几个参数都已经被bounded到了区间$[0,1]$上。除此以外，每个grid cell还产生$C$个条件概率，$P(\text{Class}_i|\text{Object})$。注意，我们不管$B$的大小，每个grid cell只产生一组这样的概率。在test的非极大值抑制阶段，对于每个bounding box，我们应该按照下式衡量该框是否应该予以保留。 \text{confidence}\times P(\text{Class}_i|\text{Object}) = P(\text{Class}_i)\times \text{IoU}_{\text{pred}}^{\text{truth}} 实际参数：在PASCAL VOC进行测试时，使用$S = 7$, $B=2$。由于共有20类，故$C=20$。所以，我们的网络输出大小为$7\times 7 \times 30$ 网络模型结构Inspired by GoogLeNet，但是没有采取inception的结构，simply使用了$1\times 1$的卷积核。base model共有24个卷积层，后面接2个全连接层，如下图所示。 另外，Fast YOLO使用了更小的网络结构（9个卷积层，且filter的数目也少了），其他部分完全一样。 训练同很多人一样，这里作者也是先在ImageNet上做了预训练。使用上图网络结构中的前20个卷积层，后面接一个average-pooling层和全连接层，在ImageNet 1000类数据集上训练了一周，达到了88%的top-5准确率。 由于Ren的论文提到给预训练的模型加上额外的卷积层和全连接层能够提升性能，所以我们加上了剩下的4个卷积层和2个全连接层，权值为随机初始化。同时，我们把网络输入的分辨率从$224\times 224$提升到了$448 \times 448$。 在最后一层，我们使用了线性激活函数；其它层使用了leaky ReLU激活函数。如下所示： f(x)= \begin{cases} x, &\text{if}\ x > 0 \\\\ 0.1x, &\text{otherwise} \end{cases}很重要的问题就是定义很好的loss function，为此，作者提出了以下几点说明： loss的形式采用误差平方和的形式（真是把回归进行到底了。。。） 由于很多的grid cell没有目标物体存在，所以给有目标存在的bounding box和没有目标存在的bounding box设置了不同的比例因子进行平衡。具体来说，\lambda_{\text{coord}} = 5，\lambda_{\text{noobj}} = 0.5 直接使用$w$和$h$，这样大的box的差异在loss中占得比重会和小的box不平衡，所以这里使用$\sqrt{w}$和$\sqrt{h}$。 上文中已经提到，同一个grid cell会提出多个bounding box。在training阶段，我们只想让一个bounding box对应object。所以，我们计算每个bounding box和ground truth的IoU，以此为标准得到最好的那个bounding box，其他的认为no obj。 loss函数的具体形式见下图（实在是不想打这一大串公式。。）。其中，$\mathbb{1}_i$表示是否有目标出现在第$i$个grid cell。 $\mathbb{1}_{i,j}$表示第$i$个grid cell的第$j$个bounding box是否对某个目标负责。 在进行反向传播时，由于loss都是二次形式，所以导数形式都是统一的。下面是Darknet中detection_layer.c中训练部分的代码。在代码中，计算了cost（也就是loss）和delta（也就是反向的导数）， 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105106107108109110111112113114115116117118119120121if(state.train)&#123; float avg_iou = 0; float avg_cat = 0; float avg_allcat = 0; float avg_obj = 0; float avg_anyobj = 0; int count = 0; *(l.cost) = 0; int size = l.inputs * l.batch; memset(l.delta, 0, size * sizeof(float)); for (b = 0; b &lt; l.batch; ++b)&#123; int index = b*l.inputs; // for each grid cell for (i = 0; i &lt; locations; ++i) &#123; // locations = S * S = 49 int truth_index = (b*locations + i)*(1+l.coords+l.classes); int is_obj = state.truth[truth_index]; // for each bbox for (j = 0; j &lt; l.n; ++j) &#123; // l.n = B = 2 int p_index = index + locations*l.classes + i*l.n + j; l.delta[p_index] = l.noobject_scale*(0 - l.output[p_index]); // 因为no obj对应的bbox很多，而responsible的只有一个 // 这里统一加上，如果一会判断该bbox responsible for object，再把它减去 *(l.cost) += l.noobject_scale*pow(l.output[p_index], 2); avg_anyobj += l.output[p_index]; &#125; int best_index = -1; float best_iou = 0; float best_rmse = 20; // 该grid cell没有目标，直接返回 if (!is_obj)&#123; continue; &#125; // 否则，找出responsible的bounding box，计算其他几项的loss int class_index = index + i*l.classes; for(j = 0; j &lt; l.classes; ++j) &#123; l.delta[class_index+j] = l.class_scale * (state.truth[truth_index+1+j] - l.output[class_index+j]); *(l.cost) += l.class_scale * pow(state.truth[truth_index+1+j] - l.output[class_index+j], 2); if(state.truth[truth_index + 1 + j]) avg_cat += l.output[class_index+j]; avg_allcat += l.output[class_index+j]; &#125; box truth = float_to_box(state.truth + truth_index + 1 + l.classes); truth.x /= l.side; truth.y /= l.side; // 找到最好的IoU，对应的bbox是responsible的，记录其index for(j = 0; j &lt; l.n; ++j)&#123; int box_index = index + locations*(l.classes + l.n) + (i*l.n + j) * l.coords; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt)&#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //iou = 0; float rmse = box_rmse(out, truth); if(best_iou &gt; 0 || iou &gt; 0)&#123; if(iou &gt; best_iou)&#123; best_iou = iou; best_index = j; &#125; &#125;else&#123; if(rmse &lt; best_rmse)&#123; best_rmse = rmse; best_index = j; &#125; &#125; &#125; if(l.forced)&#123; if(truth.w*truth.h &lt; .1)&#123; best_index = 1; &#125;else&#123; best_index = 0; &#125; &#125; if(l.random &amp;&amp; *(state.net.seen) &lt; 64000)&#123; best_index = rand()%l.n; &#125; int box_index = index + locations*(l.classes + l.n) + (i*l.n + best_index) * l.coords; int tbox_index = truth_index + 1 + l.classes; box out = float_to_box(l.output + box_index); out.x /= l.side; out.y /= l.side; if (l.sqrt) &#123; out.w = out.w*out.w; out.h = out.h*out.h; &#125; float iou = box_iou(out, truth); //printf("%d,", best_index); int p_index = index + locations*l.classes + i*l.n + best_index; *(l.cost) -= l.noobject_scale * pow(l.output[p_index], 2); // 还记得我们曾经统一加过吗？这里需要减去了 *(l.cost) += l.object_scale * pow(1-l.output[p_index], 2); avg_obj += l.output[p_index]; l.delta[p_index] = l.object_scale * (1.-l.output[p_index]); if(l.rescore)&#123; l.delta[p_index] = l.object_scale * (iou - l.output[p_index]); &#125; l.delta[box_index+0] = l.coord_scale*(state.truth[tbox_index + 0] - l.output[box_index + 0]); l.delta[box_index+1] = l.coord_scale*(state.truth[tbox_index + 1] - l.output[box_index + 1]); l.delta[box_index+2] = l.coord_scale*(state.truth[tbox_index + 2] - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(state.truth[tbox_index + 3] - l.output[box_index + 3]); if(l.sqrt)&#123; l.delta[box_index+2] = l.coord_scale*(sqrt(state.truth[tbox_index + 2]) - l.output[box_index + 2]); l.delta[box_index+3] = l.coord_scale*(sqrt(state.truth[tbox_index + 3]) - l.output[box_index + 3]); &#125; *(l.cost) += pow(1-iou, 2); avg_iou += iou; ++count; &#125; &#125; YOLO V2YOLO V2是原作者在V1基础上做出改进后提出的。为了达到题目中所称的Better，Faster，Stronger的目标，主要改进点如下。当然，具体内容还是要深入论文。 受到Faster RCNN方法的启发，引入了anchor。同时使用了K-Means方法，对anchor数量进行了讨论，在精度和速度之间做出折中； 修改了网络结构，去掉了全连接层，改成了全卷积结构； 引入了WordTree结构，将检测和分类问题做成了一个统一的框架，并充分利用ImageNet和COCO数据集的数据。 下面，还是先把论文的摘要意译如下： 我们引入了YOLO 9000模型，它是实时物体检测的State of the art的工作，能够检测超过9K类目标。首先，我们对以前的工作（YOLO V1）做出了若干改进，使其成为了一种在实时检测方法内在PASCAL VOC和COCO上State of the art的效果。通过一种新颖的多尺度训练犯法（multi-scale training method）， YOLO V2适用于不同大小尺寸输入的image，在精度和效率上达到了很好地trade-off。在67fps的速度下，VOC2007上达到了76.8mAP的精度。40fps时，达到了78.6mAP，已经超过了Faster RCNN with ResNet和SSD的精度，同时比它们更快！最后，我们提出了一种能够同时进行检测任务和分类任务的联合训练方法。使用这种方法，我们在COCO detection dataset和ImageNet classification dataset上同时训练模型。这种方法使得我们能够对那些没有label上detection data的数据做出detection的预测。我们使用ImageNet的detection任务做了验证。YOLO 9000在仅有200类中44类的detection data的情况下，仍然在ImageNet datection任务中取得了19.7mAP的成绩。对于不在COCO中的156类，YOLO9000成绩为16.0mAP。我们使得YOLO9000能够在保证实时的前提下对9K类目标进行检测。 根据论文结构的安排，将从Better，Faster和Stronger三个方面对论文提出的各条改进措施进行介绍。 Better在YOLO V1的基础上，作者提出了不少的改进来进一步提升算法的性能（mAP），主要改进措施包括网络结构的改进（第1，3，5，6条）和Anchor Box的引进（第3，4，5条）以及训练方法（第2，7条）。 改进1：引入BN层（Batch Normalization）Batch Normalization能够加快模型收敛，并提供一定的正则化。作者在每个conv层都加上了了BN层，同时去掉了原来模型中的drop out部分，这带来了2%的性能提升。 改进2：高分辨率分类器（High Resolution Classifier）YOLO V1首先在ImageNet上以$224\times 224$大小图像作为输入进行训练，之后在检测任务中提升到$448\times 448$。这里，作者在训练完224大小的分类网络后，首先调整网络大小为$448\times 448$，然后在ImageNet上进行fine tuning（10个epoch）。也就是得到了一个高分辨率的cls。再把它用detection上训练。这样，能够提升4%。 改进3：引入Anchor BoxYOLO V1中直接在CNN后面街上全连接层，直接回归bounding box的参数。这里引入了Faster RCNN中的anchor box概念，不再直接回归bounding box的参数，而是相对于anchor box的参数。 作者去掉后面的fc层和最后一个max pooling层，以期得到更高分辨率的feature map。同时，shrink网络接受$416\times 416$（而不是448）大小的输入image。这是因为我们想要最后的feature map大小是奇数，这样就能够得到一个center cell（比较大的目标，更有可能占据中间的位置）。由于YOLO conv-pooling的效应是将image downsamplig 32倍，所以最后feature map大小为$416/32 = 13$。 与YOLO V1不同的是，我们不再对同一个grid cell下的bounding box统一产生一个数量为$C$的类别概率，而是对于每一个bounding box都产生对应的$C$类概率。和YOLO V1一样的是，我们仍然产生confidence，意义也完全一样。 使用anchor后，我们的精度accuracy降低了，不过recall上来了。（这也较好理解。原来每个grid cell内部只有2个bounding box，造成recall不高。现在recall高上来了，accuracy会下降一些）。 改进4：Dimension Cluster在引入anchor box后，一个问题就是如何确定anchor的位置和大小？Faster RCNN中是手工选定的，每隔stride设定一个anchor，并根据不同的面积比例和长宽比例产生9个anchor box。在本文中，作者使用了聚类方法对如何选取anchor box做了探究。这点应该是论文中很有新意的地方。 这里对作者使用的方法不再过多赘述，强调以下两点： 作者使用的聚类方法是K-Means； 相似性度量不用欧氏距离，而是用IoU，定义如下：d(\text{box}, \text{centroid}) = 1-\text{IoU}(\text{box}, \text{centroid}) 使用不同的$k$，聚类实验结果如下，作者折中采用了$k = 5$。而且经过实验，发现当取$k=9$时候，已经能够超过Faster RCNN采用的手工固定anchor box的方法。下图右侧图是在COCO和VOC数据集上$k=5$的聚类后结果。这些box可以作为anchor box使用。 改进5：直接位置预测（Direct Location Prediction）我们仍然延续了YOLO V1中的思路，预测box相对于grid cell的位置。使用sigmoid函数作为激活函数，使得最终输出值落在$[0, 1]$这个区间上。 在output的feature map上，对于每个cell（共计$13\times 13$个），给出对应每个bounding box的输出$t_x$, $t_y$, $t_w$, $t_h$。每个cell共计$k=5$个bounding box。如何由这几个参数确定bounding box的真实位置呢？见下图。 设该grid cell距离图像左上角的offset是$(c_x, c_y)$，那么bounding box的位置和宽高计算如下。注意，box的位置是相对于grid cell的，而宽高是相对于anchor box的。 Darknet中的具体的实现代码如下（不停切换中英文输入实在是蛋疼，所以只有用我这蹩脚的英语来注释了。。。）： 1234567891011121314151617181920212223242526272829// get bounding box// x: data pointer of feature map// biases: data pointer of anchor box data// biases[2*n] = width of anchor box// biases[2*n+1] = height of anchor box// n: output bounding box for each cell in the feature map// index: output bounding box index in the cell// i: `cx` in the paper// j: 'cy' in the paper// (cx, cy) is the offset from the left top corner of the feature map// (w, h) is the size of feature map (do normalization in the code)box get_region_box(float *x, float *biases, int n, int index, int i, int j, int w, int h)&#123; box b; // i &lt;- cx, j &lt;- cy // index + 0: tx // index + 1: ty // index + 2: tw // index + 3: th // index + 4: to // not used here // index + 5, +6, ..., +(C+4) // confidence of P(class c|Object), not used here b.x = (i + logistic_activate(x[index + 0])) / w; // bx = cx+sigmoid(tx) b.y = (j + logistic_activate(x[index + 1])) / h; // by = cy+sigmoid(ty) b.w = exp(x[index + 2]) * biases[2*n] / w; // bw = exp(tw) * pw b.h = exp(x[index + 3]) * biases[2*n+1] / h; // bh = exp(th) * ph // 注意这里都做了Normalization，将值化到[0, 1]，论文里面貌似没有提到 // 也就是说YOLO 用于detection层的bounding box大小和位置的输出参数都是相对值 return b;&#125; 顺便说一下对bounding box的bp实现。具体代码如下： 12345678910111213141516171819202122232425262728293031323334353637// truth: ground truth// x: data pointer of feature map// biases: data pointer of anchor box data// n, index, i, j, w, h: same meaning with `get_region_box`// delta: data pointer of gradient// scale: just a weight, given by userfloat delta_region_box(box truth, float *x, float *biases, int n, int index, int i, int j, int w, int h, float *delta, float scale)&#123; box pred = get_region_box(x, biases, n, index, i, j, w, h); // get iou of the bbox and truth float iou = box_iou(pred, truth); // ground truth of the parameters (tx, ty, tw, th) float tx = (truth.x*w - i); float ty = (truth.y*h - j); float tw = log(truth.w*w / biases[2*n]); float th = log(truth.h*h / biases[2*n + 1]); // 这里是欧式距离损失的梯度回传 // 以tx为例。 // loss = 1/2*(bx^hat-bx)^2, 其中bx = cx + sigmoid(tx) // d(loss)/d(tx) = -(bx^hat-bx) * d(bx)/d(tx) // 注意，Darkent中的delta存储的是负梯度数值，所以下面的delta数组内数值实际是-d(loss)/d(tx) // 也就是(bx^hat-bx) * d(bx)/d(tx) // 前面的(bx^hat-bx)把cx约掉了（因为是同一个cell，偏移是一样的） // 后面相当于是求sigmoid函数对输入自变量的梯度。 // 由于当初没有缓存 sigomid(tx)，所以作者又重新计算了一次 sigmoid(tx)，也就是下面的激活函数那里 delta[index + 0] = scale * (tx - logistic_activate(x[index + 0])) * logistic_gradient(logistic_activate(x[index + 0])); delta[index + 1] = scale * (ty - logistic_activate(x[index + 1])) * logistic_gradient(logistic_activate(x[index + 1])); // tw 相似，只不过这里的 loss = 1/2(tw^hat-tw)^2，而不是和上面一样使用bw^hat 和 bw delta[index + 2] = scale * (tw - x[index + 2]); delta[index + 3] = scale * (th - x[index + 3]); return iou;&#125; 接下来，我们看一下bp的计算。主要涉及到决定bounding box大小和位置的四个参数的回归，以及置信度$t_o$，以及$C$类分类概率。上面的代码中已经介绍了bounding box大小位置的四个参数的梯度计算。对于置信度$t_o$的计算，如下所示。 1234567891011// 上面的代码遍历了所有的groundtruth，找出了与当前预测bounding box iou最大的那个// 首先，我们认为当前bounding box没有responsible for any groundtruth，// 那么，loss = 1/2*(0-sigmoid(to))^2// =&gt; d(loss)/d(to) = -(0-sigmoid(to)) * d(sigmoid)/d(to)// 由于之前的代码中已经将output取了sigmoid作用，所以就有了下面的代码// 其中，logistic_gradient(y) 是指dy/dx|(y=y0)的值。具体来说，logistic_gradient(y) = (1-y)*yl.delta[index + 4] = l.noobject_scale * ((0 - l.output[index + 4]) * logistic_gradient(l.output[index + 4]));// 如果best iou &gt; thresh, 我们认为这个bounding box有了对应的groundtruth，把梯度直接设置为0即可if (best_iou &gt; l.thresh) &#123; l.delta[index + 4] = 0;&#125; 这里额外要说明的是，阅读代码可以发现，分类loss的计算方法和V1不同，不再使用MSELoss，而是使用了交叉熵损失函数。对应地，梯度计算的方法如下所示。不过这点在论文中貌似并没有体现。12345678// for each classfor(n = 0; n &lt; classes; ++n)&#123; // P_i = \frac&#123;exp^out_i&#125;&#123;sum of exp^out_j&#125; // SoftmaxLoss = -logP(class) // ∂SoftmaxLoss/∂output = -(1(n==class)-P) delta[index + n] = scale * (((n == class)?1 : 0) - output[index + n]); if(n == class) *avg_cat += output[index + n];&#125; 改进6：Fine-Gained Features这个trick是受Faster RCNN和SSD方法中使用多个不同feature map提高算法对不同分辨率目标物体的检测能力的启发，加入了一个pass-through层，直接将倒数第二层的$26\times 26$大小的feature map加进来。 在具体实现时，是将higher resolution（也就是$26\times 26$）的feature map stacking在一起。比如，原大小为$26\times 26 \times 512$的feature map，因为我们要将其变为$13\times 13$大小，所以，将在空间上相近的点移到后面的channel上去，这部分可以参考Darknet中reorg_layer的实现。 使用这一扩展之后的feature map，提高了1%的性能提升。 改进7：多尺度训练（Multi-Scale Training）在实际应用时，输入的图像大小有可能是变化的。我们也将这一点考虑进来。因为我们的网络是全卷积神经网络，只有conv和pooling层，没有全连接层，所以可以适应不同大小的图像输入。所以网络结构上是没问题的。 具体来说，在训练的时候，我们每隔一定的epoch（例如10）就随机改变网络的输入图像大小。由于我们的网络最终降采样的比例是$32$，所以随机生成的图像大小为$32$的倍数，即$\lbrace 320, 352, \dots, 608\rbrace$。 在实际使用中，如果输入图像的分辨率较低，YOLO V2可以在不错的精度下达到很快的检测速度。这可以被用应在计算能力有限的场合（无GPU或者GPU很弱）或多路视频信号的实时处理。如果输入图像的分辨率较高，YOLO V2可以作为state of the art的检测器，并仍能获得不错的检测速度。对于目前流行的检测方法（Faster RCNN，SSD，YOLO）的精度和帧率之间的关系，见下图。可以看到，作者在30fps处画了一条竖线，这是算法能否达到实时处理的分水岭。Faster RCNN败下阵来，而YOLO V2的不同点代表了不同输入图像分辨率下算法的表现。对于详细数据，见图下的表格对比（VOC 2007上进行测试）。 总结在Better这部分的末尾，作者给出了一个表格，指出了主要提升性能的措施。例外是网络结构上改为带Anchor box的全卷积网络结构（提升了recall，但对mAP基本无影响）和使用新的网络（计算量少了~33%）。 Faster这部分的改进为网络结构的变化。包括Faster RCNN在内的很多检测方法都使用VGG-16作为base network。VGG-16精度够高但是计算量较大（对于大小为$224\times 224$的单幅输入图像，卷积层就需要30.69B次浮点运算）。在YOLO V1中，我们的网络结构较为简单，在精度上不如VGG-16（ImageNet测试，88.0% vs 90.0%）。 在YOLO V2中，我们使用了一种新的网络结构Darknet-19（因为base network有19个卷积层）。和VGG相类似，我们的卷积核也都是$3\times 3$大小，同时每次pooling操作后channel数double。另外，在NIN工作基础上，我们在网络最后使用了global average pooling层，同时使用$1\times 1$大小的卷积核来做feature map的压缩（分辨率不变，channel减小，新的元素是原来相应位置不同channel的线性组合），同时使用了Batch Normalization技术。具体的网络结构见下表。Darknet-19计算量大大减小，同时精度超过了VGG-16。 在训练过程中，首先在ImageNet 1K上进行分类器的训练。使用数据增强技术（如随机裁剪、旋转、饱和度变化等）。和上面的讨论相对应，首先使用$224\times 224$大小的图像进行训练，再使用$448\times 448$的图像进行fine tuning，具体训练参数设置可以参见论文和对应的代码。这里不再多说。 然后，我们对检测任务进行训练。对网络结构进行微调，去掉最后的卷积层（因为它是我们当初为了得到1K类的分类置信度加上去的），增加$3$个$3\times 3$大小，channel数为$1024$的卷积层，并每个都跟着一个$1\times 1$大小的卷积层，channel数由我们最终的检测任务需要的输出决定。对于VOC的检测任务来说，我们需要预测$5$个box，每个需要$5$个参数（相对位置，相对大小和置信度），同时$20$个类别的置信度，所以输出共$5\times(5+20)=125$。从YOLO V2的yolo_voc.cfg文件中，我们也可以看到如下的对应结构： 1234567891011121314[convolutional]batch_normalize=1size=3stride=1pad=1filters=1024activation=leaky[convolutional]size=1stride=1pad=1filters=125activation=linear 同时，加上上文提到的pass-through结构。 Stronger未完待续]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-立体视觉基础]]></title>
      <url>%2F2017%2F02%2F02%2Fcs131-camera%2F</url>
      <content type="text"><![CDATA[数字图像是现实世界的物体通过光学成像设备在感光材料上的投影。在3D到2D的转换过程中，深度信息丢失。如何从单幅或者多幅图像中恢复出有用的3D信息，需要使用立体视觉的知识进行分析。如下图所示。这篇博客对课程讲义中立体视觉部分做一整理，分别介绍了针孔相机模型和对极集合的基础知识。 针孔相机模型（Pinhole Camera）针孔相机是简化的相机模型。光线沿直线传播。物体反射的光线，通过针孔，在成像面形成倒立的影像。针孔与成像面的距离，称为焦距。一般而言，针孔越小，影像越清晰，但针孔太小，会导致衍射，反而令影像模糊。 投影几何的重要性质在对针孔相机详细说明之前，首先介绍投影几何（Projective Geometry）的几条性质。 在投影变换前后，长度和角度信息丢失，但直线仍然保持是直线。如下图所示。原来垂直和平行的台球桌面的边不再保持原有的角度和相对长度，但是仍然是直线。 另一方面，虽然3D世界中的平行线在投影变换后相交（我们将交点称为Vanishing point），但是3D世界中互相平行的线，在2D平面上的Vanishing point为同一点。而所有平行线的Vanishing point又在同一条直线上。如下图所示（竖直的绿色平行线在投影变换后仍然保持平行，实际上并不想交，但我们这里认为其交于无穷远的某点）。 针孔相机模型如下图所示，为针孔相机成像原理示意图。外部世界的一点$P$发出（或反射）的光线，经过小孔$O$，成像在在像平面$\Pi^\prime$的点$P^\prime$处。通过简单的相似三角形知识，可以得到图上的关系式。 由上图可以知道，3D空间的一点$P$与成像平面上的对应点$P^\prime$坐标之间的数量关系为： \left\{\begin{matrix} x^\prime = fx/z \\ y^\prime = fy/z \end{matrix}\right.可是，由于变量$z$位于分母的位置，也就是说如果直接写成变换矩阵的话，这个矩阵是和变量$z$有关的，我们不想这样，而是想得到一个完全由相机本身确定的变换矩阵。所以，我们借鉴齐次矩阵的思想，将原来的点增加一个维度，转换为齐次坐标。如下图的两个例子所示。 这样，我们可以将3D和2D之间的变换写成下面的形式。当实际计算的时候，我们首先将3D点转换为4维向量（在末尾补1），然后左乘变换矩阵。最后将得到的结果化为末尾元素为1的形式，这样，前面两个元素就代表了变换后2D点的坐标。这样做，使得变换矩阵$M$完全由相机参数（即焦距$f$）决定。 上面的式子只是理想情况下的相机模型，成立的假设条件较强，主要有以下几点假设： 内假设（和相机本身有关） 不同方向上焦距相同； 光学中心在相平面的坐标原点$(0, 0)$ 没有倾斜（no skew） 外假设（和相机位姿有关，和相机本身参数无关） 相机没有旋转（坐标轴与世界坐标系方向重合） 相机没有平移（相机中心与世界坐标系中心重合） 其中，no skew情形我也不知道中文应该如何翻译。这种情况如图所示，由于剪切变形，成像平面的坐标轴并不是严格正交，使得两个轴之间实际存在一个很小的夹角。 下面，按照slide的顺序逐渐将上述假设移去，分析变换矩阵$M$应该如何修正。 理想情况理想情况以上假设全部满足，矩阵$M$如下所示。 光学中心不在像平面的坐标原点假设光学中心位于像平面内坐标为$(u_0, v_0)$的位置。则需要在理想情况计算的坐标基础上加上这个偏移量。所以，矩阵$M$被修正为： 像素非正方形由于透镜在$x$和$y$方向上的焦距不同，使得两个方向上的比例因子不同，不能再使用同一个$f$计算。修正如下： no skew这时候$x$方向上其实有$y$轴坐标的分量，所以修正如下： 相机的旋转和平移相机外部姿态很可能经过了旋转和平移，从而不与世界坐标系完全重合。如图所示。 所以我们需要先对世界坐标系内的3D点做一个齐次变换，将其变换为一个中间坐标系，这个中间坐标系和相机坐标系是重合的。也就是我们的变换矩阵应该是下面这个形式的，其中，$H$是表征旋转和平移的齐次矩阵，$H\in \mathbb{R}^{3\times 4}$。 P^\prime = MHP首先考虑较为简单的平移。所做的修正如下所示，也就是将原来的$\mathbb{0}$矩阵变为了一个平移向量。 进一步考虑旋转时，我们将旋转分解为绕三个坐标轴的三次旋转，单次旋转矩阵如下所示： 将它们按照旋转顺序左乘起来，可以得到修正后的变换形式如下： 最终形式综上所示，变换矩阵的最终形式为： 其中，内参数矩阵中有5个自由度，外参数矩阵中有6个自由度（3平移，3旋转）。 上面的内容总结起来，如下图所示。 对极几何基础概念如下图所示，给出了对极几何中的几个基本概念。两个相机的中心分别位于$O$点和$O^{‘}$点，被观察物体位于$P$点。 极点：$e$和$e^\prime$点分别是$OO^\prime$连线与两个成像平面的交点，也是相机中心在另一台相机成像平面上对应的像。 极平面：点$O$，$O^\prime$，$P$点共同确定的平面（灰色） 极线：极平面与两个成像平面的交线，即$pe$和$p^\prime e^\prime$（蓝色） 基线：两个相机中心的连线（黄色） 极线约束从上图可以看出，如果两台相机已经给定，那么给定物体点在左边相机像平面内的成像点，则右边相机成像平面内的点被唯一确定。如何能够找到这个对应点呢？或者换种说法，在对极几何中，如何描述这种不同相机成像点的约束关系呢？ 如下图所示，从上面的讨论我们已经知道，物点$P$在左右两个相机成像平面的坐标可以使用由相机内参数矩阵和外参数矩阵确定的齐次变换矩阵得到（下图在标注时，右侧相机的点应被标注为$p^\prime$）。为了简化问题，在下面的讨论时，我们假定相机都是理想的针孔相机模型，也就是说，变换矩阵只和外参数矩阵（也即是相机的位姿决定）。再做简化，认为世界坐标系与左侧相机的相机坐标系重合。那么，就得到了图上所示的变换矩阵$M$和$M^\prime$。其中，$R$是左右两个相机之间的旋转矩阵，$T$是$OO^\prime$有向线段，表明相机中心的位移。 （下面的推导参考了博客：计算机视觉基础4——对极几何）。在下面的推导中，我们使用$p^\prime$表示在相机$O^\prime$下的向量$O\prime P$，符号$p$同理。那么，有如下关系成立：$R(p-T) = p^\prime$]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(SIFT)]]></title>
      <url>%2F2017%2F01%2F30%2Fcs131-sift%2F</url>
      <content type="text"><![CDATA[SIFT(尺度不变特征变换，Scale Invariant Feature Transform),最早由Lowe提出，目的是为了解决目标检测（Object Detection）问题中提取图像特征的问题。从名字可以看出，SIFT的优势就在于对尺度变换的不变性，同时SIFT还满足平移变换和旋转变换下的不变性，并对光照变化和3D视角变换有一定的不变性。它的主要步骤如下： scale space上的极值检测。使用DoG在不同的scaleast和image position下找到interest point。 interest point的localization。对上面的interest point进行稳定度检测，并确定其所在的scale和position。 确定方向。通过计算图像的梯度图，确定key point的方向，下一步的feature operation就是在这个方向，scale和position上进行的。 确定key point的描述子。使用图像的局部梯度作为key point的描述子，最终构成SIFT特征向量。 SIFT介绍上讲中介绍的Harris角点方法计算简便，并具有平移不变性和旋转不变性。特征$f$对某种变换$\mathcal{T}$具有不变性，是指在经过变换后原特征保持不变，也就是$f(I) = f(\mathcal{T}(I))$。但是Harris角点不具有尺度变换不变性，如下图所示。当图像被放大后，原图的角点被判定为了边缘点。 而我们想要得到一种对尺度变换保持不变性的特征计算方法。例如图像patch的像素平均亮度，如下图所示。region size缩小为原始的一半后，亮度直方图的形状不变，即平均亮度不变。 而Lowe想到了使用局部极值来作为feature来保证对scale变换的不变性。在算法的具体实现中，他使用DoG来获得局部极值。 Lowe的论文中也提到了SIFT特征的应用。SIFT特征可以产生稠密（dense）的key point，例如一张500x500像素大小的图像，一般可以产生~2000个稳定的SIFT特征。在进行image matching和recognition时，可以将ref image的SIFT特征提前计算出来保存在数据库中，并计算待处理图像的SIFT特征，根据特征向量的欧氏距离进行匹配。 这篇博客主要是Lowe上述论文的读书笔记，按照SIFT特征的计算步骤进行组织。 尺度空间极值的检测方法前人研究已经指出，在一系列合理假设下，高斯核是唯一满足尺度不变性的。所谓的尺度空间，就是指原始图像$I(x,y)$和可变尺度的高斯核$G(x,y,\sigma)$的卷积结果。如下式所示： L(x,y,\sigma) = G(x,y,\sigma)\ast I(x,y)其中，$G(x,y, \sigma) = \frac{1}{2\pi\sigma^2}\exp(-(x^2+y^2)/2\sigma^2)$。不同的$\sigma$代表不同的尺度。 DoG(difference of Gaussian)函数定义为不同尺度的高斯核与图像卷积结果之差，即， D(x,y,\sigma) = L(x,y,k\sigma) - L(x,y,\sigma)如下图所示，输入的图像重复地与高斯核进行卷积得到结果图像$L$（左侧），这样构成了一个octave。相邻的$L$之间作差得到DoG图像（右侧）。当一个octave处理完之后，将当前octave的最后一张高斯卷积结果图像降采样两倍，按照前述方法构造下一个octave。在图中，我们将一个octave划分为$s$个间隔（即$s+1$个图像）时，设$\sigma$最终变成了2倍（即$\sigma$加倍）。那么，显然有相邻两层之间的$k = 2^{1/s}$。不过为了保证首尾两张图像也能够计算差值，我们实际上需要再补上两张（做一个padding），也就是说一个octave内的总图像为$s+3$（下图中的$s=2$）。 为何我们要费力气得到DoG呢？论文中作者给出的解释是：DoG对scale-normalized后的Guassian Laplace函数$\sigma^2\Delta G$提供了足够的近似。其中前面的$\sigma^2$系数正是为了保证尺度不变性。而前人的研究则指出，$\sigma \Delta G$函数的极值，和其他一大票其他function比较时，能够提供最稳定的feature。 对于高斯核函数，有以下性质： \frac{\partial G}{\partial \sigma} = \sigma \Delta G我们将式子左侧的微分变成差分，得到了下式： \sigma\Delta G \approx \frac{G(x,y,k\sigma)-G(x,y,\sigma)}{k\sigma - \sigma}也就是： G(x,y,k\sigma)-G(x,y,\sigma) \approx (k-1)\sigma^2 \Delta G当$k=1$时，上式的近似误差为0（即上面的$s=\infty$，也就是说octave的划分是连续的）。但是当$k$较大时，如$\sqrt{2}$（这时$s=2$，octave划分十分粗糙了），仍然保证了稳定性。同时，由于相邻两层的比例$k$是定值，所以$k-1$这个因子对于检测极值没有影响。我们在计算时，无需除以$k-1$。 构造完了DoG图像金字塔，下面我们的任务就是检测其中的极值。如下图，对于每个点，我们将它与金字塔中相邻的26个点作比较，找到局部极值。 另外，我们将输入图像行列预先使用线性插值的方法resize为原来的2倍，获取更多的key point。 此外，在Lowe的论文中还提到了使用DoG的泰勒展开和海森矩阵进行key point的精细确定方法，并对key point进行过滤。这里也不再赘述了。 128维feature的获取我们需要在每个key point处提取128维的特征向量。这里结合CS131课程作业2的相关练习作出说明。对于每个key point，我们关注它位于图像金字塔中的具体层数以及像素点位置，也就是说通过索引pyramid{scale}(y, x)就可以获取key point。我们遍历key point，为它们赋予一个128维的特征向量。 我们选取point周围16x16大小的区域，称为一个patch。将其分为4x4共计16个cell。这样，每个cell内共有像素点16个。对于每个cell，我们计算它的局部梯度方向直方图，直方图共有8个bin。也就是说每个cell可以得到一个8维的特征向量。将16个cell的特征向量首尾相接，就得到了128维的SIFT特征向量。在下面的代码中，使用变量patch_mag和patch_theta分别代表patch的梯度幅值和角度，它们可以很简单地使用卷积和数学运算得到。 123patch_mag = sqrt(patch_dx.^2 + patch_dy.^2);patch_theta = atan2(patch_dy, patch_dx); % atan2的返回结果在区间[-pi, pi]上。patch_theta = mod(patch_theta, 2*pi); % 这里我们要将其转换为[0, 2pi] 之后，我们需要获取key point的主方向。其定义可见slide，即为key point扩展出来的patch的梯度方向直方图的峰值对应的角度。 所以我们首先应该设计如何构建局部梯度方向直方图。我们只要将[0, 2pi]区间划分为若干个bin，并将patch内的每个点使用其梯度大小向对应的bin内投票即可。如下所示： 12345678910111213141516171819202122232425262728293031323334function [histogram, angles] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles)% Compute a gradient histogram using gradient magnitudes and directions.% Each point is assigned to one of num_bins depending on its gradient% direction; the gradient magnitude of that point is added to its bin.%% INPUT% num_bins: The number of bins to which points should be assigned.% gradient_magnitudes, gradient angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient% for the ith point. gradient_angles ranges from 0 to 2*pi% % OUTPUT% histogram: A 1 x num_bins array containing the gradient histogram. Entry 1 is% the sum of entries in gradient_magnitudes whose corresponding% gradient_angles lie between 0 and angle_step. Similarly, entry 2 is for% angles between angle_step and 2*angle_step. Angle_step is calculated as% 2*pi/num_bins.% angles: A 1 x num_bins array which holds the histogram bin lower bounds.% In other words, histogram(i) contains the sum of the% gradient magnitudes of all points whose gradient directions fall% in the range [angles(i), angles(i + 1)) angle_step = 2 * pi / num_bins; angles = 0 : angle_step : (2*pi-angle_step); histogram = zeros(1, num_bins); num = numel(gradient_angles); for n = 1:num index = floor(gradient_angles(n) / angle_step) + 1; histogram(index) = histogram(index) + gradient_magnitudes(n); end end Lowe论文中推荐的bin数目为36个，计算主方向的函数如下： 12345678910111213141516171819202122232425function direction = ComputeDominantDirection(gradient_magnitudes, gradient_angles)% Computes the dominant gradient direction for the region around a keypoint% given the scale of the keypoint and the gradient magnitudes and gradient% angles of the pixels in the region surrounding the keypoint.%% INPUT% gradient_magnitudes, gradient_angles:% Two arrays of the same shape where gradient_magnitudes(i) and% gradient_angles(i) give the magnitude and direction of the gradient for% the ith point. % Compute a gradient histogram using the weighted gradient magnitudes. % In David Lowe's paper he suggests using 36 bins for this histogram. num_bins = 36; % Step 1: % compute the 36-bin histogram of angles using ComputeGradientHistogram() [histogram, angle_bound] = ComputeGradientHistogram(num_bins, gradient_magnitudes, gradient_angles); % Step 2: % Find the maximum value of the gradient histogram, and set "direction" % to the angle corresponding to the maximum. (To match our solutions, % just use the lower-bound angle of the max histogram bin. (E.g. return % 0 radians if it's bin 1.) [~, max_index] = max(histogram); direction = angle_bound(max_index);end 之后，我们更新patch内各点的梯度方向，计算其与主方向的夹角，作为新的方向。并将梯度进行高斯平滑。 123patch_theta = patch_theta - ComputeDominantDirection(patch_mag, patch_theta);;patch_theta = mod(patch_theta, 2*pi);patch_mag = patch_mag .* fspecial('gaussian', patch_size, patch_size / 2); % patch_size = 16 遍历cell，计算feature如下： 123456789101112131415feature = [];row_iter = 1;for y = 1:num_histograms col_iter = 1; for x = 1:num_histograms cell_mag = patch_mag(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); cell_theta = patch_theta(row_iter: row_iter + pixelsPerHistogram - 1, ... col_iter: col_iter + pixelsPerHistogram - 1); [histogram, ~] = ComputeGradientHistogram(num_angles, cell_mag, cell_theta); feature = [feature, histogram]; col_iter = col_iter + pixelsPerHistogram; end row_iter = row_iter + pixelsPerHistogram;end 最后，对feature做normalization。首先将feature化为单位长度，并将其中太大的分量进行限幅（如0.2的阈值），之后再重新将其转换为单位长度。 这样，就完成了SIFT特征的计算。在Lowe的论文中，有更多对实现细节的讨论，这里只是跟随课程slide和作业走完了算法流程，不再赘述。 应用：图像特征点匹配和Harris角点一样，SIFT特征可以用作两幅图像的特征点匹配，并且具有多种变换不变性的优点。对于两幅图像分别计算得到的特征点SIFT特征向量，可以使用下面简单的方法搜寻匹配点。计算每组点对之间的欧式距离，如果最近的距离比第二近的距离小得多，那么可以认为有一对成功匹配的特征点。其MATLAB代码如下，其中descriptor是两幅图像的SIFT特征向量。阈值默认为取做0.7。 123456789101112131415161718192021222324252627282930313233343536373839function match = SIFTSimpleMatcher(descriptor1, descriptor2, thresh)% SIFTSimpleMatcher% Match one set of SIFT descriptors (descriptor1) to another set of% descriptors (decriptor2). Each descriptor from descriptor1 can at% most be matched to one member of descriptor2, but descriptors from% descriptor2 can be matched more than once.% % Matches are determined as follows:% For each descriptor vector in descriptor1, find the Euclidean distance% between it and each descriptor vector in descriptor2. If the smallest% distance is less than thresh*(the next smallest distance), we say that% the two vectors are a match, and we add the row [d1 index, d2 index] to% the "match" array.% % INPUT:% descriptor1: N1 * 128 matrix, each row is a SIFT descriptor.% descriptor2: N2 * 128 matrix, each row is a SIFT descriptor.% thresh: a given threshold of ratio. Typically 0.7%% OUTPUT:% Match: N * 2 matrix, each row is a match.% For example, Match(k, :) = [i, j] means i-th descriptor in% descriptor1 is matched to j-th descriptor in descriptor2. if ~exist('thresh', 'var'), thresh = 0.7; end match = []; [N1, ~] = size(descriptor1); for i = 1:N1 fea = descriptor1(i, :); err = bsxfun(@minus, fea, descriptor2); dis = sqrt(sum(err.^2, 2)); [sorted_dis, ind] = sort(dis, 1); if sorted_dis(1) &lt; thresh * sorted_dis(2) match = [match; [i, ind(1)]]; end endend 接下来，我们可以使用最小二乘法计算两幅图像之间的仿射变换矩阵（齐次变换矩阵）$H$。其中$H$满足： Hp_{\text{before}} = p_{\text{after}}其中 p = \begin{bmatrix}x \\\\ y \\\\ 1\end{bmatrix}对上式稍作变形，有 p_{\text{before}}^\dagger H^\dagger = p_{\text{after}}\dagger就可以使用标准的最小二乘正则方程进行求解了。代码如下： 1234567891011121314151617181920212223242526272829303132function H = ComputeAffineMatrix( Pt1, Pt2 )%ComputeAffineMatrix% Computes the transformation matrix that transforms a point from% coordinate frame 1 to coordinate frame 2%Input:% Pt1: N * 2 matrix, each row is a point in image 1% (N must be at least 3)% Pt2: N * 2 matrix, each row is the point in image 2 that% matches the same point in image 1 (N should be more than 3)%Output:% H: 3 * 3 affine transformation matrix,% such that H*pt1(i,:) = pt2(i,:) N = size(Pt1,1); if size(Pt1, 1) ~= size(Pt2, 1), error('Dimensions unmatched.'); elseif N&lt;3 error('At least 3 points are required.'); end % Convert the input points to homogeneous coordintes. P1 = [Pt1';ones(1,N)]; P2 = [Pt2';ones(1,N)]; H = P1*P1'\P1*P2'; H = H'; % Sometimes numerical issues cause least-squares to produce a bottom % row which is not exactly [0 0 1], which confuses some of the later % code. So we'll ensure the bottom row is exactly [0 0 1]. H(3,:) = [0 0 1];end 作业中的其他例子也很有趣，这里不再多说了。贴上两张图像拼接的实验结果吧~]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-描述图像的特征(Harris 角点)]]></title>
      <url>%2F2017%2F01%2F25%2Fcs131-finding-features%2F</url>
      <content type="text"><![CDATA[feature是对图像的描述。比如，图像整体灰度值的均值和方差就可以作为feature。如果我们想在一副图像中检测足球时，可以使用滑动窗方法，逐一检查窗口内的灰度值分布是否和一张给定的典型黑白格子足球相似。可想而知，这种方法的性能一定让人捉急。而在image matching问题中，常常需要将不同视角的同一目标物进行matching，进而计算相机转过的角度。这在SLAM问题中很有意义。如下图所示，同一处景观，在不同摄影师的镜头下，不仅视角不同，而且明暗变化也有很多差别，右侧的图暖色调更浓。这也告诉我们，上面提到的只使用全局图像灰度值来做feature的做法有多么不靠谱。 那么，让我们脱离全局特征，转而将注意力集中在局部特征上。这是因为使用局部特征能够更好地处理图像中的遮挡、变形等情况，而且我们的研究对象常常是图像中的部分区域而不是图像整体。更特殊地，在这一讲中，我们主要探究key point作为local feature的描述，并介绍Harris角点检测方法。在下一节中介绍SIFT特征。 Harris角点角点，即corner，和edge类似，区别在于其在两个方向上都有较为剧烈的灰度变化（而edge只在某一个方向上灰度值变化剧烈）。如图所示。 Harris角点得名于其发明者Harris，是一种常见的角点检测方法。给定观察窗口大小，计算平移后窗口内各个像素差值的加权平方和，如下式。 E(u,v) = \sum_x\sum_yw(x,y)[I(x+u, y+v) - I(x,y)]^2其中，窗口加权函数$w$可以取做门限函数或gaussian函数。如图所示。 使用泰勒级数展开，并忽略非线性项，我们有 I(x+u,y+v) = I(x,y) + I_x(x,y)u+I_y(x,y)v所以上式可以写成（线性二次型写成了矩阵形式）， E(u,v) = \sum_{x,y}w(I_xu+I_yv)^2 = \begin{bmatrix}u&v\end{bmatrix}M\begin{bmatrix}u\\\\v\end{bmatrix}其中， M = w\begin{bmatrix}I_x^2& I_xI_y\\\\I_xI_y&I_y^2\end{bmatrix}当使用门限函数时，权值$w_{i,j} = 1$，则， M = \begin{bmatrix}\sum I_xI_x& \sum I_xI_y\\\\\sum I_xI_y&\sum I_yI_y\end{bmatrix} = \sum \begin{bmatrix}I_x \\\\I_y\end{bmatrix}\begin{bmatrix}I_x &I_y\end{bmatrix}当corner与xy坐标轴对齐时候，如下图所示。由于在黑色观察窗口内，只有上侧和左侧存在边缘，且在上边缘，$I_y$很大，而$I_x=0$，在左侧边缘，$I_x$很大而$I_y = 0$，所以，矩阵 M = \begin{bmatrix}\lambda_1 & 0 \\\\ 0&\lambda_2 \end{bmatrix} 当corner与坐标轴没有对齐时，经过旋转变换就可以将其转换到与坐标轴对齐的角度，而这种旋转操作可以使用矩阵的相似化来表示（其实是二次型的化简，可以使用合同变换，而旋转变换的矩阵是酉矩阵，转置即为矩阵的逆，所以也是相似变换）。也就是说，矩阵$M$相似于某个对角阵。 M = R^{-1}\Sigma R, \text{其中}\Sigma = \begin{bmatrix}\lambda_1&0\\\\0&\lambda_2\end{bmatrix}所以，可以根据下面这张图利用矩阵$M$的特征值来判定角点和边缘点。当两个特征值都较大时为角点；当某个分量近似为0而另一个分量较大时，可以判定为边缘点（因为某个方向的导数为0）；当两个特征值都近似为0时，说明是普通点（flat point）。（课件原图如此，空缺的问号处应分别为$\lambda_1$和$\lambda_2$）。 然而矩阵的特征值计算较为复杂，所以使用下面的方法进行近似计算。 \theta = \det(M)-\alpha\text{trace}(M)^2 = \lambda_1\lambda_2-\alpha(\lambda_1+\lambda_2)^2 为了减弱噪声的影响，常常使用gaussian窗函数。如下式所示： w(x,y) = \exp(-(x^2+y^2)/2\sigma^2)]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-边缘检测]]></title>
      <url>%2F2017%2F01%2F24%2Fcs131-edge-detection%2F</url>
      <content type="text"><![CDATA[边缘(Edge)在哺乳动物的视觉中有重要意义。在由若干卷积层构成的深度神经网络中，较低层的卷积层就被训练成为对特定形状的边缘做出响应。边缘检测也是计算机视觉和图像处理领域中一个重要的问题。 边缘的产生若仍采取将图像视作某一函数的观点，边缘是指这个函数中的不连续点。边缘检测是后续进行目标检测和形状识别的基础，也能够在立体视觉中恢复视角等。边缘的来源主要有以下几点： 物体表面不平造成灰度值的不连续； 深度值不同造成灰度值不连续； 物体表面颜色的突变造成灰度值不连续 朴素思想利用边缘是图像中不连续点的这一性质，可以通过计算图像的一阶导数，再找到一阶导数的极大值，即认为是边缘点。如下图所示，在白黑交界处，图像一阶导数的值非常大，表明此处灰度值变化剧烈，是边缘。 问题转换为如何求取图像的一阶导数（或梯度）。由于图像是离散的二元函数，所以下文不再区分求导与差分。 在$x$方向上，令$g_x = \frac{\partial f}{\partial x}$；在$y$方向上，令$g_y = \frac{\partial f}{\partial y}$。梯度的大小和方向为 g = \lbrack g_x, g_y\rbrack, \theta = \arctan(g_y/g_x)通过和Sobel算子等做卷积，可以求取两个正交方向上图像的一阶导数，并计算梯度，之后检测梯度的局部极大值就能够找出边缘点了。 只是这种方法很容易受到噪声影响。如图所示，真实的边缘点被湮没在了噪声中。 改进1：先平滑改进措施1，可以首先对图像进行高斯平滑，再按照上面的方法求取边缘点。根据卷积的性质，有： \frac{d}{dx}(f\ast g) = f\ast\frac{d}{dx}g所以我们可以先求取高斯核的一阶导数，再和原始图像直接做一次卷积就可以一举两得了。这样，引出了DoG(Deriative of Gaussian)。$x$方向的DoG如图所示。 进行高斯平滑不可避免会使图像中原本的细节部分模糊，所以需要在克服噪声和引入模糊之间做好折中。 改进2：Canny检测子改进措施2，使用Canny检测子进行检测。Canny检测方法同样基于梯度，其基本原理如下： 使用DoG计算梯度幅值和方向。 非极大值抑制，这个过程需要根据梯度方向做线性插值。如图，沿着点$q$的梯度方向找到了$p$和$r$两个点。这两个点的梯度幅值需要根据其临近的两点做插值得到。 利用梯度方向和边缘线互相垂直这一性质，如图，若已经确定点$p$为边缘点，则向它的梯度方向正交方向上寻找下一个边缘点（点$r$或$s$）。这一步也叫edge linking。 同时，为了提高算法性能，Canny中采用了迟滞阈值的方法，设定low和high两个阈值，来判定某个点是否属于强或弱边缘点。在做edge linking的时候，从强边缘点开始，如果遇到了弱边缘点，则继续，直到某点的梯度幅值甚至比low还要小，则在此停止。 改进3：RANSAC方法有的时候，我们并不是想要找到所有的边缘点，可能只是想找到图像中水平方向的某些边缘。这时候可以考虑采用RANSAC方法。 RANSAC方法的思想在于，认为已有的feature大部分都是好的。这样，每次随机抽取出若干feature，建立model，再在整个feature集合上进行验证。那么由那些好的feature得到的model一定是得分较高的。（世界上还是好人多啊！）这样就剔除了离群点的影响。 以直线拟合为例，在下图中，给出了使用RANSAC方法拟合直线的步骤。如图1所示，由于离群点的存在，如果直接使用最小二乘法进行拟合，拟合结果效果会很不理想。由于确定一条直线需要两个点，所以从点集中选取两个点，并计算拟合直线。并计算点集中的点在这条直线附近的个数，作为对模型好坏的判定，这些点是新的内点。找出最优的那条直线，使用其所有内点再进行拟合，重复上述操作，直至迭代终止。 上述RANSAC方法进行直线拟合的过程可以总结如下： 按照上述思想，我分别使用最小二乘法和RANSAC方法尝试进行直线拟合。在下面的代码中，我首先产生了正常受到一定高斯噪声污染的数据（图中的红色点），这些点的真值都落在直线$y = 2x+1$上。而后，我随机变化了斜率和截距，以期产生一些离群点（图中的蓝色点）。当然，由于随机性，这种方法生成的点有可能仍然是内点。 而后，我分别使用上述两种方法进行拟合。可以从结果图中看出，RANSAC（绿色线）能够有效避免离群点的干扰，获得更好的拟合效果。在某次实验中，两种方法的拟合结果如下：12least square: a = 3.319566, b = -1.446528ransac method: a = 1.899640, b= 1.298608 实验使用的MATLAB代码如下：1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162%% generate datax = 0:1:10;y_gt = 2*x+1;y = y_gt + randn(size(y_gt));scatter(x, y, [], [1,0,0]);hold onout_x = 0:1:10;out_y = 5*rand(size(out_x)).*out_x + 4*rand(size(out_x));scatter(out_x, out_y, [], [0,0,1]);X = [x, out_x]';Y = [y, out_y]';X = [X, ones(length(X), 1)];[a, b] = ls_fit(X, Y);plot(x, a*x+b, 'linestyle', '--', 'color', 'r');[ra, rb] = ransac_fit(X, Y, 100, 2, 0.5, 3);plot(x, ra*x+rb, 'linestyle', '-.', 'color', 'g');fprintf('least square: a = %f, b = %f\n',a, b);fprintf('ransac method: a = %f, b= %f\n', ra, rb)function [a, b] = ransac_fit(X, Y, k, n, t ,d)% ransac fit% k -- maximum iteration number% n -- smallest point numer required% t -- threshold to identify a point is fit well% d -- the number of nearby points to assert a model is finedata = [X, Y];N = size(data, 1);best_good_cnt = -1;best_a = 0;best_b = 0;for i = 1:k % sample point idx = randsample(N, n); data_sampled = data(idx, :); % fit with least square [a, b] = ls_fit(data_sampled(:, 1:2), data_sampled(:, 3)); % test model not_sampled = ones(N, 1); not_sampled(idx) = 0; not_sampled_data = data(not_sampled == 1, :); distance = abs(not_sampled_data(:, 1:2) * [a; b] - not_sampled_data(:, 3)) / sqrt(a^2+1); inner_flag = distance &lt; t; good_cnt = sum(inner_flag); if good_cnt &gt;= d &amp;&amp; good_cnt &gt; best_good_cnt best_good_cnt = good_cnt; data_refine = data(find(inner_flag), :); [a, b] = ls_fit(data_refine(:, 1:2), data_refine(:, 3)); best_a = a; best_b = b; end fprintf('iteration %d, best_a = %f, best_b = %f\n', i, best_a, best_b);enda = best_a;b = best_b;endfunction [a, b] = ls_fit(X, Y)% least square fitA = X'*X\X'*Y;a = A(1);b = A(2);end 我们对RANSAC稍作分析，可以大概了解试验次数$k$的确定方法。 仍然使用上述直线拟合的例子。如果所有点中内点所占的比例为$\omega$，每次挑选$n$个点尝试（上述demo代码中取$n=2$）。那么每次挑选的两个点全部是内点的概率为$\omega^n$。当选取的$n$个点全部为内点时，视为有效实验。那么，重复$k$次实验，有效实验次数为0的概率为$(1-\omega^n)^k$。由于底数小于1，所以我们只需尽量增大$k$，就能够降低这种倒霉的概率。下图是不同$n$和$\omega$情况下为了使得实验成功的概率大于0.99所需的$k$的分布。 RANSAC方法的有点在于能够较为鲁棒地估计模型的参数，而且实现简单。缺点在于当离群点比例较大时，为保证实验成功所需的$k$值较大。这时候，可能Hough变换等基于投票的方法更适合用于图像中的直线检测问题。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线性滤波器和矩阵的SVD分解]]></title>
      <url>%2F2017%2F01%2F23%2Fcs131-filter-svd%2F</url>
      <content type="text"><![CDATA[数字图像可以看做$\mathbb{R}^2 \rightarrow \mathbb{R}^c$的映射，其中$c$是图像的channel数。使用信号与系统的角度来看，如果单独考察某个channel，可以将图像看做是二维离散系统。 卷积卷积的概念不再详述，利用不同的kernel与原始图像做卷积，就是对图像进行线性滤波的过程。卷积操作时，以某一个点为中心点，最终结果是这个点以及它的邻域点的线性组合，组合系数由kernel决定。一般kernel的大小取成奇数。如下图所示。（图片来自博客《图像卷积与滤波的一些知识点》） 在卷积操作时，常常需要对图像做padding，常用的padding方法有： zero padding，也就是填充0值。 edge replication，也就是复制边缘值进行填充。 mirror extension，也就是将图像看做是周期性的，相当于使用对侧像素值进行填充。 作业1调整图像灰度值为0到255计算相应的k和offset值即可。另外MATLAB中的uint8函数可以将结果削顶与截底为0到255之间。123scale_ratio = 255.0 / (max_val - min_val);offset = -min_val * scale_ratio;fixedimg = scale_ratio * dark + offset; SVD图像压缩使用SVD进行图像压缩，下图是将所有奇异值按照从大到小的顺序排列的大小示意图。可以看到，第一个奇异值比其他高出一个数量级。 MATLAB实现分别使用10，50， 100个分量进行图像压缩，如下图所示。可以看到，k=10时，已经能够复原出原图像的大致轮廓。当k更大时，更多细节被复原出来。 MATLAB代码如下：12345678910111213141516171819202122232425%% read imageim = imread('./flower.bmp');im_gray = double(rgb2gray(im));[u, s, v] = svd(im_gray);%% get sigular valuesigma = diag(s);top_k = sigma(1:10);figureplot(1:length(sigma), sigma, 'r-', 'marker', 's', 'markerfacecolor', 'g');figuresubplot(2, 2, 1);imshow(uint8(im_gray));title('flower.bmp')index = 2;for k = [10, 50, 100] uk = u(:, 1:k); sk = s(1:k, 1:k); vk = v(:, 1:k); im_rec = uk * sk * vk'; subplot(2, 2, index); index = index + 1; imshow(uint8(im_rec)); title(sprintf('k = %d', k));end 图像SVD压缩中的误差分析完全是个人随手推导，不严格的说明： 将矩阵分块。由SVD分解公式$\mathbf{U}\mathbf{\Sigma} \mathbf{V^\dagger} = \mathbf{A}$，把$\mathbf{U}$按列分块，$\mathbf{V^\dagger}$按行分块，有下式成立： \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} \begin{bmatrix} v_1^\dagger\\\\ v_2^\dagger\\\\ \dots\\\\ v_m^\dagger \end{bmatrix}=\mathbf{A}由于 \begin{bmatrix} u_1 & u_2 &\vdots &u_n \end{bmatrix} \begin{bmatrix} \sigma_1 & & & \\\\ & \sigma_2& & \\\\ & & \ddots& \\\\ & & &\sigma_m \end{bmatrix} = \begin{bmatrix} \sigma_1u_1 & \sigma_2u_2 &\vdots &\sigma_nu_n \end{bmatrix}所以， \mathbf{A} = \sum_{i = 1}^{r}\sigma_iu_iv_i^\dagger上面的式子和式里面只有$r$项，是因为当$k &gt; r$时，$\sigma_k = 0$。 所以\mathbf{A} - \hat{\mathbf{A}} = \sum_{i = k+1}^{r}\sigma_iu_iv_i^\dagger 根绝矩阵范数的性质，我们有， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i\left\lVert u_i\right\rVert\left\lVert v_i^\dagger\right\rVert由于$u_i$和$v_i$都是标准正交基，所以范数小于1.故， \left\lVert\mathbf{A} - \hat{\mathbf{A}}\right\rVert \le \sum_{i=k+1}^{r}\sigma_i取无穷范数，可以知道对于误差矩阵中的任意元素（也就是压缩重建之后任意位置的像素灰度值之差），都有： e \le \sum_{i=k+1}^{r}\sigma_iSVD与矩阵范数如果某个函数$f$满足以下的性质，就可以作为矩阵的范数。 $f(\mathbf{A}) = \mathbf{0} \Leftrightarrow \mathbf{A} = \mathbf{0}$ $f(c\mathbf{A}) = c f(\mathbf{A}), \forall c \in \mathbb{R}$ $f(\mathbf{A+b}) \le f(\mathbf{A}) + f(\mathbf{B})$ 其中，矩阵的2范数可以定义为 \left\lVert\mathbf{A}\right\rVert_2 = \max{\sqrt{(\mathbf{A}x)^\dagger\mathbf{A}x}}其中，$x$是单位向量。上式的意义在于表明矩阵的2范数是对于所有向量，经过该矩阵线性变换后摸长最大的那个变换后向量的长度。 下面，给出不严格的说明，证明矩阵的2范数数值上等于其最大的奇异值。 对于空间内的任意单位向量$x$，利用矩阵的SVD分解，有（为了书写简单，矩阵不再单独加粗）： (Ax)^\dagger Ax = x^\dagger V \Sigma^\dagger \Sigma V^\dagger x其中，$U^\dagger U = I$，已经被消去了。 进一步化简，我们将$V^\dagger x$看做一个整体，令$\omega = V\dagger x$，那么有， (Ax)^\dagger Ax = (\Sigma \omega)^\dagger \Sigma \omega也就是说，矩阵的2范转换为了$\Sigma \omega$的幅值的最大值。由于$\omega$是酉矩阵和一个单位向量的乘积，所以$\omega$仍然是单位阵。 由于$\Sigma$是对角阵，所以$\omega$与其相乘后，相当于每个分量分别被放大了$\sigma_i$倍。即 \Sigma \omega = \begin{bmatrix} \sigma_1 \omega_1\\\\ \sigma_2 \omega_2\\\\ \cdots\\\\ \sigma_n \omega_n \end{bmatrix}它的幅值平方为 \left\lVert \Sigma \omega \right \rVert ^2 = \sum_{i=1}^{n}\sigma_i^2 \omega_i^2 \le \sigma_{1} \sum_{i=1}^{n}\omega_i^2 = \sigma_1^2当且仅当，$\omega_1 = 1$, $\omega_k = 0, k &gt; 1$时取得等号。 综上所述，矩阵2范数的值等于其最大的奇异值。 矩阵的另一种范数定义方法Frobenius norm定义如下： \left\lVert A \right\rVert_{F} = \sqrt{\sum_{i=1}^{m}\sum_{j=1}^{n}\left\vert a_{i,j}\right\rvert}如果我们两边平方，可以得到，矩阵的F范数实际等于某个矩阵的迹，见下式： \left\lVert A\right \rVert_F^2 = \text{trace}(A^\dagger A)利用矩阵的SVD分解，可以很容易得出，$\text{trace}(A^\dagger A) = \sum_{i=1}^{r}\sigma_i^2$ 说明如下： \text{trace}(A^\dagger A) = \text{trace}(V\Sigma^\dagger\Sigma V^\dagger)由于$V^\dagger = V^{-1}$，而且$\text{trace}(BAB^{-1}) = \text{trace}(A)$，所以， \text{trace}(A^\dagger A) = \text{trace}(\Sigma^\dagger \Sigma) = \sum_{i=1}^{r}\sigma_i^2也就是说，矩阵的F范数等于它的奇异值平方和的平方根。 \left\lVert A\right\rVert_F= \sqrt{\sum_{i=1}^{r}\sigma_i^2}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[CS131-线代基础]]></title>
      <url>%2F2017%2F01%2F22%2Fcs131-linear-alg%2F</url>
      <content type="text"><![CDATA[CS131课程(Computer Vision: Foundations and Applications)，是斯坦福大学Li Feifei实验室开设的一门计算机视觉入门基础课程，该课程目的在于为刚接触计算机视觉领域的学生提供基本原理和应用介绍。目前2016年冬季课程刚刚结束。CS131博客系列主要是关于本课的slide知识点总结与作业重点问题归纳，作为个人学习本门课程的心得体会和复习材料。 2018/03/20 Update: 这门课的2017秋季课程已经全部放出来，和上个版本相比，作业采用Python实现，同时加入了更多机器学习的内容。详细内容见：CS131 Computer Vision@Fall 2017 由于是个人项目，所以会比较随意，只对个人感兴趣的内容做一总结。这篇文章是对课前线代基础的复习与整理。 向量与矩阵数字图像可以看做二维矩阵，向量是特殊的矩阵，本课程默认的向量都是列向量。slide中给出了一些矩阵行列式和迹的性质，都比较简单，这里不再多说。 矩阵作为线性变换通过线代知识，我们知道，在线性空间中，如果给定一组基，线性变换可以通过对应的矩阵来进行描述。 scale变换对角阵可以用来表示放缩变换。 \begin{bmatrix} s_x & 0\\\\ 0 & s_y \end{bmatrix}\begin{bmatrix} x\\\\ y \end{bmatrix} = \begin{bmatrix} s_xx\\\\ s_yy \end{bmatrix}旋转变换如图所示，逆时针旋转$theta$角度，对应的旋转矩阵为： \mathbf{R} = \begin{bmatrix} \cos\theta &-\sin\theta \\\\ \sin\theta &\cos\theta \end{bmatrix}旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$旋转矩阵是酉矩阵，矩阵内的各列（或者各行）相互正交。满足如下的关系式： \mathbf{R}\mathbf{R^{\dagger}} = \mathbf{I}由于$\det{\mathbf{R}} = \det{\mathbf{R^{\dagger}}}$，所以，对于酉矩阵，$\det{\mathbf{R}} = \pm 1$. 齐次变换(Homogeneous Transform)只用上面的二维矩阵不能表达平移，使用齐次矩阵可以表达放缩，旋转和平移操作。 \mathbf{H} =\begin{bmatrix} a & b & t_x\\\\ c & d & t_y\\\\ 0 & 0 & 1 \end{bmatrix},\mathbf{H}\begin{bmatrix} x\\\\ y\\\\ 1\\\\ \end{bmatrix}=\begin{bmatrix} ax+by+t_x\\\\ cx+dy+t_y\\\\ 1 \end{bmatrix}SVD分解可以将矩阵分成若干个矩阵的乘积，叫做矩阵分解，比如QR分解，满秩分解等。SVD分解，即奇异值分解，也是一种特殊的矩阵分解方法。如下式所示，是将矩阵分解成为三个矩阵的乘积： \mathbf{U}\mathbf{\Sigma}\mathbf{V^\dagger} = \mathbf{A}其中矩阵$\mathbf{A}$大小为$m\times n$，矩阵$\mathbf{U}$是大小为$m\times m$的酉矩阵，$\mathbf{V}$是大小为$n \times n$的酉矩阵，$\mathbf{\Sigma}$是大小为$m \times n$的旋转矩阵，即只有主对角元素不为0. SVD分解在主成分分析中年很有用。由于矩阵$\mathbf{\Sigma}$一般情况下是将奇异值按照从大到小的顺序摆放，所以矩阵$\mathbf{U}$中，前面的若干列被视作主成分，后面的列显得相对不这么重要。可以抛弃后面的列，进行图像压缩。 如下图，是使用前10个分量对原图片进行压缩的效果。 12345678910im = imread('./superman.png');im_gray = rbg2gray(im);[u, s, v] = svd(double(im_gray));k = 10;uk = u(:, 1:k);sigma = diag(s);sk = diag(sigma(1:k));vk = v(:, 1:k);im_k = uk*sk*vk';imshow(uint8(im_k))]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Hello World]]></title>
      <url>%2F2016%2F12%2F16%2Fhello-world%2F</url>
      <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment Code highlightHello World! 1234#include &lt;iostream&gt;int main() &#123; std::cout &lt;&lt; "HelloWorld\n";&#125; 1print 'HelloWorld' Latex Support by MathjaxMass-energy equation by Einstein: $E = mc^2$ a linear equation: \mathbf{A}\mathbf{v} = \mathbf{y}]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[使用 Visual Studio 编译 GSL 科学计算库]]></title>
      <url>%2F2016%2F12%2F16%2Fgsl-with-vs%2F</url>
      <content type="text"><![CDATA[GSL是一个GNU支持的科学计算库，提供了很丰富的数值计算方法。本文介绍了GSL库在Windows环境下使用VisualStudio进行编译构建的过程。 GSL 的项目主页提供的说明来看，GSL支持如下的科学计算： （下面的这张表格的HTML使用的是No-Cruft Excel to HTML Table Converter生成的） Complex Numbers Roots of Polynomials Special Functions Vectors and Matrices Permutations Sorting BLAS Support Linear Algebra Eigensystems Fast Fourier Transforms Quadrature Random Numbers Quasi-Random Sequences Random Distributions Statistics Histograms N-Tuples Monte Carlo Integration Simulated Annealing Differential Equations Interpolation Numerical Differentiation Chebyshev Approximation Series Acceleration Discrete Hankel Transforms Root-Finding Minimization Least-Squares Fitting Physical Constants IEEE Floating-Point Discrete Wavelet Transforms Basis splines GSL的Linux下的配置很简单，照着它的INSTALL文件一步一步来就可以了。CMAKE大法HAO! 1234./configuremakemake installmake clean 同样的，GSL也可以在Windows环境下配置，下面记录了如何在Windows环境下使用 Visual Studio 和 CMakeGUI 编译测试GSL。 使用CMAKE编译成.SLN文件打开CMAKEGUI，将输入代码路径选为GSL源代码地址，输出路径设为自己想要的输出路径。点击 “Configure“，选择Visual Studio2013为编译器，点击Finish后会进行必要的配置。然后将表格里面的选项都打上勾，再次点击”Configure“，等待完成之后点击”Generate“。完成之后，就可以在输出路径下看到GSL.sln文件了。 使用Visual Studio生成解决方案使用 Visual Studio 打开刚才生成的.SLN文件，分别在Debug和Release模式下生成解决方案，等待完成即可。 当完成后，你应该可以在路径下看到这样一张图，我们主要关注的文件夹是\bin，\gsl，\Debug和\Release。 加入环境变量修改环境变量的Path，将\GSL_Build_Path\bin\Debug加入，这主要是为了\Debug文件夹下面的gsl.dll文件。如果不进行这一步的话，一会虽然可以编译，但是却不能运行。 这里顺便注释一句，当使用第三方库的时候，如果需要动态链接库的支持，其中一种方法就是将DLL文件的路径加入到Path中去。 建立Visual Studio属性表Visual Studio可以通过建立工程属性表的方法来配置工程选项，一个OpenCV的例子可以参见Yuanbo She的这篇博文 Opencv 完美配置攻略 2014 (Win8.1 + Opencv 2.4.8 + VS 2013)。 配置文件中主要是包含文件和静态链接库LIB的路径设置。下面把我的贴出来，只需要根据GSL的生成路径做相应修改即可。注意我的属性表中保留了OpenCV的内容，如果不需要的话，尽可以删掉。上面的博文对这张属性表如何配置讲得很清楚，有问题可以去参考。 12345678910111213141516171819&lt;?xml version="1.0" encoding="utf-8"?&gt;&lt;Project ToolsVersion="4.0" xmlns="http://schemas.microsoft.com/developer/msbuild/2003"&gt; &lt;ImportGroup Label="PropertySheets" /&gt; &lt;PropertyGroup Label="UserMacros" /&gt; &lt;PropertyGroup&gt; &lt;IncludePath&gt;$(OPENCV249)\include;E:\GSLCode\gsl-build\;$(IncludePath)&lt;/IncludePath&gt; &lt;LibraryPath Condition="'$(Platform)'=='Win32'"&gt;$(OPENCV249)\x86\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;LibraryPath Condition="'$(Platform)'=='X64'"&gt;$(OPENCV249)\x64\vc12\lib;E:\GSLCode\gsl-build\Debug;$(LibraryPath)&lt;/LibraryPath&gt; &lt;/PropertyGroup&gt; &lt;ItemDefinitionGroup&gt; &lt;Link Condition="'$(Configuration)'=='Debug'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249d.lib;opencv_contrib249d.lib;opencv_core249d.lib;opencv_features2d249d.lib;opencv_flann249d.lib;opencv_gpu249d.lib;opencv_highgui249d.lib;opencv_imgproc249d.lib;opencv_legacy249d.lib;opencv_ml249d.lib;opencv_nonfree249d.lib;opencv_objdetect249d.lib;opencv_ocl249d.lib;opencv_photo249d.lib;opencv_stitching249d.lib;opencv_superres249d.lib;opencv_ts249d.lib;opencv_video249d.lib;opencv_videostab249d.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;Link Condition="'$(Configuration)'=='Release'"&gt; &lt;AdditionalDependencies&gt;opencv_calib3d249.lib;opencv_contrib249.lib;opencv_core249.lib;opencv_features2d249.lib;opencv_flann249.lib;opencv_gpu249.lib;opencv_highgui249.lib;opencv_imgproc249.lib;opencv_legacy249.lib;opencv_ml249.lib;opencv_nonfree249.lib;opencv_objdetect249.lib;opencv_ocl249.lib;opencv_photo249.lib;opencv_stitching249.lib;opencv_superres249.lib;opencv_ts249.lib;opencv_video249.lib;opencv_videostab249.lib;gsl.lib;gslcblas.lib;%(AdditionalDependencies)&lt;/AdditionalDependencies&gt; &lt;/Link&gt; &lt;/ItemDefinitionGroup&gt; &lt;ItemGroup /&gt;&lt;/Project&gt; 在以后建立Visual Studio工程的时候，在属性窗口直接添加现有属性表就可以了！ 测试在项目网站的教程上直接找到一段代码，进行测试，输出贝塞尔函数的值。 123456789#include &lt;stdio.h&gt;#include &lt;gsl/gsl_sf_bessel.h&gt;int main(void)&#123; double x = 5.0; double y = gsl_sf_bessel_J0(x); printf("J0(%g) = %.18e\n", x, y); return 0;&#125; 控制台输出正确：]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Windows环境下使用Doxygen生成注释文档]]></title>
      <url>%2F2016%2F12%2F16%2Fuse-doxygen%2F</url>
      <content type="text"><![CDATA[Doxygen 是一种很好用的代码注释生成工具，然而和很多国外的工具软件一样，在中文环境下，它的使用总是会出现一些问题，也就是中文注释文档出现乱码。经过调试，终于是解决了这个问题。 安装 DoxygenDoxygen 在Windows平台下的安装比较简单，Doxygen的项目主页提供了下载和安装的使用说明，可以下载它们的官方使用手册进行阅读。对于Windows，提供了源代码编译安装和直接安装程序安装两种方式，可以自行选择。 安装成功后，使用命令行命令 1doxygen --help 就可以查看帮助文档，对应参数含义一目了然，降低了入手难度。 使用命令， 1doxygen -g doxygen_filename 就可以在当前目录下建立一个doxygen配置文件，用文本编辑器打开就可以编辑里面的配置选项。 使用命令， 1doxygen doxygen_filename 就可以生成注释文档了。 下面就来说一说对中文的支持。 生成 HTML 格式文档中文之所以乱码，很多时候是由于编码和译码格式不同，所以我们需要先知道自己代码文件的编码方式。我的代码都是建立在Visual Studio上的，可以通过VS的高级保存选项查看自己代码文件的存储编码格式。对于中文版的VS，一般应该是GB2312。 我们打开 Doxygen 的配置文件，将里面的 INPUT_ENCODING 改为我们代码文件的编码格式，这里就改成 GB2312。 这样一来，编译出来的 HTML 页面就不会有中文乱码了。 生成Latex 格式文档生成 Latex 需要本机上安装有 Latex 的编译环境。如果是中文用户，推荐的是CTEX套件，可以到他们的网站上去下载。 可以看到，Doxygen为Latex文件的编译生成了make文件，我们在命令行窗口中执行make命令就可以完成编译，然而这时候会发现编译出错，pdf文档无法生成。 打开生成的refman.latex文档，添加宏包 CJKutf8。然后找到 \begin{document}一行，将其改为 12\begin&#123;document&#125;\begin&#123;CJK&#125;&#123;UTF8&#125;&#123;gbsn&#125; 也就是说为正文提供了CJK环境，这样中文文本就可以正常编译了。 相应的，我们要将结尾的 \end{document)改为：12\end&#123;CJK&#125;\end&#123;document&#125; 这样，运行make命令之后，就可以看到中文的注释文档了。]]></content>
    </entry>

    
    <entry>
      <title><![CDATA[Python Regular Expressions （Python 正则表达式)]]></title>
      <url>%2F2014%2F07%2F17%2Fpython-reg-exp%2F</url>
      <content type="text"><![CDATA[本文来自于Google Developers中对于Python的介绍。https://developers.google.com/edu/python/regular-expressions。 认识正则表达式Python的正则表达式是使用 re 模块的。 12345match = re.search(pattern,str)if match: print 'found',match.group()else: print 'NOT Found!' 正则表达式的规则基本规则 a, x, 9 都是普通字符 (ordinary characters) . (一个点)可以匹配任何单个字符（除了’\n’） \w（小写的w）可以匹配一个单词里面的字母，数字或下划线 [a-zA-Z0-9_];\W （大写的W）可以匹配非单词里的这些元素 \b 匹配单词与非单词的分界 \s（小写的s）匹配一个 whitespace character，包括 space，newline，return，tab，form(\n\r\t\f)；\S（大写的S）匹配一个非 whitespace character \d 匹配十进制数字 [0-9] ^=start，$=end 用来匹配字符串的开始和结束 \ 是转义字符，用 . 来匹配串里的’.’，等一些基本的例子 12345678910## 在字符串'piiig'中查找'iii'match = re.search(r'iii', 'piiig') # found, match.group() == "iii"match = re.search(r'igs', 'piiig') # not found, match == None## . 匹配除了\n的任意字符match = re.search(r'..g', 'piiig') # found, match.group() == "iig"## \d 匹配0-9的数字字符, \w 匹配单词里的字符match = re.search(r'\d\d\d', 'p123g') # found, match.group() == "123"match = re.search(r'\w\w\w', '@@abcd!!') # found, match.group() == "abc" 重复可以用’+’ ‘*’ ‘?’来匹配0个，1个或多个重复字符。 ‘+’ 用来匹配1个或者多个字符 ‘*’ 用来匹配0个或者多个字符 ‘?’ 用来匹配0个或1个字符 注意，’+’和’*’会匹配尽可能多的字符。 一些重复字符的例子12345678910111213141516## i+ 匹配1个或者多个'i'match = re.search(r'pi+', 'piiig') # found, match.group() == "piii"## 找到字符串中最左边尽可能长的模式。## 注意，并没有匹配到第二个 'i+'match = re.search(r'i+', 'piigiiii') # found, match.group() == "ii"## \s* 匹配0个或1个空白字符 whitespacematch = re.search(r'\d\s*\d\s*\d', 'xx1 2 3xx') # found, match.group() == "1 2 3"match = re.search(r'\d\s*\d\s*\d', 'xx12 3xx') # found, match.group() == "12 3"match = re.search(r'\d\s*\d\s*\d', 'xx123xx') # found, match.group() == "123"## ^ 匹配字符串的第一个字符match = re.search(r'^b\w+', 'foobar') # not found, match == None## 与上例对比match = re.search(r'b\w+', 'foobar') # found, match.group() == "bar" Email考虑一个典型的Email地址：someone@host.com，可以用如下的方式匹配： 1match = re.search(r'\w+@\w+',str) 但是，对于这种Email地址 xyz alice-b@google.com purple monkey则不能奏效。 使用方括号方括号里面的字符表示一个字符集合。[abc]可以被用来匹配’a’或者’b’或者’c’。\w \s等都可以用在方括号里，除了’.’以外，它只能用来表示字面意义上的‘点’。所以上面的Email规则可以扩充如下： 1match = re.search('r[\w.-]+@[\w.-]+',str) 你还可以使用’-‘来指定范围，如[a-z]指示的是所有小写字母的集合。所以如果你想构造的字符集合中有’-‘，请把它放到末尾[ab-]。另外，前方加上’^’，用来表示取集合的补集，例如ab表示除了’a’和’b’之外的其他字符。 操作以Email地址为例，如果我们想要分别提取该地址的用户名’someone’和主机名’host.com’该怎么办呢？可以在模式中用圆括号指定。 123456str = 'purple alice-b@google.com monkey dishwasher'match = re.search('([\w.-]+)@([\w.-]+)', str) #用圆括号指定分割if match: print match.group() ## 'alice-b@google.com' (the whole match) print match.group(1) ## 'alice-b' (the username, group 1) print match.group(2) ## 'google.com' (the host, group 2) findall 函数与group函数只找到最左端的一个匹配不同，findall函数找到字符串中所有与模式匹配的串。 12345str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## findall返回一个包含所有匹配结果的 listemails = re.findall(r'[\w\.-]+@[\w\.-]+', str) ## ['alice@google.com', 'bob@abc.com']for email in emails: print email 在文件中使用findall当然可以读入文件的每一行，然后对每一行的内容调用findall，但是为什么不让这一切自动发生呢？ 12f = open(filename.txt,'r')matches = re.findall(pattern,f.read()) findall 和分组和group的用法相似，也可以指定分组。 12345678str = 'purple alice@google.com, blah monkey bob@abc.com blah dishwasher'## 返回了一个listtuples = re.findall(r'([\w\.-]+)@([\w\.-]+)', str)print tuples ## [('alice', 'google.com'), ('bob', 'abc.com')]## list中的元素是tuplefor tuple in tuples: print tuple[0] ## username print tuple[1] ## host 调试正则表达式异常强大，使用简单的几条规则就可以演变出很多的模式组合。在确定你的模式之前，可能需要很多的调试工作。在一个小的测试集合上测试正则表达式。 其他选项正则表达式还可以设置“选项”。 1match = re.search(pat,str,opt) 这些可选项如下： IGNORECASE 忽视大小写 DOTALL 允许’.’匹配’\n’ MULTILINE 在一个由许多行组成的字符串中，允许’^’和’$’匹配每一行的开始和结束]]></content>
    </entry>

    
  
  
</search>
