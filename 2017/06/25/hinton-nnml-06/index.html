<!doctype html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">



<meta http-equiv="Cache-Control" content="no-transform">
<meta http-equiv="Cache-Control" content="no-siteapp">












  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css">




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css">

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css">


  <meta name="keywords" content="公开课,deep learning,pytorch,">





  <link rel="alternate" href="/atom.xml" title="来呀，快活呀~" type="application/atom+xml">




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0">






<meta name="description" content="第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达">
<meta name="keywords" content="公开课,deep learning,pytorch">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法">
<meta property="og:url" content="https://xmfbit.github.io/2017/06/25/hinton-nnml-06/index.html">
<meta property="og:site_name" content="来呀，快活呀~">
<meta property="og:description" content="第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="https://xmfbit.github.io/img/contours_evaluation_optimizers.gif">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_nesterov_momentum.png">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_learningrate.png">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_tricks_for_adaptive_lr.png">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_rmsprop_improvement.png">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_summary.png">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_maanmian.jpg">
<meta property="og:image" content="https://xmfbit.github.io/img/hinton_06_adamax.png">
<meta property="og:updated_time" content="2020-05-04T14:38:02.772Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法">
<meta name="twitter:description" content="第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达">
<meta name="twitter:image" content="https://xmfbit.github.io/img/contours_evaluation_optimizers.gif">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    sidebar: {"position":"left","display":"post"},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://xmfbit.github.io/2017/06/25/hinton-nnml-06/">





  <title> Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法 | 来呀，快活呀~ </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  


<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
            (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
          m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
  ga('create', 'UA-89140122-1', 'auto');
  ga('send', 'pageview');
</script>









  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  

  <div class="custom-logo-site-title">
    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <span class="site-title">来呀，快活呀~</span>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>
    
      <p class="site-subtitle"></p>
    
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br>
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br>
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br>
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br>
            
            关于
          </a>
        </li>
      

      
        <li class="menu-item menu-item-search">
          
            <a href="javascript:;" class="popup-trigger">
          
            
              <i class="menu-item-icon fa fa-search fa-fw"></i> <br>
            
            搜索
          </a>
        </li>
      
    </ul>
  

  
    <div class="site-search">
      
  <div class="popup">
 <span class="search-icon fa fa-search"></span>
 <input type="text" id="local-search-input">
 <div id="local-search-result"></div>
 <span class="popup-btn-close">close</span>
</div>


    </div>
  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
  <link itemprop="mainEntityOfPage" href="https://xmfbit.github.io/2017/06/25/hinton-nnml-06/">

  <span style="display:none" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <meta itemprop="name" content="一个脱离了高级趣味的人">
    <meta itemprop="description" content>
    <meta itemprop="image" content="/avatar/liumengli.jpg">
  </span>

  <span style="display:none" itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
    <meta itemprop="name" content="来呀，快活呀~">
    <span style="display:none" itemprop="logo" itemscope itemtype="http://schema.org/ImageObject">
      <img style="display:none;" itemprop="url image" alt="来呀，快活呀~" src>
    </span>
  </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Neural Network for Machine Learning - Lecture 06 神经网络的“调教”方法
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="Post created" itemprop="dateCreated datePublished" datetime="2017-06-25T13:48:31+00:00">
                2017-06-25
              </time>
            

            

            
          </span>

          

          
            
          

          

          
          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>第六周的课程主要讲解了用于神经网络训练的梯度下降方法，首先对比了SGD，full batch GD和mini batch SGD方法，然后给出了几个用于神经网络训练的trick，主要包括输入数据预处理（零均值，单位方差以及PCA解耦），学习率的自适应调节以及网络权重的初始化方法（可以参考各大框架中实现的Xavier初始化方法等）。这篇文章主要记录了后续讲解的几种GD变种方法，如何合理利用梯度信息达到更好的训练效果。由于Hinton这门课确实时间已经很久了，所以文章末尾会结合一篇不错的总结性质的<a href="http://sebastianruder.com/optimizing-gradient-descent/index.html" target="_blank" rel="noopener">博客</a>和对应的<a href="https://arxiv.org/abs/1609.04747" target="_blank" rel="noopener">论文</a>以及PyTorch中的相关代码，对目前流行的梯度下降方法做个总结。</p>
<p>下图即来自上面的这篇博客。</p>
<p><img src="/img/contours_evaluation_optimizers.gif" alt="几种优化方法的可视化"><br><a id="more"></a></p>
<h2 id="Momentum"><a href="#Momentum" class="headerlink" title="Momentum"></a>Momentum</h2><p>我们可以把训练过程想象成在权重空间的一个质点（小球），移动到全局最优点的过程。不同于GD，使用梯度信息直接更新权重的位置，momentum方法是将梯度作为速度量。这样做的好处是，当梯度的方向一直不变时，速度可以加快；当梯度方向变化剧烈时，由于符号改变，所以速度减慢，起到了GD中自适应调节学习率的过程。</p>
<p>具体来说，我们利用新得到的梯度信息，采用滑动平均的方法更新速度。式子中的$\epsilon$为学习率，$\alpha$为momentum系数。</p>
<script type="math/tex; mode=display">\Delta w_t = v_t = \alpha v_{t-1} - \epsilon g_t = \Delta w_t - \epsilon g_t</script><p>为了说明momentum确实对学习过程有加速作用，假设一个简单的情形，即运动轨迹是一个斜率固定的斜面。那么我们有梯度$g$固定。根据上面的递推公式可以得到通项公式（简单的待定系数法凑出等比数列）：</p>
<script type="math/tex; mode=display">v_t = \alpha(v_{t-1} + \frac{\epsilon g}{1-\alpha}) - \frac{\epsilon g}{1-\alpha}</script><p>由于$\alpha &lt; 0$，所以当$t = \infty$时，只剩下了后面的常数项，即：</p>
<script type="math/tex; mode=display">v_\infty = -\frac{\epsilon}{1-\alpha}g</script><p>也就是说，权重更新的幅度变成了原来的$\frac{1}{1-\alpha}$倍。若取$\alpha=0.99$，则加速$100$倍。</p>
<p>Hinton给出的建议是由于训练开头梯度值比较大，所以momentum系数一开始不要过大，例如可以取$0.5$。当梯度值较小，训练过程被困在一个峡谷的时候，可以适当提升。</p>
<p>一种改进方法由Nesterov提出。在上面的方法中，我们首先更新了在该处的累积梯度信息，然后向前移动。而Nesterov方法中，我们首先沿着累计梯度信息向前走，然后根据梯度信息进行更正。</p>
<p><img src="/img/hinton_06_nesterov_momentum.png" alt="Nesterov方法"></p>
<h2 id="Adaptive-Learning-Rate"><a href="#Adaptive-Learning-Rate" class="headerlink" title="Adaptive Learning Rate"></a>Adaptive Learning Rate</h2><p>这种方法起源于这样的观察：在网络中，不同layer之间的权重更新需要不同的学习率。因为浅层和深层的layer梯度幅值很可能不同。所以，对不同的权重乘上不同的因子是个更加合理的选择。</p>
<p>例如，我们可以根据梯度是否发生符号变化按照下面的方式调节某个权重$w_{ij}$的增益。注意$0.95$和$0.05$的和是$1$。这样可以使得平衡点在$1$附近。<br><img src="/img/hinton_06_learningrate.png" alt="Different learning rate gain"></p>
<p>下面是使用这种方法的几个trick，包括限幅，较大的batch size以及和momentum的结合。</p>
<p><img src="/img/hinton_06_tricks_for_adaptive_lr.png" alt="Tricks for adaptive lr"></p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>rprop利用梯度的符号，如果符号保持不变，则相应增大step size；否则减小。但是只能用于full batch GD。RMSProp就是一种可以结合mini batch SGD和rprop的一种方法。</p>
<p>我们使用滑动平均方法更新梯度的mean square（即RMS中的MS得来）。</p>
<script type="math/tex; mode=display">\text{MeanSquare}(w, t) = 0.9 \text{MeanSquare}(w, t-1) + 0.1g_t^2</script><p>然后，将梯度除以上面的得到的Mean Square值。</p>
<p>RMSProp还有一些变种，列举如下：<br><img src="/img/hinton_06_rmsprop_improvement.png" alt="Otehr RMSProp"></p>
<h2 id="课程总结"><a href="#课程总结" class="headerlink" title="课程总结"></a>课程总结</h2><ul>
<li>对于小数据集，使用full batch GD（LBFGS或adaptive learning rate如rprop）。</li>
<li>对于较大数据集，使用mini batch SGD。并可以考虑加上momentmum和RMSProp。</li>
</ul>
<p>如何选择学习率是一个较为依赖经验的任务（网络结构不同，任务不同）。<br><img src="/img/hinton_06_summary.png" alt="总结"></p>
<h2 id="“Modern”-SGD"><a href="#“Modern”-SGD" class="headerlink" title="“Modern” SGD"></a>“Modern” SGD</h2><p>从本部分开始，我将转向总结摘要中提到的那篇博客中的主要内容。首先，给出当前基于梯度的优化方法的一些问题。可以看到，之后人们提出的改进方法就是想办法解决对应问题的。由于与Hinton课程相比，这些方法提出时间（也许称之为流行时间更合适？做数学的那帮人可能很早就知道这些优化方法了吧？）较短，所以这里仿照Modern C++之称呼，就把它们统一叫做Modern SGD吧。。。</p>
<ul>
<li>学习率通常很难确定。学习率太大？容易扯到蛋（loss直接爆炸）；学习率太小，训练到天荒地老。。。</li>
<li>学习率如何在训练中调整。目前常用的方法是退火，要么是固定若干次迭代之后把学习率调小，要么是观察loss到某个阈值后把学习率调小。总之，都是在训练开始前，人工预先定义好的。而这没有考虑到数据集自身的特点。</li>
<li>学习率对每个网络参数都一样。这点在上面课程中Hinton已经提到，引出了自适应学习率的方法。</li>
<li>高度非凸函数的优化难题。以前人们多是认为网络很容易收敛到局部极小值。后来有人提出，网络之所以难训练，更多是由于遇到了鞍点。也就是某个方向上它是极小值；而另一个方向却是极大值（高数中介绍过的，马鞍面）</li>
</ul>
<p><img src="/img/hinton_06_maanmian.jpg" alt="马鞍面"></p>
<h3 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h3><p><a href="http://jmlr.org/papers/v12/duchi11a.html" target="_blank" rel="noopener">Adagrad</a>对不同的参数采用不同的学习率，也是其Ada（Adaptive）的名字得来。我们记时间步$t$时标号为$i$的参数对应的梯度为$g_{i}$，即：</p>
<script type="math/tex; mode=display">g_{i} = \bigtriangledown_{\theta_i} J(\theta)</script><p>Adagrad使用一个系数来为不同的参数修正学习率，如下：</p>
<script type="math/tex; mode=display">\hat{g_i} = \frac{1}{\sqrt{G_i+\epsilon}}g_i</script><p>其中，$G_i$是截止到当前时间步$t$时，参数$\theta_i$对应梯度$g_i$的平方和。</p>
<p>我们可以把上面的式子写成矩阵形式。其中，$\odot$表示逐元素的矩阵相乘（element-wise product）。同时，$G_t = g_t \odot g_t$。</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{G_t+\epsilon}}\odot g_t</script><p>我们再来看PyTorch中的相关实现：</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for each gradient of parameters:</span></span><br><span class="line"><span class="comment"># addcmul(t, alpha, t1, t2): t = t1*t2*alpha + t</span></span><br><span class="line"><span class="comment"># let epsilon = 1E-10</span></span><br><span class="line">state[<span class="string">'sum'</span>].addcmul_(<span class="number">1</span>, grad, grad)   <span class="comment"># 计算 G</span></span><br><span class="line">std = state[<span class="string">'sum'</span>].sqrt().add_(<span class="number">1e-10</span>)  <span class="comment"># 计算 \sqrt(G)</span></span><br><span class="line">p.data.addcdiv_(-clr, grad, std)       <span class="comment"># 更新</span></span><br></pre></td></tr></table></figure>
<p>由于Adagrad对不同的梯度给了不同的学习率修正值，所以使用这种方法时，我们可以不用操心学习率，只是给定一个初始值（如$0.01$）就够了。尤其是对稀疏的数据，Adagrad方法能够自适应调节其梯度更新信息，给那些不常出现（非零）的梯度对应更大的学习率。PyTorch中还为稀疏数据特别优化了更新算法。</p>
<p>Adagrad的缺点在于由于$G_t$矩阵是平方和，所以分母会越来越大，造成训练后期学习率会变得很小。下面的Adadelta方法针对这个问题进行了改进。</p>
<h3 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h3><p><a href="https://arxiv.org/abs/1212.5701" target="_blank" rel="noopener">Adadelta</a>给出的改进方法是不再记录所有的历史时刻的$g$的平方和，而是最近一个有限的观察窗口$w$的累积梯度平方和。在实际使用时，这种方法使用了一个参数$\gamma$（如$0.9$）作为遗忘因子，对$E[g_t^2]$进行统计。</p>
<script type="math/tex; mode=display">E[g_t^2] = \gamma E[g_{t-1}^2] + (1-\gamma)g_t^2</script><p>由于$\sqrt{E[g_t^2]}$就是$g$的均方根RMS，所以，修正后的梯度如下。注意到，这正是Hinton在课上所讲到的RMSprop的优化方法。</p>
<script type="math/tex; mode=display">\hat{g}_t = \frac{1}{\text{RMS}[g]}g_t</script><p>作者还观察到，这样更新的话，其实$\theta$和$\Delta \theta$的单位是不一样的（此时$\Delta \theta$是无量纲数）。所以，作者提出再乘上一个$\text{RMS}[\Delta \theta]$来平衡（同时去掉了学习率$\eta$），所以，最终的参数更新如下：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\text{RMS}[\Delta \theta]}{\text{RMS}[g]}g_t</script><p>这种方法甚至不再需要学习率。下面是PyTorch中的实现，其中仍然保有学习率<code>lr</code>这一参数设定，默认值为$1.0$。代码注释中，我使用<code>MS</code>来指代$E[x^2]$。即，$\text{RMS}[x] = \sqrt{\text{MS}[x]+\epsilon}$。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># update: MS[g] = MS[g]*\rho + g*g*(1-\rho)</span></span><br><span class="line">square_avg.mul_(rho).addcmul_(<span class="number">1</span> - rho, grad, grad)</span><br><span class="line"><span class="comment"># current RMS[g] = sqrt(MS[g] + \epsilon)</span></span><br><span class="line">std = square_avg.add(eps).sqrt_()</span><br><span class="line"><span class="comment"># \Delta \theta = RMS[\Delta \theta] / RMS[g]) * g</span></span><br><span class="line">delta = acc_delta.add(eps).sqrt_().div_(std).mul_(grad)</span><br><span class="line"><span class="comment"># update parameter: \theta -= lr * \Delta \theta</span></span><br><span class="line">p.data.add_(-group[<span class="string">'lr'</span>], delta)</span><br><span class="line"><span class="comment"># update MS[\Delta \theta] = MS[\Delta \theta] * \rho + \Delta \theta^2 * (1-\rho)</span></span><br><span class="line">acc_delta.mul_(rho).addcmul_(<span class="number">1</span> - rho, delta, delta)</span><br></pre></td></tr></table></figure></p>
<h3 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h3><p><a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">Adaptive momen Estimation（Adam，自适应矩估计）</a>，是另一种为不同参数自适应设置不同学习率的方法。Adam方法不止存储过往的梯度平方均值（二阶矩）信息，还存储过往的梯度均值信息（一阶矩）。</p>
<script type="math/tex; mode=display">\begin{aligned}m_t&=\beta_1 m_{t-1}+(1-\beta_1)g_t\\v_t&=\beta_2 v_{t-1}+(1-\beta_2)g_t^2\end{aligned}</script><p>作者观察到上述估计是有偏的（biase towards $0$），所以给出如下修正：</p>
<script type="math/tex; mode=display">\begin{aligned}\hat{m} &= \frac{m}{1-\beta_1}\\ \hat{v}&=\frac{v}{1-\beta_2}\end{aligned}</script><p>参数的更新如下：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t - \frac{\eta}{\sqrt{\hat{v_t} + \epsilon}}\hat{m_t}</script><p>作者给出$\beta_1 = 0.9$，$\beta_2=0.999$，$\epsilon=10^{-8}$。</p>
<p>为了更好地理解PyTorch中的实现方式，需要对上式进行变形：</p>
<script type="math/tex; mode=display">\Delta \theta = \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta \frac{m_t}{\sqrt{v_t}}</script><p>代码中令$\text{step_size} =  \frac{\sqrt{1-\beta_2}}{1-\beta_1}\eta$。同时，$\beta$也要以指数规律衰减，即：$\beta_t = \beta_0^t$。</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># exp_avg is `m`: expected average of g</span></span><br><span class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line"><span class="comment"># exp_avg_sq is `v`: expected average of g's square</span></span><br><span class="line">exp_avg_sq.mul_(beta2).addcmul_(<span class="number">1</span> - beta2, grad, grad)</span><br><span class="line"></span><br><span class="line"><span class="comment"># \sqrt&#123;v_t + \epsilon&#125;</span></span><br><span class="line">denom = exp_avg_sq.sqrt().add_(group[<span class="string">'eps'</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1 - \beta_1^t</span></span><br><span class="line">bias_correction1 = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line"><span class="comment"># 1 - \beta_2^t</span></span><br><span class="line">bias_correction2 = <span class="number">1</span> - beta2 ** state[<span class="string">'step'</span>]</span><br><span class="line"><span class="comment"># get step_size</span></span><br><span class="line">step_size = group[<span class="string">'lr'</span>] * math.sqrt(bias_correction2) / bias_correction1</span><br><span class="line"><span class="comment"># delta = -step_size * m / sqrt(v)</span></span><br><span class="line">p.data.addcdiv_(-step_size, exp_avg, denom)</span><br></pre></td></tr></table></figure>
<h3 id="AdaMax"><a href="#AdaMax" class="headerlink" title="AdaMax"></a>AdaMax</h3><p>上面Adam中，实际上我们是用梯度$g$的$2$范数（$\sqrt{\hat{v_t}}$）去对$g$进行Normalization。那么为什么不用其他形式的范数$p$来试试呢？然而，对于$1$范数和$2$范数，数值是稳定的。对于再大的$p$，数值不稳定。不过，当取无穷范数的时候，又是稳定的了。</p>
<p>由于无穷范数就是求绝对值最大的分量，所以这种方法叫做<a href="https://arxiv.org/abs/1412.6980" target="_blank" rel="noopener">AdaMax</a>。其对应的$\hat{v_t}$为（这里为了避免混淆，使用$u_t$指代）：</p>
<script type="math/tex; mode=display">u_t = \beta_2^\infty u_{t-1} + (1-\beta_2^\infty) g_t^\infty</script><p>我们将$u_t$按照时间展开，可以得到（直接摘自论文的图）。其中最后一步递推式的得来：根据$u_t$把$u_{t-1}$的展开形式也写出来，就不难发现最下面的递推形式。</p>
<p><img src="/img/hinton_06_adamax.png" alt="Adamax中ut的推导"></p>
<p>相应的更新权重操作为：</p>
<script type="math/tex; mode=display">\theta_{t+1} = \theta_t -\frac{\eta}{u_t}\hat{m}_t</script><p>在PyTorch中的实现如下：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Update biased first moment estimate, which is \hat&#123;m&#125;_t</span></span><br><span class="line">exp_avg.mul_(beta1).add_(<span class="number">1</span> - beta1, grad)</span><br><span class="line"><span class="comment"># 下面这种用来逐元素求取 max(A, B) 的方法可以学习一个</span></span><br><span class="line"><span class="comment"># Update the exponentially weighted infinity norm.</span></span><br><span class="line">norm_buf = torch.cat([</span><br><span class="line">    exp_inf.mul_(beta2).unsqueeze(<span class="number">0</span>),</span><br><span class="line">    grad.abs().add_(eps).unsqueeze_(<span class="number">0</span>)</span><br><span class="line">], <span class="number">0</span>)</span><br><span class="line"><span class="comment">## 找到 exp_inf 和 g之间的较大者（只需要在刚刚聚合的这个维度上找即可~）</span></span><br><span class="line">torch.max(norm_buf, <span class="number">0</span>, keepdim=<span class="literal">False</span>, out=(exp_inf, exp_inf.new().long()))</span><br><span class="line"></span><br><span class="line"><span class="comment">## beta1 correction</span></span><br><span class="line">bias_correction = <span class="number">1</span> - beta1 ** state[<span class="string">'step'</span>]</span><br><span class="line">clr = group[<span class="string">'lr'</span>] / bias_correction</span><br><span class="line"></span><br><span class="line">p.data.addcdiv_(-clr, exp_avg, exp_inf)</span><br></pre></td></tr></table></figure></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/公开课/" rel="tag"># 公开课</a>
          
            <a href="/tags/deep-learning/" rel="tag"># deep learning</a>
          
            <a href="/tags/pytorch/" rel="tag"># pytorch</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2017/06/23/effective-cpp-07/" rel="next" title="Effective CPP 阅读 - Chapter 7 模板与泛型编程">
                <i class="fa fa-chevron-left"></i> Effective CPP 阅读 - Chapter 7 模板与泛型编程
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2017/07/03/effective-cpp-08/" rel="prev" title="Effective CPP 阅读 - Chapter 8 定制new和delete">
                Effective CPP 阅读 - Chapter 8 定制new和delete <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <!-- JiaThis Button BEGIN -->
<div class="jiathis_style">
  <a class="jiathis_button_tsina"></a>
  <a class="jiathis_button_tqq"></a>
  <a class="jiathis_button_weixin"></a>
  <a class="jiathis_button_cqq"></a>
  <a class="jiathis_button_douban"></a>
  <a class="jiathis_button_renren"></a>
  <a class="jiathis_button_qzone"></a>
  <a class="jiathis_button_kaixin001"></a>
  <a class="jiathis_button_copy"></a>
  <a href="http://www.jiathis.com/share" class="jiathis jiathis_txt jiathis_separator jtico jtico_jiathis" target="_blank"></a>
  <a class="jiathis_counter_style"></a>
</div>
<script type="text/javascript">
  var jiathis_config={
    hideMore:false
  }
</script>
<script type="text/javascript" src="http://v3.jiathis.com/code/jia.js" charset="utf-8"></script>
<!-- JiaThis Button END -->

      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
        <div id="lv-container" data-id="city" data-uid="MTAyMC8yODMwOS80ODgx"></div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image" src="/avatar/liumengli.jpg" alt="一个脱离了高级趣味的人">
          <p class="site-author-name" itemprop="name">一个脱离了高级趣味的人</p>
          <p class="site-description motion-element" itemprop="description">相与枕藉乎舟中，不知东方之既白</p>
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">89</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">34</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/xmfbit" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                  GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/2629935075/" target="_blank" title="微博">
                  
                    <i class="fa fa-fw fa-globe"></i>
                  
                  微博
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#Momentum"><span class="nav-number">1.</span> <span class="nav-text">Momentum</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Adaptive-Learning-Rate"><span class="nav-number">2.</span> <span class="nav-text">Adaptive Learning Rate</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RMSProp"><span class="nav-number">3.</span> <span class="nav-text">RMSProp</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#课程总结"><span class="nav-number">4.</span> <span class="nav-text">课程总结</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#“Modern”-SGD"><span class="nav-number">5.</span> <span class="nav-text">“Modern” SGD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Adagrad"><span class="nav-number">5.1.</span> <span class="nav-text">Adagrad</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adadelta"><span class="nav-number">5.2.</span> <span class="nav-text">Adadelta</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Adam"><span class="nav-number">5.3.</span> <span class="nav-text">Adam</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#AdaMax"><span class="nav-number">5.4.</span> <span class="nav-text">AdaMax</span></a></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">一个脱离了高级趣味的人</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Muse
  </a>
</div>


        

        
      </div>
    </footer>

    <div class="back-to-top">
      <i class="fa fa-arrow-up"></i>
    </div>
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



    
    <script type="text/javascript">
      (function(d, s) {
        var j, e = d.getElementsByTagName(s)[0];
        if (typeof LivereTower === 'function') { return; }
        j = d.createElement(s);
        j.src = 'https://cdn-city.livere.com/js/embed.dist.js';
        j.async = true;
        e.parentNode.insertBefore(j, e);
      })(document, 'script');
    </script>
  





  




	





  





  

  
      <!-- UY BEGIN -->
      <script type="text/javascript" src="http://v2.uyan.cc/code/uyan.js?uid="></script>
      <!-- UY END -->
  




  
  
  <script type="text/javascript">
    // Popup Window;
    var isfetched = false;
    // Search DB path;
    var search_path = "search.xml";
    if (search_path.length == 0) {
      search_path = "search.xml";
    }
    var path = "/" + search_path;
    // monitor main search box;

    function proceedsearch() {
      $("body").append('<div class="popoverlay">').css('overflow', 'hidden');
      $('.popup').toggle();
    }
    // search function;
    var searchFunc = function(path, search_id, content_id) {
      'use strict';
      $.ajax({
        url: path,
        dataType: "xml",
        async: true,
        success: function( xmlResponse ) {
          // get the contents from search data
          isfetched = true;
          $('.popup').detach().appendTo('.header-inner');
          var datas = $( "entry", xmlResponse ).map(function() {
            return {
              title: $( "title", this ).text(),
              content: $("content",this).text(),
              url: $( "url" , this).text()
            };
          }).get();
          var $input = document.getElementById(search_id);
          var $resultContent = document.getElementById(content_id);
          $input.addEventListener('input', function(){
            var matchcounts = 0;
            var str='<ul class=\"search-result-list\">';
            var keywords = this.value.trim().toLowerCase().split(/[\s\-]+/);
            $resultContent.innerHTML = "";
            if (this.value.trim().length > 1) {
              // perform local searching
              datas.forEach(function(data) {
                var isMatch = false;
                var content_index = [];
                var data_title = data.title.trim().toLowerCase();
                var data_content = data.content.trim().replace(/<[^>]+>/g,"").toLowerCase();
                var data_url = decodeURIComponent(data.url);
                var index_title = -1;
                var index_content = -1;
                var first_occur = -1;
                // only match artiles with not empty titles and contents
                if(data_title != '') {
                  keywords.forEach(function(keyword, i) {
                    index_title = data_title.indexOf(keyword);
                    index_content = data_content.indexOf(keyword);
                    if( index_title >= 0 || index_content >= 0 ){
                      isMatch = true;
                      if (i == 0) {
                        first_occur = index_content;
                      }
                    }

                  });
                }
                // show search results
                if (isMatch) {
                  matchcounts += 1;
                  str += "<li><a href='"+ data_url +"' class='search-result-title'>"+ data_title +"</a>";
                  var content = data.content.trim().replace(/<[^>]+>/g,"");
                  if (first_occur >= 0) {
                    // cut out 100 characters
                    var start = first_occur - 20;
                    var end = first_occur + 80;
                    if(start < 0){
                      start = 0;
                    }
                    if(start == 0){
                      end = 50;
                    }
                    if(end > content.length){
                      end = content.length;
                    }
                    var match_content = content.substring(start, end);
                    // highlight all keywords
                    keywords.forEach(function(keyword){
                      var regS = new RegExp(keyword, "gi");
                      match_content = match_content.replace(regS, "<b class=\"search-keyword\">"+keyword+"</b>");
                    });

                    str += "<p class=\"search-result\">" + match_content +"...</p>"
                  }
                  str += "</li>";
                }
              })};
            str += "</ul>";
            if (matchcounts == 0) { str = '<div id="no-result"><i class="fa fa-frown-o fa-5x" /></div>' }
            if (keywords == "") { str = '<div id="no-result"><i class="fa fa-search fa-5x" /></div>' }
            $resultContent.innerHTML = str;
          });
          proceedsearch();
        }
      });}

    // handle and trigger popup window;
    $('.popup-trigger').click(function(e) {
      e.stopPropagation();
      if (isfetched == false) {
        searchFunc(path, 'local-search-input', 'local-search-result');
      } else {
        proceedsearch();
      };
    });

    $('.popup-btn-close').click(function(e){
      $('.popup').hide();
      $(".popoverlay").remove();
      $('body').css('overflow', '');
    });
    $('.popup').click(function(e){
      e.stopPropagation();
    });
  </script>


  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

  


</body>
</html>
