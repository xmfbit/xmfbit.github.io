<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>来呀，快活呀~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xmfbit.github.io/"/>
  <updated>2020-03-21T02:02:44.776Z</updated>
  <id>https://xmfbit.github.io/</id>
  
  <author>
    <name>一个脱离了高级趣味的人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>MIT Missing Semester - Shell</title>
    <link href="https://xmfbit.github.io/2020/03/13/mit-missing-semester-02-shell/"/>
    <id>https://xmfbit.github.io/2020/03/13/mit-missing-semester-02-shell/</id>
    <published>2020-03-13T14:05:30.000Z</published>
    <updated>2020-03-21T02:02:44.776Z</updated>
    
    <content type="html"><![CDATA[<p>工欲善其事，必先利其器。<a href="https://missing.csail.mit.edu/" target="_blank" rel="external">MIT Missing Semester</a>就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！</p><p>这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。</p><p>这里我跳过了第一节课，直接从bash shell开始。</p><a id="more"></a><h2 id="一些零散的点"><a href="#一些零散的点" class="headerlink" title="一些零散的点"></a>一些零散的点</h2><ul><li>bash中双引号和单引号的区别</li></ul><p>双引号会发生变量替换，单引号不会。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">foo=bar</div><div class="line"></div><div class="line"><span class="comment"># output: hello, bar</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"hello, <span class="variable">$foo</span>"</span></div><div class="line"><span class="comment"># output: hello, $foo</span></div><div class="line"><span class="built_in">echo</span> <span class="string">'hello, $foo'</span></div></pre></td></tr></table></figure><ul><li>bangbang</li></ul><p>使用<code>!!</code>可以执行上一条命令。</p><ul><li>bash 中的特殊变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line">$? <span class="comment"># 上一条命令的返回值，正常退出是0，否则是非0</span></div><div class="line"><span class="variable">$@</span> <span class="comment"># 所有输入的参数</span></div><div class="line"><span class="variable">$#</span> <span class="comment"># 输入参数的个数</span></div><div class="line">$$ <span class="comment"># pid of current script</span></div><div class="line"></div><div class="line"><span class="comment"># 检查上条命令是否正常退出</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ $? <span class="_">-ne</span> 0 ]; <span class="keyword">then</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"fail"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"success"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><ul><li>如何忽略命令的输出</li></ul><p>有的时候，我们只想要命令的返回值。例如使用<code>grep foo bar</code>来查看文件<code>bar</code>中是否含有字符串<code>foo</code>，可以将标准输出和标准错误重定向到<code>/dev/null</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 第一个是标准输出，第二个是标准错误</span></div><div class="line">grep <span class="string">"foo"</span> bar &gt; /dev/null 2&gt; /dev/null</div><div class="line"></div><div class="line"><span class="comment"># 或者可以这样：</span></div><div class="line"></div><div class="line"><span class="comment"># grep "name" test_lazy.cpp 2&gt;&amp;1 &gt; /dev/null</span></div><div class="line"></div><div class="line"><span class="keyword">if</span> [ <span class="string">"$?"</span> <span class="_">-ne</span> 0 ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"found foo in bar"</span></div><div class="line"><span class="keyword">else</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"not found foo in bar"</span></div><div class="line"><span class="keyword">fi</span></div></pre></td></tr></table></figure><h2 id="globbing"><a href="#globbing" class="headerlink" title="globbing"></a>globbing</h2><ul><li>任意字符：<code>*</code></li><li>单个字符：<code>?</code></li><li>使用<code>{}</code>给定可选元素的集合。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">a.&#123;hpp,cpp&#125;  =&gt; a.hpp a.cpp</div><div class="line">a&#123;,0,1,2&#125;   =&gt; a a0 a1 a2</div><div class="line"><span class="comment"># 支持多层级</span></div><div class="line">touch proj&#123;1,2&#125;/&#123;a,b&#125;.txt</div><div class="line"><span class="comment"># 还支持range</span></div><div class="line">touch proj&#123;1,2&#125;/&#123;a..g&#125;.txt</div></pre></td></tr></table></figure><h2 id="bash-中的函数"><a href="#bash-中的函数" class="headerlink" title="bash 中的函数"></a>bash 中的函数</h2><ul><li>如何写函数</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="title">mycd</span></span> () &#123;</div><div class="line">    <span class="built_in">cd</span> <span class="variable">$1</span></div><div class="line">    <span class="built_in">pwd</span></div><div class="line">&#125;</div></pre></td></tr></table></figure><ul><li>如何在bash中导入脚本中的函数</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">source</span> your_bash_script.sh</div><div class="line"><span class="comment"># then use the function defined in the bash script</span></div><div class="line"><span class="comment"># 你可以这样理解：from your_bash_script import *</span></div></pre></td></tr></table></figure><h2 id="for-loop"><a href="#for-loop" class="headerlink" title="for-loop"></a>for-loop</h2><h3 id="遍历给定的元素序列"><a href="#遍历给定的元素序列" class="headerlink" title="遍历给定的元素序列"></a>遍历给定的元素序列</h3><p>使用<code>for item in xxx; do yyy; done</code>来遍历给定的序列，并施加具体操作于序列元素：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1 2 3</div><div class="line"><span class="keyword">do</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$i</span>"</span></div><div class="line"><span class="keyword">done</span></div><div class="line"><span class="comment"># welcome 1</span></div><div class="line"><span class="comment"># welcome 2</span></div><div class="line"><span class="comment"># welcome 3</span></div></pre></td></tr></table></figure><p>注意，列表元素是通过空格来隔离的。如果这样写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1, 2, 3</div></pre></td></tr></table></figure><p>那么最终输出也是<code>welcome 1, welcome 2, welcome 3</code></p><p>还可以使用for-range的方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</div></pre></td></tr></table></figure><h3 id="c-style-for-loop"><a href="#c-style-for-loop" class="headerlink" title="c-style for-loop"></a>c-style for-loop</h3><p>也可以像C语言那样使用for-loop：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (( Exp1; Exp2; Exp3)); <span class="keyword">do</span> xxx; <span class="keyword">done</span></div></pre></td></tr></table></figure><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (( c=1; c&lt;=3; c++ )); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span>; <span class="keyword">done</span></div></pre></td></tr></table></figure><p>还可以使用这种风格构造无穷循环，<code>for (( ; ; )); do xxx; done</code>。</p><h3 id="break-continue"><a href="#break-continue" class="headerlink" title="break / continue"></a>break / continue</h3><p>当满足一定条件时，使用<code>break</code>退出循环，或使用<code>continue</code>继续循环。</p><h3 id="while"><a href="#while" class="headerlink" title="while"></a>while</h3><p>除了for-loop，还可以使用<code>while</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">while</span> CONDITION; <span class="keyword">do</span> xxx; <span class="keyword">done</span></div></pre></td></tr></table></figure><h3 id="until"><a href="#until" class="headerlink" title="until"></a>until</h3><p><code>until</code>和<code>while</code>的用法一致，不同点在于：</p><ul><li><code>while</code>是CONDITION为<code>true</code>执行，当<code>false</code>是退出循环</li><li><code>until</code>是CONDITION为<code>false</code>执行，当<code>true</code>时退出循环</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">c=1</div><div class="line">until [ <span class="variable">$c</span> <span class="_">-gt</span> 3 ]; <span class="keyword">do</span></div><div class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span></div><div class="line">  ((c++))</div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure><h2 id="数学表达式"><a href="#数学表达式" class="headerlink" title="数学表达式"></a>数学表达式</h2><p>在上面for-loop中，已经看到了我们使用<code>((exp))</code>的形式进行数学表达式运算。一般来说，在bash shell中进行数学表达式的运算可以采用：</p><ul><li>使用<code>expr</code>，如<code>c=$(expr 1 + 1); echo $c</code>，注意操作数与操作符之间都是有空格的。</li><li>使用<code>let</code>，如<code>c=1; let c=$c+1; echo $c</code>，注意操作数与操作符之间没有空格。</li><li>使用双括号<code>(())</code>，就像上面看到的那样：<code>c=1; echo $((c += 1)); echo $c</code>。这时候，操作数与操作符之间的空格可有可无。</li></ul><p>最后一种双括号可能更为常用，支持的操作符：<code>+/-/++/--/*/%/**</code>，也支持逻辑运算符：<code>&gt;=/&lt;=/&gt;/&lt;/==/!=/!/||/&amp;&amp;</code>。</p><p>如果希望进行浮点数运算，bash本身是不支持的，可以使用<code>bc</code>命令，将表达式作为字符串传入就可以了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">echo</span> <span class="string">"1.0+2.0"</span> | bc</div><div class="line">c=$(r=1.5;<span class="built_in">echo</span> <span class="string">"<span class="variable">$r</span> + 2.5"</span>|bc); <span class="built_in">echo</span> <span class="variable">$c</span></div></pre></td></tr></table></figure><h2 id="调试工具"><a href="#调试工具" class="headerlink" title="调试工具"></a>调试工具</h2><p>shellcheck可以用来帮助静态分析shell脚本。用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">shellcheck your_bash_script.sh</div></pre></td></tr></table></figure><p>可以去网站上试用：<a href="https://www.shellcheck.net/#" target="_blank" rel="external">Shellcheck</a></p><h2 id="几个有用的命令"><a href="#几个有用的命令" class="headerlink" title="几个有用的命令"></a>几个有用的命令</h2><p>这里列出一些常用的命令工具，都是和查找有关。更多内容，可以通过<code>man</code>或者<code>tldr</code>查看。</p><h3 id="查找文件-find"><a href="#查找文件-find" class="headerlink" title="查找文件 find"></a>查找文件 find</h3><p>最常用的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 递归地查找当前目录及子目录下所有的python文件</span></div><div class="line">find . -name=<span class="string">"*.py"</span></div><div class="line"><span class="comment"># -type d 表示过滤结果为所有目录</span></div><div class="line"><span class="comment"># -type f 表示过滤结果为所有文件</span></div><div class="line">find . -name=<span class="string">"test"</span> -type d</div><div class="line"><span class="comment"># Find all files modified in the last day</span></div><div class="line">find . -mtime -1</div><div class="line"><span class="comment"># Find all zip files with size in range 500k to 10M</span></div><div class="line">find . -size +500k -size -10M -name <span class="string">'*.tar.gz'</span></div></pre></td></tr></table></figure><p><code>find</code>还可以通过<code>-exec</code>来接后续处理，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Delete all files with .tmp extension</span></div><div class="line"><span class="comment"># 注意最后的 \</span></div><div class="line">find . -name <span class="string">'*.tmp'</span> -exec rm &#123;&#125; \;</div><div class="line"><span class="comment"># Find all PNG files and convert them to JPG</span></div><div class="line">find . -name <span class="string">'*.png'</span> -exec convert &#123;&#125; &#123;.&#125;.jpg \;</div></pre></td></tr></table></figure><p>你也可以用<code>fd</code>作为<code>find</code>的改进版。具体用法可以参考<a href="https://github.com/sharkdp/fd" target="_blank" rel="external">fd</a>，这里不再多说了。</p><h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>如果你想按照名字去查找文件，还可以试试<code>locate</code>。一个简单的比较：</p><ul><li><code>locate</code>只支持按名字查找，<code>find</code>可以更加多样</li><li><code>locate</code>通过周期性更新的database来查找，时效性不如<code>find</code></li><li><code>locate</code>使用更简单，默认会查找所有符合要求的文件，<code>find</code>一般是查找给定路径下的文件</li></ul><p>由于上述原因，我一般是使用<code>locate</code>查找系统自带的某个lib等文件。比如有时候我可能不知道<code>libcudart.so</code>在哪里，这时候就可以通过<code>locate libcudart.so</code>来查找。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">locate libcudart.so | grep <span class="string">"/usr"</span></div><div class="line"></div><div class="line"><span class="comment"># output:</span></div><div class="line"><span class="comment"># /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7</span></div><div class="line"><span class="comment"># /usr/local/cuda-10.0/lib64/libcudart.so</span></div><div class="line"><span class="comment"># ...</span></div></pre></td></tr></table></figure><h3 id="在文件中查找字符串-grep"><a href="#在文件中查找字符串-grep" class="headerlink" title="在文件中查找字符串 grep"></a>在文件中查找字符串 grep</h3><p><code>grep</code> 用来在文件中正则匹配字符串，比如某个变量或函数定义啥的。<code>grep</code>命令很有用，在后续课程中还会着重介绍。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在文件中查找xxx，并打印其所在的行</span></div><div class="line">grep xxx file</div><div class="line"></div><div class="line"><span class="comment"># 在所有文件中递归地查找</span></div><div class="line">grep -R xxx .</div></pre></td></tr></table></figure><p>常用的一些flag，可以是<code>-C +number</code>（用来显示match的context，number是行数），<code>-v</code>是反转（不包含所给pattern的行）</p><p>和<code>find</code>一样，<code>grep</code>也有一些更好用的替代品，如<code>rg</code>，<code>ag</code>，<code>ack</code>等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># Find all python files where I used the requests library</span></div><div class="line">rg -t py <span class="string">'import requests'</span></div><div class="line"><span class="comment"># Find all files (including hidden files) without a shebang line</span></div><div class="line">rg -u --files-without-match <span class="string">"^#!"</span></div><div class="line"><span class="comment"># Find all matches of foo and print the following 5 lines</span></div><div class="line">rg foo -A 5</div><div class="line"><span class="comment"># Print statistics of matches (# of matched lines and files )</span></div><div class="line">rg --stats PATTERN</div></pre></td></tr></table></figure><h3 id="查找历史命令-history"><a href="#查找历史命令-history" class="headerlink" title="查找历史命令 history"></a>查找历史命令 history</h3><p><code>history</code>可以打印历史的shell命令，和<code>grep</code>配合能够找到历史上曾经用过的某给定命令。不过这个我在使用<code>zsh</code>的时候，一般是通过光标上下键来联想查找的。</p><p>一个有用的工具：<code>fzf</code>（which means 模糊查找）。</p><p>另外，这里讲师推荐了一款基于历史命令的自动补全（看lecture时候觉得很酷）。如果你和我一样使用<code>zsh</code>，可以参考这个插件：<a href="https://github.com/zsh-users/zsh-autosuggestions" target="_blank" rel="external">zsh-autosuggestions</a>。</p><h3 id="关于目录"><a href="#关于目录" class="headerlink" title="关于目录"></a>关于目录</h3><p>因为shell环境下没有GUI，所以查看一个目录内的内容，包括跳转目录都很不方便。对此也有一些好用的工具：</p><ul><li>查看目录内容：<code>tree</code>（最经典），<code>broot</code>，<code>nnn</code>，<code>ranger</code></li><li>跳转目录：<code>autojump</code>（在用），<code>fasd</code></li></ul><h2 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h2><h3 id="关于ls的用法"><a href="#关于ls的用法" class="headerlink" title="关于ls的用法"></a>关于ls的用法</h3><ul><li>Includes all files, including hidden files：<code>ls -al</code></li><li>Sizes are listed in human readable format (e.g. 454M instead of 454279954)：<code>ls -lh</code></li><li>Files are ordered by recency：<code>ls -lt</code></li><li>Output is colorized：<code>ls -l --color</code> （zsh自动colorized，所以这个没有验证）</li></ul><h3 id="bash函数"><a href="#bash函数" class="headerlink" title="bash函数"></a>bash函数</h3><p>Write bash functions <code>marco</code> and <code>polo</code> that do the following. Whenever you execute <code>marco</code> the current working directory should be saved in some manner, then when you execute <code>polo</code>, no matter what directory you are in, <code>polo</code> should cd you back to the directory where you executed <code>marco</code>. For ease of debugging you can write the code in a file <code>marco.sh</code> and (re)load the definitions to your shell by executing <code>source marco.sh</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line"><span class="comment"># 使用文件存储要cd的path</span></div><div class="line"><span class="function"><span class="title">macro</span></span> () &#123;</div><div class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$(pwd)</span>"</span> &gt; <span class="variable">$HOME</span>/.macro.history</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="function"><span class="title">polo</span></span> () &#123;</div><div class="line">    <span class="built_in">cd</span> <span class="string">"<span class="variable">$(cat "$HOME/.macro.history")</span>"</span> || <span class="built_in">exit</span> 1</div><div class="line">&#125;</div></pre></td></tr></table></figure><h3 id="循环和程序返回值判断"><a href="#循环和程序返回值判断" class="headerlink" title="循环和程序返回值判断"></a>循环和程序返回值判断</h3><p>Say you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#!/bin/bash</span></div><div class="line"></div><div class="line"><span class="keyword">for</span> ((i=1; ; i++)); <span class="keyword">do</span></div><div class="line">  <span class="comment"># save the script to `fail_rarely.sh`</span></div><div class="line">  ./fail_rarely.sh 2&amp;&gt; out.log</div><div class="line">  <span class="keyword">if</span> [ $? <span class="_">-ne</span> 0 ]; <span class="keyword">then</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"fail after run <span class="variable">$i</span> times"</span></div><div class="line">    <span class="built_in">echo</span> <span class="string">"stdout and stderr message:"</span></div><div class="line">    cat out.log</div><div class="line">    <span class="built_in">break</span></div><div class="line">  <span class="keyword">fi</span></div><div class="line"><span class="keyword">done</span></div></pre></td></tr></table></figure><h3 id="xargs和管道"><a href="#xargs和管道" class="headerlink" title="xargs和管道"></a>xargs和管道</h3><p>As we covered in lecture find’s <code>-exec</code> can be very powerful for performing operations over the files we are searching for. However, what if we want to do something with all the files, like creating a zip file? As you have seen so far commands will take input from both arguments and STDIN. When piping commands, we are connecting STDOUT to STDIN, but some commands like tar take inputs from arguments. To bridge this disconnect there’s the <code>xargs</code> command which will execute a command using STDIN as arguments. For example <code>ls | xargs rm</code> will delete the files in the current directory.</p><p>Your task is to write a command that recursively finds all HTML files in the folder and makes a zip with them. Note that your command should work even if the files have spaces (hint: check <code>-d</code> flag for <code>xargs</code>)</p><h4 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a>xargs</h4><p>先来看下<code>xargs</code>和管道的区别。这里已经给了一个例子：<code>ls | xargs rm</code>。由于<code>rm</code>命令比较危险，所以下面会换成<code>cat</code>（删除文件变成了打印文件内容）。</p><p>为什么不能用管道呢，比如<code>ls | cat</code>。我们先建立一个空目录作为playground：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">mkdir <span class="built_in">test</span></div><div class="line"><span class="built_in">cd</span> <span class="built_in">test</span></div><div class="line"><span class="built_in">echo</span> <span class="string">"simgple test"</span> &gt; a.txt</div></pre></td></tr></table></figure><p>执行<code>ls | cat</code>，会发现它只是把当前目录下的所有文件名打印了出来，并没有打印<code>a.txt</code>的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ls | cat</div><div class="line"><span class="comment"># a.txt</span></div></pre></td></tr></table></figure><p>这是因为管道只是把STDOUT作为<code>cat</code>的STDIN。在linux中，STDOUT和STDIN是两个特殊的文件，<code>ls</code>将把它的输出结果写入到STDOUT中，同时我们就会在屏幕上看到对应输出。而<code>cat</code>从STDIN中接受输入。当没有管道时，由用户输入并写入STDIN。由于管道，<code>cat</code>将直接从STDOUT中读取。也就是<code>ls</code>的输出，也就是当前目录下的文件列表。拆解后想当于下面：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ls &gt; stdout_ls</div><div class="line">cat &lt; stdout_ls</div></pre></td></tr></table></figure><p>所以，如果我们想要打印<code>a.txt</code>的内容，管道就不够用了。也就是上面说的，我们要把<code>ls</code>的输出作为<code>cat</code>的参数。这时候需要使用<code>xargs</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">ls | xargs cat</div><div class="line"><span class="comment"># simple test</span></div></pre></td></tr></table></figure><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>先准备一些测试文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line">mkdir htmls</div><div class="line"><span class="built_in">cd</span> htmls</div><div class="line">mkdir htmls/&#123;1..3&#125;</div><div class="line">touch htmls/1/1.html</div><div class="line">touch htmls/2/2\ 2.html</div><div class="line">touch htmls/root.html</div></pre></td></tr></table></figure><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>题目说明中的<code>-d</code>没找到，在<a href="https://www.tecmint.com/xargs-command-examples/" target="_blank" rel="external">12 Practical Examples of Linux Xargs Command for Beginners</a>找到了如下用法，使用<br><code>-print0</code>和<code>-0</code>（是数字<code>0</code>）配合，具体可以参考<code>man xargs</code>中的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="comment">#-0      Change xargs to expect NUL (``\0'') characters as separators, instead of spaces and newlines.  </span></div><div class="line"><span class="comment">#        This is expected to be used in concert with the -print0 function in find(1).</span></div><div class="line"></div><div class="line">find htmls -name <span class="string">"*.html"</span> -print0 | xargs -0 tar vcf html.zip</div></pre></td></tr></table></figure><h3 id="命令组合"><a href="#命令组合" class="headerlink" title="命令组合"></a>命令组合</h3><p>Write a command or script to recursively find the most recently modified file in a directory. More generally, can you list all files by recency?</p><p>首先递归地列出当前目录下的所有文件，再使用<code>ls -lt</code>将其按照时间排序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">find -L . -type f -print0 | xargs -0 ls <span class="_">-lt</span></div><div class="line"></div><div class="line"><span class="comment"># 如果只需要最新的那个，使用 head命令只打印第一行</span></div><div class="line">find -L . -type f -print0 | xargs -0 ls <span class="_">-lt</span> | head -1</div></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工欲善其事，必先利其器。&lt;a href=&quot;https://missing.csail.mit.edu/&quot;&gt;MIT Missing Semester&lt;/a&gt;就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！&lt;/p&gt;
&lt;p&gt;这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。&lt;/p&gt;
&lt;p&gt;这里我跳过了第一节课，直接从bash shell开始。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
      <category term="bash" scheme="https://xmfbit.github.io/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>Debian9 编译Caffe的一个坑</title>
    <link href="https://xmfbit.github.io/2019/11/24/caffe-compiling-debian9/"/>
    <id>https://xmfbit.github.io/2019/11/24/caffe-compiling-debian9/</id>
    <published>2019-11-24T11:12:48.000Z</published>
    <updated>2020-03-21T02:02:44.776Z</updated>
    
    <content type="html"><![CDATA[<p>记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">In file included from /usr/local/cuda/include/cuda_runtime.h:120:0,</div><div class="line">                 from &lt;command-line&gt;:0:</div><div class="line">/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &quot;&quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER</div><div class="line">_BUILD__ instead.&quot;&quot; is not valid in preprocessor expressions</div><div class="line"> #define __CUDACC_VER__ &quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&quot;</div></pre></td></tr></table></figure><p>如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。</p><p>解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个<a href="https://github.com/NVIDIA/caffe/issues/408" target="_blank" rel="external">github issue</a>，修改<code>include/caffe/common.hpp</code>即可。</p><a id="more"></a><p><img src="/img/fix_caffe_for_boost_CUDACC_VER_error.png" alt="diff修改"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;1&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;2&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;3&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;4&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;5&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;div class=&quot;line&quot;&gt;In file included from /usr/local/cuda/include/cuda_runtime.h:120:0,&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;                 from &amp;lt;command-line&amp;gt;:0:&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &amp;quot;&amp;quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER&lt;/div&gt;&lt;div class=&quot;line&quot;&gt;_BUILD__ instead.&amp;quot;&amp;quot; is not valid in preprocessor expressions&lt;/div&gt;&lt;div class=&quot;line&quot;&gt; #define __CUDACC_VER__ &amp;quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&amp;quot;&lt;/div&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。&lt;/p&gt;
&lt;p&gt;解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个&lt;a href=&quot;https://github.com/NVIDIA/caffe/issues/408&quot;&gt;github issue&lt;/a&gt;，修改&lt;code&gt;include/caffe/common.hpp&lt;/code&gt;即可。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文 - MetaPruning：Meta Learning for Automatic Neural Network Channel Pruning</title>
    <link href="https://xmfbit.github.io/2019/10/25/paper-meta-pruning/"/>
    <id>https://xmfbit.github.io/2019/10/25/paper-meta-pruning/</id>
    <published>2019-10-25T16:33:37.000Z</published>
    <updated>2020-03-21T02:02:44.784Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：<a href="https://github.com/liuzechun/MetaPruning" target="_blank" rel="external">MetaPruning</a>。</p><p>这篇文章首发于<a href="https://wemp.app/accounts/fd027dce-bcd1-4eaf-9e64-88bffd7ca8a2" target="_blank" rel="external">Paper Weekly公众号</a>，欢迎关注。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>模型剪枝是一种能够减少模型大小和计算量的方法。模型剪枝一般可以分为三个步骤：</p><ul><li>训练一个参数量较多的大网络</li><li>将不重要的权重参数剪掉</li><li>剪枝后的小网络做fine tune</li></ul><p>其中第二步是模型剪枝中的关键。有很多paper围绕“怎么判断权重是否重要”以及“如何剪枝”等问题进行讨论。困扰模型剪枝落地的一个问题就是剪枝比例的确定。传统的剪枝方法常常需要人工layer by layer地去确定每层的剪枝比例，然后进行fine tune，用起来很耗时，而且很不方便。不过最近的<a href="https://arxiv.org/abs/1810.05270" target="_blank" rel="external">Rethinking the Value of Network Pruning</a>指出，剪枝后的权重并不重要，对于channel pruning来说，更重要的是找到剪枝后的网络结构，具体来说就是每层留下的channel数量。受这个发现启发，文章提出可以用一个PruningNet，对于给定的剪枝网络，自动生成weight，无需进行retrain，然后评测剪枝网络在验证集上的性能，从而选出最优的网络结构。</p><p>具体来说，PruningNet的输入是剪枝后的网络结构，必须首先对网络结构进行编码，转换为coding vector。这里可以直接用剪枝后网络每层的channel数来编码。在搜索剪枝网络的时候，我们可以尝试各种coding vector，用PruningNet生成剪枝后的网络权重。网络结构和权重都有了，就可以去评测网络的性能。进而用进化算法搜索最优的coding vector，也就是最优的剪枝结构。在用进化算法搜索的时候，可以使用自定义的目标函数，包括将网络的accuracy，latency，FLOPS等考虑进来。</p><p><img src="/img/paper_metapruning_pruningnet.jpg" alt="PruningNet的训练和使用"></p><h2 id="PruningNet"><a href="#PruningNet" class="headerlink" title="PruningNet"></a>PruningNet</h2><p>从上一小节已经可以知道，PruningNet是整个算法的关键。那么怎么才能找到这样一个“神奇网络”呢？</p><p>先做一下符号约定，使用$c_i$表示剪枝之后第$i$层的channel数量，$l$为网络的层数，$W$表示剪枝后网络的权重。那么PruningNet的输入输出如下所示：</p><p>$W = \text{PruningNet}(c_1, c_2, \dots, c_l)$</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>先结合下图看一下forward部分。PruningNet是由$l$个PruningBlock组成的，每个PruningBlock是一个两层的MLP。首先看图b，编码着网络结构信息的coding vector输入到当前block后，输出经过Reshape，成了一个Weight Matrix。注意哦，这里的WeightMatrix是固定大小的（也就是未剪枝的原始Weight shape大小），和剪枝网络结构无关。再看图a，因为要对网络进行剪枝，所以WeightMatrix要进行Crop。对应到图b，可以看到，Crop是在两个维度上进行的。首先，由于上一层也进行了剪枝，所以input channel数变少了；其次，由于当前层进行了剪枝，所以output channel数变少了。这样经过Crop，就生成了剪枝后的网络weight。我们再输入一个mini batch的训练图片，就可以得到剪枝后的网络的loss。</p><p><img src="/img/paper_metapruning_pruningnet_forward.jpg" alt="PruningNet train forward"></p><p>在backward部分，我们不更新剪枝后网络的权重，而是更新PruningNet的权重。由于上面的操作都是可微分的，所以直接用链式法则传过去就行。如果你使用PyTorch等支持自动微分的框架，这是很容易的。</p><p>下图所示是训练过程的整个PruningNet（左侧）和剪枝后网络（右侧，即PrunedNet）。训练过程中的coding vector在状态空间里随机采样，随机选取每层的channel数量。</p><p>PS：和原始论文相比，下图和上图顺序是颠倒的。这里从底向上介绍了PruningNet的训练，而论文则是自顶向下。</p><p><img src="/img/paper_metapruning_whole_meta_learning.jpg" alt="整个PruningNet"></p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>训练好PruningNet后，就可以用它来进行搜索了！我们只需要输入某个coding vector，PruningNet就会为我们生成对应每层的WeightMatrix。别忘了coding vector是编码的网络结构，现在又有了weight，我们就可以在验证集上测试网络的性能了。进而，可以使用进化算法等优化方法去搜索最优的coding vector。当我们得到了最优结构的剪枝网络后，再from scratch地训练它。</p><p>进化算法这里不再赘述，很多优化的书中包括网上都有资料。这里把整个算法流程贴出来：</p><p><img src="/img/paper_metapruning_evaluation_algorithm.jpg" alt="进化算法流程"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在ImageNet上用MobileNet和ResNet进行了实验。训练PruningNet用了$\frac{1}{4}$的原模型的epochs。数据增强使用常见的标准流程，输入image大小为$224\times 224$。</p><p>将原始ImageNet的训练集做分割，每个类别选50张组成sub-validation（共计50000），其余作为sub-training。在训练时，我们使用sub-training训练PruningNet。在搜索时，使用sub-validation评估剪枝网络的性能。不过，还要注意，在搜索时，使用20000张sub-training中的图片重新计算BatchNorm layer中的running mean和running variance。</p><h3 id="shortcut剪枝"><a href="#shortcut剪枝" class="headerlink" title="shortcut剪枝"></a>shortcut剪枝</h3><p>在进行模型剪枝时，一个比较难处理的问题是ResNet中的shortcut结构。因为最后有一个element-wise的相加操作，必须保证两路feature map是严格shape相同的，所以不能随意剪枝，否则会造成channel不匹配。下面对几种论文中用到的网络结构分别讨论。</p><h4 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet-v1"></a>MobileNet-v1</h4><p>MobileNet-v1是没有shortcut结构的。我们为每个conv layer都配上相应的PruningBlock——一个两层的MLP。PruningNet的输入coding vector中的元素是剪枝后每层的channel数量。而输入第$i$个PruningBlock的是一个2D vector，由归一化的第$i-1$层和第$i$层的剪枝比例构成。这部分可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv1/training/mobilenet_v1.py#L15" target="_blank" rel="external">MetaPruning</a>来看。注意第$1$个conv layer的输入是1D vector，因为它是第一个被剪枝的layer。在训练时，coding vector的搜索空间被以一定步长划分为grid，采样就是在这些格点上进行的。</p><h4 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet-v2"></a>MobileNet-v2</h4><p>MobileNet-v2引入了类似ResNet的shortcut结构，这种resnet block必须统一看待。具体来说，对于没有在resnet block中的conv，处理方法如MobileNet-v1。对每个resnet block，配上一个相应的PruningBlock。由于每个resnet block中只有一个中间层（$3\times 3$的conv），所以输出第$i$个PruningBlock的是一个3D vector，由归一化的第$i-1$个resnet block，第$i$个resnet block和中间conv层的剪枝比例构成。其他设置和MobileNet-v1相同。这里可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv2/training/mobilenet_v2.py#L109" target="_blank" rel="external">MetaPruning</a>来看。</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>处理方法如MobileNet-v2所示。可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/resnet/training/resnet.py#L75" target="_blank" rel="external">MetaPruning</a>来看。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在相近FLOPS情况下，和MobileNet论文中改变ratio参数得到的模型比较，MetaPruning得到的模型accuracy更高。尤其是压缩比例更大时，该方法更有优势。</p><p><img src="/img/paper_metapruning_compare_with_mobilenet_baseline.jpg" alt="MobileNet baseline比较"></p><p>和其他剪枝方法（如<a href="https://arxiv.org/abs/1802.03494" target="_blank" rel="external">AMC</a>）等方法比较，该方法也得到了SOTA的结果。MetaPruning方法能够以一种统一的方法处理ResNet中的shortcut结构，并且不需要人工调整太多的参数。</p><p><img src="/img/paper_metapruning_compare_with_other_pruning_automl.jpg" alt="和其他剪枝方法比较"></p><p>上面的比较都是基于理论FLOPS，现在更多人在关注网络在实际硬件上的latency怎么样。文章对此也进行了讨论。如何测试网络的latency？当然可以每个网络都实际跑一下，不过有些麻烦。基于每个layer的inference时间是互相独立的这个假设，作者首先构造了各个layer inference latency的查找表（参见论文<a href="https://arxiv.org/abs/1812.03443" target="_blank" rel="external">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</a>），以此来估计实际网络的latency。作者这里和MobileNet baseline做了比较，结果也证明了该方法更优。</p><p><img src="/img/paper_metapruning_latency_compare_with_mobilenet_baseline.jpg" alt="latency比较"></p><h3 id="PruningNet结果分析"><a href="#PruningNet结果分析" class="headerlink" title="PruningNet结果分析"></a>PruningNet结果分析</h3><p>此外，作者还对PruningNet的预测结果进行可视化，试图找出一些可解释性，并找出剪枝参数的一些规律。</p><ul><li>down-sampling的部分PruningNet倾向于保留更多的channel，如MobileNet-v2 block中间的那个conv</li><li>优先剪浅层layer的channel，FLOPS约束太强剪深层的channel，但可能会造成网络accuracy下降比较多</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章从“剪枝后的weight作用不大”的现象出发，将剪枝和NAS结合，提出了PruningNet为剪枝后的网络预测weight，避免了网络的retrain，从而可以快速衡量剪枝网络的性能。并在编码网络信息的coding vector状态空间进行搜索，找到给定约束条件下的最优网络结构，在ImageNet数据集和ResNet/MobileNet-v1/v2上取得了比之前剪枝算法更好的效果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：&lt;a href=&quot;https://github.com/liuzechun/MetaPruning&quot;&gt;MetaPruning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章首发于&lt;a href=&quot;https://wemp.app/accounts/fd027dce-bcd1-4eaf-9e64-88bffd7ca8a2&quot;&gt;Paper Weekly公众号&lt;/a&gt;，欢迎关注。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
    <link href="https://xmfbit.github.io/2019/07/06/bag-of-tricks-for-image-cls/"/>
    <id>https://xmfbit.github.io/2019/07/06/bag-of-tricks-for-image-cls/</id>
    <published>2019-07-06T05:52:45.000Z</published>
    <updated>2020-03-21T02:02:44.778Z</updated>
    
    <content type="html"><![CDATA[<p>这是<a href="https://arxiv.org/abs/1812.01187" target="_blank" rel="external">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。</p><p><img src="/img/bag_of_tricks_no_silver_bullet.jpeg" alt="软工里面没有银弹，数据科学同样这样"></p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇文章主要讨论了训练图片分类模型的tricks，包括data augmentation，（lr，batch size等）超参设置，模型架构微调和模型蒸馏等技术。可以在增加少许计算量的情况下，把ResNet-50的top 1 acc提升4个点，从而打败许多后起之秀。Talk is cheap, show me the code. 论文讨论的方法对应代码，都已经在GluonCV中开源，所以建议在阅读论文的时候，对照<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py" target="_blank" rel="external">代码</a>进行学习。</p><p><img src="/img/bag_of_tricks_resnet50_overperform_others.jpg" alt="ResNet-50的效果提升"></p><h2 id="Baseline-Training"><a href="#Baseline-Training" class="headerlink" title="Baseline Training"></a>Baseline Training</h2><p>这里介绍了一些（已经不算trick的）训练ResNet-50可以注意的地方。使用这些方法，应该可以复现论文中给出的结果。</p><h3 id="Data-Argumentation"><a href="#Data-Argumentation" class="headerlink" title="Data Argumentation"></a>Data Argumentation</h3><p>这里都是老生常谈了，可以直接参看代码<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py#L203" target="_blank" rel="external">gluon cv/image classification</a>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line">jitter_param = <span class="number">0.4</span></div><div class="line">lighting_param = <span class="number">0.1</span></div><div class="line">mean_rgb = [<span class="number">123.68</span>, <span class="number">116.779</span>, <span class="number">103.939</span>]</div><div class="line">std_rgb = [<span class="number">58.393</span>, <span class="number">57.12</span>, <span class="number">57.375</span>]</div><div class="line">train_data = mx.io.ImageRecordIter(</div><div class="line">    path_imgrec         = rec_train,</div><div class="line">    path_imgidx         = rec_train_idx,</div><div class="line">    preprocess_threads  = num_workers,</div><div class="line">    shuffle             = <span class="keyword">True</span>,</div><div class="line">    batch_size          = batch_size,</div><div class="line">    data_shape          = (<span class="number">3</span>, input_size, input_size),</div><div class="line">    mean_r              = mean_rgb[<span class="number">0</span>],</div><div class="line">    mean_g              = mean_rgb[<span class="number">1</span>],</div><div class="line">    mean_b              = mean_rgb[<span class="number">2</span>],</div><div class="line">    std_r               = std_rgb[<span class="number">0</span>],</div><div class="line">    std_g               = std_rgb[<span class="number">1</span>],</div><div class="line">    std_b               = std_rgb[<span class="number">2</span>],</div><div class="line">    rand_mirror         = <span class="keyword">True</span>,</div><div class="line">    random_resized_crop = <span class="keyword">True</span>,</div><div class="line">    max_aspect_ratio    = <span class="number">4.</span> / <span class="number">3.</span>,</div><div class="line">    min_aspect_ratio    = <span class="number">3.</span> / <span class="number">4.</span>,</div><div class="line">    max_random_area     = <span class="number">1</span>,</div><div class="line">    min_random_area     = <span class="number">0.08</span>,</div><div class="line">    brightness          = jitter_param,</div><div class="line">    saturation          = jitter_param,</div><div class="line">    contrast            = jitter_param,</div><div class="line">    pca_noise           = lighting_param,</div><div class="line">)</div></pre></td></tr></table></figure><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><ul><li>使用Xavier初始化卷积层和全连接层的权重，也就是$w\sim \mathcal{U}(-a, a)$，其中$a = \sqrt{6/(d_{in} + d_{out})}$，$d$是输入和输出的channel size。偏置项初始化为$0$。</li><li>BatchNorm的$\gamma$初始化为$1$，偏置项为$0$。</li></ul><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>8卡V100，batch size = 256，使用NAG梯度下降，lr从0.1，在30，60，90epoch处除以10。</p><p>使用上述设置，得到的ResNet-50模型比原始论文更好，不过Inception-V3（输入为$229\times 229$大小）和MobileNet稍差于原始论文。</p><h2 id="更快地训练"><a href="#更快地训练" class="headerlink" title="更快地训练"></a>更快地训练</h2><p>主要讨论使用低精度（FP16）和大batch size对训练的影响。</p><h3 id="大batch-size"><a href="#大batch-size" class="headerlink" title="大batch size"></a>大batch size</h3><p>大的batch size经常会导致模型的val acc降低（一个简单的解释是，大batch size造成iteration次数减少，导致模型效果变差。当然，实际训练中，大batch size常常搭配较大的lr，所以问题并不是这么简单），可以考虑使用下面的方法解决这个问题。</p><h4 id="（成比例）提高lr"><a href="#（成比例）提高lr" class="headerlink" title="（成比例）提高lr"></a>（成比例）提高lr</h4><p>上面说的iteration次数减少是一个方面。另一个考虑是大的batch size会造成对梯度的估计方差变小，我们可以乘上一个较大的lr，让方差的不确定性增大一些。一个经验之谈是，lr随着batch size成比例扩大。比如在训练ResNet-50的时候，He给出的在$B = 256$时，lr取为$0.1$。那么如果$B = 512$，那么lr也相应扩大为$0.2$。</p><h4 id="lr-warm-up"><a href="#lr-warm-up" class="headerlink" title="lr warm up"></a>lr warm up</h4><p>如果lr初始设置的很大，可能会带来数值不稳定。因为刚开始的时候权重是随机初始化的，gradient也比较大。可以给lr做warm up，也就是开始若干个迭代用较小的lr，等训练稳定了再用回那个大的lr。一种方法是线性warm up，也就是在warm up阶段，lr是线性地从0涨到给定的那个大lr。</p><h4 id="设置-gamma-0"><a href="#设置-gamma-0" class="headerlink" title="设置$\gamma = 0$"></a>设置$\gamma = 0$</h4><p>这个操作比较新奇，在初始阶段，BN的$\beta$参数是设置为$0$的。如果我们再设置$\gamma = 0$，说明BN的输出就是$0$了。这是什么操作？！</p><p>作者指出，可以在ResNet这种有by-pass的结构中使用这个trick。在ResNet block的最后一层，我们经常做$y = x + res(x)$，可以考虑将res这一路的最后一个BN层的$\gamma$参数设置为0。这时候，相当于只有输入$x$传到后面，相当于减少了网络的层深。之后的训练中，$\gamma$会逐渐变大，也就逐渐恢复了res通路。</p><p>这种方法也是试图解决网络训练初始阶段不稳定的问题。不过这个操作还是挺骚的。。。类似的方法（利用BN层的$\gamma$参数）也见到过被用在模型剪枝上，如Net Sliming等方法。可以参见博客中的相关文章讨论。</p><h4 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h4><p>给weight加上L2 norm来做weight decay，是缓解网络过拟合的标准解决办法之一。不过，最好只对conv和fc的kernel做，而不要对它们的bias，BN的$\gamma$和$\beta$做。</p><p>上面的方法，在batch size不大于2K的时候，应该是够用了的。</p><h3 id="低精度"><a href="#低精度" class="headerlink" title="低精度"></a>低精度</h3><p>很多新GPU都加入了FP16的硬件支持，例如V100上使用FP16比FP32，训练能够加速$2$到$3$倍。FP16的问题是表示范围变小了，同时分辨率变小。对应地会造成两个问题，溢出和无法更新（梯度过小，不到FP16的最小表示）。一种解决办法是使用FP16来做forward和backward，但是在FP32上更新梯度（防止梯度过小）。同时给loss乘上一个系数，让它更好地契合FP16能表示的数据范围。</p><p>这里简要介绍下FP16精度的相关内容。关于Nvidia GPU FP16的更多信息，可以参考<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="external">Nvidia文档混合精度训练</a>。</p><h4 id="FP16数据表示"><a href="#FP16数据表示" class="headerlink" title="FP16数据表示"></a>FP16数据表示</h4><p>FP16，顾名思义，就是使用16个bit表示浮点数。具体编码方式上，和FP32基本一致，只不过位数有了缩水。</p><blockquote><p>IEEE 754 standard defines the following 16-bit half-precision floating point format: 1 sign bit, 5 exponent bits, and 10 fractional bits.</p></blockquote><p>TODO: FP32和FP16的比较</p><h4 id="FP16-in-MXNet"><a href="#FP16-in-MXNet" class="headerlink" title="FP16 in MXNet"></a>FP16 in MXNet</h4><p>在MXNet中，使用混合精度训练还是挺简单的。具体可以参考<a href="https://mxnet.incubator.apache.org/versions/master/faq/float16.html" target="_blank" rel="external">Mixed precision training using float16</a></p><p>下面是使用gluon训练时候要注意的几个地方：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## optimizer 开启混合精度选项</span></div><div class="line"><span class="comment">## 这会使optimizer为参数保存一份FP32拷贝，在上面进行梯度的更新，</span></div><div class="line"><span class="comment">## 防止梯度过小无法更新FP16</span></div><div class="line"><span class="keyword">if</span> opt.dtype != <span class="string">'float32'</span>:</div><div class="line">    optimizer_params[<span class="string">'multi_precision'</span>] = <span class="keyword">True</span></div><div class="line"><span class="comment">## net cast到给定的数值精度</span></div><div class="line">net = get_model(model_name, **kwargs)</div><div class="line">net.cast(opt.dtype)</div><div class="line"><span class="comment">## 训练过程中，将输入也cast到指定精度</span></div><div class="line"><span class="keyword">while</span> in_training:</div><div class="line">    <span class="comment">## blablabla</span></div><div class="line">    outputs = [net(X.astype(opt.dtype, copy=<span class="keyword">False</span>)) <span class="keyword">for</span> X <span class="keyword">in</span> data]</div><div class="line">    <span class="comment">## 计算loss也把label cast到指定精度</span></div></pre></td></tr></table></figure><p>使用MXNet老的symbolic接口时候，因为静态图一旦写好就固定了，所以我们需要在建图的时候，考虑FP16精度。</p><ul><li>在原始输入node后面接一个<code>cast</code> op，将FP32转成FP16。</li><li>最好在<code>SoftmaxOutput</code>之前，插入一个<code>cast</code> op，将FP16转回FP32，以便有更高的精度。</li><li><code>optimizer</code>打开<code>multi_precision</code>开关，这里和上面gluon是一致的。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## 建图</span></div><div class="line">data = mx.sym.Variable(name=<span class="string">"data"</span>)</div><div class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</div><div class="line">    data = mx.sym.Cast(data=data, dtype=np.float16)</div><div class="line"><span class="comment"># ... the rest of the network</span></div><div class="line">net_out = net(data)</div><div class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</div><div class="line">    net_out = mx.sym.Cast(data=net_out, dtype=np.float32)</div><div class="line">output = mx.sym.SoftmaxOutput(data=net_out, name=<span class="string">'softmax'</span>)</div><div class="line"><span class="comment">## 优化器设置</span></div><div class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>, multi_precision=<span class="keyword">True</span>, lr=<span class="number">0.01</span>)</div></pre></td></tr></table></figure><p>下面有几条额外的建议：</p><ul><li>FP16加速主要来源于新GPU上的Tensor Core计算$D = A * B + C$这种运算，且它们的维度是$8$的倍数。所以如果不满足$8$倍数这个条件，FP16的计算速度可能不会很快，或者说和FP32相比没多少优势。尤其是当你在CIFAR10这种输入图片size比较小的数据集上训练的时候。</li><li>针对上面这种情况，你可以使用<code>nvprof</code>工具来check是否Tensor Core被使用了，那些名字里面带有<code>s884cudnn</code>的操作就是了。</li><li>确保data io和preprocessing不要成为瓶颈，不然面对这些扯后腿的地方，FP16男默女泪。</li><li>batch size最好设置为8的倍数，2的幂次是坠吼的。</li><li>如果GPU memory还算充足，可以设置<code>MXNET_CUDNN_AUTOTUNE_DEFAULT = 2</code>，来让MXNet有更多的测试来选用最快的卷积算法，代价就是更多的显存占用。</li><li>最好为BatchNorm和SoftmaxOutput使用FP32精度。Gluon里面这些都是自动的，MXNet中BN层是自动的，但是SoftmaxOutput需要自己设置一下，见上。</li></ul><h4 id="loss-scaling"><a href="#loss-scaling" class="headerlink" title="loss scaling"></a>loss scaling</h4><p>再说一下上面提到的loss scaling。</p><p>为啥要做loss scaling呢？主要是由于FP16的精度比较差，而能够表示的较大的数对于CNN网络来说又基本用不到（虽然说FP16的表示范围相比FP32已经缩水不少了），所以可能出现这样一种情形，loss对FP16 weight或activation求梯度，梯度太小，以至于FP16无法表示。那其实我们可以给loss乘上一个系数，放大gradient，以便FP16能够表示。在梯度更新之前，再把这个梯度scale回去，就可以了。如下图所示。</p><p><img src="/img/bag_of_tricks_fp16_range_dismatch.jpg" alt="FP16和FP32的range不匹配"></p><p>使用gluon或MXNet设置loss scaling的方法如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## gluon</span></div><div class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss(weight=<span class="number">128</span>)</div><div class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</div><div class="line">                                multi_precision=<span class="keyword">True</span>,</div><div class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</div><div class="line"><span class="comment">## mxnet</span></div><div class="line">mxnet.sym.SoftmaxOutput(other_args, grad_scale=<span class="number">128.0</span>)</div><div class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</div><div class="line">                                multi_precision=<span class="keyword">True</span>,</div><div class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</div></pre></td></tr></table></figure><p>经验来看，对于Multibox SSD, R-CNN, bigLSTM and Seq2seq这些任务，loss scaling是比较有必要的。这里有个疑问，loss scaling应该是在训练过程中不断变化的，但上面的使用都是直接把loss scaling写死了（gluon还好，再手动给loss乘上一个因子），那如何修改loss scaling呢？后面指出可以使用constant的loss scaling（一般取2的幂次64，128等），但是不知道实际训练会不会有问题。<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="external">Nvidia guide</a>中给出的建议是：</p><blockquote><p>If you encounter precision problems, it is beneficial to scale the loss up by 128, and scale the application of the gradients down by 128.</p></blockquote><p>当然，最好的办法是自己看一下FP32 gradient的分布。</p><p>当当当。。。说了这么多，那么具体加速效果如何呢？使用batch size = $1024$，和batch size = $256$的baseline相比，从下表可知，三种不同的网络结构，分别加速了$1.6X$到$3X$，而且acc还涨了一些。</p><p><img src="/img/bag_of_tricks_accelarate_training.jpg" alt="加速效果"></p><p>具体的acc影响的ablation实验如下。可以看到，只是使用lr线性增大的情况下，大（batch size的）网络稍逊于小（batch size的）网络。不过当使用上面几个技术综合来看的时候，大小网络的性能差异已经抹去了，而且大网络的训练速度更快。</p><p><img src="/img/bag_of_tricks_ablation_of_accelarate_train.jpg" alt="ablation实验结果"></p><h2 id="更好的网络"><a href="#更好的网络" class="headerlink" title="更好的网络"></a>更好的网络</h2><p>TODO: 未完待续</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是&lt;a href=&quot;https://arxiv.org/abs/1812.01187&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt;的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/bag_of_tricks_no_silver_bullet.jpeg&quot; alt=&quot;软工里面没有银弹，数据科学同样这样&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello TVM</title>
    <link href="https://xmfbit.github.io/2019/06/29/tvm-helloworld/"/>
    <id>https://xmfbit.github.io/2019/06/29/tvm-helloworld/</id>
    <published>2019-06-29T05:55:43.000Z</published>
    <updated>2020-03-21T02:02:44.779Z</updated>
    
    <content type="html"><![CDATA[<p>TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！)</p><blockquote><p>TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.</p></blockquote><p>compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。</p><p>我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM!</p><p><img src="/img/tvm_introduction.jpg" alt="TVM概念图"></p><a id="more"></a><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>随着深度学习逐渐从研究所的“伊甸园”迅速在工业界的铺开，摆在大家面前的问题是如何将深度学习模型部署到目标硬件平台上，能够多快好省地完成前向计算，从而提供更好的用户体验，<del>同时为老板省钱，还能减少碳排放来造福子孙</del>。</p><p>和单纯做研究相比，在工业界我们主要遇到了两个问题：</p><ul><li>深度学习框架实在是太$^{\text{TM}}$多了。caffe / mxnet / tensorflow / pytorch训练出来的模型都彼此有不同的分发格式。如果你和我一样，做过不同框架的TensorRT的部署，我想你会懂的。。。</li><li>GPU实在是太$^{\text{TM}}$贵了。深度学习春风吹满地，老黄股票真争气。另一方面，一些嵌入式平台没有使用GPU的条件。同时一些人也开始在做FPGA/ASIC的深度学习加速卡。如何将深度学习模型部署适配到多样的硬件平台上？</li></ul><p>为了解决第一个问题，TVM内部实现了自己的IR，可以将上面这些主流深度学习框架的模型转换为统一的内部表示，以便后续处理。若想要详细了解，可以看下NNVM这篇博客：<a href="https://tvm.ai/2017/10/06/nnvm-compiler-announcement" target="_blank" rel="external">NNVM Compiler: Open Compiler for AI Frameworks</a>。这张图应该能够说明NNVM在TVM中起到的作用。</p><p><img src="/img/tvm_hello_nnvm_as_a_bridge.jpg" alt="NNVM在TVM中的作用"></p><p>为了解决第二个问题，TVM内部有多重机制来做优化。其中一个特点是，使用机器学习（结合专家知识）的方法，通过在目标硬件上跑大量trial，来获得该硬件上相关运算（例如卷积）的最优实现。这使得TVM能够做到快速为新型硬件或新的op做优化。我们知道，在GPU上我们站在Nvidia内部专家的肩膀上，使用CUDA / CUDNN / CUBLAS编程。但相比于Conv / Pooling等Nvidia已经优化的很好了的op，我们自己写的op很可能效率不高。或者在新的硬件上，没有类似CUDA的生态，如何对网络进行调优？TVM这种基于机器学习的方法给出了一个可行的方案。我们只需给定参数的搜索空间（少量的人类专家知识），就可以将剩下的工作交给TVM。如果对此感兴趣，可以阅读TVM中关于AutoTuner的介绍和tutorial：<a href="https://docs.tvm.ai/tutorials/autotvm/tune_nnvm_arm.html" target="_blank" rel="external">Auto-tuning a convolutional network for ARM CPU</a>。</p><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>我的环境为Debian 8，CUDA 9。</p><h3 id="准备代码"><a href="#准备代码" class="headerlink" title="准备代码"></a>准备代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">git <span class="built_in">clone</span> https://github.com/dmlc/tvm.git</div><div class="line"><span class="built_in">cd</span> tvm</div><div class="line">git checkout e22b5802</div><div class="line">git submodule update --init --recursive</div></pre></td></tr></table></figure><h3 id="config文件"><a href="#config文件" class="headerlink" title="config文件"></a>config文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> tvm</div><div class="line">mkdir build</div><div class="line">cp ../cmake/config.cmake ./build</div><div class="line"><span class="built_in">cd</span> build</div></pre></td></tr></table></figure><p>编辑config文件，打开CUDA / BLAS / cuBLAS / CUDNN的开关。注意下LLVM的开关。LLVM可以从这个页面<a href="http://releases.llvm.org/download.html" target="_blank" rel="external">LLVM Download</a>下载，我之前就已经下载好，版本为7.0。如果你像我一样是Debian8，可以使用for Ubuntu14.04的那个版本。由于是已经编译好的二进制包，下载之后解压即可。</p><p>找到这一行，改成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">set(USE_LLVM /path/to/llvm/bin/llvm-config)</div></pre></td></tr></table></figure></p><h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>这里有个坑，因为我们使用了LLVM，最好使用LLVM中的clang。否则可能导致tvm生成的代码无法二次导入。见这个讨论帖：<a href="https://discuss.tvm.ai/t/runtime-llvm-cc-create-shared-error-while-run-tune-simple-template/1037" target="_blank" rel="external">_cc.create_shared error while run tune_simple_template</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">export</span> LLVM=/path/to/llvm</div><div class="line">cmake -DCMAKE_C_COMPILER=<span class="variable">$LLVM</span>/bin/clang -DCMAKE_CXX_COMPILER=<span class="variable">$LLVM</span>/bin/clang++ ..</div><div class="line"><span class="comment"># 火力全开，let's rock</span></div><div class="line">make -j$(nproc)</div></pre></td></tr></table></figure><h3 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">cd</span> /path/to/tvm</div><div class="line"><span class="comment"># 我一般用清华的镜像，你呢。。。</span></div><div class="line"><span class="built_in">export</span> THU_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple</div><div class="line">pip install tornado tornado psutil xgboost numpy decorator attrs  --user -i <span class="variable">$THU_MIRROR</span></div><div class="line"><span class="built_in">cd</span> python; python setup.py install --user; <span class="built_in">cd</span> ..</div><div class="line"><span class="built_in">cd</span> topi/python; python setup.py install --user; <span class="built_in">cd</span> ../..</div><div class="line"><span class="built_in">cd</span> nnvm/python; python setup.py install --user; <span class="built_in">cd</span> ../..</div></pre></td></tr></table></figure><h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><p>使用tvm为mxnet symbol计算图生成CUDA代码，并进行前向计算。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> numpy</div><div class="line"><span class="keyword">import</span> tvm</div><div class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> relay</div><div class="line"><span class="keyword">from</span> tvm.relay <span class="keyword">import</span> testing</div><div class="line"><span class="keyword">from</span> tvm.contrib <span class="keyword">import</span> graph_runtime</div><div class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</div><div class="line"></div><div class="line"><span class="comment">## load mxnet model</span></div><div class="line">prefix = <span class="string">'/your/mxnet/checkpoint/prefix'</span></div><div class="line">epoch = <span class="number">0</span></div><div class="line">mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)</div><div class="line"></div><div class="line"><span class="comment">## import model into tvm from mxnet</span></div><div class="line">shape_dict = &#123;<span class="string">'data'</span>: (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)&#125;</div><div class="line"><span class="comment">## tvm提供了 frontend.from_XXX 接口，从不同的框架中导入模型</span></div><div class="line">relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict,</div><div class="line">        arg_params=arg_params, aux_params=aux_params)</div><div class="line"></div><div class="line"><span class="comment"># 设定目标硬件为 GPU，生成TVM模型</span></div><div class="line"><span class="comment">## ---------------------------- </span></div><div class="line"><span class="comment"># graph：execution graph in json format</span></div><div class="line"><span class="comment"># lib: tvm module library of compiled functions for the graph on the target hardware</span></div><div class="line"><span class="comment"># params: parameter blobs</span></div><div class="line"><span class="comment">## ---------------------------</span></div><div class="line">target = <span class="string">'cuda'</span></div><div class="line"><span class="keyword">with</span> relay.build_config(opt_level=<span class="number">3</span>):</div><div class="line">    graph, lib, params = relay.build(relay_func, target, params=relay_params)</div><div class="line"></div><div class="line"><span class="comment"># run forward</span></div><div class="line"><span class="comment">## 直接使用tvm提供的cat示例图片</span></div><div class="line"><span class="keyword">from</span> tvm.contrib.download <span class="keyword">import</span> download_testdata</div><div class="line">img_url = <span class="string">'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'</span></div><div class="line">img_path = download_testdata(img_url, <span class="string">'cat.png'</span>, module=<span class="string">'data'</span>)</div><div class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</div><div class="line">image = Image.open(img_path).resize((<span class="number">224</span>, <span class="number">224</span>))</div><div class="line"></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_image</span><span class="params">(im)</span>:</span></div><div class="line">    im = np.array(im).astype(np.float32)</div><div class="line">    im = np.transpose(im, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</div><div class="line">    im = im[np.newaxis, :]</div><div class="line">    <span class="keyword">return</span> im</div><div class="line"></div><div class="line">x = transform_image(image)</div><div class="line"></div><div class="line"><span class="comment"># let's go</span></div><div class="line">ctx = tvm.gpu(<span class="number">0</span>)</div><div class="line">dtype = <span class="string">'float32'</span></div><div class="line"><span class="comment">## 加载模型</span></div><div class="line">m = graph_runtime.create(graph, lib, ctx)</div><div class="line"><span class="comment">## set input data</span></div><div class="line">m.set_input(<span class="string">'data'</span>, tvm.nd.array(x.astype(dtype)))</div><div class="line"><span class="comment">## set input params</span></div><div class="line">m.set_input(**params)</div><div class="line">m.run()</div><div class="line"><span class="comment"># get output</span></div><div class="line">outputs = m.get_output(<span class="number">0</span>)</div><div class="line">top1 = np.argmax(outputs.asnumpy()[<span class="number">0</span>])</div><div class="line"></div><div class="line"><span class="comment"># save model</span></div><div class="line"><span class="comment">## lib存为tar包文件，解压后可以发现，就是打包了动态链接库</span></div><div class="line">path_lib = <span class="string">'./deploy_resnet50_v2_lib.tar'</span></div><div class="line">lib.export_library(path_lib)</div><div class="line"></div><div class="line"><span class="comment">## 计算图存为json文件</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(graph)</div><div class="line"><span class="comment">## 权重存为二进制文件</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_params'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</div><div class="line">    f.write(relay.save_param_dict(params))</div><div class="line"></div><div class="line"><span class="comment"># load model back</span></div><div class="line">loaded_json = open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>).read()</div><div class="line">loaded_lib = tvm.module.load(path_lib)</div><div class="line">loaded_params = bytearray(open(<span class="string">'./deploy_params'</span>, <span class="string">'rb'</span>).read())</div><div class="line">module = graph_runtime.create(loaded_json, loaded_lib, ctx)</div><div class="line"><span class="comment">## 好了，剩下的就都一样了</span></div></pre></td></tr></table></figure><h2 id="最后的话"><a href="#最后的话" class="headerlink" title="最后的话"></a>最后的话</h2><p>我个人的观点，TVM是一个很有意思的项目。在深度学习模型的优化和部署上做了很多探索，在官方放出的benchmark上表现还是不错的。如果使用非GPU进行模型的部署，TVM值得一试。不过在GPU上，得益于Nvidia的CUDA生态，目前TensorRT仍然用起来更方便，综合性能更好。如果你和我一样，主要仍然在GPU上搞事情，可以密切关注TVM的发展，并尝试使用在自己的项目中，不过我觉得还是优先考虑TensorRT。<del>另一方面，TVM的代码实在是看不太懂啊。。。</del></p><h2 id="想要更多"><a href="#想要更多" class="headerlink" title="想要更多"></a>想要更多</h2><ul><li>TVM paper：<a href="https://arxiv.org/abs/1802.04799" target="_blank" rel="external">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li><li>TVM 项目主页：<a href="https://tvm.ai/" target="_blank" rel="external">TVM</a></li></ul><p>后续TVM的介绍，不知道啥时候有时间再写。。。随缘吧。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。&lt;/p&gt;
&lt;p&gt;我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/tvm_introduction.jpg&quot; alt=&quot;TVM概念图&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="tvm" scheme="https://xmfbit.github.io/tags/tvm/"/>
    
  </entry>
  
  <entry>
    <title>重读 C++ Primer</title>
    <link href="https://xmfbit.github.io/2019/05/01/cpp-primer-review/"/>
    <id>https://xmfbit.github.io/2019/05/01/cpp-primer-review/</id>
    <published>2019-05-01T05:32:58.000Z</published>
    <updated>2020-03-21T02:02:44.798Z</updated>
    
    <content type="html"><![CDATA[<p>重读C++ Primer第五版，整理一些糊涂的语法知识点。</p><div align="center">     <img src="/img/cpp_is_terrible.jpg" width="200" height="300" alt="入门到放弃" align="center"></div><a id="more"></a><h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><p>总结一些比较容易搞乱的基础语法。</p><h3 id="const-限定说明符"><a href="#const-限定说明符" class="headerlink" title="const 限定说明符"></a>const 限定说明符</h3><ul><li><code>const</code>对象一般只在当前文件可见，如果希望在其他文件访问，在声明和定义时，均需加上<code>extern</code>关键字。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">100</span>;</div></pre></td></tr></table></figure><ul><li>顶层<code>const</code>和底层<code>const</code></li></ul><p>指针本身也是对象，所以有所谓的“常量指针”（指针本身不能赋值）和“指向常量的指针”（指针指向的那个对象不能赋值）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> a = <span class="number">10</span>;</div><div class="line"><span class="comment">// 指针指向的对象不能经由指针赋值</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span>* p1 = &amp;a;</div><div class="line">*p = <span class="number">0</span>;  <span class="comment">// 错误</span></div><div class="line"><span class="comment">// 指针本身不能再赋值</span></div><div class="line"><span class="keyword">int</span>* <span class="keyword">const</span> p2 = &amp;a;</div><div class="line"><span class="keyword">int</span> b = <span class="number">0</span>;</div><div class="line">p2 = &amp;p;   <span class="comment">// 错误</span></div></pre></td></tr></table></figure><p>如何记住这条规则？c++中类型说明从右向左读即可。例如<code>p1</code>，其左侧首先遇到<code>int*</code>，故其是个“普通”指针（没有被<code>const</code>修饰），再往左才读到<code>const</code>，故这个指针指向的内容是常量，不能修改。<code>p2</code>同理。</p><p>把“指针本身是常量”的行为称为“顶层const”（top-level），把“指针指向内容是常量”的行为称为“底层const”（low-level）。</p><h3 id="auto-和-decltype"><a href="#auto-和-decltype" class="headerlink" title="auto 和 decltype"></a>auto 和 decltype</h3><ul><li><code>auto</code>类型推断的规则</li></ul><p>编译器推断<code>auto</code>声明变量的类型时，可能和初始值类型不一样。当初始值类型为引用时，编译器以被引用对象的类型作为<code>auto</code>的类型，除非显式指明。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</div><div class="line"><span class="keyword">int</span>&amp; ri = i;</div><div class="line"><span class="comment">// type of j: int</span></div><div class="line"><span class="keyword">auto</span> j = ri;</div><div class="line"><span class="comment">// type of rj: int&amp;</span></div><div class="line"><span class="keyword">auto</span>&amp; rj = ri;</div></pre></td></tr></table></figure><p>另外，<code>auto</code>只会保留底层<code>const</code>，忽略顶层<code>const</code>，除非显式指定。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</div><div class="line"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="keyword">const</span> p = &amp;a;</div><div class="line"><span class="comment">// type of b: int</span></div><div class="line"><span class="keyword">auto</span> b = a;</div><div class="line"><span class="comment">// type of p1: const int*</span></div><div class="line"><span class="keyword">auto</span> p1 = p;</div><div class="line"><span class="comment">// type of p2: const int* const</span></div><div class="line"><span class="keyword">const</span> <span class="keyword">auto</span> p2 = p;</div><div class="line">p1 = &amp;b;   <span class="comment">// ok, p1 本身已经不是const的了</span></div><div class="line">p2 = &amp;b;   <span class="comment">// wrong! 显式指定了 p2 本身是 const</span></div><div class="line">*p1 = <span class="number">10</span>;  <span class="comment">// wrong! p1 保留了底层const，指向的内容仍然不可改变</span></div></pre></td></tr></table></figure><ul><li><code>decltype</code> 类型推断规则</li></ul><p>和<code>auto</code>不同，<code>decltype</code>保留表达式的顶层<code>const</code>和引用。</p><ol><li>如果表达式是变量，那么返回该变量的类型；</li><li>如果表达式不是纯变量，返回表达式结果的类型；</li><li>如果表达式是解引用，返回引用类型。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">int</span> i = <span class="number">42</span>, *p = &amp;i, &amp;r = i;</div><div class="line"><span class="keyword">decltype</span>(i) j;    <span class="comment">// ok, j is a int</span></div><div class="line"><span class="keyword">decltype</span>(r) y;    <span class="comment">// wrong! y是引用类型，必须初始化</span></div><div class="line"><span class="keyword">decltype</span>(r + <span class="number">0</span>) z;  <span class="comment">// ok, r+0 返回值是int</span></div><div class="line"><span class="keyword">decltype</span>(*p) c;   <span class="comment">// wrong! 解引用的结果是引用，必须初始化</span></div></pre></td></tr></table></figure><p>有一种情况特殊，如果是春变量，但是变量名加上括号，结果将是引用。原因：变量加上括号，将会被当做表达式。而变量又可以被赋值，所以得到了引用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">dectype((i)) d;  <span class="comment">// wrong! d是引用</span></div></pre></td></tr></table></figure><h2 id="泛型算法"><a href="#泛型算法" class="headerlink" title="泛型算法"></a>泛型算法</h2><p>C++的标准库中实现了很多泛型算法，如<code>find</code>, <code>sort</code>等。它们大多定义在<code>&lt;algorithm&gt;</code>头文件中，一些数值相关的定义在<code>&lt;numeric&gt;</code>中。通过“迭代器”这一层抽象，泛型算法可以不关心所操作数据实际储存的容器，不过仍然受制于实际数据类型。例如<code>find</code>中，为了比较当前元素是否为所求值，要求元素类型实现<code>==</code>运算。好在这些算法大多支持自定义操作。</p><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>在标准库的<code>&lt;iterator&gt;</code>中，定义了如下几种通用迭代器。</p><ul><li>插入迭代器</li></ul><p>插入器是一个迭代器的适配器，接受一个容器，生成一个用于该容器的迭代器，能够实现向该容器插入元素。插入迭代器有三种，区别在于插入元素的位置：</p><ol><li><code>back_inserter</code>，创建一个使用<code>push_back</code>插入的迭代器</li><li><code>front_inserter</code>，创建一个使用<code>push_front</code>插入的迭代器</li><li><code>inserter</code>，创建一个使用<code>insert</code>的迭代器，在给定的迭代器前面插入元素</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div></pre></td><td class="code"><pre><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span></span></div><div class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></div><div class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</div><div class="line"></div><div class="line"><span class="comment">// 使用back_inserter插入数据</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b;</div><div class="line">    <span class="comment">// copy a -&gt; b, 动态改变b的大小</span></div><div class="line">    copy(begin(a), end(a), back_inserter(b));</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v: b) &#123;</div><div class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// b: 1, 2, 3, 4, 5</span></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div><div class="line"></div><div class="line"><span class="comment">// 使用inserter，将数据插入指定位置</span></div><div class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</div><div class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</div><div class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b &#123;<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>&#125;;</div><div class="line">    <span class="comment">// find iter of value 8</span></div><div class="line">    <span class="keyword">auto</span> iter = find(b.begin(), b.end(), <span class="number">8</span>);</div><div class="line">    <span class="comment">// copy a -&gt; b before value 8</span></div><div class="line">    copy(begin(a), end(a), inserter(b, iter));</div><div class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v : b) &#123;</div><div class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="comment">// b: 6, 7, 1, 2, 3, 4, 5, 8</span></div><div class="line">    <span class="keyword">return</span> <span class="number">0</span>;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>这里要注意的是，当使用<code>front_inserter</code>时，由于插入总是在容器头部发生，所以最后的插入结果是原始数据序列的逆序。</p><ul><li>流迭代器</li></ul><p>虽然输入输出流不是容器，不过也有用于这些IO对象的迭代器：<code>istream_iterator</code>和<code>ostream_iterator</code>。这样，我们可以通过它们向对应的输入输出流读写数据，</p><p>创建输入流迭代器时，必须指定其要操作的数据类型，并将其绑定到某个流（标准输入输出流或文件流），或使用默认初始化，得到当做尾后值使用的迭代器。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; in_iter(<span class="built_in">cin</span>);</div><div class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; in_eof;</div><div class="line"><span class="comment">// 使用迭代器构建vector</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; values(in_iter, in_eof);</div></pre></td></tr></table></figure><p>创建输出流迭代器时，必须指定其要操作的数据类型，并向其绑定到某个流，还可以传入第二个参数，类型是C风格的字符串（字符串字面常量或指向<code>\0</code>结尾的字符数组指针），表示在输出数据之后，还会输出此字符串。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</div><div class="line"><span class="comment">// 输出：1       2       3       4       5 </span></div><div class="line">copy(v.begin(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">"\t"</span>));</div></pre></td></tr></table></figure><ul><li>反向迭代器</li></ul><p>顾名思义，反向迭代器的迭代顺序和正常的迭代器是相反的。使用<code>rbegin</code>和<code>rend</code>可以获得绑定在该容器的反向迭代器。不过<code>forward_list</code>和流对象，由于没有同时实现<code>++</code>和<code>--</code>，所以没有反向迭代器。</p><p>反向迭代器常常用来在容器中查找最后一个满足条件的元素。这时候要注意，如果继续使用该迭代器，顺序仍然是反向的。如果需要正向迭代器，可以使用<code>.base()</code>方法得到对应的正向迭代器。不过要注意，正向迭代器和反向迭代器的位置会不一样哦~</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 找到数组中最后一个5,并将其后数字打印出来</span></div><div class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v &#123;<span class="number">10</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</div><div class="line"><span class="keyword">auto</span> iter = find(v.rbegin(), v.rend(), <span class="number">5</span>);</div><div class="line"><span class="comment">// 输出：5,4,5,10,</span></div><div class="line">copy(iter, v.rend(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</div><div class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"\n"</span>;</div><div class="line"><span class="comment">// 输出：1,2, 注意并没有输出5</span></div><div class="line">copy(iter.base(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</div></pre></td></tr></table></figure><h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2><p>拖延症发作。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;重读C++ Primer第五版，整理一些糊涂的语法知识点。&lt;/p&gt;
&lt;div  align=&quot;center&quot;&gt;    
 &lt;img src=&quot;/img/cpp_is_terrible.jpg&quot; width = &quot;200&quot; height = &quot;300&quot; alt=&quot;入门到放弃&quot; align=center /&gt;
&lt;/div&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="https://xmfbit.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>YOLO Caffe模型转换BN的坑</title>
    <link href="https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/"/>
    <id>https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/</id>
    <published>2019-03-09T04:51:18.000Z</published>
    <updated>2020-03-21T02:02:44.784Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。</p><a id="more"></a><h1 id="Darknet中BN的计算"><a href="#Darknet中BN的计算" class="headerlink" title="Darknet中BN的计算"></a>Darknet中BN的计算</h1><p>以CPU代码为例，在Darknet中，BN做normalization的操作如下，<a href="https://github.com/pjreddie/darknet/blob/master/src/blas.c#L147" target="_blank" rel="external">normalize_cpu</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial)</span></span></div><div class="line">&#123;</div><div class="line">    <span class="keyword">int</span> b, f, i;</div><div class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</div><div class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</div><div class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; spatial; ++i)&#123;</div><div class="line">                <span class="keyword">int</span> index = b*filters*spatial + f*spatial + i;</div><div class="line">                x[index] = (x[index] - mean[f])/(<span class="built_in">sqrt</span>(variance[f]) + <span class="number">.000001</span>f);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>可以看到，Darknet中的BN计算如下：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2} + \epsilon}</script><p>而且，$\epsilon$参数是固定的，为$1\times 10^{-6}$。</p><h1 id="问题和解决"><a href="#问题和解决" class="headerlink" title="问题和解决"></a>问题和解决</h1><p>然而，在Caffe（以及大部分其他框架）中，$\epsilon$的位置是在根号里面的，也就是：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><p>另外，查看<code>caffe.proto</code>可以知道，Caffe默认的$\epsilon$值为$1\times 10^{-5}$。</p><p>所以，在转换为caffe prototxt时，需要设置<code>batch_norm_param</code>如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">batch_norm_param &#123;</div><div class="line">  use_global_stats: true</div><div class="line">  eps: 1e-06</div><div class="line">&#125;</div></pre></td></tr></table></figure><p>另外，需要重新求解$\sigma^2$，按照layer输出要相等的等量关系，可以求得：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_running_var</span><span class="params">(var, eps=DARKNET_EPS)</span>:</span></div><div class="line">    <span class="keyword">return</span> np.square(np.sqrt(var) + eps) - eps</div></pre></td></tr></table></figure><p>这里调整之后，转换后的Caffe模型和原始Darknet模型的输出误差已经是$1\times 10^{-7}$量级，可以认为转换成功。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>MacOS Mojave更新之后一定要做这几件事！</title>
    <link href="https://xmfbit.github.io/2018/10/27/mac-update-mojave/"/>
    <id>https://xmfbit.github.io/2018/10/27/mac-update-mojave/</id>
    <published>2018-10-27T06:57:12.000Z</published>
    <updated>2020-03-21T02:02:44.774Z</updated>
    
    <content type="html"><![CDATA[<p>很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。<br><a id="more"></a></p><h2 id="Git等工具"><a href="#Git等工具" class="headerlink" title="Git等工具"></a>Git等工具</h2><p>试图使用<code>git</code>是出现了如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">git clone xx.git</div><div class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</div></pre></td></tr></table></figure><p>解决办法参考<a href="https://apple.stackexchange.com/questions/254380/macos-mojave-invalid-active-developer-path" target="_blank" rel="external">macOS Mojave: invalid active developer path</a>中的最高赞回答：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">xcode-select --install</div></pre></td></tr></table></figure></p><h2 id="osxfuse"><a href="#osxfuse" class="headerlink" title="osxfuse"></a>osxfuse</h2><p>参考Github讨论帖<a href="https://github.com/osxfuse/osxfuse/issues/542" target="_blank" rel="external">osxfuse not compatible with MacOS Mojave</a>，从官网下载最新的3.8.2版本安装即可。</p><h2 id="VSCode等编辑器字体变“瘦”"><a href="#VSCode等编辑器字体变“瘦”" class="headerlink" title="VSCode等编辑器字体变“瘦”"></a>VSCode等编辑器字体变“瘦”</h2><p>更新之后，发现VSCode编辑器中的字体变得“很瘦”，不美观。执行下面的命令，并重启机器，应该可以恢复。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">defaults write -g CGFontRenderingFontSmoothingDisabled -bool NO</div></pre></td></tr></table></figure></p><h2 id="Mos-Caffine-IINA-等APP"><a href="#Mos-Caffine-IINA-等APP" class="headerlink" title="Mos Caffine IINA 等APP"></a>Mos Caffine IINA 等APP</h2><p>Mos可以平滑Mac上外接鼠标的滚动，并调整鼠标滚动方向和Windows相同。更新后发现Mos失灵。这应该是和新版本中更强的权限管理有关，解决办法是在”安全隐私设置” -&gt; “辅助功能”中，先把Mos的勾勾去掉，然后重新勾选。Caffine同样的操作。</p><p>IINA是一款Mac上的播放器软件，是我在Mac上的默认播放器。更新后点击媒体文件，发现只是弹出IINA软件的界面，却没有自动播放。解决办法是在媒体文件上右键，在打开方式中重新选择IINA，并勾选默认打开方式选项。</p><p>更新新系统后，遇到的坑暂时就这么多。希望能够帮助到需要的人。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。&lt;br&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Rethinking The Value of Network Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/</id>
    <published>2018-10-22T14:25:42.000Z</published>
    <updated>2020-03-21T02:02:44.796Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://openreview.net/forum?id=rJlnB3C5Ym" target="_blank" rel="external">这篇文章</a>是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/">论文总结 - 模型剪枝 Model Pruning</a>。</p><p>这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。</p><p>然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：<a href="https://github.com/Eric-mingjie/rethinking-network-pruning" target="_blank" rel="external">Eric-mingjie/rethinking-network-pruning</a>。</p><a id="more"></a><h2 id="FLOP的计算"><a href="#FLOP的计算" class="headerlink" title="FLOP的计算"></a>FLOP的计算</h2><p>代码中有关于PyTorch模型的FLOPs的计算，见<a href="https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py" target="_blank" rel="external">compute_flops.py</a>。可以很方便地应用到自己的代码中。</p><h2 id="ThiNet的实现"><a href="#ThiNet的实现" class="headerlink" title="ThiNet的实现"></a>ThiNet的实现</h2><h2 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>几个仍然有疑问的地方：</p><ol><li><p>作者已经证明在ImageNet/CIFAR等样本分布均衡的数据集上的结论，如果样本分布不均衡呢？有三种思路有待验证：</p><ul><li>prune模型需要从大模型处继承权重，然后直接在不均衡数据集上训练即可；</li><li>prune模型不需要从大模型处继承权重， 但是需要先在ImageNet数据集上训练，然后再在不均衡数据集上训练；</li><li>prune模型直接在不均衡数据集上训练（以我的经验，这种思路应该是不work的）</li></ul></li><li><p>prune前的大模型权重不重要，结构重要，这是本文的结论之一。自动搜索树的prune算法可以看做是模型结构搜索，但是大模型给出了搜索空间的一个很好的初始点。这个初始点是否是任务无关的？也就是说，对A任务有效的小模型，是否在B任务上也是很work的？</p></li><li><p>现在的网络搜索中应用了强化学习/遗传算法等方法，这些方法怎么能够和prune结合？ECCV 2018中HanSong和He Yihui发表了AMC方法。</p></li></ol><p>总之，作者用自己辛勤的实验，给我们指出了一个”可能的”（毕竟文章还没被接收）误区，但是仍然有很多乌云漂浮在上面，需要更多的实验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=rJlnB3C5Ym&quot;&gt;这篇文章&lt;/a&gt;是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考&lt;a href=&quot;https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/&quot;&gt;论文总结 - 模型剪枝 Model Pruning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。&lt;/p&gt;
&lt;p&gt;然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：&lt;a href=&quot;https://github.com/Eric-mingjie/rethinking-network-pruning&quot;&gt;Eric-mingjie/rethinking-network-pruning&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文总结 - 模型剪枝 Model Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/</id>
    <published>2018-10-03T08:31:07.000Z</published>
    <updated>2020-03-21T02:02:44.797Z</updated>
    
    <content type="html"><![CDATA[<p>模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。</p><p><img src="/img/paper-summary-model-pruning-joke.jpg" alt="剪枝的学问"></p><a id="more"></a><h2 id="Deep-Compression-Han-Song"><a href="#Deep-Compression-Han-Song" class="headerlink" title="Deep Compression, Han Song"></a>Deep Compression, Han Song</h2><p>抛去LeCun等人在90年代初的几篇论文，HanSong是这个领域的先行者。发表了一系列关于模型压缩的论文。其中NIPS 2015上的这篇<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="external">Learning both weights and connections for efficient neural network</a>着重讨论了对模型进行剪枝的方法。这篇论文之前我已经写过了<a href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/">阅读总结</a>，比较详细。</p><p>概括来说，作者提出的主要观点包括，L1 norm作为neuron是否重要的metric，train -&gt; pruning -&gt; retrain三阶段方法以及iteratively pruning。需要注意的是，作者的方法只能得到非结构化的稀疏，对于作者的专用硬件EIE可能会很有帮助。但是如果想要在通用GPU或CPU上用这种方法做加速，是不太现实的。</p><h2 id="SSL，WenWei"><a href="#SSL，WenWei" class="headerlink" title="SSL，WenWei"></a>SSL，WenWei</h2><p>既然非结构化稀疏对现有的通用GPU/CPU不友好，那么可以考虑构造结构化的稀疏。将Conv中的某个filter或filter的某个方形区域甚至是某个layer直接去掉，应该是可以获得加速效果的。WenWei<a href="https://arxiv.org/abs/1608.03665" target="_blank" rel="external">论文Learning Structured Sparsity in Deep Neural Networks</a>发表在NIPS 2016上，介绍了如何使用LASSO，给损失函数加入相应的惩罚，进行结构化稀疏。这篇论文之前也已经写过博客，可以参考<a href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/">博客文章</a>。</p><p>概括来说，作者引入LASSO正则惩罚项，通过不同的具体形式，构造了对不同结构化稀疏的损失函数。</p><h2 id="L1-norm-Filter-Pruning，Li-Hao"><a href="#L1-norm-Filter-Pruning，Li-Hao" class="headerlink" title="L1-norm Filter Pruning，Li Hao"></a>L1-norm Filter Pruning，Li Hao</h2><p>在通用GPU/CPU上，加速效果最好的还是整个Filter直接去掉。作者发表在ICLR 2017上的<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="external">论文Pruning Filters for Efficient ConvNets</a>提出了一种简单的对卷积层的filter进行剪枝的方法。</p><p>这篇论文真的很简单。。。主要观点就是通过Filter的L1 norm来判断这个filter是否重要。人为设定剪枝比例后，将该层不重要的那些filter直接去掉，并进行fine tune。在确定剪枝比例的时候，假定每个layer都是互相独立的，分别对其在不同剪枝比例下进行剪枝，并评估模型在验证集上的表现，做sensitivity分析，然后确定合理的剪枝比例。在实现的时候要注意，第$i$个layer中的第$j$个filter被去除，会导致其输出的feature map中的第$j$个channel缺失，所以要相应调整后续的BN层和Conv层的对应channel上的参数。</p><p>另外，实现起来还有一些细节，这些可以参见原始论文。提一点，在对ResNet这种有旁路结构的网络进行剪枝时，每个block中的最后一个conv不太好处理。因为它的输出要与旁路做加和运算。如果channel数量不匹配，是没法做的。作者在这里的处理方法是，听identity那一路的。如果那一路确定了剪枝后剩余的index是多少，那么$\mathcal{F}(x)$那一路的最后那个conv也这样剪枝。</p><p>这里给出一张在ImageNet上做sensitivity analysis的图表。需要对每个待剪枝的layer进行类似的分析。</p><p><img src="/img/paper-model-pruning-filter-pruning-sensitivity-results.png" alt="sensitivity分析"></p><h2 id="Automated-Gradual-Pruning-Gupta"><a href="#Automated-Gradual-Pruning-Gupta" class="headerlink" title="Automated Gradual Pruning, Gupta"></a>Automated Gradual Pruning, Gupta</h2><p>这篇文章发表在NIPS 2017的一个关于移动设备的workshop上，名字很有意思（这些人起名字为什么都这么熟练啊）：<a href="https://arxiv.org/abs/1710.01878" target="_blank" rel="external">To prune, or not to prune: exploring the efficacy of pruning for model compression</a>。TensorFlow的repo中已经有了对应的实现（亲儿子。。）：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning" target="_blank" rel="external">Model pruning: Training tensorflow models to have masked connections</a>。哈姆雷特不能回答的问题，作者的答案则是Yes。</p><p><img src="/img/paper-model-pruning-why-so-baixue.jpg" alt="为什么你们起名字这么熟练啊"></p><p>这篇文章主要有两个贡献。一是比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但是compact-small模型的性能，得出结论：对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，都是前者更好。具体的数据参考论文。</p><p>二是提出了一个渐进的自动调节的pruning策略。首先，作者也着眼于非结构化稀疏。同时和上面几篇文章一样，作者也使用绝对值大小作为衡量importance的标准，作者提出，sparsity可以按照下式自动调节：</p><script type="math/tex; mode=display">s_t = s_f + (s_i-s_f)(1-\frac{t-t_0}{n\Delta t})^3 \quad \text{for}\quad t \in \{t_0, t_0+\Delta t,\dots,t_0+n\Delta t\}</script><p>其中，$s_i$是初始剪枝比例，一般为$0$。$s_f$为最终的剪枝比例，开始剪枝的迭代次数为$t_0$，剪枝间隔为$\Delta t$，共进行$n$次。</p><h2 id="Net-Sliming-Liu-Zhuang-amp-Huang-Gao"><a href="#Net-Sliming-Liu-Zhuang-amp-Huang-Gao" class="headerlink" title="Net Sliming, Liu Zhuang &amp; Huang Gao"></a>Net Sliming, Liu Zhuang &amp; Huang Gao</h2><p>这篇文章<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="external">Learning Efficient Convolutional Networks through Network Slimming</a>发表在ICCV 2017，利用CNN网络中的必备组件——BN层中的gamma参数，实现端到端地学习剪枝参数，决定某个layer中该去除掉哪些channel。作者中有DenseNet的作者——姚班学生刘壮和康奈尔大学博士后黄高。代码已经开源：<a href="https://github.com/liuzhuang13/slimming" target="_blank" rel="external">liuzhuang13/slimming</a>。</p><p>作者的主要贡献是提出可以使用BN层的gamma参数，标志其前面的conv输出的feature map的某个channel是否重要，相应地，也是conv参数中的那个filter是否重要。</p><p>首先，需要给BN的gamma参数加上L1 正则惩罚训练模型，新的损失函数变为$L= \sum_{(x,y)}l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma}g(\gamma)$。</p><p>接着将该网络中的所有gamma进行排序，根据人为给出的剪枝比例，去掉那些gamma很小的channel，也就是对应的filter。最后进行finetune。这个过程可以反复多次，得到更好的效果。如下所示：<br><img src="/img/paper-model-pruning-net-sliming-procedure.png" alt="Net Sliming的大致流程"></p><p>还是上面遇到过的问题，如果处理ResNet或者DenseNet Feature map会多路输出的问题。这里作者提出使用一个”channel selection layer”，统一对该feature map的输出进行处理，只选择没有被mask掉的那些channel输出。具体实现可以参见开源代码<a href="https://github.com/Eric-mingjie/network-slimming/blob/master/models/channel_selection.py#L6" target="_blank" rel="external">channel selection layer</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">channel_selection</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="string">"""</span></div><div class="line">    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.</div><div class="line">    The output shape of this layer is determined by the number of 1 in `self.indexes`.</div><div class="line">    """</div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Initialize the `indexes` with all one vector with the length same as the number of channels.</div><div class="line">        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.</div><div class="line">        """</div><div class="line">        super(channel_selection, self).__init__()</div><div class="line">        self.indexes = nn.Parameter(torch.ones(num_channels))</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_tensor)</span>:</span></div><div class="line">        <span class="string">"""</span></div><div class="line">        Parameter</div><div class="line">        ---------</div><div class="line">        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.</div><div class="line">        """</div><div class="line">        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))</div><div class="line">        <span class="keyword">if</span> selected_index.size == <span class="number">1</span>:</div><div class="line">            selected_index = np.resize(selected_index, (<span class="number">1</span>,))</div><div class="line">        output = input_tensor[:, selected_index, :, :]</div><div class="line">        <span class="keyword">return</span> output</div></pre></td></tr></table></figure><p>略微解释一下：在开始加入L1正则，惩罚gamma的时候，相当于identity变换；当确定剪枝参数后，相应index会被置为$0$，被mask掉，这样输出就没有这个channel了。后面的几路都可以用这个共同的输出。</p><h2 id="AutoPruner-Wu-Jianxin"><a href="#AutoPruner-Wu-Jianxin" class="headerlink" title="AutoPruner, Wu Jianxin"></a>AutoPruner, Wu Jianxin</h2><p>这篇文章<a href="https://arxiv.org/abs/1805.08941" target="_blank" rel="external">AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference</a>是南大Wu Jianxin组新进发的文章，还没有投稿到任何学术会议或期刊，只是挂在了Arvix上，应该是还不够完善。他们还有一篇文章ThiNet：<a href="https://arxiv.org/abs/1707.06342" target="_blank" rel="external">ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression</a>发表在ICCV 2017上。</p><p>这篇文章的主要贡献是提出了一种端到端的模型剪枝方法，如下图所示。为第$i$个Conv输出加上一个旁路，输入为其输出的Feature map，依次经过Batch-wise Pooling -&gt; FC -&gt; scaled sigmoid的变换，按channel输出取值在$[0,1]$范围的向量作为mask，与Feature map做积，mask掉相应的channel。通过学习FC的参数，就可以得到适当的mask，判断该剪掉第$i$个Conv的哪个filter。其中，scaled sigmoid变换是指$y = \sigma(\alpha x)$。通过训练过程中不断调大$\alpha$，就可以控制sigmoid的“硬度”，最终实现$0-1$门的效果。<br><img src="/img/paper-summary-autopruner-arch.png" alt="AutoPruner框图"></p><p>构造损失函数$\mathcal{L} = \mathcal{L}_{\text{cross-entropy}} + \lambda \Vert \frac{\Vert v \Vert_1}{C} - r \Vert_2^2$。其中，$v$是sigmoid输出的mask，$C$为输出的channel数量，$r$为目标稀疏度。</p><p>不过在具体的细节上，作者表示要注意的东西很多。主要是FC层的初始化和几个超参数的处理。作者在论文中提出了相应想法：</p><ul><li>FC层初始化权重为$0$均值，方差为$10\sqrt{\frac{2}{n}}$的高斯分布，其中$n = C\times H \times W$。</li><li>上述$\alpha$的控制，如何增长$\alpha$。作者设计了一套if-else的规则。</li><li>上述损失函数中的比例$\lambda$，作者使用了$\lambda = 100 \vert r_b - r\vert$的自适应调节方法。</li></ul><p><img src="/img/paper-summary-model-compression-autopruner-alg.png" alt="AutoPruner Alg"></p><h2 id="Rethinking-Net-Pruning-匿名"><a href="#Rethinking-Net-Pruning-匿名" class="headerlink" title="Rethinking Net Pruning, 匿名"></a>Rethinking Net Pruning, 匿名</h2><p>这篇文章<a href="https://openreview.net/pdf?id=rJlnB3C5Ym" target="_blank" rel="external">Rethinking the Value of Network Pruning</a>有意思了。严格说来，它还在ICLR 2019的匿名评审阶段，并没有被接收。不过这篇文章的炮口已经瞄准了之前提出的好几个model pruning方法，对它们的结果提出了质疑。上面的链接中，也有被diss的方法之一的作者He Yihui和本文作者的交流。</p><p>之前的剪枝算法大多考虑两个问题：</p><ol><li>怎么求得一个高效的剪枝模型结构，如何确定剪枝方式和剪枝比例：在哪里剪，剪多少</li><li>剪枝模型的参数求取：如何保留原始模型中重要的weight，对进行补偿，使得accuracy等性能指标回复到原始模型</li></ol><p>而本文的作者check了六种SOA的工作，发现：在剪枝算法得到的模型上进行finetune，只比相同结构，但是使用random初始化权重的网络performance好了一点点，甚至有的时候还不如。作者的结论是：</p><ol><li>训练一个over parameter的model对最终得到一个efficient的小模型不是必要的</li><li>为了得到剪枝后的小模型，求取大模型中的important参数其实并不打紧</li><li>剪枝得到的结构，相比求得的weight，更重要。所以不如将剪枝算法看做是网络结构搜索的一种特例。</li></ol><p>作者立了两个论点来打：</p><ol><li>要先训练一个over-parameter的大模型，然后在其基础上剪枝。因为大模型有更强大的表达能力。</li><li>剪枝之后的网络结构和权重都很重要，是剪枝模型finetune的基础。</li></ol><p>作者试图通过实验证明，很多剪枝方法并没有他们声称的那么有效，很多时候，无需剪枝之后的权重，而是直接随机初始化并训练，就能达到这些论文中的剪枝方法的效果。当然，这些论文并不是一无是处。作者提出，是剪枝之后的结构更重要。这些剪枝方法可以看做是网络结构的搜索。</p><p>论文的其他部分就是对几种现有方法的实验和diss。我还没有细看，如果后续这篇论文得到了接收，再做总结吧~夹带一些私货，基于几篇论文的实现经验和在真实数据集上的测试，这篇文章的看法我是同意的。</p><p>更新：这篇文章的作者原来正是Net Sliming的作者Liu Zhuang和Huang Gao，那实验和结论应该是很有保障的。最近这篇文章确实也引起了大家的注意，值得好好看一看。</p><h2 id="其他论文等资源"><a href="#其他论文等资源" class="headerlink" title="其他论文等资源"></a>其他论文等资源</h2><ul><li><a href="https://nervanasystems.github.io/distiller/index.html" target="_blank" rel="external">Distiller</a>：一个使用PyTorch实现的剪枝工具包</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-summary-model-pruning-joke.jpg&quot; alt=&quot;剪枝的学问&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>VIM安装YouCompleteMe和Jedi进行自动补全</title>
    <link href="https://xmfbit.github.io/2018/10/02/vim-you-complete-me/"/>
    <id>https://xmfbit.github.io/2018/10/02/vim-you-complete-me/</id>
    <published>2018-10-02T14:30:54.000Z</published>
    <updated>2020-03-21T02:02:44.774Z</updated>
    
    <content type="html"><![CDATA[<p>这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~</p><p><img src="/img/vim-config-demo.png" alt="我的VIM"></p><a id="more"></a><h2 id="使用Jedi和YouCompleteMe配置Vim"><a href="#使用Jedi和YouCompleteMe配置Vim" class="headerlink" title="使用Jedi和YouCompleteMe配置Vim"></a>使用Jedi和YouCompleteMe配置Vim</h2><p>在远程开发机上调试代码时，我的习惯是大型项目使用sshfs将其镜像到本地，然后使用VSCode打开编辑。VSCode中有终端可以方便的ssh到远端开发机，我将”CTRL+`”配置成了编辑器和终端之间的切换快捷键。加上vim插件，就可以实现不用鼠标，不离开当前编辑环境进行代码编写和调试了。</p><p>然而，如果是想在开发机上写一段小的代码，上述方法就显得太麻烦了。</p><h2 id="编译Vim"><a href="#编译Vim" class="headerlink" title="编译Vim"></a>编译Vim</h2><p>编译Vim，注意我们要设定其安装目录为anaconda下的bin目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">./configure --with-features=huge --enable-multibyte --enable-pythoninterp=yes --with-python-config-dir=/path/to/anaconda/bin/python-config --enable-gui=gtk2 --prefix=/path/to/anaconda</div></pre></td></tr></table></figure><p>编译并安装：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div></pre></td><td class="code"><pre><div class="line">make -j4 VIMRUNTIMEDIR=/path/to/anaconda/share/vim/vim81</div><div class="line">make install</div></pre></td></tr></table></figure></p><p>安装后，可以查看vim的version进行确认。安装没有问题，会提示刚才编译的版本信息。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">vim --version</div></pre></td></tr></table></figure></p><p>使用Vundle管理插件，这个没有什么问题，直接按照README提示即可，见：<a href="https://github.com/VundleVim/Vundle.vim" target="_blank" rel="external">Vundle@Github</a>。</p><p>使用Vundle进行插件管理，只需要以下面的形式指明插件目录或Github仓库名称，进入vim后，在Normal状态，输入<code>:PluginInstall</code>即可。</p><h2 id="Jedi"><a href="#Jedi" class="headerlink" title="Jedi"></a>Jedi</h2><p>首先需要安装jedi的python包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">pip install jedi</div></pre></td></tr></table></figure></p><p>使用Vbudle安装<a href="https://github.com/davidhalter/jedi-vim" target="_blank" rel="external">jedi-vim</a>，并在<code>.vimrc</code>中添加以下内容。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">let g:jedi#force_py_version=2.7</div></pre></td></tr></table></figure></p><h2 id="YouCompleteMe"><a href="#YouCompleteMe" class="headerlink" title="YouCompleteMe"></a>YouCompleteMe</h2><p>使用Vundle安装<a href="https://github.com/Valloric/YouCompleteMe#ubuntu-linux-x64" target="_blank" rel="external">YouCompleteMe</a>。</p><p>之后，进入目录<code>.vim/bundle/YouCompleteMe</code>，执行<code>./install.py</code>。如果需要C++支持，执行<code>./install.py --clang-completer</code>。</p><p>但是，其中遇到了问题，找不到Python.h文件。使用<code>locate Python.h</code>，明确该文件确实存在，且其位于<code>/path/to/anaconda/include/python2.7</code>后，手动修改CMakeLists.txt，指定该文件目录位置即可。</p><p>修改这个：<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/CMakeLists.txt</code><br>和<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/CMakeLists.txt</code>，向其中添加：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line"><span class="built_in">set</span>( CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -I/path/to/anaconda/include/python2.7"</span> )</div></pre></td></tr></table></figure><p>强行指定头文件包含目录。</p><h2 id="括号自动补全"><a href="#括号自动补全" class="headerlink" title="括号自动补全"></a>括号自动补全</h2><p>虽然SO上有人指出可以直接通过设置<code>.vimrc</code>的方法实现，不过还是直接用现成的插件吧。推荐使用<a href="https://github.com/jiangmiao/auto-pairs" target="_blank" rel="external">jiangmiao/auto-pairs</a>。可以按照README的说明进行安装。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/vim-config-demo.png&quot; alt=&quot;我的VIM&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="vim" scheme="https://xmfbit.github.io/tags/vim/"/>
    
      <category term="tools" scheme="https://xmfbit.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>MXNet fit介绍</title>
    <link href="https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/"/>
    <id>https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/</id>
    <published>2018-10-02T14:11:15.000Z</published>
    <updated>2020-03-21T02:02:44.798Z</updated>
    
    <content type="html"><![CDATA[<p>在MXNet中，<code>Module</code>提供了训练模型的方便接口。使用<code>symbol</code>将计算图建好之后，用<code>Module</code>包装一下，就可以通过<code>fit()</code>方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用<code>fit()</code>接口就不太合适。这里把<code>fit()</code>代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。</p><p>其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个<a href="https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks" target="_blank" rel="external">Slide分享</a>，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。</p><p><img src="/img/differences-of-deep-learning-frameworks-22-638.jpg" alt="Differences of Deep Learning Frameworks"></p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先，需要将数据绑定到计算图上，并初始化模型的参数，并初始化求解器。这些是求解模型必不可少的。</p><p>其次，还会建立训练的metric，方便我们掌握训练进程和当前模型在训练任务的表现。</p><p>这些是在为后续迭代进行梯度下降更新做准备。</p><h2 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a>迭代更新</h2><p>使用SGD进行训练的时候，我们需要不停地从数据迭代器中获取包含data和label的batch，并将其feed到网络模型中。进行forward computing后进行bp，获得梯度，并根据具体的优化方法（SGD, SGD with momentum, RMSprop等）进行参数更新。</p><p>这部分可以抽成：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># in an epoch</span></div><div class="line"><span class="keyword">while</span> <span class="keyword">not</span> end_epoch:</div><div class="line">    batch = next(train_iter)</div><div class="line">    m.forward_backward(batch)</div><div class="line">    m.update()</div><div class="line">    <span class="keyword">try</span>:</div><div class="line">        next_batch = next(data_iter)</div><div class="line">        m.prepare(next_batch)</div><div class="line">    <span class="keyword">except</span> StopIteration:</div><div class="line">        end_epoch = <span class="keyword">True</span></div></pre></td></tr></table></figure></p><h2 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h2><p>在训练的时候，观察输出的各种metric是必不可少的。我们对训练过程的把握就是通过metric给出的信息。通常在分类任务中常用到的metric有Accuracy，TopK-Accuracy以及交叉熵损失等，这些已经在MXNet中有了现成的实现。而在<code>fit</code>中，调用了<code>m.update_metric(eval_metric, data_batch.label)</code>实现。这里的<code>eval_metric</code>就是我们指定的metric，而<code>label</code>是batch提供的label。注意，在MXNet中，label一般都是以<code>list</code>的形式给出（对应于多任务学习），也就是说这里的label是<code>list of NDArray</code>。当自己魔改的时候要注意。</p><h2 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h2><p>计算了eval_metric等信息，我们需要将其在屏幕上打印出来。MXNet中可以通过callback实现。另外，保存模型checkpoint这样的功能也是通过callback实现的。一种常用的场景是每过若干个batch，做一次logging，打印当前的metric信息，如交叉熵损失降到多少了，准确率提高到多少了等。MXNet会将以下信息打包成<code>BatchEndParam</code>类型（其实是一个自定义的<code>namedtuple</code>）的变量，包括当前epoch，当前迭代次数，评估的metric。如果你需要更多的信息或者更自由的logging监控，也可以参考代码自己实现。</p><p>我们以常用的<code>Speedometer</code>看一下如何使用这些信息，其功能如下，将训练的速度和metric打印出来。</p><blockquote><p>Logs training speed and evaluation metrics periodically</p></blockquote><p>PS:这里有个隐藏的坑。MXNet中的<code>Speedometer</code>每回调一次，会把<code>metric</code>的内容清除。这在训练的时候当然没问题。但是如果是在validation上跑，就会有问题了。这样最终得到的只是最后一个回调周期那些batch的metric，而不是整个验证集上的。如果在<code>fit</code>方法中传入了<code>eval_batch_end_callback</code>参数就要注意这个问题了。解决办法一是在<code>Speedometer</code>实例初始化时传入<code>auto_reset=False</code>，另一种干脆就不要加这个参数，默认为<code>None</code>好了。同样的问题也发生在调用<code>Module.score()</code>方法来获取模型在验证集上metric的时候。</p><p>可以在<code>Speedometer</code>代码中寻找下面这几行，会更清楚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">if</span> param.eval_metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">    name_value = param.eval_metric.get_name_value()</div><div class="line">    <span class="keyword">if</span> self.auto_reset:</div><div class="line">        param.eval_metric.reset()</div></pre></td></tr></table></figure><h2 id="在验证集上测试"><a href="#在验证集上测试" class="headerlink" title="在验证集上测试"></a>在验证集上测试</h2><p>当在训练集上跑过一个epoch后，如果提供了验证集的迭代器，会在验证集上对模型进行测试。这里，MXNet直接封装了<code>score()</code>方法。在<code>score</code>中，基本流程和<code>fit()</code>相同，只是我们只需要forward computing即可。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><p>用了一段时间的MXNet，给我的最大的感觉是MXNet就像一个写计算图的前端，提供了很方便的python接口生成静态图，以及很多“可插拔”的插件（虽然可能不是很全，更像是一份guide而不是拿来即用的tool），如上文中的metric等，使其更适合做成流程化的基础DL平台，供给更上层方便地配置使用。缺点就是隐藏了比较多的实现细节（当然，你完全可以从代码中自己学习，比如从<code>fit()</code>代码了解神经网络的大致训练流程）。至于MXNet宣扬的诸如速度快，图优化，省计算资源等优点，因为我没有过数据对比，就不说了。</p><p>缺点就是写图的时候有时不太灵活（可能也是我写的看的还比较少），即使是和TensorFlow这种同为静态图的DL框架比。另外，貌似MXNet中很多东西都没有跟上最新的论文等，比如Cosine的learning rate decay就没有。Model Zoo也比较少(gluon可能会好一点，Gluon-CV和Gluon-NLP貌似是在搞一些论文复现的工作)。对开发来讲，很多东西都需要阅读代码才能知道是怎么回事，只是读文档的话容易踩坑。</p><p>说到这里，感觉MXNet的python训练接口（包括module，optimizer，metric等）更像是一份example代码，是在教你怎么去用MXNet，而不像一个灵活地强大的工具箱。当然，很多东西不能得兼，希望MXNet越来越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在MXNet中，&lt;code&gt;Module&lt;/code&gt;提供了训练模型的方便接口。使用&lt;code&gt;symbol&lt;/code&gt;将计算图建好之后，用&lt;code&gt;Module&lt;/code&gt;包装一下，就可以通过&lt;code&gt;fit()&lt;/code&gt;方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用&lt;code&gt;fit()&lt;/code&gt;接口就不太合适。这里把&lt;code&gt;fit()&lt;/code&gt;代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。&lt;/p&gt;
&lt;p&gt;其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个&lt;a href=&quot;https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks&quot;&gt;Slide分享&lt;/a&gt;，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/differences-of-deep-learning-frameworks-22-638.jpg&quot; alt=&quot;Differences of Deep Learning Frameworks&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="mxnet" scheme="https://xmfbit.github.io/tags/mxnet/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Like What You Like - Knowledge Distill via Neuron Selectivity Transfer</title>
    <link href="https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/"/>
    <id>https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/</id>
    <published>2018-10-02T13:32:05.000Z</published>
    <updated>2020-03-21T02:02:44.775Z</updated>
    
    <content type="html"><![CDATA[<p>好长时间没有写博客了，国庆假期把最近看的东西整理一下。<a href="https://arxiv.org/abs/1707.01219" target="_blank" rel="external">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a>这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。</p><p><img src="/img/paper-nst-kt-like-what-you-like.gif" alt="jump if you jump"></p><p>另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：<a href="https://arxiv.org/abs/1707.01213" target="_blank" rel="external">Data-driven sparse structure selection for deep neural networks</a>。</p><p>另另外，其实这两篇文章挂在Arxiv的时间很接近，<a href="https://www.zhihu.com/question/62068158" target="_blank" rel="external">知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？</a>有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>KD的一个痛点在于其只适用于softmax分类问题。这样，对于Detection。Segmentation等一系列问题，没有办法应用。另一个问题在于当分类类别数较少时，KD效果不理想。这个问题比较好理解，假设我们面对一个二分类问题，那么我们并不关心类间的similarity，而是尽可能把两类分开即可。同时，这篇文章的实验部分也验证了这个猜想：当分类问题分类类别数较多时，使用KD能够取得best的结果。</p><p>作者联想到，我们是否可以把CNN中某个中间层输出的feature map利用起来呢？Student输出的feature map要和Teacher的相似，相当于是Student学习到了Teacher提取特征的能力。在CNN中，每个filter都是在和一个feature map上的patch做卷积得到输出，很多个filter都做卷积运算，就得到了feature（map）。另外，当filter和patch有相似的结构时，得到的激活比较大。举个例子，如果filter是Sobel算子，那么当输入image是边缘的时候，得到的响应是最大的。filter学习出来的是输入中的某些模式。当模式匹配上时，激活。这里也可以参考一些对CNN中filter做可视化的研究。</p><p>顺着上面的思路，有人提出了Attention Transfer的方法，可以参见这篇文章：<a href="https://arxiv.org/abs/1612.03928" target="_blank" rel="external">Improving the Performance of Convolutional Neural Networks via Attention Transfer</a>。而在NST这篇文章中，作者引入了新的损失函数，用于衡量Student和Teacher对相同输入的激活Feature map的不同，可以说除了下面要介绍的数学概念以外，没有什么难理解的地方。整个训练的网络结构如下所示：<br><img src="/img/paper-nst-student-and-teacher.png" alt="NST知识蒸馏的整体框图结构"></p><h2 id="Maximum-Mean-Discrepancy"><a href="#Maximum-Mean-Discrepancy" class="headerlink" title="Maximum Mean Discrepancy"></a>Maximum Mean Discrepancy</h2><p>MMD 是用来衡量sampled data之间分布差异的距离量度。如果有两个不同的分布$p$和$q$，以及从两个分布中采样得到的Data set$\mathcal{X}$和$\mathcal{Y}$。那么MMD距离如下：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2</script><p>其中，$\phi$表示某个mapping function。变形之后（内积打开括号），可以得到：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)</script><p>其中，$k$是某个kernel function，$k(x, y) = \phi(x)^{T}\phi(y)$。</p><p>我们可以使用MMD来衡量Student模型和Teacher模型中间输出的激活feature map的相似程度。通过优化这个损失函数，使得S的输出分布接近T。通过引入MMD，将NST loss定义如下，下标$S$表示Student的输出，$T$表示Teacher的输出。第一项$\mathcal{H}$是指由样本类别标签计算的CrossEntropy Loss。第二项即为上述的MMD Loss。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{H}(y, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)</script><p>注意，为了确保后一项有意义，需要保证$F_T$和$F_S$有相同长度。具体来说，对于网络中间输出的feature map，我们将每个channel上的$HW$维的feature vector作为分布$\mathcal{X}$的一个采样。按照作者的设定，我们需要保证S和T对应的feature map在spatial dimension上必须一样大。如果不一样，可以使用插值方法进行扩展。</p><p>为了不受相对幅值大小的影响，需要对feature vector做normalization。</p><p>对于kernal的选择，作者提出了三种可行方案：线性，多项式和高斯核。在后续通过实验对比了它们的性能。</p><h2 id="和其他方法的关联"><a href="#和其他方法的关联" class="headerlink" title="和其他方法的关联"></a>和其他方法的关联</h2><p>如果使用线性核函数，也就是$\phi$是一个identity mapping，那么MMD就成了直接比较两个样本分布质心的距离。这时候，和上文提到的AT方法的一种形式是类似的。（这个我觉得有点强行扯关系。。。）</p><p>如果使用二次多项式核函数，可以得到，$\mathcal{L}_{MMD}(F_T, F_S) = \Vert G_T - G_S\Vert_F^2$。其中，$G \in \mathbb{R}^{HW\times HW}$为Gram矩阵，其中的元素$g_{ij} = (f^i)^Tf^j$。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在CIFAR10，ImageNet等数据集上进行了实验。Student均使用Inception-BN网络，Teacher分别使用了ResNet-1001和ResNet-101。一些具体的参数设置参考论文即可。</p><p>下面是CIFAR10上的结果。可以看到，单一方法下，CIFAR10分类NST效果最好，CIFAR100分类KD最好。组合方法中，KD+NST最好。<br><img src="/img/paper-nst-cifar10-results.png" alt="CIFAR10 结果"></p><p>下面是ImageNet上的结果。KD+NST的组合仍然是效果最好的。<br><img src="/img/paper-nst-imagenet-results.png" alt="ImageNet结果"></p><p>作者还对NST前后，Student和Teacher的输出Feature map做了聚类，发现NST确实能够使得S的输出去接近T的输出分布。如下图所示：<br><img src="/img/paper-nst-visulization-teacher-student-feature-map.png" alt="NST减小了T和S的激活feature map的distance"></p><p>此外，作者还实验了在Detection任务上的表现。在PASCAL VOC2007数据集上基于Faster RCNN方法进行了实验。backbone网络仍然是Inception BN，从<code>4b</code>layer获取feature map，此时stide为16。</p><p><img src="/img/paper-nst-pascal-voc-results.png" alt="PASCAL VOC结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好长时间没有写博客了，国庆假期把最近看的东西整理一下。&lt;a href=&quot;https://arxiv.org/abs/1707.01219&quot;&gt;Like What You Like: Knowledge Distill via Neuron Selectivity Transfer&lt;/a&gt;这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-nst-kt-like-what-you-like.gif&quot; alt=&quot;jump if you jump&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：&lt;a href=&quot;https://arxiv.org/abs/1707.01213&quot;&gt;Data-driven sparse structure selection for deep neural networks&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另另外，其实这两篇文章挂在Arxiv的时间很接近，&lt;a href=&quot;https://www.zhihu.com/question/62068158&quot;&gt;知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？&lt;/a&gt;有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Distilling the Knowledge in a Neural Network</title>
    <link href="https://xmfbit.github.io/2018/06/07/knowledge-distilling/"/>
    <id>https://xmfbit.github.io/2018/06/07/knowledge-distilling/</id>
    <published>2018-06-07T13:56:12.000Z</published>
    <updated>2020-03-21T02:02:44.781Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="external">Distilling the Knowledge in a Neural Network</a>。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这里我将Wang Naiyang在知乎相关问题的<a href="https://www.zhihu.com/question/50519680/answer/136363665" target="_blank" rel="external">回答</a>粘贴如下，将KD方法的motivation讲的很清楚。图森也发了论文对KD进行了改进，下篇笔记总结。</p><blockquote><p>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</p></blockquote><h2 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h2><p>这篇论文很好阅读。论文中实现蒸馏是靠soften softmax prob实现的。在分类任务中，常常使用交叉熵作为损失函数，使用one-hot编码的标注好的类别标签${1,2,\dots,K}$作为target，如下所示：</p><script type="math/tex; mode=display">\mathcal{L} = -\sum_{i=1}^{K}t_i\log p_i</script><p>作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，$2$和$3$就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字$2$时，很有可能它给出的概率是：数字$2$为$0.99$，数字$3$为$10^{-2}$，数字$7$为$10^{-4}$。如何能够利用训练好的Teacher模型给出的这种信息呢？</p><p>可以使用带温度的softmax函数。对于softmax的输入（下文统一称为logit），我们按照下式给出输出：</p><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>其中，当$T = 1$时，就是普通的softmax变换。这里令$T &gt; 1$，就得到了软化的softmax。（这个很好理解，除以一个比$1$大的数，相当于被squash了，线性的sqush被指数放大，差距就不会这么大了）。OK，有了这个东西，我们将Teacher网络和Student的最后充当分类器的那个全连接层的输出都做这个处理。</p><p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。<br>可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失：</p><script type="math/tex; mode=display">\mathcal{L}_{soft}=-\sum_{i=1}^{K}p_i\log q_i</script><p>其中，$p_i$为Teacher模型给出的soft target，$q_i$为Student模型给出的soft output。作者发现，最好的方式是做一个multi task learning，将上面这个损失函数和真正的交叉熵损失加权相加。相应地，我们将其称为hard target。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}</script><p>其中，$\mathcal{L}_{hard}$是分类问题中经典的交叉熵损失。由于做softened softmax计算时，需要除以$T$，导致soft target关联的梯度幅值被缩小了$T^2$倍，所以有必要在$\lambda$中预先考虑到$T^2$这个因子。</p><p>PS:这里有一篇地平线烫叔关于多任务中loss函数设计的回答：<a href="https://www.zhihu.com/question/268105631/answer/335246543" target="_blank" rel="external">神经网络中，设计loss function有哪些技巧? - Alan Huang的回答 - 知乎</a>。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>这里给出一个开源的MXNet的实现:<a href="https://github.com/TuSimple/neuron-selectivity-transfer/blob/master/symbol/transfer.py#L4" target="_blank" rel="external">kd loss by mxnet</a>。MXNet中的<code>SoftmaxOutput</code>不仅能直接支持one-hot编码类型的array作为label输入，甚至label的<code>dtype</code>也可以不是整型！</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd</span><span class="params">(student_hard_logits, teacher_hard_logits, temperature, weight_lambda, prefix)</span>:</span></div><div class="line">    student_soft_logits = student_hard_logits / temperature</div><div class="line">    teacher_soft_logits = teacher_hard_logits / temperature</div><div class="line">    teacher_soft_labels = mx.symbol.SoftmaxActivation(teacher_soft_logits,</div><div class="line">        name=<span class="string">"teacher%s_soft_labels"</span> % prefix)</div><div class="line">    kd_loss = mx.symbol.SoftmaxOutput(data=student_soft_logits, label=teacher_soft_labels,</div><div class="line">                                      grad_scale=weight_lambda, name=<span class="string">"%skd_loss"</span> % prefix)</div><div class="line">    <span class="keyword">return</span> kd_loss</div></pre></td></tr></table></figure><h2 id="matching-logit是特例"><a href="#matching-logit是特例" class="headerlink" title="matching logit是特例"></a>matching logit是特例</h2><p>（这部分没什么用，练习推导了一下交叉熵损失的梯度计算）</p><p>在Hinton之前，有学者提出可以匹配Teacher和Student输出的logit，Hinton指出这是本文方法在一定假设下的近似。为了和论文中的符号相同，下面我们使用$C$表示soft target带来的loss，Teacher和Student第$i$个神经元输出的logit分别为$v_i$和$z_i$，输出的softened softmax分别为$p_i$和$q_i$。那么我们有：</p><script type="math/tex; mode=display">C = -\sum_{j=1}^{C}p_j \log q_j</script><p>而且，</p><script type="math/tex; mode=display">p_i = \frac{\exp(v_i/T)}{\sum_j \exp(v_j/T)}</script><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>让我们暂时忽略$T$（最后我们乘上$\frac{1}{T}$即可），我们有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} = -\sum_{j=1}^{K}p_j\frac{1}{q_j}\frac{\partial q_j}{\partial z_i}</script><p>分情况讨论，当$i = j$时，有：</p><script type="math/tex; mode=display">\frac{\partial q_j}{\partial z_i} = q_i (1-q_i)</script><p>当$i \neq j$时，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial q_j}{\partial z_i} &= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2}  \\&=-q_iq_j\end{aligned}</script><p>这样，我们有：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial C}{\partial z_i} &= - p_i\frac{1}{q_i}q_i(1-q_i) + \sum_{j=1, j\neq i}^{K}p_j\frac{1}{q_j}q_iq_j  \\&= -p_i + p_iq_i + \sum_{j=1, j\neq i}^K p_jq_i \\&= q_i -p_i\end{aligned}</script><p>当然，其实上面的推导过程只不过是重复了一遍one-hot编码的交叉熵损失的计算。</p><p>这样，如果我们假设logit是零均值的，也就是说$\sum_j z_j = \sum_j v_j = 0$，那么有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} \sim \frac{1}{NT^2}(z_i - v_i)</script><p>所以说，MSE下进行logit的匹配，是本文方法的一个特例。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者使用了MNIST进行图片分类的实验，一个有趣的地方在于（和论文前半部分举的$2$和$3$识别的例子呼应），作者在数据集中有意地去除了标签为$3$的样本。没有KD的student网络不能识别测试时候提供的$3$，有KD的student网络能够识别一些$3$（虽然它从来没有在训练样本中出现过！）。后面，作者在语音识别和一个Google内部的很大的图像分类数据集（JFT dataset）上做了实验，</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li>知乎上关于soft target的讨论，有Wang Naiyan和Zhou Bolei的分析：<a href="https://www.zhihu.com/question/50519680" target="_blank" rel="external">如何理解soft target这一做法？</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>（译）PyTorch 0.4.0 Migration Guide</title>
    <link href="https://xmfbit.github.io/2018/04/27/pytorch-040-migration-guide/"/>
    <id>https://xmfbit.github.io/2018/04/27/pytorch-040-migration-guide/</id>
    <published>2018-04-27T02:49:31.000Z</published>
    <updated>2020-03-21T02:02:44.796Z</updated>
    
    <content type="html"><![CDATA[<p>PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个<a href="http://pytorch.org/2018/04/22/0_4_0-migration-guide.html" target="_blank" rel="external">转换指导</a>，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有：</p><ul><li><code>Tensor</code>和<code>Variable</code>合并，<code>autograd</code>的机制有所不同，变得更简单，使用<code>requires_grad</code>和上下文相关环境管理。</li><li>Numpy风格的<code>Tensor</code>构建。</li><li>提出了<code>device</code>，更简单地在cpu和gpu中移动数据。</li></ul><a id="more"></a><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><p>在0.4.0版本中，PyTorch引入了许多令人兴奋的新特性和bug fixes。为了方便以前版本的使用者转换到新的版本，我们编写了此指导，主要包括以下几个重要的方面：</p><ul><li><code>Tensors</code> 和 <code>Variables</code> 已经merge到一起了</li><li>支持0维的Tensor（即标量scalar）</li><li>弃用了 <code>volatile</code> 标志</li><li><code>dtypes</code>, <code>devices</code>, 和 Numpy 风格的 Tensor构造函数</li><li>（更好地编写）设备无关代码</li></ul><p>下面分条介绍。</p><h2 id="Tensor-和-Variable-合并"><a href="#Tensor-和-Variable-合并" class="headerlink" title="Tensor 和 Variable 合并"></a><code>Tensor</code> 和 <code>Variable</code> 合并</h2><p>在PyTorch以前的版本中，<code>Tensor</code>类似于<code>numpy</code>中的<code>ndarray</code>，只是对多维数组的抽象。为了能够使用自动求导机制，必须使用<code>Variable</code>对其进行包装。而现在，这两个东西已经完全合并成一个了，以前<code>Variable</code>的使用情境都可以使用<code>Tensor</code>。所以以前训练的时候总要额外写的warpping语句用不到了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> data, target <span class="keyword">in</span> data_loader:</div><div class="line">    <span class="comment">## 用不到了</span></div><div class="line">    data, target = Variable(data), Variable(target)</div><div class="line">    loss = criterion(model(data), target)</div></pre></td></tr></table></figure><h3 id="Tensor的类型type"><a href="#Tensor的类型type" class="headerlink" title="Tensor的类型type()"></a><code>Tensor</code>的类型<code>type()</code></h3><p>以前我们可以使用<code>type()</code>获取<code>Tensor</code>的data type（FloatTensor，LongTensor等）。现在需要使用<code>x.type()</code>获取类型或<code>isinstance()</code>判别类型。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.DoubleTensor([<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(type(x))  <span class="comment"># 曾经会给出 torch.DoubleTensor</span></div><div class="line"><span class="string">"&lt;class 'torch.Tensor'&gt;"</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(x.type())  <span class="comment"># OK: 'torch.DoubleTensor'</span></div><div class="line"><span class="string">'torch.DoubleTensor'</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>print(isinstance(x, torch.DoubleTensor))  <span class="comment"># OK: True</span></div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="autograd现在如何追踪计算图的历史"><a href="#autograd现在如何追踪计算图的历史" class="headerlink" title="autograd现在如何追踪计算图的历史"></a><code>autograd</code>现在如何追踪计算图的历史</h3><p><code>Tensor</code>和<code>Variable</code>的合并，简化了计算图的构建，具体规则见本条和以下几条说明。</p><p><code>requires_grad</code>, 这个<code>autograd</code>中的核心标志量,现在成了<code>Tensor</code>的属性。之前的<code>Variable</code>使用规则可以同样应用于<code>Tensor</code>，<code>autograd</code>自动跟踪那些至少有一个input的<code>requires_grad==True</code>的计算节点构成的图。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.ones(<span class="number">1</span>)  <span class="comment">## 默认requires_grad = False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = torch.ones(<span class="number">1</span>)  <span class="comment">## 同样，y的requires_grad标志也是False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z = x + y</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所有的输入节点都不要求梯度，所以z的requires_grad也是False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 所以如果试图对z做梯度反传，会抛出Error</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.backward()</div><div class="line">RuntimeError: element <span class="number">0</span> of tensors does <span class="keyword">not</span> require grad <span class="keyword">and</span> does <span class="keyword">not</span> have a grad_fn</div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 通过手动指定的方式创建 requires_grad=True 的Tensor</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w = torch.ones(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 把它和之前requires_grad=False的节点相加，得到输出</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total = w + z</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 由于w需要梯度，所以total也需要</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 可以做bp</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>total.backward()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>w.grad</div><div class="line">tensor([ <span class="number">1.</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="comment">## 不用有时间浪费在求取 x y z的梯度上，因为它们没有 require grad，它们的grad == None</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>z.grad == x.grad == y.grad == <span class="keyword">None</span></div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="操作-requires-grad-标志"><a href="#操作-requires-grad-标志" class="headerlink" title="操作 requires_grad 标志"></a>操作 <code>requires_grad</code> 标志</h3><p>除了直接设置这个属性，你可以使用<code>my_tensor.requires_grad_()</code>就地修改这个标志（还记得吗，以<code>_</code>结尾的方法名表示in-place的操作）。或者就在构造的时候传入此参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad_()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>existing_tensor.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor = torch.zeros(<span class="number">3</span>, <span class="number">4</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>my_tensor.requires_grad</div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="data怎么办？What-about-data"><a href="#data怎么办？What-about-data" class="headerlink" title=".data怎么办？What about .data?"></a><code>.data</code>怎么办？What about .data?</h3><p>原来版本中，对于某个<code>Variable</code>，我们可以通过<code>x.data</code>的方式获取其包装的<code>Tensor</code>。现在两者已经merge到了一起，如果你调用<code>y = x.data</code>仍然和以前相似，<code>y</code>现在会共享<code>x</code>的data，并与<code>x</code>的计算历史无关，且其<code>requires_grad</code>标志为<code>False</code>。</p><p>然而，<code>.data</code>有的时候可能会成为代码中不安全的一个点。对<code>x.data</code>的任何带动都不会被<code>aotograd</code>跟踪。所以，当做反传的时候，计算的梯度可能会不对，一种更安全的替代方法是调用<code>x.detach()</code>，仍然会返回一个共享<code>x</code>data的Tensor，且<code>requires_grad=False</code>，但是当<code>x</code>需要bp的时候，会报告那些in-place的操作。</p><blockquote><p>However, .data can be unsafe in some cases. Any changes on x.data wouldn’t be tracked by autograd, and the computed gradients would be incorrect if x is needed in a backward pass. A safer alternative is to use x.detach(), which also returns a Tensor that shares data with requires_grad=False, but will have its in-place changes reported by autograd if x is needed in backward.</p></blockquote><p>这里有些绕，可以看下下面的示例代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 一个简单的计算图：y = sum(x**2)</span></div><div class="line">x = torch.ones((<span class="number">1</span> ,<span class="number">2</span>))</div><div class="line">x.requires_grad_()</div><div class="line">y = torch.sum(x**<span class="number">2</span>)</div><div class="line">y.backward()</div><div class="line">x.grad   <span class="comment"># grad: [2, 2, 2]</span></div><div class="line"><span class="comment"># 使用.data，在计算完y之后，又改动了x，会造成梯度计算错误</span></div><div class="line">x.grad.zero_()</div><div class="line">y = torch.sum(x**<span class="number">2</span>)</div><div class="line">data = x.data</div><div class="line">data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">2</span></div><div class="line">y.backward()</div><div class="line">x.grad   <span class="comment"># grad: [4, 2, 2] 错了哦~</span></div><div class="line"><span class="comment"># 使用detach，同样的操作，会抛出异常</span></div><div class="line">x.grad.zero_()</div><div class="line">y = torch.sum(x**<span class="number">2</span>)</div><div class="line">data = x.detach()</div><div class="line">data[<span class="number">0</span>, <span class="number">0</span>] = <span class="number">2</span></div><div class="line">y.backward()</div><div class="line"><span class="comment"># 抛出如下异常</span></div><div class="line"><span class="comment"># RuntimeError: one of the variables needed for gradient </span></div><div class="line"><span class="comment"># computation has been modified by an inplace operation</span></div></pre></td></tr></table></figure><h2 id="支持0维-scalar-的Tensor"><a href="#支持0维-scalar-的Tensor" class="headerlink" title="支持0维(scalar)的Tensor"></a>支持0维(scalar)的Tensor</h2><p>原来的版本中，对Tensor vector（1D Tensor）做索引得到的结果是一个python number，但是对一个Variable vector来说，得到的就是一个<code>size(1,)</code>的vector!对于reduction function（如<code>torch.sum</code>，<code>torch.max</code>）也有这样的问题。</p><p>所以我们引入了scalar（0D Tensor）。它可以使用<code>torch.tensor()</code> 函数来创建，现在你可以这样做：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>)         <span class="comment"># 直接创建scalar</span></div><div class="line">tensor(<span class="number">3.1416</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">3.1416</span>).size()  <span class="comment"># scalar 是 0D</span></div><div class="line">torch.Size([])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">3</span>]).size()     <span class="comment"># 和1D对比</span></div><div class="line">torch.Size([<span class="number">1</span>])</div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector = torch.arange(<span class="number">2</span>, <span class="number">6</span>)  <span class="comment"># 1D的vector</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector</div><div class="line">tensor([ <span class="number">2.</span>,  <span class="number">3.</span>,  <span class="number">4.</span>,  <span class="number">5.</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector.size()</div><div class="line">torch.Size([<span class="number">4</span>])</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>]                    <span class="comment"># 对1D的vector做indexing，得到的是scalar</span></div><div class="line">tensor(<span class="number">5.</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>vector[<span class="number">3</span>].item()             <span class="comment"># 使用.item()获取python number</span></div><div class="line"><span class="number">5.0</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum = torch.tensor([<span class="number">2</span>, <span class="number">3</span>]).sum()</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum</div><div class="line">tensor(<span class="number">5</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>mysum.size()</div><div class="line">torch.Size([])</div></pre></td></tr></table></figure><h3 id="累积losses"><a href="#累积losses" class="headerlink" title="累积losses"></a>累积losses</h3><p>我们在训练的时候，经常有这样的用法：<code>total_loss += loss.data[0]</code>。<code>loss</code>通常都是由损失函数计算出来的一个标量，也就是包装了<code>(1,)</code>大小<code>Tensor</code>的<code>Variable</code>。在新的版本中，<code>loss</code>则变成了0D的scalar。对一个scalar做indexing是没有意义的，应该使用<code>loss.item()</code>获取python number。</p><p>注意，如果你在做累加的时候没有转换为python number，你的程序可能会出现不必要的内存占用。因为<code>autograd</code>会记录调用过程，以便做反向传播。所以，你现在应该写成 <code>total_loss += loss.item()</code>。</p><h2 id="弃用volatile标志"><a href="#弃用volatile标志" class="headerlink" title="弃用volatile标志"></a>弃用<code>volatile</code>标志</h2><p><code>volatile</code> 标志被弃用了，现在没有任何效果。以前的版本中，一个设置<code>volatile=True</code>的<code>Variable</code> 表明其不会被<code>autograd</code>追踪。现在，被替换成了一个更灵活的上下文管理器，如<code>torch.no_grad()</code>，<code>torch.set_grad_enable(grad_mode)</code>等。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">1</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.no_grad():    <span class="comment"># 使用 torch,no_grad()构建不需要track的上下文环境</span></div><div class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line">&gt;&gt;&gt;</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>is_train = <span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span><span class="keyword">with</span> torch.set_grad_enabled(is_train):   <span class="comment"># 在inference的时候，设置不要track</span></div><div class="line"><span class="meta">... </span>    y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">True</span>)  <span class="comment"># 当然也可以不用with构建上下文环境，而单独这样用</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">True</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.set_grad_enabled(<span class="keyword">False</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y = x * <span class="number">2</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>y.requires_grad</div><div class="line"><span class="keyword">False</span></div></pre></td></tr></table></figure><h2 id="dtypes-devices-和NumPy风格的构建函数"><a href="#dtypes-devices-和NumPy风格的构建函数" class="headerlink" title="dtypes, devices 和NumPy风格的构建函数"></a><code>dtypes</code>, <code>devices</code> 和NumPy风格的构建函数</h2><p>以前的版本中，我们需要以”tensor type”的形式给出对data type（如<code>float</code>或<code>double</code>），device type（如cpu或gpu）以及layout（dense或sparse）的限定。例如，<code>torch.cuda.sparse.DoubleTensor</code>用来构造一个data type是<code>double</code>，在GPU上以及sparse的tensor。</p><p>现在我们引入了<code>torch.dtype</code>，<code>torch.device</code>和<code>torch.layout</code>来更好地使用Numpy风格的构建函数。</p><h3 id="torch-dtype"><a href="#torch-dtype" class="headerlink" title="torch.dtype"></a><code>torch.dtype</code></h3><p>下面是可用的 <code>torch.dtypes</code> (data types) 和它们对应的tensor types。可以使用<code>x.dtype</code>获取。</p><table>   <tr>      <td>data type</td>      <td>torch.dtype</td>      <td>Tensor types</td>   </tr>   <tr>      <td>32-bit floating point</td>      <td>torch.float32 or torch.float</td>      <td>torch.*.FloatTensor</td>   </tr>   <tr>      <td>64-bit floating point</td>      <td>torch.float64 or torch.double</td>      <td>torch.*.DoubleTensor</td>   </tr>   <tr>      <td>16-bit floating point</td>      <td>torch.float16 or torch.half</td>      <td>torch.*.HalfTensor</td>   </tr>   <tr>      <td>8-bit integer (unsigned)</td>      <td>torch.uint8</td>      <td>torch.*.ByteTensor</td>   </tr>   <tr>      <td>8-bit integer (signed)</td>      <td>torch.int8</td>      <td>torch.*.CharTensor</td>   </tr>   <tr>      <td>16-bit integer (signed)</td>      <td>torch.int16 or torch.short</td>      <td>torch.*.ShortTensor</td>   </tr>   <tr>      <td>32-bit integer (signed)</td>      <td>torch.int32 or torch.int</td>      <td>torch.*.IntTensor</td>   </tr>   <tr>      <td>64-bit integer (signed)</td>      <td>torch.int64 or torch.long</td>      <td>torch.*.LongTensor</td>   </tr></table><h3 id="torch-device"><a href="#torch-device" class="headerlink" title="torch.device"></a><code>torch.device</code></h3><p><code>torch.device</code>包含了device type（如cpu或cuda）和可能的设备id。使用<code>torch.device(&#39;{device_type}&#39;)</code>或<code>torch.device(&#39;{device_type}:{device_ordinal}&#39;)</code>的方式来初始化。 </p><p>如果没有指定<code>device ordinal</code>，那么默认是当前的device。例如，<code>torch.device(&#39;cuda&#39;)</code>相当于<code>torch.device(&#39;cuda:X&#39;)</code>，其中，<code>X</code>是<code>torch.cuda.current_device()</code>的返回结果。</p><p>使用<code>x.device</code>来获取。</p><h3 id="torch-layout"><a href="#torch-layout" class="headerlink" title="torch.layout"></a><code>torch.layout</code></h3><p><code>torch.layout</code>代表了<code>Tensor</code>的data layout。 目前支持的是<code>torch.strided</code> (dense，也是默认的) 和 <code>torch.sparse_coo</code> (COOG格式的稀疏tensor)。</p><p>使用<code>x.layout</code>来获取。</p><h3 id="创建Tensor（Numpy风格）"><a href="#创建Tensor（Numpy风格）" class="headerlink" title="创建Tensor（Numpy风格）"></a>创建<code>Tensor</code>（Numpy风格）</h3><p>你可以使用<code>dtype</code>，<code>device</code>，<code>layout</code>和<code>requires_grad</code>更好地控制<code>Tensor</code>的创建。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>device = torch.device(<span class="string">"cuda:1"</span>) </div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, <span class="number">3</span>, dtype=torch.float64, device=device)</div><div class="line">tensor([[<span class="number">-0.6344</span>,  <span class="number">0.8562</span>, <span class="number">-1.2758</span>],</div><div class="line">        [ <span class="number">0.8414</span>,  <span class="number">1.7962</span>,  <span class="number">1.0589</span>],</div><div class="line">        [<span class="number">-0.1369</span>, <span class="number">-1.0462</span>, <span class="number">-0.4373</span>]], dtype=torch.float64, device=<span class="string">'cuda:1'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad  <span class="comment"># default is False</span></div><div class="line"><span class="keyword">False</span></div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.zeros(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.requires_grad</div><div class="line"><span class="keyword">True</span></div></pre></td></tr></table></figure><h3 id="torch-tensor-data"><a href="#torch-tensor-data" class="headerlink" title="torch.tensor(data, ...)"></a><code>torch.tensor(data, ...)</code></h3><p><code>torch.tensor</code>是新加入的<code>Tesnor</code>构建函数。它接受一个”array-like”的参数，并将其value copy到一个新的<code>Tensor</code>中。可以将它看做<code>numpy.array</code>的等价物。不同于<code>torch.*Tensor</code>方法，你可以创建0D的Tensor（也就是scalar）。此外，如果<code>dtype</code>参数没有给出，它会自动推断。推荐使用这个函数从已有的data，如Python List创建<code>Tensor</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>cuda = torch.device(<span class="string">"cuda"</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([[<span class="number">1</span>], [<span class="number">2</span>], [<span class="number">3</span>]], dtype=torch.half, device=cuda)</div><div class="line">tensor([[ <span class="number">1</span>],</div><div class="line">        [ <span class="number">2</span>],</div><div class="line">        [ <span class="number">3</span>]], device=<span class="string">'cuda:0'</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor(<span class="number">1</span>)               <span class="comment"># scalar</span></div><div class="line">tensor(<span class="number">1</span>)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2.3</span>]).dtype  <span class="comment"># type inferece</span></div><div class="line">torch.float32</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.tensor([<span class="number">1</span>, <span class="number">2</span>]).dtype    <span class="comment"># type inferece</span></div><div class="line">torch.int64</div></pre></td></tr></table></figure><p>我们还加了更多的<code>Tensor</code>创建方法。其中有一些<code>torch.*_like</code>，<code>tensor.new_*</code>这样的形式。</p><ul><li><p><code>torch.*_like</code>的参数是一个input tensor， 它返回一个相同属性的tensor，除非有特殊指定。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x)</div><div class="line">tensor([ <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>], dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>torch.zeros_like(x, dtype=torch.int)</div><div class="line">tensor([ <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">0</span>], dtype=torch.int32)</div></pre></td></tr></table></figure></li><li><p><code>tensor.new_*</code>类似，不过它通常需要接受一个指定shape的参数。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="meta">&gt;&gt;&gt; </span>x = torch.randn(<span class="number">3</span>, dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">2</span>)</div><div class="line">tensor([ <span class="number">1.</span>,  <span class="number">1.</span>], dtype=torch.float64)</div><div class="line"><span class="meta">&gt;&gt;&gt; </span>x.new_ones(<span class="number">4</span>, dtype=torch.int)</div><div class="line">tensor([ <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>,  <span class="number">1</span>], dtype=torch.int32)</div></pre></td></tr></table></figure></li></ul><p>为了指定shape参数，你可以使用<code>tuple</code>，如<code>torch.zeros((2, 3))</code>（Numpy风格）或者可变数量参数<code>torch.zeros(2, 3)</code>（以前的版本只支持这种）。</p><table>   <tr>      <td>Name</td>      <td>Returned Tensor</td>      <td>torch.*_likevariant</td>      <td>tensor.new_*variant</td>   </tr>   <tr>      <td>torch.empty</td>      <td>unintialized memory</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.zeros</td>      <td>all zeros</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.ones</td>      <td>all ones</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.full</td>      <td>filled with a given value</td>      <td>✔</td>      <td>✔</td>   </tr>   <tr>      <td>torch.rand</td>      <td>i.i.d. continuous Uniform[0, 1)</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randn</td>      <td>i.i.d. Normal(0, 1)</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randint</td>      <td>i.i.d. discrete Uniform in given range</td>      <td>✔</td>      <td></td>   </tr>   <tr>      <td>torch.randperm</td>      <td>random permutation of {0, 1, ..., n - 1}</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.tensor</td>      <td>copied from existing data (list, NumPy ndarray, etc.)</td>      <td></td>      <td>✔</td>   </tr>   <tr>      <td>torch.from_numpy*</td>      <td>from NumPy ndarray (sharing storage without copying)</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.arange, torch.range and torch.linspace</td>      <td>uniformly spaced values in a given range</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.logspace</td>      <td>logarithmically spaced values in a given range</td>      <td></td>      <td></td>   </tr>   <tr>      <td>torch.eye</td>      <td>identity matrix</td>      <td></td>      <td></td>   </tr></table><p>注：<code>torch.from_numpy</code>只接受NumPy <code>ndarray</code>作为输入参数。</p><h2 id="书写设备无关代码（device-agnostic-code）"><a href="#书写设备无关代码（device-agnostic-code）" class="headerlink" title="书写设备无关代码（device-agnostic code）"></a>书写设备无关代码（device-agnostic code）</h2><p>以前版本很难写设备无关代码。我们使用两种方法使其变得简单：</p><ul><li><code>Tensor</code>的<code>device</code>属性可以给出其<code>torch.device</code>（<code>get_device</code>只能获取CUDA tensor）</li><li>使用<code>x.to()</code>方法，可以很容易将<code>Tensor</code>或者<code>Module</code>在devices间移动（而不用调用<code>x.cpu()</code>或者<code>x.cuda()</code>。</li></ul><p>推荐使用下面的模式：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 在脚本开始的地方，指定device</span></div><div class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</div><div class="line"></div><div class="line"><span class="comment">## 一些代码</span></div><div class="line"></div><div class="line"><span class="comment"># 当你想创建新的Tensor或者Module时候，使用下面的方法</span></div><div class="line"><span class="comment"># 如果已经在相应的device上了，将不会发生copy</span></div><div class="line">input = data.to(device)</div><div class="line">model = MyModule(...).to(device)</div></pre></td></tr></table></figure><h2 id="在nn-Module中对于submodule，parameter和buffer名字新的约束"><a href="#在nn-Module中对于submodule，parameter和buffer名字新的约束" class="headerlink" title="在nn.Module中对于submodule，parameter和buffer名字新的约束"></a>在<code>nn.Module</code>中对于submodule，parameter和buffer名字新的约束</h2><p>当使用<code>module.add_module(name, value)</code>, <code>module.add_parameter(name, value)</code> 或者 <code>module.add_buffer(name, value)</code>时候不要使用空字符串或者包含<code>.</code>的字符串，可能会导致<code>state_dict</code>中的数据丢失。如果你在load这样的<code>state_dict</code>，注意打补丁，并且应该更新代码，规避这个问题。</p><h2 id="一个具体的例子"><a href="#一个具体的例子" class="headerlink" title="一个具体的例子"></a>一个具体的例子</h2><p>下面是一个code snippet，展示了从0.3.1跨越到0.4.0的不同。</p><h3 id="0-3-1-version"><a href="#0-3-1-version" class="headerlink" title="0.3.1 version"></a>0.3.1 version</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div></pre></td><td class="code"><pre><div class="line">model = MyRNN()</div><div class="line"><span class="keyword">if</span> use_cuda:</div><div class="line">    model = model.cuda()</div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">total_loss = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</div><div class="line">    input, target = Variable(input), Variable(target)</div><div class="line">    hidden = Variable(torch.zeros(*h_shape))  <span class="comment"># init hidden</span></div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        input, target, hidden = input.cuda(), target.cuda(), hidden.cuda()</div><div class="line">    ...  <span class="comment"># get loss and optimize</span></div><div class="line">    total_loss += loss.data[<span class="number">0</span>]</div><div class="line"></div><div class="line"><span class="comment"># evaluate</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</div><div class="line">    input = Variable(input, volatile=<span class="keyword">True</span>)</div><div class="line">    <span class="keyword">if</span> use_cuda:</div><div class="line">        ...</div><div class="line">    ...</div></pre></td></tr></table></figure><h3 id="0-4-0-version"><a href="#0-4-0-version" class="headerlink" title="0.4.0 version"></a>0.4.0 version</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># torch.device object used throughout this script</span></div><div class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> use_cuda <span class="keyword">else</span> <span class="string">"cpu"</span>)</div><div class="line"></div><div class="line">model = MyRNN().to(device)</div><div class="line"></div><div class="line"><span class="comment"># train</span></div><div class="line">total_loss = <span class="number">0</span></div><div class="line"><span class="keyword">for</span> input, target <span class="keyword">in</span> train_loader:</div><div class="line">    input, target = input.to(device), target.to(device)</div><div class="line">    hidden = input.new_zeros(*h_shape)  <span class="comment"># has the same device &amp; dtype as `input`</span></div><div class="line">    ...  <span class="comment"># get loss and optimize</span></div><div class="line">    total_loss += loss.item()           <span class="comment"># get Python number from 1-element Tensor</span></div><div class="line"></div><div class="line"><span class="comment"># evaluate</span></div><div class="line"><span class="keyword">with</span> torch.no_grad():                   <span class="comment"># operations inside don't track history</span></div><div class="line">    <span class="keyword">for</span> input, target <span class="keyword">in</span> test_loader:</div><div class="line">        ...</div></pre></td></tr></table></figure><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li><a href="https://github.com/pytorch/pytorch/releases/tag/v0.4.0" target="_blank" rel="external">Release Note</a></li><li><a href="http://pytorch.org/docs/stable/index.html" target="_blank" rel="external">Documentation</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;PyTorch在前两天官方发布了0.4.0版本。这个版本与之前相比，API发生了较大的变化，所以官方也出了一个&lt;a href=&quot;http://pytorch.org/2018/04/22/0_4_0-migration-guide.html&quot;&gt;转换指导&lt;/a&gt;，这篇博客是这篇指导的中文翻译版。归结起来，对我们代码影响最大的地方主要有：&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;Tensor&lt;/code&gt;和&lt;code&gt;Variable&lt;/code&gt;合并，&lt;code&gt;autograd&lt;/code&gt;的机制有所不同，变得更简单，使用&lt;code&gt;requires_grad&lt;/code&gt;和上下文相关环境管理。&lt;/li&gt;
&lt;li&gt;Numpy风格的&lt;code&gt;Tensor&lt;/code&gt;构建。&lt;/li&gt;
&lt;li&gt;提出了&lt;code&gt;device&lt;/code&gt;，更简单地在cpu和gpu中移动数据。&lt;/li&gt;
&lt;/ul&gt;
    
    </summary>
    
    
      <category term="pytorch" scheme="https://xmfbit.github.io/tags/pytorch/"/>
    
  </entry>
  
  <entry>
    <title>JupyterNotebook设置Python环境</title>
    <link href="https://xmfbit.github.io/2018/04/09/set-env-in-jupyternotebook/"/>
    <id>https://xmfbit.github.io/2018/04/09/set-env-in-jupyternotebook/</id>
    <published>2018-04-09T05:44:04.000Z</published>
    <updated>2020-03-21T02:02:44.788Z</updated>
    
    <content type="html"><![CDATA[<p>使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。<br><a id="more"></a></p><h2 id="conda的虚拟环境"><a href="#conda的虚拟环境" class="headerlink" title="conda的虚拟环境"></a>conda的虚拟环境</h2><p>在Anaconda中，我们可以使用<code>conda create -n your_env_name python=your_python_version</code>的方法创建虚拟环境，并使用<code>source activate your_env_name</code>方式激活该虚拟环境，并在其中安装与默认（主）python环境不同的软件包等。</p><p>当激活该虚拟环境时，ipython下是可以正常加载的。但是打开Jupyter Notebook，会发现其加载的仍然是默认的Python kernel，而我们需要在notebook中也能使用新添加的虚拟环境。</p><h2 id="解决方法"><a href="#解决方法" class="headerlink" title="解决方法"></a>解决方法</h2><p>解决方法见这个帖子：<a href="https://stackoverflow.com/questions/39604271/conda-environments-not-showing-up-in-jupyter-notebook" target="_blank" rel="external">Conda environments not showing up in Jupyter Notebook</a>.</p><p>首先，安装<code>nb_conda_kernels</code>包：<br><figure class="highlight bash"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">conda install nb_conda_kernels</div></pre></td></tr></table></figure></p><p>然后，打开Notebook，点击<code>New</code>，会出现当前所有安装的虚拟环境以供选择，如下所示。<br><img src="/img/set-env-in-notebook-choose-kernel.png" alt="选择特定的kernel加载"></p><p>如果是已经编辑过的notebook，只需要打开该笔记本，在菜单栏中选择<code>Kernel -&gt; choose kernel -&gt; your env kernel</code>即可。<br><img src="/img/set-env-in-notebook-change-kernel.png" alt="改变当前notebook的kernel"></p><p>关于<code>nb_conda_kernels</code>的详细信息，可以参考其GitHub页面：<a href="https://github.com/Anaconda-Platform/nb_conda_kernels" target="_blank" rel="external">nb_conda_kernels</a>。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;使用Python时，常遇到的一个问题就是Python和库的版本不同。Anaconda的env算是解决这个问题的一个好用的方法。但是，在使用Jupyter Notebook的时候，我却发现加载的仍然是默认的Python Kernel。这篇博客记录了如何在Jupyter Notebook中也能够设置相应的虚拟环境。&lt;br&gt;
    
    </summary>
    
    
      <category term="python" scheme="https://xmfbit.github.io/tags/python/"/>
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>数值优化之牛顿方法</title>
    <link href="https://xmfbit.github.io/2018/04/03/newton-method/"/>
    <id>https://xmfbit.github.io/2018/04/03/newton-method/</id>
    <published>2018-04-03T13:43:16.000Z</published>
    <updated>2020-03-21T02:02:44.783Z</updated>
    
    <content type="html"><![CDATA[<p>简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于<a href="https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95" target="_blank" rel="external">Wiki页面</a>。<br><img src="/img/newton-method-demo.gif" width="400" height="300" alt="牛顿法动图" align="center"><br><a id="more"></a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>牛顿方法，是一种用来求解方程$f(x) = 0$的根的方法。从题图可以看出它是如何使用的。</p><p>首先，需要给定根的初始值$x_0$。接下来，在函数曲线上找到其所对应的点$(x_0, f(x_0))$，并过该点做切线交$x$轴于一点$x_1$。从$x_1$出发，重复上述操作，直至收敛。</p><p>根据图上的几何关系和导数的几何意义，有：</p><script type="math/tex; mode=display">x_{n+1} = x_n - \frac{f(x_n)}{f^\prime(x_n)}</script><h2 id="优化上的应用"><a href="#优化上的应用" class="headerlink" title="优化上的应用"></a>优化上的应用</h2><p>做优化的时候，我们常常需要的是求解某个损失函数$L$的极值。在极值点处，函数的导数为$0$。所以这个问题被转换为了求解$L$的导数的零点。我们有</p><script type="math/tex; mode=display">\theta_{n+1} = \theta_n - \frac{L^\prime(\theta_n)}{L^{\prime\prime}(\theta_n)}</script><h2 id="推广到向量形式"><a href="#推广到向量形式" class="headerlink" title="推广到向量形式"></a>推广到向量形式</h2><p>机器学习中的优化问题常常是在高维空间进行，可以将其推广到向量形式：</p><script type="math/tex; mode=display">\theta_{n+1} = \theta_n - H^{-1}\nabla_\theta L(\theta_n)</script><p>其中，$H$表示海森矩阵，是一个$n\times n$的矩阵，其中元素为：</p><script type="math/tex; mode=display">H_{ij} = \frac{\partial^2 L}{\partial \theta_i \partial \theta_j}</script><p>特别地，当海森矩阵为正定时，此时的极值为极小值（可以使用二阶的泰勒展开式证明）。</p><p>PS:忘了什么是正定矩阵了吗？想想二次型的概念，对于$\forall x$不为$0$向量，都有$x^THx &gt; 0$。</p><h2 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h2><p>牛顿方法的收敛速度较SGD为快（二阶收敛），但是会涉及到求解一个$n\times n$的海森矩阵的逆，所以虽然需要的迭代次数更少，但反而可能比较耗时（$n$的大小）。</p><h2 id="L-BFGS"><a href="#L-BFGS" class="headerlink" title="L-BFGS"></a>L-BFGS</h2><p>由于牛顿方法中需要计算海森矩阵的逆，所以很多时候并不实用。大家就想出了一些近似计算$H^{-1}$的方法，如L-BFGS等。</p><p><em>推导过程待续。。。</em></p><p>L-BFGS的资料网上还是比较多的，这里有一个PyTorch中L-BFGS方法的实现：<a href="https://github.com/pytorch/pytorch/blob/master/torch/optim/lbfgs.py" target="_blank" rel="external">optim.lbfgs</a>。</p><p>这里有一篇不错的文章<a href="http://www.hankcs.com/ml/l-bfgs.html" target="_blank" rel="external">数值优化：理解L-BFGS算法</a>，本博客写作过程参考很多。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;简要介绍一下优化方法中的牛顿方法（Newton’s Method）。下面的动图demo来源于&lt;a href=&quot;https://zh.wikipedia.org/wiki/%E7%89%9B%E9%A1%BF%E6%B3%95&quot;&gt;Wiki页面&lt;/a&gt;。&lt;br&gt;&lt;img src=&quot;/img/newton-method-demo.gif&quot; width = &quot;400&quot; height = &quot;300&quot; alt=&quot;牛顿法动图&quot; align=center /&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="math" scheme="https://xmfbit.github.io/tags/math/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Feature Pyramid Networks for Object Detection (FPN)</title>
    <link href="https://xmfbit.github.io/2018/04/02/paper-fpn/"/>
    <id>https://xmfbit.github.io/2018/04/02/paper-fpn/</id>
    <published>2018-04-02T02:12:03.000Z</published>
    <updated>2020-03-21T02:02:44.783Z</updated>
    
    <content type="html"><![CDATA[<p>图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取<a href="https://xmfbit.github.io/2017/01/30/cs131-sift/">SIFT特征</a>就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。<br><a id="more"></a></p><h2 id="Pyramid-or-not-It’s-a-question"><a href="#Pyramid-or-not-It’s-a-question" class="headerlink" title="Pyramid or not? It’s a question."></a>Pyramid or not? It’s a question.</h2><p>在DL席卷CV之前，特征大多需要研究人员手工设计，如SIFT/Harr/HoG等。人们在使用这些特征的时候发现，往往需要使用图像金字塔，在multi scale下进行检测，才能得到不错的结果。然而，使用CNN时，由于其本身具有的一定的尺度不变性，大家常常是只在单一scale下（也就是原始图像作为输入），就可以达到不错的结果。不过很多时候参加COCO等竞赛的队伍还是会在TEST的时候使用这项技术，能够取得更好的成绩。但是这样会造成计算时间的巨大开销，TRAIN和TEST的不一致。TRAIN中引入金字塔，内存就会吃紧。所以主流的Fast/Faster RCNN并没有使用金字塔。</p><p>换个角度，我们知道在CNN中，输入会逐层处理，经过Conv/Pooling的操作后，不同深度的layer产生的feature map的spatial dimension是不一样的，这就是作者在摘要中提到的“inherent multi-scale pyramidal hierarchy of deep CNN”。不过，还有一个问题，就是深层和浅层的feature map虽然构成了一个feature pyramid，但是它们的语义并不对等：深层layer的feature map有更抽象的语义信息，而浅层feature map有较高的resolution，但是语义信息还是too yong too simple。</p><p>SSD做过这方面的探索。但是它采用的方法是从浅层layer引出，又加了一些layer，导致无法reuse high resolution的feature map。我们发现，浅层的high resolution feature map对检测小目标很有用处。</p><p>那我们想要怎样呢？</p><ul><li>高层的low resolution，strong semantic info特征如何和浅层的high resolution，weak semantic info自然地结合？</li><li>不引入过多的额外计算，最好也只需要用single scale的原始输入。</li></ul><p>用一张图总结一下。下图中蓝色的轮廓线框起来的就是不同layer输出的feature map。蓝色线越粗，代表其语义信息越强。在（a）中，是将图像做成金字塔，分别跑一个NN来做，这样计算量极大。（b）中是目前Faster RCNN等采用的方法，只在single scale上做。（c）中是直接将各个layer输出的层级feature map自然地看做feature pyramid来做。（d）是本文的方法，不同层级的feature map做了merge，能够使得每个level的语义信息都比较强（注意看蓝色线的粗细）。<br><img src="/img/paper-fpn-different-pyramids.png" alt="不同金字塔方法"></p><p>我们使用这种名为FPN的技术，不用什么工程上的小花招，就打败了目前COCO上的最好结果。不止detection，FPN也能用在图像分割上（当然，现在我们知道，MaskRCNN中的关键技术之一就是FPN）。</p><h2 id="FPN"><a href="#FPN" class="headerlink" title="FPN"></a>FPN</h2><p>有人可能会想，其实前面的网络有人也做过不同深度layer的merge啊，通过skip connection就可以了。作者指出，那种方法仍然是只能在最终的single scale的output feature map上做，而我们的方法是在all level上完成，如下图所示。<br><img src="/img/paper-fpn-different-with-related-work.png" alt="我们才是真正的金字塔"></p><h3 id="Bottom-up-pathway"><a href="#Bottom-up-pathway" class="headerlink" title="Bottom-up pathway"></a>Bottom-up pathway</h3><p> Bottom-up pathway指的是网络的前向计算部分，会产生一系列scale相差2x的feature map。当然，在这些downsample中间，还会有layer的输出spatial dimension是一致的。那些连续的有着相同spatial dimension输出的layer是一个stage。这样，我们就完成了传统金字塔方法和CNN网络的名词的对应。</p><p> 以ResNet为例，我们用每个stage中最后一个residual block的输出作为构建金字塔的feature map，也就是<code>C2~C5</code>。它们的步长分别是$4, 8, 16, 32$。我们没用<code>conv1</code>。</p><h3 id="Top-down-pathway和lateral-connection"><a href="#Top-down-pathway和lateral-connection" class="headerlink" title="Top-down pathway和lateral connection"></a>Top-down pathway和lateral connection</h3><p>Top-down pathway是指将深层的有更强语义信息的feature经过upsampling变成higher resolution的过程。然后再与bottom-up得到的feature经过lateral connection（侧边连接）进行增强。</p><p>下面这张图展示了做lateral connection的过程。注意在图中，越深的layer位于图的上部。我们以框出来放大的那部分举例子。从更深的层输出的feature经过<code>2x up</code>处理（spatial dimension一致了），从左面来的浅层的feature经过<code>1x1 conv</code>处理（channel dimension一致了），再进行element-wise的相加，得到了该stage最后用于prediction的feature（其实还要经过一个<code>3x3 conv</code>的处理，见下引文）。<br><img src="/img/paper-fpn-lateral-connection.png" alt="lateral connection"></p><p>一些细节，直接引用：</p><blockquote><p>To start the iteration, we simply attach a 1x1 convolutional layer on C5 to produce the coarsest resolution map. Finally, we append a 3x3 convolution on each merged map to generate the final feature map, which is to reduce the aliasing effect of upsampling.</p></blockquote><p>此外，由于金字塔上的所有feature共享classifier和regressor，要求它们的channel dimension必须一致。本文固定使用$256$。而且这些外的conv layer没有使用非线性激活。</p><p>这里给出一个基于PyTorch的FPN的第三方实现<a href="https://github.com/kuangliu/pytorch-fpn/blob/master/fpn.py" target="_blank" rel="external">kuangliu/pytorch-fpn</a>，可以对照论文捋一遍。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div><div class="line">60</div><div class="line">61</div><div class="line">62</div><div class="line">63</div><div class="line">64</div><div class="line">65</div><div class="line">66</div><div class="line">67</div><div class="line">68</div><div class="line">69</div><div class="line">70</div><div class="line">71</div><div class="line">72</div><div class="line">73</div><div class="line">74</div><div class="line">75</div><div class="line">76</div><div class="line">77</div><div class="line">78</div><div class="line">79</div><div class="line">80</div><div class="line">81</div><div class="line">82</div><div class="line">83</div><div class="line">84</div><div class="line">85</div><div class="line">86</div><div class="line">87</div><div class="line">88</div><div class="line">89</div><div class="line">90</div><div class="line">91</div><div class="line">92</div><div class="line">93</div><div class="line">94</div><div class="line">95</div><div class="line">96</div><div class="line">97</div><div class="line">98</div><div class="line">99</div><div class="line">100</div><div class="line">101</div><div class="line">102</div><div class="line">103</div><div class="line">104</div><div class="line">105</div><div class="line">106</div><div class="line">107</div><div class="line">108</div><div class="line">109</div></pre></td><td class="code"><pre><div class="line"><span class="comment">## ResNet的block</span></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Bottleneck</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    expansion = <span class="number">4</span></div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, in_planes, planes, stride=<span class="number">1</span>)</span>:</span></div><div class="line">        super(Bottleneck, self).__init__()</div><div class="line">        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn1 = nn.BatchNorm2d(planes)</div><div class="line">        self.conv2 = nn.Conv2d(planes, planes, kernel_size=<span class="number">3</span>, stride=stride, padding=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn2 = nn.BatchNorm2d(planes)</div><div class="line">        self.conv3 = nn.Conv2d(planes, self.expansion*planes, kernel_size=<span class="number">1</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn3 = nn.BatchNorm2d(self.expansion*planes)</div><div class="line"></div><div class="line">        self.shortcut = nn.Sequential()</div><div class="line">        <span class="keyword">if</span> stride != <span class="number">1</span> <span class="keyword">or</span> in_planes != self.expansion*planes:</div><div class="line">            self.shortcut = nn.Sequential(</div><div class="line">                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="keyword">False</span>),</div><div class="line">                nn.BatchNorm2d(self.expansion*planes)</div><div class="line">            )</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        out = F.relu(self.bn1(self.conv1(x)))</div><div class="line">        out = F.relu(self.bn2(self.conv2(out)))</div><div class="line">        out = self.bn3(self.conv3(out))</div><div class="line">        out += self.shortcut(x)</div><div class="line">        out = F.relu(out)</div><div class="line">        <span class="keyword">return</span> out</div><div class="line"></div><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">FPN</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, block, num_blocks)</span>:</span></div><div class="line">        super(FPN, self).__init__()</div><div class="line">        self.in_planes = <span class="number">64</span></div><div class="line"></div><div class="line">        self.conv1 = nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>, padding=<span class="number">3</span>, bias=<span class="keyword">False</span>)</div><div class="line">        self.bn1 = nn.BatchNorm2d(<span class="number">64</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Bottom-up layers, backbone of the network</span></div><div class="line">        self.layer1 = self._make_layer(block,  <span class="number">64</span>, num_blocks[<span class="number">0</span>], stride=<span class="number">1</span>)</div><div class="line">        self.layer2 = self._make_layer(block, <span class="number">128</span>, num_blocks[<span class="number">1</span>], stride=<span class="number">2</span>)</div><div class="line">        self.layer3 = self._make_layer(block, <span class="number">256</span>, num_blocks[<span class="number">2</span>], stride=<span class="number">2</span>)</div><div class="line">        self.layer4 = self._make_layer(block, <span class="number">512</span>, num_blocks[<span class="number">3</span>], stride=<span class="number">2</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Top layer</span></div><div class="line">        <span class="comment"># 我们需要在C5后面接一个1x1, 256 conv，得到金字塔最顶端的feature</span></div><div class="line">        self.toplayer = nn.Conv2d(<span class="number">2048</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)  <span class="comment"># Reduce channels</span></div><div class="line"></div><div class="line">        <span class="comment"># Smooth layers</span></div><div class="line">        <span class="comment"># 这个是上面引文中提到的抗aliasing的3x3卷积</span></div><div class="line">        self.smooth1 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        self.smooth2 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line">        self.smooth3 = nn.Conv2d(<span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</div><div class="line"></div><div class="line">        <span class="comment"># Lateral layers</span></div><div class="line">        <span class="comment"># 为了匹配channel dimension引入的1x1卷积</span></div><div class="line">        <span class="comment"># 注意这些backbone之外的extra conv，输出都是256 channel</span></div><div class="line">        self.latlayer1 = nn.Conv2d(<span class="number">1024</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line">        self.latlayer2 = nn.Conv2d( <span class="number">512</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line">        self.latlayer3 = nn.Conv2d( <span class="number">256</span>, <span class="number">256</span>, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>, padding=<span class="number">0</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_make_layer</span><span class="params">(self, block, planes, num_blocks, stride)</span>:</span></div><div class="line">        strides = [stride] + [<span class="number">1</span>]*(num_blocks<span class="number">-1</span>)</div><div class="line">        layers = []</div><div class="line">        <span class="keyword">for</span> stride <span class="keyword">in</span> strides:</div><div class="line">            layers.append(block(self.in_planes, planes, stride))</div><div class="line">            self.in_planes = planes * block.expansion</div><div class="line">        <span class="keyword">return</span> nn.Sequential(*layers)</div><div class="line"></div><div class="line">    <span class="comment">## FPN的lateral connection部分: upsample以后，element-wise相加</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_upsample_add</span><span class="params">(self, x, y)</span>:</span></div><div class="line">        <span class="string">'''Upsample and add two feature maps.</span></div><div class="line">        Args:</div><div class="line">          x: (Variable) top feature map to be upsampled.</div><div class="line">          y: (Variable) lateral feature map.</div><div class="line">        Returns:</div><div class="line">          (Variable) added feature map.</div><div class="line">        Note in PyTorch, when input size is odd, the upsampled feature map</div><div class="line">        with `F.upsample(..., scale_factor=2, mode='nearest')`</div><div class="line">        maybe not equal to the lateral feature map size.</div><div class="line">        e.g.</div><div class="line">        original input size: [N,_,15,15] -&gt;</div><div class="line">        conv2d feature map size: [N,_,8,8] -&gt;</div><div class="line">        upsampled feature map size: [N,_,16,16]</div><div class="line">        So we choose bilinear upsample which supports arbitrary output sizes.</div><div class="line">        '''</div><div class="line">        _,_,H,W = y.size()</div><div class="line">        <span class="keyword">return</span> F.upsample(x, size=(H,W), mode=<span class="string">'bilinear'</span>) + y</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        <span class="comment"># Bottom-up</span></div><div class="line">        c1 = F.relu(self.bn1(self.conv1(x)))</div><div class="line">        c1 = F.max_pool2d(c1, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, padding=<span class="number">1</span>)</div><div class="line">        c2 = self.layer1(c1)</div><div class="line">        c3 = self.layer2(c2)</div><div class="line">        c4 = self.layer3(c3)</div><div class="line">        c5 = self.layer4(c4)</div><div class="line">        <span class="comment"># Top-down</span></div><div class="line">        <span class="comment"># P5: 金字塔最顶上的feature</span></div><div class="line">        p5 = self.toplayer(c5)</div><div class="line">        <span class="comment"># P4: 上一层 p5 + 侧边来的 c4</span></div><div class="line">        <span class="comment"># 其余同理</span></div><div class="line">        p4 = self._upsample_add(p5, self.latlayer1(c4))</div><div class="line">        p3 = self._upsample_add(p4, self.latlayer2(c3))</div><div class="line">        p2 = self._upsample_add(p3, self.latlayer3(c2))</div><div class="line">        <span class="comment"># Smooth</span></div><div class="line">        <span class="comment"># 输出做一下smooth</span></div><div class="line">        p4 = self.smooth1(p4)</div><div class="line">        p3 = self.smooth2(p3)</div><div class="line">        p2 = self.smooth3(p2)</div><div class="line">        <span class="keyword">return</span> p2, p3, p4, p5</div></pre></td></tr></table></figure></p><h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>下面作者会把FPN应用到FasterRCNN的两个重要步骤：RPN和Fast RCNN。</p><h3 id="FPN加持的RPN"><a href="#FPN加持的RPN" class="headerlink" title="FPN加持的RPN"></a>FPN加持的RPN</h3><p>在Faster RCNN中，RPN用来提供ROI的proposal。backbone网络输出的single feature map上接了$3\times 3$大小的卷积核来实现sliding window的功能，后面接两个$1\times 1$的卷积分别用来做objectness的分类和bounding box基于anchor box的回归。我们把最后的classifier和regressor部分叫做head。</p><p>使用FPN时，我们在金字塔每层的输出feature map上都接上这样的head结构（$3\times 3$的卷积 + two sibling $1\times 1$的卷积）。同时，我们不再使用多尺度的anchor box，而是在每个level上分别使用不同大小的anchor box。具体说，对应于特征金字塔的$5$个level的特征，<code>P2 - P6</code>，anchor box的大小分别是$32^2, 64^2, 128^2, 256^2, 512^2$。不过每层的anchor box仍然要照顾到不同的长宽比例，我们使用了$3$个不同的比例：$1:2, 1:1, 2:1$（和原来一样）。这样，我们一共有$5\times 3 = 15$个anchor box。</p><p>训练过程中，我们需要给anchor boxes赋上对应的正负标签。对于那些与ground truth有最大IoU或者与任意一个ground truth的IoU超过$0.7$的anchor boxes，是positive label；那些与所有ground truth的IoU都小于$0.3$的是negtive label。</p><p>有一个疑问是head的参数是否要在不同的level上共享。我们试验了共享与不共享两个方法，accuracy是相近的。这也说明不同level之间语义信息是相似的，只是resolution不同。</p><h3 id="FPN加持的Fast-RCNN"><a href="#FPN加持的Fast-RCNN" class="headerlink" title="FPN加持的Fast RCNN"></a>FPN加持的Fast RCNN</h3><p>Fast RCNN的原始方法是只在single scale的feature map上做的，要想使用FPN，首先应该解决的问题是前端提供的ROI proposal应该对应到pyramid的哪一个label。由于我们的网络基本都是在ImageNet训练的网络上做transfer learning得到的，我们就以base model在ImageNet上训练时候的输入$224\times 224$作为参考，依据当前ROI和它的大小比例，确定该把这个ROI对应到哪个level。如下所示：</p><script type="math/tex; mode=display">k = \lfloor k_0 + \log_2(\sqrt{wh}/224)\rfloor</script><p>后面接的predictor head我们这里直接连了两个$1024d$的fc layer，再接final classification和regression的部分。同样的，这些参数对于不同level来说是共享的。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;图像金字塔或特征金字塔是传统CV方法中常用的技巧，例如求取&lt;a href=&quot;https://xmfbit.github.io/2017/01/30/cs131-sift/&quot;&gt;SIFT特征&lt;/a&gt;就用到了DoG图像金字塔。但是在Deep Learning统治下的CV detection下，这种方法变得无人问津。一个重要的问题就是计算量巨大。而本文提出了一种仅用少量额外消耗建立特征金字塔的方法，提高了detector的性能。&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="detection" scheme="https://xmfbit.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文 - YOLO v3</title>
    <link href="https://xmfbit.github.io/2018/04/01/paper-yolov3/"/>
    <id>https://xmfbit.github.io/2018/04/01/paper-yolov3/</id>
    <published>2018-04-01T08:48:45.000Z</published>
    <updated>2020-03-21T02:02:44.787Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">YOLOv3: An Incremental Improvement</a>。下面这张图是YOLO V3与RetinaNet的比较。<br><img src="/img/paper-yolov3-comparison-retinanet.png" alt="YOLO v3和RetinaNet的比较"></p><p>可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。<br><a id="more"></a></p><h2 id="YOLO-v3比你们不知道高到哪里去了"><a href="#YOLO-v3比你们不知道高到哪里去了" class="headerlink" title="YOLO v3比你们不知道高到哪里去了"></a>YOLO v3比你们不知道高到哪里去了</h2><p>YOLO v3在保持其一贯的检测速度快的特点前提下，性能又有了提升：输入图像为$320\times 320$大小的图像，可以在$22$ms跑完，mAP达到了$28.2$，这个数据和SSD相同，但是快了$3$倍。在TitanX上，YOLO v3可以在$51$ms内完成，$AP_{50}$的值为$57.9$。而RetinaNet需要$198$ms，$AP_{50}$近似却略低，为$57.5$。</p><h3 id="ps：啥是AP"><a href="#ps：啥是AP" class="headerlink" title="ps：啥是AP"></a>ps：啥是AP</h3><p>AP就是average precision啦。在detection中，我们认为当预测的bounding box和ground truth的IoU大于某个阈值（如取为$0.5$）时，认为是一个True Positive。如果小于这个阈值，就是一个False Positive。</p><p>所谓precision，就是指检测出的框框中有多少是True Positive。另外，还有一个指标叫做recall，是指所有的ground truth里面，有多少被检测出来了。这两个概念都是来自于classification问题，通过设定上面IoU的阈值，就可以迁移到detection中了。</p><p>我们可以取不同的阈值，这样就可以绘出一条precisio vs recall的曲线，计算曲线下的面积，就是AP值。COCO中使用了<code>0.5:0.05:0.95</code>十个离散点近似计算（参考<a href="http://cocodataset.org/#detections-eval" target="_blank" rel="external">COCO的说明文档网页</a>）。detection中常常需要同时检测图像中多个类别的物体，我们将不同类别的AP求平均，就是mAP。</p><p>如果我们只看某个固定的阈值，如$0.5$，计算所有类别的平均AP，那么就用$AP_{50}$来表示。所以YOLO v3单拿出来$AP_{50}$说事，是为了证明虽然我的bounding box不如你RetinaNet那么精准（IoU相对较小），但是如果你对框框的位置不是那么敏感（$0.5$的阈值很多时候够用了），那么我是可以做到比你更好更快的。</p><h2 id="Bounding-Box位置的回归"><a href="#Bounding-Box位置的回归" class="headerlink" title="Bounding Box位置的回归"></a>Bounding Box位置的回归</h2><p>这里和原来v2基本没区别。仍然使用聚类产生anchor box的长宽（下式的$p_w$和$p_h$）。网络预测四个值：$t_x$，$t_y$，$t_w$，$t_h$。我们知道，YOLO网络最后输出是一个$M\times M$的feature map，对应于$M \times M$个cell。如果某个cell距离image的top left corner距离为$(c_x, c_y)$（也就是cell的坐标），那么该cell内的bounding box的位置和形状参数为：</p><script type="math/tex; mode=display">\begin{aligned}b_x &= \sigma(t_x) + c_x\\ b_y &= \sigma(t_y) + c_y\\ b_w &= p_w e^{t_w}\\ b_h &= p_h e^{t_h}\end{aligned}</script><p>PS：这里有一个问题，不管FasterRCNN还是YOLO，都不是直接回归bounding box的长宽（就像这样：$b_w = p_w t_w^\prime$），而是要做一个对数变换，实际预测的是$\log(\cdot)$。这里小小解释一下。</p><p>这是因为如果不做变换，直接预测相对形变$t_w^\prime$，那么要求$t_w^\prime &gt; 0$，因为你的框框的长宽不可能是负数。这样，是在做一个有不等式条件约束的优化问题，没法直接用SGD来做。所以先取一个对数变换，将其不等式约束去掉，就可以了。</p><p><img src="/img/paper=yolov3-bbox-regression.png" alt="bounding box的回归"></p><p>在训练的时候，使用平方误差损失。</p><p>另外，YOLO会对每个bounding box给出是否是object的置信度预测，用来区分objects和背景。这个值使用logistic回归。当某个bounding box与ground truth的IoU大于其他所有bounding box时，target给$1$；如果某个bounding box不是IoU最大的那个，但是IoU也大于了某个阈值（我们取$0.5$），那么我们忽略它（既不惩罚，也不奖励），这个做法是从Faster RCNN借鉴的。我们对每个ground truth只分配一个最好的bounding box与其对应（这与Faster RCNN不同）。如果某个bounding box没有倍assign到任何一个ground truth对应，那么它对边框位置大小的回归和class的预测没有贡献，我们只惩罚它的objectness，即试图减小其confidence。</p><h2 id="分类预测"><a href="#分类预测" class="headerlink" title="分类预测"></a>分类预测</h2><p>我们不用softmax做分类了，而是使用独立的logisitc做二分类。这种方法的好处是可以处理重叠的多标签问题，如Open Image Dataset。在其中，会出现诸如<code>Woman</code>和<code>Person</code>这样的重叠标签。</p><h2 id="FPN加持的多尺度预测"><a href="#FPN加持的多尺度预测" class="headerlink" title="FPN加持的多尺度预测"></a>FPN加持的多尺度预测</h2><p>之前YOLO的一个弱点就是缺少多尺度变换，使用<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">FPN</a>中的思路，v3在$3$个不同的尺度上做预测。在COCO上，我们每个尺度都预测$3$个框框，所以一共是$9$个。所以输出的feature map的大小是$N\times N\times [3\times (4+1+80)]$。</p><p>然后我们从两层前那里拿feature map，upsample 2x，并与更前面输出的feature map通过element-wide的相加做merge。这样我们能够从后面的层拿到更多的高层语义信息，也能从前面的层拿到细粒度的信息（更大的feature map，更小的感受野）。然后在后面接一些conv做处理，最终得到和上面相似大小的feature map，只不过spatial dimension变成了$2$倍。</p><p>照上一段所说方法，再一次在final scale尺度下给出预测。</p><h2 id="代码实现"><a href="#代码实现" class="headerlink" title="代码实现"></a>代码实现</h2><p>在v3中，作者新建了一个名为<code>yolo</code>的layer，其参数如下：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">[yolo]</div><div class="line">mask = 0,1,2</div><div class="line">## 9组anchor对应9个框框</div><div class="line">anchors = 10,13,  16,30,  33,23,  30,61,  62,45,  59,119,  116,90,  156,198,  373,326</div><div class="line">classes=20   ## VOC20类</div><div class="line">num=9</div><div class="line">jitter=.3</div><div class="line">ignore_thresh = .5</div><div class="line">truth_thresh = 1</div><div class="line">random=1</div></pre></td></tr></table></figure></p><p>打开<code>yolo_layer.c</code>文件，找到<code>forward</code><a href="">部分代码</a>。可以看到，首先，对输入进行activation。注意，如论文所说，对类别进行预测的时候，没有使用v2中的softmax或softmax tree，而是直接使用了logistic变换。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (b = <span class="number">0</span>; b &lt; l.batch; ++b)&#123;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.n; ++n)&#123;</div><div class="line">        <span class="keyword">int</span> index = entry_index(l, b, n*l.w*l.h, <span class="number">0</span>);</div><div class="line">        <span class="comment">// 对 tx, ty进行logistic变换</span></div><div class="line">        activate_array(l.output + index, <span class="number">2</span>*l.w*l.h, LOGISTIC);</div><div class="line">        index = entry_index(l, b, n*l.w*l.h, <span class="number">4</span>);</div><div class="line">        <span class="comment">// 对confidence和C类进行logistic变换</span></div><div class="line">        activate_array(l.output + index, (<span class="number">1</span>+l.classes)*l.w*l.h, LOGISTIC);</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我们看一下如何计算梯度。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">for</span> (j = <span class="number">0</span>; j &lt; l.h; ++j) &#123;</div><div class="line">    <span class="keyword">for</span> (i = <span class="number">0</span>; i &lt; l.w; ++i) &#123;</div><div class="line">        <span class="keyword">for</span> (n = <span class="number">0</span>; n &lt; l.n; ++n) &#123;</div><div class="line">            <span class="comment">// 对每个预测的bounding box</span></div><div class="line">            <span class="comment">// 找到与其IoU最大的ground truth</span></div><div class="line">            <span class="keyword">int</span> box_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</div><div class="line">            box pred = get_yolo_box(l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.w*l.h);</div><div class="line">            <span class="keyword">float</span> best_iou = <span class="number">0</span>;</div><div class="line">            <span class="keyword">int</span> <span class="keyword">best_t</span> = <span class="number">0</span>;</div><div class="line">            <span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</div><div class="line">                box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">                <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</div><div class="line">                <span class="keyword">float</span> iou = box_iou(pred, truth);</div><div class="line">                <span class="keyword">if</span> (iou &gt; best_iou) &#123;</div><div class="line">                    best_iou = iou;</div><div class="line">                    <span class="keyword">best_t</span> = t;</div><div class="line">                &#125;</div><div class="line">            &#125;</div><div class="line">            <span class="keyword">int</span> obj_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</div><div class="line">            avg_anyobj += l.output[obj_index];</div><div class="line">            <span class="comment">// 计算梯度</span></div><div class="line">            <span class="comment">// 如果大于ignore_thresh, 那么忽略</span></div><div class="line">            <span class="comment">// 如果小于ignore_thresh，target = 0</span></div><div class="line">            <span class="comment">// diff = -gradient = target - output</span></div><div class="line">            <span class="comment">// 为什么是上式，见下面的数学分析</span></div><div class="line">            l.delta[obj_index] = <span class="number">0</span> - l.output[obj_index];</div><div class="line">            <span class="keyword">if</span> (best_iou &gt; l.ignore_thresh) &#123;</div><div class="line">                l.delta[obj_index] = <span class="number">0</span>;</div><div class="line">            &#125;</div><div class="line">            <span class="comment">// 这里仍然有疑问，为何使用truth_thresh?这个值是1</span></div><div class="line">            <span class="comment">// 按道理，iou无论如何不可能大于1啊。。。</span></div><div class="line">            <span class="keyword">if</span> (best_iou &gt; l.truth_thresh) &#123;</div><div class="line">                <span class="comment">// confidence target = 1</span></div><div class="line">                l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</div><div class="line">                <span class="keyword">int</span> <span class="keyword">class</span> = net.truth[<span class="keyword">best_t</span>*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths + <span class="number">4</span>];</div><div class="line">                <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="keyword">class</span> = l.<span class="built_in">map</span>[<span class="keyword">class</span>];</div><div class="line">                <span class="keyword">int</span> class_index = entry_index(l, b, n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</div><div class="line">                <span class="comment">// 对class进行求导</span></div><div class="line">                delta_yolo_class(l.output, l.delta, class_index, <span class="keyword">class</span>, l.classes, l.w*l.h, <span class="number">0</span>);</div><div class="line">                box truth = float_to_box(net.truth + <span class="keyword">best_t</span>*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">                <span class="comment">// 对box位置参数进行求导</span></div><div class="line">                delta_yolo_box(truth, l.output, l.biases, l.mask[n], box_index, i, j, l.w, l.h, net.w, net.h, l.delta, (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</div><div class="line">            &#125;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>我们首先来说一下为何confidence（包括后面的classification）的<code>diff</code>计算为何是<code>target - output</code>的形式。对于logistic regression，假设logistic函数的输入是$o = f(x;\theta)$。其中，$\theta$是网络的参数。那么输出$y = h(o)$，其中$h$指logistic激活函数（或sigmoid函数）。那么，我们有：</p><script type="math/tex; mode=display">\begin{aligned}P(y=1|x) &= h(o)\\ P(y=0|x) &= 1-h(o)\end{aligned}</script><p>写出对数极大似然函数，我们有：</p><script type="math/tex; mode=display">\log L = \sum y\log h+(1-y)\log(1-h)</script><p>为了使用SGD，上式两边取相反数，我们有损失函数：</p><script type="math/tex; mode=display">J = -\log L = \sum -y\log h-(1-y)\log(1-h)</script><p>对第$i$个输入$o_i$求导，我们有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial J}{\partial o_i} &= \frac{\partial J}{\partial h_i}\frac{\partial h_i}{\partial o_i}\\&= [-y_i/h_i-(y_i-1)/(1-h_i)] \frac{\partial h_i}{\partial o_i} \\&= \frac{h_i-y_i}{h_i(1-h_i)} \frac{\partial h_i}{\partial o_i}\end{aligned}</script><p>根据logistic函数的求导性质，有：</p><script type="math/tex; mode=display">\frac{\partial h_i}{\partial o_i} = h_i(1-h_i)</script><p>所以，有</p><script type="math/tex; mode=display">\frac{\partial J}{\partial o_i} = h_i-y_i</script><p>其中，$h_i$即为logistic激活后的输出，$y_i$为target。由于YOLO代码中均使用<code>diff</code>，也就是<code>-gradient</code>，所以有<code>delta = target - output</code>。</p><p>关于logistic回归，还可以参考我的博客：<a href="https://xmfbit.github.io/2018/03/21/cs229-supervised-learning/">CS229 简单的监督学习方法</a>。</p><p>下面，我们看下两个关键的子函数，<code>delta_yolo_class</code>和<code>delta_yolo_box</code>的实现。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// class是类别的ground truth</span></div><div class="line"><span class="comment">// classes是类别总数</span></div><div class="line"><span class="comment">// index是feature map一维数组里面class prediction的起始索引</span></div><div class="line"><span class="function"><span class="keyword">void</span> <span class="title">delta_yolo_class</span><span class="params">(<span class="keyword">float</span> *output, <span class="keyword">float</span> *delta, <span class="keyword">int</span> index, </span></span></div><div class="line">  <span class="keyword">int</span> <span class="keyword">class</span>, <span class="keyword">int</span> classes, <span class="keyword">int</span> stride, <span class="keyword">float</span> *avg_cat) &#123;</div><div class="line">    <span class="keyword">int</span> n;</div><div class="line">    <span class="comment">// 这里暂时不懂</span></div><div class="line">    <span class="keyword">if</span> (delta[index])&#123;</div><div class="line">        delta[index + stride*<span class="keyword">class</span>] = <span class="number">1</span> - output[index + stride*<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">if</span>(avg_cat) *avg_cat += output[index + stride*<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">return</span>;</div><div class="line">    &#125;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; classes; ++n)&#123;</div><div class="line">        <span class="comment">// 见上，diff = target - prediction</span></div><div class="line">        delta[index + stride*n] = ((n == <span class="keyword">class</span>)?<span class="number">1</span> : <span class="number">0</span>) - output[index + stride*n];</div><div class="line">        <span class="keyword">if</span>(n == <span class="keyword">class</span> &amp;&amp; avg_cat) *avg_cat += output[index + stride*n];</div><div class="line">    &#125;</div><div class="line">&#125;</div><div class="line"><span class="comment">// box delta这里没什么可说的，就是square error的求导</span></div><div class="line"><span class="function"><span class="keyword">float</span> <span class="title">delta_yolo_box</span><span class="params">(box truth, <span class="keyword">float</span> *x, <span class="keyword">float</span> *biases, <span class="keyword">int</span> n, </span></span></div><div class="line">  <span class="keyword">int</span> index, <span class="keyword">int</span> i, <span class="keyword">int</span> j, <span class="keyword">int</span> lw, <span class="keyword">int</span> lh, <span class="keyword">int</span> w, <span class="keyword">int</span> h, </div><div class="line">  <span class="keyword">float</span> *delta, <span class="keyword">float</span> scale, <span class="keyword">int</span> stride) &#123;</div><div class="line">    box pred = get_yolo_box(x, biases, n, index, i, j, lw, lh, w, h, stride);</div><div class="line">    <span class="keyword">float</span> iou = box_iou(pred, truth);</div><div class="line">    <span class="keyword">float</span> tx = (truth.x*lw - i);</div><div class="line">    <span class="keyword">float</span> ty = (truth.y*lh - j);</div><div class="line">    <span class="keyword">float</span> tw = <span class="built_in">log</span>(truth.w*w / biases[<span class="number">2</span>*n]);</div><div class="line">    <span class="keyword">float</span> th = <span class="built_in">log</span>(truth.h*h / biases[<span class="number">2</span>*n + <span class="number">1</span>]);</div><div class="line"></div><div class="line">    delta[index + <span class="number">0</span>*stride] = scale * (tx - x[index + <span class="number">0</span>*stride]);</div><div class="line">    delta[index + <span class="number">1</span>*stride] = scale * (ty - x[index + <span class="number">1</span>*stride]);</div><div class="line">    delta[index + <span class="number">2</span>*stride] = scale * (tw - x[index + <span class="number">2</span>*stride]);</div><div class="line">    delta[index + <span class="number">3</span>*stride] = scale * (th - x[index + <span class="number">3</span>*stride]);</div><div class="line">    <span class="keyword">return</span> iou;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><p>上面，我们遍历了每一个prediction的bounding box，下面我们还要遍历每个ground truth，根据IoU，为其分配一个最佳的匹配。<br><figure class="highlight cpp"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div></pre></td><td class="code"><pre><div class="line"><span class="comment">// 遍历ground truth</span></div><div class="line"><span class="keyword">for</span>(t = <span class="number">0</span>; t &lt; l.max_boxes; ++t)&#123;</div><div class="line">    box truth = float_to_box(net.truth + t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths, <span class="number">1</span>);</div><div class="line">    <span class="keyword">if</span>(!truth.x) <span class="keyword">break</span>;</div><div class="line">    <span class="comment">// 找到iou最大的那个bounding box</span></div><div class="line">    <span class="keyword">float</span> best_iou = <span class="number">0</span>;</div><div class="line">    <span class="keyword">int</span> best_n = <span class="number">0</span>;</div><div class="line">    i = (truth.x * l.w);</div><div class="line">    j = (truth.y * l.h);</div><div class="line">    box truth_shift = truth;</div><div class="line">    truth_shift.x = truth_shift.y = <span class="number">0</span>;</div><div class="line">    <span class="keyword">for</span>(n = <span class="number">0</span>; n &lt; l.total; ++n)&#123;</div><div class="line">        box pred = &#123;<span class="number">0</span>&#125;;</div><div class="line">        pred.w = l.biases[<span class="number">2</span>*n]/net.w;</div><div class="line">        pred.h = l.biases[<span class="number">2</span>*n+<span class="number">1</span>]/net.h;</div><div class="line">        <span class="keyword">float</span> iou = box_iou(pred, truth_shift);</div><div class="line">        <span class="keyword">if</span> (iou &gt; best_iou)&#123;</div><div class="line">            best_iou = iou;</div><div class="line">            best_n = n;</div><div class="line">        &#125;</div><div class="line">    &#125;</div><div class="line">    </div><div class="line">    <span class="keyword">int</span> mask_n = int_index(l.mask, best_n, l.n);</div><div class="line">    <span class="keyword">if</span>(mask_n &gt;= <span class="number">0</span>)&#123;</div><div class="line">        <span class="keyword">int</span> box_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">0</span>);</div><div class="line">        <span class="keyword">float</span> iou = delta_yolo_box(truth, l.output, l.biases, best_n, </div><div class="line">          box_index, i, j, l.w, l.h, net.w, net.h, l.delta, </div><div class="line">          (<span class="number">2</span>-truth.w*truth.h), l.w*l.h);</div><div class="line">        <span class="keyword">int</span> obj_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span>);</div><div class="line">        avg_obj += l.output[obj_index];</div><div class="line">        <span class="comment">// 对应objectness target = 1</span></div><div class="line">        l.delta[obj_index] = <span class="number">1</span> - l.output[obj_index];</div><div class="line">        <span class="keyword">int</span> <span class="keyword">class</span> = net.truth[t*(<span class="number">4</span> + <span class="number">1</span>) + b*l.truths + <span class="number">4</span>];</div><div class="line">        <span class="keyword">if</span> (l.<span class="built_in">map</span>) <span class="keyword">class</span> = l.<span class="built_in">map</span>[<span class="keyword">class</span>];</div><div class="line">        <span class="keyword">int</span> class_index = entry_index(l, b, mask_n*l.w*l.h + j*l.w + i, <span class="number">4</span> + <span class="number">1</span>);</div><div class="line">        delta_yolo_class(l.output, l.delta, class_index, <span class="keyword">class</span>, l.classes, l.w*l.h, &amp;avg_cat);</div><div class="line">        ++count;</div><div class="line">        ++class_count;</div><div class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.5</span>) recall += <span class="number">1</span>;</div><div class="line">        <span class="keyword">if</span>(iou &gt; <span class="number">.75</span>) recall75 += <span class="number">1</span>;</div><div class="line">        avg_iou += iou;</div><div class="line">    &#125;</div><div class="line">&#125;</div></pre></td></tr></table></figure></p><h2 id="Darknet网络架构"><a href="#Darknet网络架构" class="headerlink" title="Darknet网络架构"></a>Darknet网络架构</h2><p>引入了ResidualNet的思路（$3\times 3$和$1\times 1$的卷积核，shortcut连接），构建了Darknet-53网络。<br><img src="/img/paper-yolov3-darknet53.png" alt="darknet-63"></p><h2 id="YOLO的优势和劣势"><a href="#YOLO的优势和劣势" class="headerlink" title="YOLO的优势和劣势"></a>YOLO的优势和劣势</h2><p>把YOLO v3和其他方法比较，优势在于快快快。当你不太在乎IoU一定要多少多少的时候，YOLO可以做到又快又好。作者还在文章的结尾发起了这样的牢骚：</p><blockquote><p>Russakovsky et al report that that humans have a hard time distinguishing an IOU of .3 from .5! “Training humans to visually inspect a bounding box with IOU of 0.3 and distinguish it from one with IOU 0.5 is surprisingly difficult.” [16] If humans have a hard time telling the difference, how much does it matter?</p></blockquote><p>使用了多尺度预测，v3对于小目标的检测结果明显变好了。不过对于medium和large的目标，表现相对不好。这是需要后续工作进一步挖局的地方。</p><p>下面是具体的数据比较。<br><img src="/img/paper-yolov3-comparisons.png" alt="具体数据比较"></p><h2 id="我们是身经百战，见得多了"><a href="#我们是身经百战，见得多了" class="headerlink" title="我们是身经百战，见得多了"></a>我们是身经百战，见得多了</h2><p>作者还贴心地给出了什么方法没有奏效。</p><ul><li>anchor box坐标$(x, y)$的预测。预测anchor box的offset，no stable，不好。</li><li>线性offset预测，而不是logistic。精度下降。</li><li>focal loss。精度下降。</li><li>双IoU阈值，像Faster RCNN那样。效果不好。</li></ul><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><p>下面是一些可供利用的参考资料：</p><ul><li>YOLO的项目主页<a href="https://pjreddie.com/darknet/yolo/" target="_blank" rel="external">Darknet YOLO</a></li><li>作者主页上的<a href="https://pjreddie.com/media/files/papers/YOLOv3.pdf" target="_blank" rel="external">paper链接</a></li><li>知乎专栏上的<a href="https://zhuanlan.zhihu.com/p/34945787" target="_blank" rel="external">全文翻译</a></li><li>FPN论文<a href="https://arxiv.org/abs/1612.03144" target="_blank" rel="external">Feature pyramid networks for object detection</a></li><li>知乎上的解答：<a href="https://www.zhihu.com/question/41540197" target="_blank" rel="external">AP是什么，怎么计算</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO的作者又放出了V3版本，在之前的版本上做出了一些改进，达到了更好的性能。这篇博客介绍这篇论文：&lt;a href=&quot;https://pjreddie.com/media/files/papers/YOLOv3.pdf&quot;&gt;YOLOv3: An Incremental Improvement&lt;/a&gt;。下面这张图是YOLO V3与RetinaNet的比较。&lt;br&gt;&lt;img src=&quot;/img/paper-yolov3-comparison-retinanet.png&quot; alt=&quot;YOLO v3和RetinaNet的比较&quot;&gt;&lt;/p&gt;
&lt;p&gt;可以使用搜索功能，在本博客内搜索YOLO前作的论文阅读和代码。&lt;br&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="yolo" scheme="https://xmfbit.github.io/tags/yolo/"/>
    
      <category term="detection" scheme="https://xmfbit.github.io/tags/detection/"/>
    
  </entry>
  
  <entry>
    <title>论文 - SqueezeNet, AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</title>
    <link href="https://xmfbit.github.io/2018/03/24/paper-squeezenet/"/>
    <id>https://xmfbit.github.io/2018/03/24/paper-squeezenet/</id>
    <published>2018-03-24T06:02:53.000Z</published>
    <updated>2020-03-21T02:02:44.773Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://arxiv.org/abs/1602.07360" target="_blank" rel="external">SqueezeNet</a>由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。</p><a id="more"></a><h2 id="模型压缩相关工作"><a href="#模型压缩相关工作" class="headerlink" title="模型压缩相关工作"></a>模型压缩相关工作</h2><p>模型压缩的好处主要有以下几点：</p><ul><li>更好的分布式训练。server之间的通信往往限制了分布式训练的提速比例，较少的网络参数能够降低对server间通信需求。</li><li>云端向终端的部署，需要更低的带宽，例如手机app更新或无人车的软件包更新。</li><li>更易于在FPGA等硬件上部署，因为它们往往都有着非常受限的片上RAM。</li></ul><p>相关工作主要有两个方向，即模型压缩和模型结构自身探索。</p><p>模型压缩方面的工作主要有，使用SVD分解，Deep Compression等。模型结构方面比较有意义的工作是GoogLeNet的Inception module（可在博客内搜索<em>Xception</em>查看Xception的作者是如何受此启发发明Xception结构的）。</p><p>本文的作者从网络设计角度出发，提出了名为SqueezeNet的网络结构，使用比AlexNet少$50$倍的参数，在ImageNet上取得了comparable的结果。此外，还探究了CNN的arch是如何影响model size和最终的accuracy的。主要从两个方面进行了探索，分别是<em>CNN microarch</em>和<em>CNN macroarch</em>。前者意为在更小的粒度上，如每一层的layer怎么设计，来考察；后者是在更为宏观的角度，如一个CNN中的不同layer该如何组织来考察。</p><p><em>PS: 吐槽：看完之后觉得基本没探索出什么太有用的可以迁移到其他地方的规律。。。只是比较了自己的SqueezeNet在不同参数下的性能，有些标题党之嫌，题目很大，但是里面的内容并不完全是这样。CNN的设计还是实验实验再实验。</em></p><h2 id="SqueezeNet"><a href="#SqueezeNet" class="headerlink" title="SqueezeNet"></a>SqueezeNet</h2><p>为了简单，下文简称<em>SNet</em>。SNet的基本组成是叫做<em>Fire</em>的module。我们知道，对于一个CONV layer，它的参数数量计算应该是：$K \times K \times M \times N$。其中，$K$是filter的spatial size，$M$和$N$分别是输入feature map和输出activation的channel size。由此，设计SNet时，作者的依据主要是以下几点：</p><ul><li>把$3\times 3$的卷积替换成$1\times 1$，相当于减小上式中的$K$。</li><li>减少$3\times 3$filter对应的输入feature map的channel，相当于减少上式的$M$。</li><li>delayed downsample。使得activation的feature map能够足够大，这样对提高accuracy有益。CNN中的downsample主要是通过CONV layer或pooling layer中stride设置大于$1$得到的，作者指出，应将这种操作尽量后移。</li></ul><blockquote><p>Our intuition is that large activation maps (due to delayed downsampling) can lead to higher classification accuracy, with all else held equal.</p></blockquote><h3 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h3><p>Fire Module是SNet的基本组成单元，如下图所示。可以分为两个部分，一个是上面的<em>squeeze</em>部分，是一组$1\times 1$的卷积，用来将输入的channel squeeze到一个较小的值。后面是<em>expand</em>部分，由$1\times 1$和$3\times 3$卷积mix起来。使用$s_{1 x 1}$，$e_{1x1}$和$e_{3x3}$表示squeeze和expand中两种不同卷积的channel数量，令$s_{1x1} &lt; e_{1x1} + e_{3x3}$，用来实现上述策略2.<br><img src="/img/paper-squeezenet-fire-module.png" alt="Fire Module示意"></p><p>下面，对照PyTorch实现的SNet代码看下Fire的实现，注意上面说的CONV后面都接了ReLU。<br><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">Fire</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inplanes, squeeze_planes,</span></span></div><div class="line">                 expand1x1_planes, expand3x3_planes):</div><div class="line">        super(Fire, self).__init__()</div><div class="line">        self.inplanes = inplanes</div><div class="line">        <span class="comment">## squeeze 部分</span></div><div class="line">        self.squeeze = nn.Conv2d(inplanes, squeeze_planes, kernel_size=<span class="number">1</span>)</div><div class="line">        self.squeeze_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 1x1 部分</span></div><div class="line">        self.expand1x1 = nn.Conv2d(squeeze_planes, expand1x1_planes,</div><div class="line">                                   kernel_size=<span class="number">1</span>)</div><div class="line">        self.expand1x1_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line">        <span class="comment">## expand 3x3部分</span></div><div class="line">        self.expand3x3 = nn.Conv2d(squeeze_planes, expand3x3_planes,</div><div class="line">                                   kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</div><div class="line">        self.expand3x3_activation = nn.ReLU(inplace=<span class="keyword">True</span>)</div><div class="line"></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></div><div class="line">        x = self.squeeze_activation(self.squeeze(x))</div><div class="line">        <span class="comment">## 将expand 部分1x1和3x3的cat到一起</span></div><div class="line">        <span class="keyword">return</span> torch.cat([</div><div class="line">            self.expand1x1_activation(self.expand1x1(x)),</div><div class="line">            self.expand3x3_activation(self.expand3x3(x))], <span class="number">1</span>)</div></pre></td></tr></table></figure></p><h3 id="SNet"><a href="#SNet" class="headerlink" title="SNet"></a>SNet</h3><p>有了Fire Module这个基础材料，我们就可以搭建SNet了。一个单独的<code>conv1</code> layer，后面接了$8$个连续的Fire Module，最后再接一个<code>conv10</code> layer。此外，在<code>conv1</code>，<code>fire4</code>, <code>fire8</code>和<code>conv10</code>后面各有一个<code>stride=2</code>的MAX Pooling layer。这些pooling的位置相对靠后，是对上述策略$3$的实践。我们还可以在不同的Fire Module中加入ResNet中的bypass结构。这样，形成了下图三种不同的SNet结构。<br><img src="/img/paper-squeezenet-macroarch.png" alt="SNet的三种形式"></p><p>一些细节：</p><ul><li>为了使得$1\times 1$和$3\times 3$的卷积核能够有相同spatial size的输出，$3\times 3$的卷积输入加了<code>padding=1</code>。</li><li>在squeeze layer和expand layer中加入了ReLU。</li><li>在<code>fire 9</code>后加入了drop ratio为$0.5$的Dropout layer。</li><li>受NIN启发，SNet中没有fc层。</li><li>更多的细节和训练参数的设置可以参考GitHub上的<a href="https://github.com/DeepScale/SqueezeNet" target="_blank" rel="external">官方repo</a>。</li></ul><p>同样的，我们可以参考PyTorch中的实现。注意下面实现了v1.0和v1.1版本，两者略有不同。v1.1版本参数更少，也能够达到v1.0的精度。</p><blockquote><p>SqueezeNet v1.1 (in this repo), which requires 2.4x less computation than SqueezeNet v1.0 without diminshing accuracy.</p></blockquote><figure class="highlight py"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">SqueezeNet</span><span class="params">(nn.Module)</span>:</span></div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, version=<span class="number">1.0</span>, num_classes=<span class="number">1000</span>)</span>:</span></div><div class="line">        super(SqueezeNet, self).__init__()</div><div class="line">        <span class="keyword">if</span> version <span class="keyword">not</span> <span class="keyword">in</span> [<span class="number">1.0</span>, <span class="number">1.1</span>]:</div><div class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Unsupported SqueezeNet version &#123;version&#125;:"</span></div><div class="line">                             <span class="string">"1.0 or 1.1 expected"</span>.format(version=version))</div><div class="line">        self.num_classes = num_classes</div><div class="line">        <span class="keyword">if</span> version == <span class="number">1.0</span>:</div><div class="line">            self.features = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">96</span>, kernel_size=<span class="number">7</span>, stride=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">96</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">            )</div><div class="line">        <span class="keyword">else</span>:</div><div class="line">            self.features = nn.Sequential(</div><div class="line">                nn.Conv2d(<span class="number">3</span>, <span class="number">64</span>, kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>),</div><div class="line">                nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">64</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">16</span>, <span class="number">64</span>, <span class="number">64</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">128</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">32</span>, <span class="number">128</span>, <span class="number">128</span>),</div><div class="line">                nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">2</span>, ceil_mode=<span class="keyword">True</span>),</div><div class="line">                Fire(<span class="number">256</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">48</span>, <span class="number">192</span>, <span class="number">192</span>),</div><div class="line">                Fire(<span class="number">384</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">                Fire(<span class="number">512</span>, <span class="number">64</span>, <span class="number">256</span>, <span class="number">256</span>),</div><div class="line">            )</div><div class="line">        <span class="comment"># Final convolution is initialized differently form the rest</span></div><div class="line">        final_conv = nn.Conv2d(<span class="number">512</span>, self.num_classes, kernel_size=<span class="number">1</span>)</div><div class="line">        self.classifier = nn.Sequential(</div><div class="line">            nn.Dropout(p=<span class="number">0.5</span>),</div><div class="line">            final_conv,</div><div class="line">            nn.ReLU(inplace=<span class="keyword">True</span>),</div><div class="line">            nn.AvgPool2d(<span class="number">13</span>, stride=<span class="number">1</span>)</div><div class="line">)</div></pre></td></tr></table></figure><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>把SNet和AlexNet分别经过Deep Compression，在ImageNet上测试结果如下。可以看到，未被压缩时，SNet比AlexNet少了$50$倍，accuracy是差不多的。经过压缩，SNet更是可以进一步瘦身成不到$0.5$M，比原始的AlexNet瘦身了$500+$倍。<br><img src="/img/paper-squeezenet-benchmark.png" alt="性能比较"></p><p>注意上述结果是使用HanSong的Deep Compression技术（聚类+codebook）得到的。这种方法得到的模型在通用计算平台（CPU/GPU）上的优势并不明显，需要在作者提出的EIE硬件上才能充分发挥其性能。对于线性的量化（直接用量化后的$8$位定点存储模型），<a href="http://lepsucd.com/?page_id=630" target="_blank" rel="external">Ristretto</a>实现了SNet的量化，但是有一个点的损失。</p><h2 id="Micro-Arch探索"><a href="#Micro-Arch探索" class="headerlink" title="Micro Arch探索"></a>Micro Arch探索</h2><p>所谓CNN的Micro Arch，是指如何确定各层的参数，如filter的个数，kernel size的大小等。在SNet中，主要是filter的个数，即上文提到的$s_{1x1}$，$e_{1x1}$和$e_{3x3}$。这样，$8$个Fire Module就有$24$个超参数，数量太多，我们需要加一些约束，暴露主要矛盾，把问题变简单一点。</p><p>我们设定$base_e$是第一个Fire Module的expand layer的filter个数，每隔$freq$个Fire Module，会加上$incr_e$这么多。那么任意一个Fire Module的expand layer filter的个数为$e_i = base_e + (incr_e \times \lfloor \frac{i}{freq}\rfloor)$。</p><p>在expand layer，我们有$e_i = e_{i,1x1} + e_{i,3x3}$，设定$pct_{3x3} = e_{i,3x3}/e_i$为$3\times 3$的conv占的比例。</p><p>设定$SR = s_{i,1x1} / e_i$，为squeeze和expand filter个数比例。</p><h3 id="SR的影响"><a href="#SR的影响" class="headerlink" title="SR的影响"></a>SR的影响</h3><p>$SR$于区间$[0.125, 1]$之间取，accuracy基本随着$SR$增大而提升，同时模型的size也在变大。但$SR$从$0.75$提升到$1.0$，accuracy无提升。publish的SNet使用了$SR=0.125$。<br><img src="/img/paper-squeeze-sr-impact.png" alt="SR"></p><h3 id="1X1和3x3的比例pct的影响"><a href="#1X1和3x3的比例pct的影响" class="headerlink" title="1X1和3x3的比例pct的影响"></a>1X1和3x3的比例pct的影响</h3><p>为了减少参数，我们把部分$3\times 3$的卷积换成了$1\times 1$的，构成了expand layer。那么两者的比例对模型的影响？$pct$在$[0.01, 0.99]$之间变化。同样，accuracy和model size基本都随着$pct$增大而提升。当大于$0.5$时，模型的accuracy基本无提升。<br><img src="/img/paper-squeezenet-pct-impact.png" alt="pct"></p><h2 id="Macro-Arch探索"><a href="#Macro-Arch探索" class="headerlink" title="Macro Arch探索"></a>Macro Arch探索</h2><p>这里主要讨论了是否使用ResNet中的bypass结构。<br><img src="/img/paper-squeezenet-bypass.png" alt="bypass比较"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/abs/1602.07360&quot;&gt;SqueezeNet&lt;/a&gt;由HanSong等人提出，和AlexNet相比，用少于$50$倍的参数量，在ImageNet上实现了comparable的accuracy。比较本文和HanSoing其他的工作，可以看出，其他工作，如Deep Compression是对已有的网络进行压缩，减小模型size；而SqueezeNet是从网络设计入手，从设计之初就考虑如何使用较少的参数实现较好的性能。可以说是模型压缩的两个不同思路。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
</feed>
