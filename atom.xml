<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>来呀，快活呀~</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://xmfbit.github.io/"/>
  <updated>2020-05-02T07:17:54.452Z</updated>
  <id>https://xmfbit.github.io/</id>
  
  <author>
    <name>一个脱离了高级趣味的人</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>论文 - Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</title>
    <link href="https://xmfbit.github.io/2020/04/15/paper-tf-training-aweare-quantization/"/>
    <id>https://xmfbit.github.io/2020/04/15/paper-tf-training-aweare-quantization/</id>
    <published>2020-04-15T14:19:42.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>Google比较早的关于training-aware-quantization的模型量化的paper，不过提供了很多模型量化的基本知识。后面不管是TFLite还是TensorRT，都能在这篇文章中找到对应的基础知识。Arxiv: <a href="https://arxiv.org/abs/1712.05877" target="_blank" rel="noopener">Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference</a></p><a id="more"></a><h2 id="Quantization-Scheme-如何量化"><a href="#Quantization-Scheme-如何量化" class="headerlink" title="Quantization Scheme - 如何量化"></a>Quantization Scheme - 如何量化</h2><p>在这个小节，我们更考虑如何在数学上去设计量化的conv操作。在下一个小节，会从更实际的角度来考虑一个CNN的量化。</p><p>把量化值$q$映射到浮点数$r$的式子很简单，其中$S$和$Z$是量化参数，分别为零点（zero-point）和scaling factor：</p><script type="math/tex; mode=display">r = S(q - Z)</script><p>$S$和$Z$作为量化参数，每个weight array或者activation array是共享的，不同的array之间不共享。</p><p>这里的$S$决定了分辨率，也就是最小量化误差。而$Z$可以用来平衡掉数据偏离$0$的bias。</p><p>有了上述变换规则， CNN中常见的矩阵相乘可以表示为下图。其中，$\alpha$的值为$1,2,3$，分别表示输入矩阵1，输入矩阵2，结果矩阵3。式4给出了在量化后的$Q$上计算原始浮点$R$的矩阵的方法。$\Sigma$内都是整数运算，只需要最后乘上一个scaling factor $M$即可，从而加速了计算。</p><p><img src="/img/paper_tf_taq_matmul.png" alt="矩阵乘法"></p><h2 id="CNN的量化-以Conv为例"><a href="#CNN的量化-以Conv为例" class="headerlink" title="CNN的量化 - 以Conv为例"></a>CNN的量化 - 以Conv为例</h2><p>以卷积op为例，说明在实际的CNN模型中量化是如何做的。</p><h3 id="layer-fusion"><a href="#layer-fusion" class="headerlink" title="layer fusion"></a>layer fusion</h3><p>首先要做的是layer fusion，例如把常见的conv + bn + relu的3个op简化为一个op。relu这种op的fusion不多说，BN的fusion需要考虑权重，参见下个小节。</p><p>卷积实际上还是在进行矩阵乘法。在相乘的时候，用<code>uint8</code>来存储两个操作数，用<code>int32</code>存储结果，以防止相加的时候溢出。如：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">int32 += uint8 * uint8</span><br></pre></td></tr></table></figure><p>对于bias，也使用<code>int32</code>存储。作者在这里指出，使用较高精度存储bias，是很有必要的。因为bias的每个值都要加到对应channel的所有activation上去，所以它的比较小的量化误差也会对结果造成比较大的影响。综合起来，对于bias，我们使用$S = S_1S_2$（和conv的结果的scaling factor相同），且zero-point为$0$。</p><p>如下图所示，当conv / +bias / relu等操作做完之后，再进行量化。最终完成了fusion之后的conv op的计算。</p><p><img src="/img/paper_tf_taq_conv_quantization.png" alt="conv的计算"></p><h2 id="Training-aware-Quantization"><a href="#Training-aware-Quantization" class="headerlink" title="Training-aware Quantization"></a>Training-aware Quantization</h2><p>在训练时，是使用float精度模拟量化模型，并使用float更新梯度。在inference的时候，直接在支持INT8的硬件上跑inference。这时候就是原生的量化模型在前向计算了。如下图所示：</p><p><img src="/img/paper_tf_taq_train_inference.png" alt="train &amp;&amp; inference"></p><p>这里作者首先分析了量化模型相对于原始精度模型可能的掉点原因：</p><ul><li>同一组weight或activation的不同channel之间的差异较大。因为我们上面已经说过，它们会共享同一个scaling factor。所以如果某个channel的weight特别小，就会造成相对误差很大。</li><li>某些离群点(outlier)影响，把整个分布带偏。</li></ul><p>要使用float模拟定点量化模型前向传播，要特别注意量化究竟是在哪里发生的。要注意的是下面两种情况：</p><ul><li>对于weight来说，如果有BN的话，要先把BN和conv的weight做fusion，再量化，再前向计算，这里下面会详细说明一下</li><li>对于activation来说，一般是激活函数之后，还有bypass的相加或concat之后（对ResNet这种结构）</li></ul><h3 id="BN的fusion"><a href="#BN的fusion" class="headerlink" title="BN的fusion"></a>BN的fusion</h3><p>前面提到，BN要和conv fusion到一起。按channel做如下操作即可：</p><script type="math/tex; mode=display">w_{\text{fold}} = \frac{\gamma w}{\sqrt{\sigma^2 + \epsilon}}</script><p>再加上量化，训练时候的整个计算图如下所示：</p><p><img src="/img/paper_tf_taq_conv_bn_folding_in_taining.png" alt="conv/bn folding"></p><h3 id="带饱和的量化"><a href="#带饱和的量化" class="headerlink" title="带饱和的量化"></a>带饱和的量化</h3><p>为了避免outlier的影响，在量化之前要先进行一波饱和操作，然后将值域均匀地映射到定点数表示的范围，例如8bit量化为256个stage。</p><p><img src="/img/tf_paper_taq_quantize_with_clamp.png" alt="quantization_with_clamp"></p><p>不过新的问题出现了。对于weight，可以很容易地找到这样的$a$和$b$，但是对于activation，只有模型跑起来才能知道其范围。文章指出可以使用指数滑动平均来做（就是BN在训练时更新moving_mean和moving_variance的方式），要注意的点在于：</p><ul><li>滑动平均的系数应该定的很接近$1$，有利于变化比较平滑</li><li>开始的若干步训练，可以暂时去掉activation的quantization，有利于网络稳定</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;Google比较早的关于training-aware-quantization的模型量化的paper，不过提供了很多模型量化的基本知识。后面不管是TFLite还是TensorRT，都能在这篇文章中找到对应的基础知识。Arxiv: &lt;a href=&quot;https://arxiv.org/abs/1712.05877&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Quantization and Training of Neural Networks for Efficient Integer-Arithmetic-Only Inference&lt;/a&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
      <category term="model quantization" scheme="https://xmfbit.github.io/tags/model-quantization/"/>
    
  </entry>
  
  <entry>
    <title>论文 - DARTS</title>
    <link href="https://xmfbit.github.io/2020/04/14/paper-darts/"/>
    <id>https://xmfbit.github.io/2020/04/14/paper-darts/</id>
    <published>2020-04-14T21:12:52.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>NAS的文章很多了，这篇介绍DARTS：<a href="https://arxiv.org/abs/1806.09055" target="_blank" rel="noopener">DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH</a></p><p><img src="/img/paper_darts_basic_idea.png" alt="darts的基本思路"></p><a id="more"></a><h1 id="搜索空间"><a href="#搜索空间" class="headerlink" title="搜索空间"></a>搜索空间</h1><p>基本沿用前人工作的基本设定：</p><ul><li>每个cell是由$N$个有序的节点组成的DAG。其中，每个节点$x^{(i)}$表示一个feature map，节点之间的有向弧$E(i,j)$表示由$x^{(i)}$到$x^{(j)}$的某种操作（operation）$o^{(i,j)}$</li><li>每个cell有两个输入，一个输出。两个输入分别是第$i-1$个和$i-2$个cell的输出（假设当前cell是第$i$个）。输出是cell内所有节点应用某种Reduction操作（如concat）得到的</li><li>每个节点的值是由它前面的所有节点决定的：</li></ul><script type="math/tex; mode=display">x^{(j)} = \sum_{i<j}o^{(i,j)}x^{(i)}</script><ul><li>特殊op“Zero”，表示两个节点之间其实没有连接。</li></ul><p>遵循上面的设定，网络结构的搜索，就转换为了搜索节点之间operation的问题。下面，我们就对这个问题建模，将它转换为一个可微分用梯度下降搜索的优化问题。</p><h1 id="松弛"><a href="#松弛" class="headerlink" title="松弛"></a>松弛</h1><p>“松弛”是一种优化中常用的技巧。</p><p>设搜索空间内的所有可能op的集合为$\mathcal{O}$。本来$x^{(i)}$到$x^{(j)}$的op是在这个集合中离散地取值，但是现在我们把它松弛为一个连续问题：</p><script type="math/tex; mode=display">\bar{o} = \sum_{o\in\mathcal{O}}\frac{\exp(\alpha_o)}{\sum_{o^{\prime}\in\mathcal{O}}\exp(\alpha_{o^{\prime}})}o</script><p>其中，$\alpha$是一个长度为$|\mathcal{O}|$的向量，$\alpha_o$是里面对应于操作$o$的权重。</p><p>当搜索过程结束后，选取$\mathcal{O}$中能够使得$\alpha$中分量最大的那个元素$o^\ast$就是最终两个节点的连接op：</p><script type="math/tex; mode=display">o^{\ast} =\underset{o\in\mathcal{O}}{\operatorname{argmax}} \alpha_o</script><p>当然，除了网络结构，我们还需要去学习网络的权重参数$w$。所以整个问题是一个<a href="https://en.wikipedia.org/wiki/Bilevel_optimization" target="_blank" rel="noopener">bi-level</a>的优化问题，$\alpha$是upper-level变量，$w$是lower-level变量。PS：超参数搜索也有相关工作将其建模为bi-level的优化问题求解。</p><p>也就是说，给定某个$\alpha$（也就是某个确定的网络结构），在训练集上得到最优的$w$，并将当前的网络结构和权重在验证集上做评估。那个在验证集上得到最好的结果对应的网络结构，就是我们要找的$\alpha$，而网络的权重$w$也对应得出。</p><p><img src="/img/paper_darts_optimization_goal.png" alt="优化目标"></p><p>也就是说，我们需要最小化模型在验证集上的损失函数；其中，$w$是$\alpha$的某个函数（在这里，$\alpha$是和某个网络结构一一对应的），需要满足训练集上的损失函数最小。</p><h1 id="求解"><a href="#求解" class="headerlink" title="求解"></a>求解</h1><p>我们已经把DARTS抽象成了一个优化问题，下面考虑如何高效求解。</p><p>显然，按照上面的想法，给定网络结构后，在训练集上得到最优的$w$，再去验证集上跑评估，是不现实的。一是搜索空间巨大，耗时太长；二是仍然无法根据当前的$\alpha$，得到下一步该向哪里走，难道仍然要用启发式或诸如进化算法等方法？如果是求导，那这个链路也太长了，根本不现实。这里作者指出，可以用如下的方式近似梯度：</p><p><img src="/img/paper_darts_approximate_gd.png" alt="论文中使用的approximate gradient descent"></p><p>看起来括号里面的内容是把求解$w^\ast$的$N$步迭代只取了一步。</p><script type="math/tex; mode=display">w^\ast = w - \sum_{N}\xi\frac{\partial\mathcal{L}_{\text{train}}(w,\alpha)}{\partial w}|_{w_i}</script><p>为什么能这样近似？似乎没有什么严密的理论支撑，作者对这个“瑕疵”的处理方法是：</p><ul><li>拿出CIFAR10和ImageNet以及PTB等数据集上的结果，证明算法在实际上是可以work的，而且效果很好</li><li>后面给出了一个在简单优化问题上的讨论</li><li>给出了关于超参数设置的经验技巧，最重要的还放出了源码</li></ul><p>这无疑让整个文章的可信度大大增强。</p><p>算法迭代步骤可以描述如下：</p><p><img src="/img/paper_darts_alg_precedure.png" alt="迭代"></p><p>不过上面的算法描述在实际中并不好用，因为$\alpha$的那一坨梯度一看就很不好求。我们可以通过链式求导法则将其展开。</p><p>考虑函数$f(x, g(x))$对$x$的导数$\frac{df}{dx}$。首先令$y = g(x)$，有全微分：</p><script type="math/tex; mode=display">df = \frac{\partial f}{\partial x}dx + \frac{\partial f}{\partial y}dy</script><p>而且，有$dy = \frac{dg}{dx} dx$。所以，</p><script type="math/tex; mode=display">\frac{df}{dx} = \frac{\partial f}{\partial x} + \frac{\partial f}{\partial y} \frac{dg}{dx}</script><p>回归我们的问题，令$x = \alpha$，$w^\prime = g(\alpha) = w - \xi\nabla_w\mathcal{L}_{train}(w, \alpha)$。</p><p>有</p><script type="math/tex; mode=display">\frac{dw^\prime}{d\alpha} = -\xi \nabla^2_{w,\alpha}\mathcal{L}_{train}(w,\alpha)</script><p>将它代回上面$\frac{df}{dx}$，就得到了论文里面的形式：</p><p><img src="/img/paper_darts_apply_chain_rule.png" alt="论文给出的形式"></p><p>化简还没有结束。考虑到$\nabla_w\mathcal{L}_{train}$已经是一个$\mathbb{R}^n$的向量，再对$\alpha$求导，就是一个雅克比矩阵。再和后面那个梯度向量相乘，导致计算量很大。这里作者采用了差分近似微分的方法：</p><p><img src="/img/paper_darts_diff_as_gradient.png" alt="差分近似微分"></p><p>下面是作者的具体计算代码：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 更新 \alpha</span></span><br><span class="line">architect.step(input, target, input_search, target_search, lr, optimizer, unrolled=args.unrolled)</span><br><span class="line"><span class="comment"># 更新w</span></span><br><span class="line">optimizer.zero_grad()</span><br></pre></td></tr></table></figure><p>下面进入<code>Architect</code>类的内部看下<code>step</code>的实现。当令$\xi=0$时，$w$不用前进一步，<code>architect.step</code>比较简单（对应于<code>unrolled=False</code>）:</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_backward_step</span><span class="params">(self, input_valid, target_valid)</span>:</span></span><br><span class="line">  <span class="comment"># loss = L_val(w, alpha)</span></span><br><span class="line">  loss = self.model._loss(input_valid, target_valid)</span><br><span class="line">  loss.backward()</span><br></pre></td></tr></table></figure><p>当$\xi\neq 0$时，$w$要在train集合上前进一步，对应于<code>unrolled=True</code>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_backward_step_unrolled</span><span class="params">(self, input_train, target_train, input_valid, target_valid, eta, network_optimizer)</span>:</span></span><br><span class="line">  <span class="comment"># 在train上更新w = w - \xi * dL_train(w, alpha) / dw</span></span><br><span class="line">  unrolled_model = self._compute_unrolled_model(input_train, target_train, eta, network_optimizer)</span><br><span class="line">  <span class="comment"># 在target上计算loss，然后对alpha求导</span></span><br><span class="line">  unrolled_loss = unrolled_model._loss(input_valid, target_valid)</span><br><span class="line"></span><br><span class="line">  unrolled_loss.backward()</span><br><span class="line">  dalpha = [v.grad <span class="keyword">for</span> v <span class="keyword">in</span> unrolled_model.arch_parameters()]</span><br><span class="line">  vector = [v.grad.data <span class="keyword">for</span> v <span class="keyword">in</span> unrolled_model.parameters()]</span><br><span class="line">  <span class="comment"># 就是那个差分替代微分的式子</span></span><br><span class="line">  implicit_grads = self._hessian_vector_product(vector, input_train, target_train)</span><br><span class="line">  </span><br><span class="line">  <span class="keyword">for</span> g, ig <span class="keyword">in</span> zip(dalpha, implicit_grads):</span><br><span class="line">    g.data.sub_(eta, ig.data)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 更新alpha</span></span><br><span class="line">  <span class="keyword">for</span> v, g <span class="keyword">in</span> zip(self.model.arch_parameters(), dalpha):</span><br><span class="line">    <span class="keyword">if</span> v.grad <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">      v.grad = Variable(g.data)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      v.grad.data.copy_(g.data)</span><br></pre></td></tr></table></figure><p>差分的计算具体是这里。注意到作者提到了两个超参数的经验值：</p><p><img src="/img/paper_darts_hyper_param_exp_value.png" alt="经验值设置"></p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_hessian_vector_product</span><span class="params">(self, vector, input, target, r=<span class="number">1e-2</span>)</span>:</span></span><br><span class="line">  <span class="comment"># R = \epsilon，按照上面的经验值公式求取</span></span><br><span class="line">  R = r / _concat(vector).norm()</span><br><span class="line">  <span class="comment"># 这是前面那一项</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.add_(R, v)</span><br><span class="line">  loss = self.model._loss(input, target)</span><br><span class="line">  grads_p = torch.autograd.grad(loss, self.model.arch_parameters())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 这是后面那一项</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.sub_(<span class="number">2</span>*R, v)</span><br><span class="line">  loss = self.model._loss(input, target)</span><br><span class="line">  grads_n = torch.autograd.grad(loss, self.model.arch_parameters())</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 别忘把w复原</span></span><br><span class="line">  <span class="keyword">for</span> p, v <span class="keyword">in</span> zip(self.model.parameters(), vector):</span><br><span class="line">    p.data.add_(R, v)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># 最后的近似结果</span></span><br><span class="line">  <span class="keyword">return</span> [(x-y).div_(<span class="number">2</span>*R) <span class="keyword">for</span> x, y <span class="keyword">in</span> zip(grads_p, grads_n)]</span><br></pre></td></tr></table></figure><h1 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h1><h2 id="算法收敛的讨论"><a href="#算法收敛的讨论" class="headerlink" title="算法收敛的讨论"></a>算法收敛的讨论</h2><p>这里并没有数学上的证明保证方法的收敛性。作者也没有回避这一点，并指出，\xi 的选取对于是否收敛很重要。总之，实验效果很好，说明这个近似是可以work的。</p><blockquote><p>While we are not currently aware of the convergence guarantees for our optimization algorithm, in practice it is able to reach a fixed point with a suitable choice of ξ</p></blockquote><p>作者对其做在简单问题下的收敛做了讨论。</p><p><img src="/img/paper_darts_convergence_discussion.png" alt="简单问题上的讨论"></p><h2 id="CNN-CIFAR10"><a href="#CNN-CIFAR10" class="headerlink" title="CNN @ CIFAR10"></a>CNN @ CIFAR10</h2><h3 id="operation-set"><a href="#operation-set" class="headerlink" title="operation set"></a>operation set</h3><p>选择$3\times 3$，$5\times 5$的kernel size大小的分离卷积和pooling等，再加上identity和zero。具体可以参考代码：<a href="https://github.com/quark0/darts/blob/master/cnn/operations.py" target="_blank" rel="noopener">darts/cnn/operations.py</a>。例如，$3\times 3$的分离卷积如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 给定input channel和stride，生成3x3分离卷积</span></span><br><span class="line"><span class="comment"># 'sep_conv_3x3' : lambda C, stride, affine: SepConv(C, C, 3, stride, 1, affine=affine),</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 可以看到是如下更小op的串联：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># relu -&gt; 3x3 seperable conv -&gt; bn -&gt; relu -&gt; 3x3 seperable conv(stride=1) -&gt; bn</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">SepConv</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, C_in, C_out, kernel_size, stride, padding, affine=True)</span>:</span></span><br><span class="line">    super(SepConv, self).__init__()</span><br><span class="line">    self.op = nn.Sequential(</span><br><span class="line">      nn.ReLU(inplace=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=stride, padding=padding, groups=C_in, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.BatchNorm2d(C_in, affine=affine),</span><br><span class="line">      nn.ReLU(inplace=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_in, kernel_size=kernel_size, stride=<span class="number">1</span>, padding=padding, groups=C_in, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.Conv2d(C_in, C_out, kernel_size=<span class="number">1</span>, padding=<span class="number">0</span>, bias=<span class="literal">False</span>),</span><br><span class="line">      nn.BatchNorm2d(C_out, affine=affine),</span><br><span class="line">      )</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.op(x)</span><br></pre></td></tr></table></figure><p>主要特点是：</p><ul><li>顺序为relu -&gt; conv -&gt; bn</li><li>可分离卷积重复两次</li></ul><p>这也是前面NAS文章的惯常操作。</p><h3 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h3><p>在Cell尺度上，每个cell由$N=7$个node组成。输入节点如前所述（有时候可能需要$1\times 1$节点），输出节点是该cell的所有中间节点（不包括input节点）在channel上的concat。</p><p>在网络宏观尺度上，cell堆叠形成最后的网络。Cell也被分为<code>normal</code>和<code>reduce</code>两种。后者会对输入节点取stride为$2$，从而downsampling。在网络的$1/3$和$2/3$深度处为reduce cell，其他为normal cell。normal和reduce cell分别有一套共享的$\alpha$参数。从而，整个网络的结构可以被两组$\alpha_{\text{normal}}$和$\alpha_{\text{reduce}}$完全描述。</p><p>下图直观地展示了在CIFAR10上搜索出来的cell结构：</p><ul><li>两个输入，一个输出，四个中间节点，它们通过concat操作成了输出</li><li>每个中间节点入度都是2，也就是我们选取的是$\alpha$中top $K=2$的op</li></ul><p><img src="/img/paper_darts_net_arch_on_cifar10.png" alt="DARTS在CIFAR10上搜出的cell"></p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在CIFAR10上，搜出的网络性能和之前基于RL或进化算法的SOTA方法是可比的，而且GPU小时数明显缩短。DARTS方法和ENAS是少数能够在&lt;10 GPU*days的计算资源下做出比较好结果的方法。其中DARTS又比ENAS有较好的TestError。作者对此也做了说明：</p><blockquote><p>DARTS outperformed ENAS (Pham et al., 2018b) by discovering cells with comparable error rates but lessparameters. The longer search time is due to the fact that we have repeated the search process fourtimes for cell selection. This practice is less important for convolutional cells however, because theperformance of discovered architectures does not strongly depend on initialization</p></blockquote><p><img src="/img/paper_darts_sota_comparision_cnn.png" alt="CNN搜索与SOTA比较"></p><h2 id="ImageNet"><a href="#ImageNet" class="headerlink" title="ImageNet"></a>ImageNet</h2><p><img src="/img/paper_darts_result_imagenet.png" alt="在ImageNet上的结果"></p><h2 id="讨论"><a href="#讨论" class="headerlink" title="讨论"></a>讨论</h2><p>文章的主要工作：</p><ul><li>将NAS问题通过松弛建模为一个关于模型结构的优化问题</li><li>提出了一个不错的解决该优化问题的梯度下降解法</li><li>在相关任务上证明了方法的有效性</li><li>给大家在堆GPU资源用强化学习 / 进化算法之外，指出了一条可行的NAS求解之路</li></ul><p>文章的作者应该是有比较多的数学优化方面的知识。引入权重向量并softmax求取top K op应该是还算让人容易想到，但后面的优化求解就很容易出错劝退。</p><h1 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h1><ul><li>PyTorch实现的DARTS：<a href="https://github.com/quark0/darts" target="_blank" rel="noopener">quark0/darts</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;NAS的文章很多了，这篇介绍DARTS：&lt;a href=&quot;https://arxiv.org/abs/1806.09055&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;DARTS: DIFFERENTIABLE ARCHITECTURE SEARCH&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper_darts_basic_idea.png&quot; alt=&quot;darts的基本思路&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="nas" scheme="https://xmfbit.github.io/tags/nas/"/>
    
  </entry>
  
  <entry>
    <title>MIT Missing Semester - Command-line Environment</title>
    <link href="https://xmfbit.github.io/2020/04/06/mit-missing-semester-05-commandline-env/"/>
    <id>https://xmfbit.github.io/2020/04/06/mit-missing-semester-05-commandline-env/</id>
    <published>2020-04-06T18:52:29.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>这是<a href="https://missing.csail.mit.edu/2020/command-line/" target="_blank" rel="noopener">MIT Missing Semester系列</a>的第五讲，主要关于shell下对于进程（Process)的控制。</p><a id="more"></a><h1 id="Job-Control"><a href="#Job-Control" class="headerlink" title="Job Control"></a>Job Control</h1><h2 id="Kill-Process"><a href="#Kill-Process" class="headerlink" title="Kill Process"></a>Kill Process</h2><p>最简单的，要想结束当前正在进行的进程，使用<code>Ctrl+c</code>即可。而实际上，这是通过向其发送了<code>SIGINT</code>（INT是interrupt的意思）的信号量实现的。例如，执行如下脚本时候，如果按下<code>Ctrl+c</code>，并不会退出，而只是会执行<code>handler</code>函数。更多信号量的文档可以参考<code>man signal</code>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="keyword">import</span> signal, time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">handler</span><span class="params">(signum, time)</span>:</span></span><br><span class="line">    <span class="comment"># 中断会进这里</span></span><br><span class="line">    print(<span class="string">"\nI got a SIGINT, but I am not stopping"</span>)</span><br><span class="line"></span><br><span class="line">signal.signal(signal.SIGINT, handler)</span><br><span class="line">i = <span class="number">0</span></span><br><span class="line"><span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">    time.sleep(<span class="number">.1</span>)</span><br><span class="line">    print(<span class="string">"\r&#123;&#125;"</span>.format(i), end=<span class="string">""</span>)</span><br><span class="line">    i += <span class="number">1</span></span><br></pre></td></tr></table></figure><h2 id="休眠-suspend"><a href="#休眠-suspend" class="headerlink" title="休眠 (suspend)"></a>休眠 (suspend)</h2><p>使用<code>Ctrl-z</code>发送<code>SIGSTOP</code>信号可以使得进程暂时休眠，并可以后续继续运行。我们可以用下面的代码做实验。在<code>Ctrl+z</code>后，程序暂时休眠停止运行，计数器的值也不再更新，直到使用<code>fg</code>命令，才重新开始。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 每隔1s打印当前计数器的值</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> itertools <span class="keyword">import</span> count</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> count(<span class="number">0</span>):</span><br><span class="line">        print(<span class="string">'current time: &#123;&#125;'</span>.format(i))</span><br><span class="line">        time.sleep(<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></table></figure><p><img src="/img/mit_missing_semester_05_example_suspend_process.png" alt="运行实例"></p><p>除了<code>fg</code>以外，还可以使用<code>bg</code>来开始被休眠的进程。只不过<code>bg</code>会让重新开始的进程在后台运行。</p><p>这里直接贴出讲师的一个例子：</p><p><img src="/img/mit_missing_semester_05_example_job_control.png" alt="job control"></p><p>需要注意的是：</p><ul><li><code>bg</code>的使用：将进程状态从suspending转到running</li><li><code>jobs</code>可以列出当前所有未完成的进程</li><li>可以使用<code>%number</code>的形式引用<code>jobs</code>列出的进程</li><li>除了快捷键，也可以使用<code>kill</code>向指定进程发送信号量，具体可以参考<code>man kill</code></li></ul><h1 id="SSH"><a href="#SSH" class="headerlink" title="SSH"></a>SSH</h1><p>远程ssh到开发机是常见的操作，这里有一些比较零散的知识点记录。</p><ul><li>使用<code>ssh-copy-id $host_name</code>将当前机器的ssh public key拷贝到给定的远程host机器（之前我都是通过手动copy）</li><li>将文件拷贝到远程机器的N种方法：<ul><li>使用<code>ssh + tee</code>：<code>cat local_file | ssh remote_server tee server_file</code>，这是利用了<code>ssh remote_server</code>可以后接shell命令</li><li>使用<code>scp</code>，这也是我最常用的命令了</li><li>使用<code>rsync</code>，更强大的<code>scp</code>，可以跳过重复文件等</li></ul></li><li>端口转发，例如在远程服务器的<code>8888</code>端口启动了jupyter notebook，可以使用<code>ssh -L 9999:localhost:8888 foobar@remote_server</code>将其转发到本机的<code>9999</code>端口，这样在本机浏览<code>localhost:9999</code>即可访问笔记本</li></ul><p><img src="/img/mit_missing_semester_05_local_port_forward.png" alt="local port forwarding"><br><img src="/img/mit_missing_semester_05_remote_port_forward.png" alt="remote port forwarding"></p><h1 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h1><h2 id="创建alias"><a href="#创建alias" class="headerlink" title="创建alias"></a>创建alias</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">alias</span> dc=<span class="string">"cd"</span></span><br><span class="line"><span class="comment"># 列出最常用的10个命令</span></span><br><span class="line"><span class="comment">## 将第一列（表示序号）设为空字符串</span></span><br><span class="line"><span class="built_in">history</span> | awk <span class="string">'&#123;$1="";print substr($0,2)&#125;'</span> | sort | uniq -c | sort -n | tail -n 10</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是&lt;a href=&quot;https://missing.csail.mit.edu/2020/command-line/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MIT Missing Semester系列&lt;/a&gt;的第五讲，主要关于shell下对于进程（Process)的控制。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>TFRecord 简介</title>
    <link href="https://xmfbit.github.io/2020/04/03/tfrecord-introduction/"/>
    <id>https://xmfbit.github.io/2020/04/03/tfrecord-introduction/</id>
    <published>2020-04-03T23:36:29.000Z</published>
    <updated>2020-05-02T07:17:54.456Z</updated>
    
    <content type="html"><![CDATA[<p>TFRecord是TensorFlow中常用的数据打包格式。通过将训练数据或测试数据打包成TFRecord文件，就可以配合TF中相关的DataLoader / Transformer等API实现数据的加载和处理，便于高效地训练和评估模型。</p><p>TF官方tutorial：<a href="https://www.tensorflow.org/tutorials/load_data/tfrecord" target="_blank" rel="noopener">TFRecord and tf.Example</a></p><p><img src="/img/tfrecord_logo.jpeg" alt="TFRecord好！"></p><a id="more"></a><h1 id="组成TFReocrd的砖石：tf-Example"><a href="#组成TFReocrd的砖石：tf-Example" class="headerlink" title="组成TFReocrd的砖石：tf.Example"></a>组成TFReocrd的砖石：<code>tf.Example</code></h1><p><code>tf.Example</code>是一个Protobuffer定义的message，表达了一组string到bytes value的映射。TFRecord文件里面其实就是存储的序列化的<code>tf.Example</code>。如果对Protobuffer不熟悉，可以去看下Google的<a href="https://developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="noopener">文档</a>和<a href="https://developers.google.com/protocol-buffers/docs/pythontutorial" target="_blank" rel="noopener">教程</a>。</p><h2 id="Example-是什么"><a href="#Example-是什么" class="headerlink" title="Example 是什么"></a>Example 是什么</h2><p>我们可以具体到相关代码去详细地看下<code>tf.Example</code>的构成。作为一个Protobuffer message，它被定义在文件<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/core/example/example.proto#L88" target="_blank" rel="noopener">core/example/example.proto</a>中：</p><figure class="highlight protobuf"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">message</span> <span class="title">Example</span> </span>&#123;</span><br><span class="line">  Features features = <span class="number">1</span>;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>好吧，原来只是包了一层<code>Features</code>的message。我们还需要进一步去查找<code>Features</code>的message<a href="https://github.com/tensorflow/tensorflow/blob/v2.1.0/tensorflow/core/example/feature.proto#L85-L88" target="_blank" rel="noopener">定义</a>：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">message Features &#123;</span><br><span class="line">  // Map from feature name to feature.</span><br><span class="line">  map&lt;string, Feature&gt; feature = 1;</span><br><span class="line">&#125;;</span><br></pre></td></tr></table></figure><p>到这里，我们可以看出，<code>tf.Example</code>确实表达了一组string到Feature的映射。其中，这个string表示feature name，后面的Feature又是一个message。继续寻找：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">// Containers for non-sequential data.</span><br><span class="line">message Feature &#123;</span><br><span class="line">  // Each feature can be exactly one kind.</span><br><span class="line">  oneof kind &#123;</span><br><span class="line">    BytesList bytes_list = 1;</span><br><span class="line">    FloatList float_list = 2;</span><br><span class="line">    Int64List int64_list = 3;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;;</span><br><span class="line"></span><br><span class="line">// 这里摘一个 Int64List 的定义如下，float/bytes同理</span><br><span class="line">message Int64List &#123;</span><br><span class="line">  // 可以看到，如其名所示，表示的是int64数值的列表</span><br><span class="line">  repeated int64 value = 1 [packed = true];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>看起来，是描述了一组各种数据类型的list，包括二进制字节流，float或者int64的数值列表。</p><h2 id="属于自己的Example"><a href="#属于自己的Example" class="headerlink" title="属于自己的Example"></a>属于自己的Example</h2><p>有了上面的分解，要想构造自己数据集的<code>tf.Example</code>，就可以一步步组合起来。</p><p>首先用下面的几个帮助函数，将给定的Python类型数据转换为对应的Feature。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># The following functions can be used to convert a value to a type compatible</span></span><br><span class="line"><span class="comment"># with tf.Example.</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_bytes_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a bytes_list from a string / byte."""</span></span><br><span class="line">  <span class="keyword">if</span> isinstance(value, type(tf.constant(<span class="number">0</span>))):</span><br><span class="line">    value = value.numpy() <span class="comment"># BytesList won't unpack a string from an EagerTensor.</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这里我们直接认为value是个标量，如果是tf.Tensor，可以使用</span></span><br><span class="line"><span class="comment"># `tf.io.serialize_tensor`将其序列化为bytes</span></span><br><span class="line"><span class="comment"># `tf.io.parse_tensor`可以反序列化为tf.Tensor</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_float_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns a float_list from a float / double."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(float_list=tf.train.FloatList(value=[value]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">_int64_feature</span><span class="params">(value)</span>:</span></span><br><span class="line">  <span class="string">"""Returns an int64_list from a bool / enum / int / uint."""</span></span><br><span class="line">  <span class="keyword">return</span> tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))</span><br></pre></td></tr></table></figure><p>有了<code>Feature</code>，就可以组成<code>Features</code>，只要把对应的名字作为string传进去就行了。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">features_dict = &#123;<span class="string">'image'</span>: _bytes_feature(image_data), <span class="string">'label'</span>: _int64_feature(label)&#125;</span><br><span class="line">features = tf.train.Features(feature=features_dict)</span><br></pre></td></tr></table></figure><p><code>Example</code>自然也就有了：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">example = tf.train.Example(features=features)</span><br></pre></td></tr></table></figure><h1 id="TFRecord"><a href="#TFRecord" class="headerlink" title="TFRecord"></a>TFRecord</h1><p>TFRecord是一个二进制文件，只能顺序读取。它的数据打包格式如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">uint64 length</span><br><span class="line">uint32 masked_crc32_of_length</span><br><span class="line">byte   data[length]</span><br><span class="line">uint32 masked_crc32_of_data</span><br></pre></td></tr></table></figure><p>其中，<code>data[length]</code>通常是一个<code>Example</code>序列化之后的数据。</p><h2 id="将Example写入TFRecord"><a href="#将Example写入TFRecord" class="headerlink" title="将Example写入TFRecord"></a>将<code>Example</code>写入TFRecord</h2><p>可以使用python API，将<code>Example</code>proto写入TFRecord文件。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> tf.io.TFRecordWriter(filename) <span class="keyword">as</span> writer:</span><br><span class="line">    <span class="keyword">for</span> image_file <span class="keyword">in</span> image_files: </span><br><span class="line">        image_data = open(image_file, <span class="string">'rb'</span>).read()</span><br><span class="line">        features = tf.train.Features(feature=&#123;<span class="string">'image'</span>: _bytes_feature(image_Data)&#125;)</span><br><span class="line">        <span class="comment"># 得到 example</span></span><br><span class="line">        example = tf.train.Example(features=features)</span><br><span class="line">        <span class="comment"># 通过调用message.SerializeToString() 将其序列化</span></span><br><span class="line">        writer.write(example.SerializeToString())</span><br></pre></td></tr></table></figure><h2 id="读取TFRecord中的Example"><a href="#读取TFRecord中的Example" class="headerlink" title="读取TFRecord中的Example"></a>读取TFRecord中的<code>Example</code></h2><p>通过<code>tf.data.TFRecordDataset</code>得到<code>Dataset</code>，然后遍历它，并反序列化，就可以得到原始数据。下面的代码段从TFRecord文件中读取刚刚写入的image：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">parse_from_single_example</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="string">""" 从example message反序列化得到当初写入的内容 """</span></span><br><span class="line">    <span class="comment"># 描述features</span></span><br><span class="line">    desc = &#123;<span class="string">'image'</span>: tf.io.FixedLenFeature([], dtype=tf.string)&#125;</span><br><span class="line">    <span class="comment"># 使用tf.io.parse_single_example反序列化</span></span><br><span class="line">    <span class="keyword">return</span> tf.io.parse_single_example(example_proto, desc)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">decode_image_from_bytes</span><span class="params">(image_data)</span>:</span></span><br><span class="line">    <span class="string">""" use cv2.imdecode decode image from raw binary data """</span></span><br><span class="line">    bytes_array = np.array(bytearray(image_data))</span><br><span class="line">    <span class="keyword">return</span> cv2.imdecode(bytes_array, cv2.IMREAD_COLOR)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_image_from_single_example</span><span class="params">(example_proto)</span>:</span></span><br><span class="line">    <span class="string">""" get image fom example serialized data """</span></span><br><span class="line">    data = parse_from_single_example(example_proto)</span><br><span class="line">    image_data = data[<span class="string">'image'</span>].numpy()</span><br><span class="line">    <span class="comment"># the image_data is str</span></span><br><span class="line">    <span class="comment"># decode the binary bytes to get the image</span></span><br><span class="line">    <span class="keyword">return</span> decode_image_from_bytes(image_data)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">dataset = tf.data.TFRecordDataset(tfrecord_file)</span><br><span class="line">data_iter = iter(dataset)</span><br><span class="line">first_example = next(data_iter)</span><br><span class="line"></span><br><span class="line">first_image = get_image_from_single_example(first_example)</span><br></pre></td></tr></table></figure><p>或者可以用<code>map</code>来将parser的pipeline应用于原dataset：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 注意这里不能用get_image_from_single_example</span></span><br><span class="line"><span class="comment"># 因为 `.numpy()` 不能用于静态 Map</span></span><br><span class="line">image_data = dataset.map(parse_from_single_example)</span><br><span class="line"></span><br><span class="line">first_image_data = next(iter(image_data))</span><br><span class="line">image = decode_image_from_bytes(first_image_data[<span class="string">'image'</span>].numpy())</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TFRecord是TensorFlow中常用的数据打包格式。通过将训练数据或测试数据打包成TFRecord文件，就可以配合TF中相关的DataLoader / Transformer等API实现数据的加载和处理，便于高效地训练和评估模型。&lt;/p&gt;
&lt;p&gt;TF官方tutorial：&lt;a href=&quot;https://www.tensorflow.org/tutorials/load_data/tfrecord&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;TFRecord and tf.Example&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/tfrecord_logo.jpeg&quot; alt=&quot;TFRecord好！&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="tf" scheme="https://xmfbit.github.io/tags/tf/"/>
    
  </entry>
  
  <entry>
    <title>使用 travis 发布博客</title>
    <link href="https://xmfbit.github.io/2020/03/21/publish-blog-with-travis/"/>
    <id>https://xmfbit.github.io/2020/03/21/publish-blog-with-travis/</id>
    <published>2020-03-21T14:51:31.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>我的博客之前是通过手动调用<code>hexo generate</code>生成<code>public</code>，并更新到<code>master</code>分支的。最近试了下使用travis直接发布，发现省事了不少。官方的文档其实还是挺全的，不过也碰到了一些坑，记录在这里。</p><p><img src="/img/travis_logo.png" alt="travis logo"><br><a id="more"></a></p><h2 id="Travis-简介"><a href="#Travis-简介" class="headerlink" title="Travis 简介"></a>Travis 简介</h2><p>Travis（/‘trævis/）是一款CI（持续集成）工具。<a href="https://www.zhihu.com/question/23444990" target="_blank" rel="noopener">这里</a>有一篇关于持续集成的知乎问答。</p><blockquote><p>持续集成强调开发人员提交了新代码之后，立刻进行构建、（单元）测试。根据测试结果，我们可以确定新代码和原有代码能否正确地集成在一起。</p></blockquote><p>在公司内团队内开发某个项目的时候，我们常常也会使用jenkins等工具作为CI的工具。比如当某位同学试图向<code>master</code>分支merge代码时，就会触发测试。机器人会在相关MR下评论，通知build和test的结果。</p><p><img src="/img/what_is_ci.jpg" alt="什么是持续集成"></p><p>在GitHub的很多项目中，都有CI的身影。如下图所示，在caffe项目的README页面中就显示了该项目目前的CI状态。至于如何在项目中添加这个功能，可以参考<a href="https://developer.github.com/v3/repos/statuses/" target="_blank" rel="noopener">这个页面</a>，这里暂时不多说。</p><p><img src="/img/caffe_build_status.png" alt="caffe example"></p><p>而如果我们想在Github自己的项目中使用CI，就可以考虑<a href="https://travis-ci.org/" target="_blank" rel="noopener">Travis</a>。</p><p>上面介绍了什么是CI以及travis可以帮助我们进行CI。那为什么可以使用这个功能发布博客吗？因为我们的博客本身是一个依托于Github page功能的静态网站。首先我们有了<code>username.github.io</code>这个repo，然后在其<code>master</code>分支下放置了hexo生成的静态HTML，就可以看到博客了。想一下之前发布博客的步骤：</p><ul><li>编写内容</li><li>使用<code>hexo generate</code>生成HTML等（会放在一个<code>public/</code>文件夹下）</li><li>将<code>public/</code>发布到repo的master分支下</li></ul><p>现在我们就可以把后两步使用travis完成。当我们编写好内容后，将其推送到repo的非master分支，并触发CI的构建，就可以自动完成后两步。</p><h2 id="配置-Travis"><a href="#配置-Travis" class="headerlink" title="配置 Travis"></a>配置 Travis</h2><p>阮一峰的博客里面有一篇<a href="http://www.ruanyifeng.com/blog/2017/12/travis_ci_tutorial.html" target="_blank" rel="noopener">travis教程</a>可以参考。我们这里因为只是一个比较简单的博客发布功能，所以不再展开。</p><p>如果你还没有自己的博客，可以去搜索如何使用hexo搭建博客系统，首先确保本地能够跑起来，成功访问自己的博客页面。如果已经有了博客，可以参考<a href="https://github.com/hexojs/hexo-starter" target="_blank" rel="noopener">这个repo</a>，整理下自己的目录结构，尤其是<code>.gitignore</code>。注意，这个包含原始网页内容的分支不能是<code>master</code>。你可以建立一个叫<code>hexo</code>的分支来做这件事。</p><p>接下来，在你的账户内添加<code>Travis CI</code>：<a href="https://github.com/marketplace/travis-ci" target="_blank" rel="noopener">Travis CI</a>。并在<a href="https://github.com/settings/installations" target="_blank" rel="noopener">Applications settings</a>页面确认Travis可以访问你的repo。这时候应该会重定向到Travis的页面。</p><p>打开新窗口，去往<a href="https://github.com/settings/tokens" target="_blank" rel="noopener">Github token</a>生成new token。</p><p>在Travis中，找到repo setting，并在<code>Environment Variables</code>中，设置name为<code>GH_TOKEN</code>，并将上面的token加入。</p><p>在你的repo中，checkout到<code>hexo</code>分支。并添加<code>.travis.yml</code>文件，内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">sudo: false</span><br><span class="line">language: node_js</span><br><span class="line">node_js:</span><br><span class="line">  - 10 # use nodejs v10 LTS</span><br><span class="line">cache: npm</span><br><span class="line">branches:</span><br><span class="line">  only:</span><br><span class="line">    - hexo # build hexo branch only</span><br><span class="line">script:</span><br><span class="line">  - hexo generate # generate static files</span><br><span class="line">deploy:</span><br><span class="line">  provider: pages</span><br><span class="line">  skip-cleanup: true</span><br><span class="line">  github-token: $GH_TOKEN</span><br><span class="line">  keep-history: true</span><br><span class="line">  on:</span><br><span class="line">    branch: hexo</span><br><span class="line">  target_branch: master  # 这行很重要</span><br><span class="line">  local-dir: public</span><br></pre></td></tr></table></figure><p>注意上面一定要设置<code>target_branch</code>一项，因为我们需要生成的内容写入<code>master</code>分支。关于这些选项的含义，<a href="https://docs.travis-ci.com/user/deployment/pages/" target="_blank" rel="noopener">Travis</a>有相关介绍。不过我是看的<a href="https://bookdown.org/yihui/blogdown/travis-github.html" target="_blank" rel="noopener">这个</a>。这里面的解释更加针对博客部署的场景，建议读一下。</p><p>接下来，我们往<code>hexo</code>分支上推送内容，就会触发CI并生成网页了！</p><h2 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h2><p>你可以前往<a href="https://travis-ci.com/dashboard" target="_blank" rel="noopener">Travis Dashboard</a>查看自己项目的构建情况。</p><p><img src="/img/travis_pane.png" alt="travis dashboard"></p><p>另外，如果你在hexo中也用了<code>landscape</code>主题，可能会报fail。解决方法很粗暴，直接删除这个文件就行了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rm themes/landscape/README.md</span><br></pre></td></tr></table></figure><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ul><li><a href="https://hexo.io/docs/github-pages" target="_blank" rel="noopener">官方教程</a></li></ul><p>上面的官方教程已经说的比较详细了，但是有坑，就是需要设置<code>target_branch</code>没有说明。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;我的博客之前是通过手动调用&lt;code&gt;hexo generate&lt;/code&gt;生成&lt;code&gt;public&lt;/code&gt;，并更新到&lt;code&gt;master&lt;/code&gt;分支的。最近试了下使用travis直接发布，发现省事了不少。官方的文档其实还是挺全的，不过也碰到了一些坑，记录在这里。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/travis_logo.png&quot; alt=&quot;travis logo&quot;&gt;&lt;br&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
      <category term="travis" scheme="https://xmfbit.github.io/tags/travis/"/>
    
  </entry>
  
  <entry>
    <title>MIT Missing Semester - Data Wrangling</title>
    <link href="https://xmfbit.github.io/2020/03/15/mit-missing-semester-04-data-wrangling/"/>
    <id>https://xmfbit.github.io/2020/03/15/mit-missing-semester-04-data-wrangling/</id>
    <published>2020-03-15T21:47:18.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>这是<a href="https://missing.csail.mit.edu/2020/data-wrangling/" target="_blank" rel="noopener">MIT Missing Semester系列</a>的第四讲。关于vim的第三讲跳过。Data Wrangling在这里的意思是对数据做变换（Transformation）。例如将一个MP4格式的视频转换为AVI，或或者是从日志中提取所需要的结构化文本信息。具体到本课，主要是处理文本信息：如何匹配到我们感兴趣的信息，如果构建一个处理的pipeline等。</p><a id="more"></a><p>`</p><h2 id="正则表达式"><a href="#正则表达式" class="headerlink" title="正则表达式"></a>正则表达式</h2><p>在很久以前，总结了一篇关于python中的正则表达式的常用用法，竟然也是博客的第一篇文章：<a href="https://xmfbit.github.io/2014/07/17/python-reg-exp/">python正则表达式</a>。</p><p>推荐一个<a href="https://regexone.com/" target="_blank" rel="noopener">交互式的正则表达式学习网站</a>。这里有一些简单的规则：</p><ul><li><code>.</code>匹配任意字符，除了<code>\n</code></li><li><code>*</code>匹配前缀的任意个，包括0个</li><li><code>+</code>匹配前缀的任意个，不包括0个</li><li><code>?</code>匹配前缀的0个或1个</li><li><code>[abc]</code>匹配给定集合里面的元素，例如这里匹配<code>a</code>或<code>b</code>或<code>c</code></li><li><code>(ab)</code>匹配给定的组合，例如这里匹配<code>ab</code></li><li><code>(exp1|exp2)</code>匹配<code>exp1</code>或<code>exp2</code></li><li><code>^</code>指示一行的开头</li><li><code>$</code>指示一行的结尾</li></ul><p>要注意的是，<code>(</code>在下面的sed中如果没有特殊说明，被视为普通字符，需要加上<code>-E</code>选项。</p><p>如果我们想要指定具体的次数呢？可以使用<code>.{n}</code>的形式。例如，<code>a{3}</code>表明匹配3个<code>a</code>；<code>[ab]{4}</code>匹配4个<code>a</code>或<code>b</code>。使用range表达式，表明在某个范围内：<code>.{2,5}</code>表示2到5个任意字符。</p><h2 id="sed"><a href="#sed" class="headerlink" title="sed"></a>sed</h2><p>sed(Stream Editor)可以帮助我们变换文本。sed每次从输入流中读入一行，作相应变换，并输出。</p><blockquote><p>A stream editor is used to perform basic text transformations on an input stream (a file or input from a pipeline)</p></blockquote><p><img src="/img/sed_workflow.png" alt="sed workflow"></p><p><a href="https://www.tutorialspoint.com/sed/index.htm" target="_blank" rel="noopener">这里</a>是一个sed的教程，下面结合该教程和讲师的实例，对sed使用做一说明。</p><h3 id="sed的其他用法"><a href="#sed的其他用法" class="headerlink" title="sed的其他用法"></a>sed的其他用法</h3><p>这里首先对sed的其他用法做一说明。</p><p>使用<code>-e</code>可以传入一些命令，例如<code>1d</code>就是删除第一行。通过串联，可以删除多行。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 删除第一行和第五行</span></span><br><span class="line">sed -e <span class="string">'1d'</span> -e `5d` input</span><br></pre></td></tr></table></figure><p>还可以使用<code>-f</code>指示从某个文件内读取命令，</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1d\n2d"</span> &gt; arg.txt</span><br><span class="line">sed -f arg.txt input</span><br></pre></td></tr></table></figure><h3 id="sed对文本进行查找替换"><a href="#sed对文本进行查找替换" class="headerlink" title="sed对文本进行查找替换"></a>sed对文本进行查找替换</h3><p>sed最常用的场景之一，使用如下命令，将文本文件中的<code>pattern</code>（一个正则表达式）替换为<code>new</code>。当<code>new</code>为空时，将直接删去<code>pattern</code>。最后的<code>g</code>如果不加，则只匹配一次，加上<code>g</code>表示全局。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sed <span class="string">'s/pattern/new/ filename/g'</span></span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># .* 表示任意多个任意字符，包括0个。所以下面会把 says hello以及它前面的内容都删掉</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to dog"</span> | sed <span class="string">'s/.*says hello//'</span></span><br><span class="line"><span class="comment"># output: to dog</span></span><br></pre></td></tr></table></figure><p>注意，<code>.*</code>组合是greedy的。这意味着它会尽可能多地去匹配任意字符。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to says hello to dog"</span> | sed <span class="string">'s/.*says hello//'</span></span><br><span class="line"><span class="comment">#output: to dog</span></span><br></pre></td></tr></table></figure><h3 id="capture-group"><a href="#capture-group" class="headerlink" title="capture group"></a>capture group</h3><p>capture group是指我希望记住匹配到的值。在正则表达式中，使用<code>()</code>括起来的就是capture group。我们可以使用<code>\1</code>，<code>\2</code>来引用它们。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 想知道cat对谁打招呼了</span></span><br><span class="line"><span class="comment"># 我们使用`(.*)`来匹配任意多的字符，也就是dog</span></span><br><span class="line"><span class="comment"># 并将整行替换为`\1`，也就是capture group</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"cat says hello to dog"</span> | sed -E <span class="string">'s/.*says hello to (.*)/\1/'</span></span><br><span class="line"><span class="comment"># output：dog</span></span><br></pre></td></tr></table></figure><h2 id="sort和uniq"><a href="#sort和uniq" class="headerlink" title="sort和uniq"></a>sort和uniq</h2><p>sort，顾名思义，读入input，排序，再将它们输出。uniq，可以将<strong>紧邻的</strong>相同行进行合并。因为uniq只能合并紧邻的向同行，所以常常和sort配合使用。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -c 会在每一行前面加上一列，显示重复出现的次数</span></span><br><span class="line"><span class="comment"># 继续排序，并输出出现频率最高的10个 （sort升序排列，使用tail找到末尾，也就是最大的）</span></span><br><span class="line"><span class="comment"># sort -k        -k, --key=KEYDEF</span></span><br><span class="line"><span class="comment">#                sort via a key; KEYDEF gives location and type</span></span><br><span class="line"><span class="comment">#      -n        numeric sort</span></span><br><span class="line">xxx | sort | uniq -c | sort -nk1,1 | tail -10</span><br></pre></td></tr></table></figure><h2 id="awk"><a href="#awk" class="headerlink" title="awk"></a>awk</h2><p>awk是另一种stream editor。与sed相比，它更针对于成列的数据。例如，我们可以打印文本文件的第一列：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> input</span><br></pre></td></tr></table></figure><p>awk的一般用法还会加上<code>pattern</code>，例如<code>awk &#39;pattern {action}&#39; input</code>。例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># $0表示非特定列，而是整行</span></span><br><span class="line"><span class="comment"># 找出第一列符合给定pattern的那些行，并打印这个整行</span></span><br><span class="line">awk <span class="string">'$1 ~ /pattern/ &#123;print $0&#125;'</span> input</span><br><span class="line"></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello world\nsmile world"</span> | awk <span class="string">'$1 ~ /^h.*o$/ &#123;print $1&#125;'</span></span><br><span class="line"><span class="comment"># output: hello</span></span><br><span class="line"><span class="comment"># 这里smile这行因为不符合pattern，所以被filter掉了</span></span><br></pre></td></tr></table></figure><p>awk的功能还远不止此。awk中可以使用循环，分支等语句构成更复杂的逻辑。</p><p>paste命令可以用来合并多行为一行。下面的命令将输入文件的第一列顺序拼接为一行，并使用逗号分隔。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk <span class="string">'&#123;print $1&#125;'</span> input | paste -sd,</span><br></pre></td></tr></table></figure><blockquote><p>The paste utility concatenates the corresponding lines of the given input files, replacing all but the last file’s newline characters with a single tab character, and writes the resulting lines to standard output.</p></blockquote><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>本课主要介绍了一些常用的文本处理命令，包括sed, awk, sort, uniq, paste等。下面使用tldr命令给出这些命令的常用用法供参考。</p><h3 id="sed-1"><a href="#sed-1" class="headerlink" title="sed"></a>sed</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tldr sed</span><br><span class="line"><span class="comment"># sed</span></span><br><span class="line"></span><br><span class="line">  Edit text <span class="keyword">in</span> a scriptable manner.</span><br><span class="line"></span><br><span class="line">- Replace the first occurrence of a regular expression <span class="keyword">in</span> each line of a file, and <span class="built_in">print</span> the result:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'s/regex/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace all occurrences of an extended regular expression <span class="keyword">in</span> a file, and <span class="built_in">print</span> the result:</span><br><span class="line"></span><br><span class="line">  sed -r <span class="string">'s/regex/replace/g'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace all occurrences of a string <span class="keyword">in</span> a file, overwriting the file (i.e. <span class="keyword">in</span>-place):</span><br><span class="line"></span><br><span class="line">  sed -i <span class="string">'s/find/replace/g'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace only on lines matching the line pattern:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'/line_pattern/s/find/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Delete lines matching the line pattern:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'/line_pattern/d'</span> filename</span><br><span class="line"></span><br><span class="line">- Print only text between n-th line till the next empty line:</span><br><span class="line"></span><br><span class="line">  sed -n <span class="string">'n,/^$/p'</span> filename</span><br><span class="line"></span><br><span class="line">- Apply multiple find-replace expressions to a file:</span><br><span class="line"></span><br><span class="line">  sed -e <span class="string">'s/find/replace/'</span> -e <span class="string">'s/find/replace/'</span> filename</span><br><span class="line"></span><br><span class="line">- Replace separator / by any other character not used <span class="keyword">in</span> the find or replace patterns, e.g., <span class="comment">#:</span></span><br><span class="line"></span><br><span class="line">  sed <span class="string">'s#find#replace#'</span> filename</span><br><span class="line"></span><br><span class="line">- Print only the n-th line of a file:</span><br><span class="line"></span><br><span class="line">  sed <span class="string">'nq;d'</span> filename</span><br></pre></td></tr></table></figure><h3 id="awk-1"><a href="#awk-1" class="headerlink" title="awk"></a>awk</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tldr awk</span><br><span class="line"><span class="comment"># awk</span></span><br><span class="line"></span><br><span class="line">  A versatile programming language <span class="keyword">for</span> working on files.</span><br><span class="line"></span><br><span class="line">- Print the fifth column (a.k.a. field) <span class="keyword">in</span> a space-separated file:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;print $5&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print the second column of the lines containing <span class="string">"something"</span> <span class="keyword">in</span> a space-separated file:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'/something/ &#123;print $2&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print the last column of each line <span class="keyword">in</span> a file, using a comma (instead of space) as a field separator:</span><br><span class="line"></span><br><span class="line">  awk -F <span class="string">','</span> <span class="string">'&#123;print $NF&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Sum the values <span class="keyword">in</span> the first column of a file and <span class="built_in">print</span> the total:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;s+=$1&#125; END &#123;print s&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Sum the values <span class="keyword">in</span> the first column and pretty-print the values and <span class="keyword">then</span> the total:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'&#123;s+=$1; print $1&#125; END &#123;print "--------"; print s&#125;'</span> filename</span><br><span class="line"></span><br><span class="line">- Print every third line starting from the first line:</span><br><span class="line"></span><br><span class="line">  awk <span class="string">'NR%3==1'</span> filename</span><br></pre></td></tr></table></figure><h3 id="sort"><a href="#sort" class="headerlink" title="sort"></a>sort</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tldr tldr</span><br><span class="line"><span class="comment"># sort</span></span><br><span class="line"></span><br><span class="line">  Sort lines of text files.</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> ascending order:</span><br><span class="line"></span><br><span class="line">  sort filename</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> descending order:</span><br><span class="line"></span><br><span class="line">  sort -r filename</span><br><span class="line"></span><br><span class="line">- Sort a file <span class="keyword">in</span> <span class="keyword">case</span>-insensitive way:</span><br><span class="line"></span><br><span class="line">  sort --ignore-case filename</span><br><span class="line"></span><br><span class="line">- Sort a file using numeric rather than alphabetic order:</span><br><span class="line"></span><br><span class="line">  sort -n filename</span><br><span class="line"></span><br><span class="line">- Sort the passwd file by the 3rd field, numerically:</span><br><span class="line"></span><br><span class="line">  sort -t: -k 3n /etc/passwd</span><br><span class="line"></span><br><span class="line">- Sort a file preserving only unique lines:</span><br><span class="line"></span><br><span class="line">  sort -u filename</span><br><span class="line"></span><br><span class="line">- Sort human-readable numbers (<span class="keyword">in</span> this <span class="keyword">case</span> the 5th field of `ls -lh`):</span><br><span class="line"></span><br><span class="line">  ls -lh | sort -h -k 5</span><br></pre></td></tr></table></figure><h3 id="uniq"><a href="#uniq" class="headerlink" title="uniq"></a>uniq</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tldr uniq</span><br><span class="line"><span class="comment"># uniq</span></span><br><span class="line"></span><br><span class="line">  Output the unique lines from the given input or file.</span><br><span class="line">  Since it does not detect repeated lines unless they are adjacent, we need to sort them first.</span><br><span class="line"></span><br><span class="line">- Display each line once:</span><br><span class="line"></span><br><span class="line">  sort file | uniq</span><br><span class="line"></span><br><span class="line">- Display only unique lines:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -u</span><br><span class="line"></span><br><span class="line">- Display only duplicate lines:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -d</span><br><span class="line"></span><br><span class="line">- Display number of occurrences of each line along with that line:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -c</span><br><span class="line"></span><br><span class="line">- Display number of occurrences of each line, sorted by the most frequent:</span><br><span class="line"></span><br><span class="line">  sort file | uniq -c | sort -nr</span><br></pre></td></tr></table></figure><h3 id="paste"><a href="#paste" class="headerlink" title="paste"></a>paste</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">➜  ~ tldr paste</span><br><span class="line"># paste</span><br><span class="line"></span><br><span class="line">  Merge lines of files.</span><br><span class="line"></span><br><span class="line">- Join all the lines into a single line, using TAB as delimiter:</span><br><span class="line"></span><br><span class="line">  paste -s file</span><br><span class="line"></span><br><span class="line">- Join all the lines into a single line, using the specified delimiter:</span><br><span class="line"></span><br><span class="line">  paste -s -d delimiter file</span><br><span class="line"></span><br><span class="line">- Merge two files side by side, each in its column, using TAB as delimiter:</span><br><span class="line"></span><br><span class="line">  paste file1 file2</span><br><span class="line"></span><br><span class="line">- Merge two files side by side, each in its column, using the specified delimiter:</span><br><span class="line"></span><br><span class="line">  paste -d delimiter file1 file2</span><br><span class="line"></span><br><span class="line">- Merge two files, with lines added alternatively:</span><br><span class="line"></span><br><span class="line">  paste -d &apos;\n&apos; file1 file2</span><br></pre></td></tr></table></figure><h2 id="Exercise"><a href="#Exercise" class="headerlink" title="Exercise"></a>Exercise</h2><h3 id="关于正则匹配的应用"><a href="#关于正则匹配的应用" class="headerlink" title="关于正则匹配的应用"></a>关于正则匹配的应用</h3><ul><li>Find the number of words (in <code>/usr/share/dict/words</code>) that contain at least three <code>a</code>s and don’t have a <code>&#39;s</code> ending.</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cat /usr/share/dict/words | grep -E <span class="string">'(.*a)&#123;3&#125;'</span> | grep -Ev <span class="string">"\'s$"</span> | wc -l</span><br><span class="line"><span class="comment">#    7047</span></span><br></pre></td></tr></table></figure><h3 id="sed-使用"><a href="#sed-使用" class="headerlink" title="sed 使用"></a>sed 使用</h3><p>就地修改文件，使用<code>-i</code>。最好在后面指定backup文件的后缀名，否则将不会做backup。有丢失原始数据的风险。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">-i extension</span><br><span class="line">         Edit files <span class="keyword">in</span>-place, saving backups with the specified extension.  If a zero-length extension is given, no </span><br><span class="line">         backup will be saved. It is not recommended to give a zero-length extension when <span class="keyword">in</span>-place editing files, as </span><br><span class="line">         you risk corruption or partial content <span class="keyword">in</span> situations <span class="built_in">where</span> disk space is exhausted, etc.</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是&lt;a href=&quot;https://missing.csail.mit.edu/2020/data-wrangling/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MIT Missing Semester系列&lt;/a&gt;的第四讲。关于vim的第三讲跳过。Data Wrangling在这里的意思是对数据做变换（Transformation）。例如将一个MP4格式的视频转换为AVI，或或者是从日志中提取所需要的结构化文本信息。具体到本课，主要是处理文本信息：如何匹配到我们感兴趣的信息，如果构建一个处理的pipeline等。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>MIT Missing Semester - Shell</title>
    <link href="https://xmfbit.github.io/2020/03/13/mit-missing-semester-02-shell/"/>
    <id>https://xmfbit.github.io/2020/03/13/mit-missing-semester-02-shell/</id>
    <published>2020-03-13T22:05:30.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>工欲善其事，必先利其器。<a href="https://missing.csail.mit.edu/" target="_blank" rel="noopener">MIT Missing Semester</a>就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！</p><p>这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。</p><p>这里我跳过了第一节课，直接从bash shell开始。</p><a id="more"></a><h2 id="一些零散的点"><a href="#一些零散的点" class="headerlink" title="一些零散的点"></a>一些零散的点</h2><ul><li>bash中双引号和单引号的区别</li></ul><p>双引号会发生变量替换，单引号不会。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">foo=bar</span><br><span class="line"></span><br><span class="line"><span class="comment"># output: hello, bar</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"hello, <span class="variable">$foo</span>"</span></span><br><span class="line"><span class="comment"># output: hello, $foo</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">'hello, $foo'</span></span><br></pre></td></tr></table></figure><ul><li>bangbang</li></ul><p>使用<code>!!</code>可以执行上一条命令。</p><ul><li>bash 中的特殊变量</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$? <span class="comment"># 上一条命令的返回值，正常退出是0，否则是非0</span></span><br><span class="line"><span class="variable">$@</span> <span class="comment"># 所有输入的参数</span></span><br><span class="line"><span class="variable">$#</span> <span class="comment"># 输入参数的个数</span></span><br><span class="line">$$ <span class="comment"># pid of current script</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查上条命令是否正常退出</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"fail"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"success"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><ul><li>如何忽略命令的输出</li></ul><p>有的时候，我们只想要命令的返回值。例如使用<code>grep foo bar</code>来查看文件<code>bar</code>中是否含有字符串<code>foo</code>，可以将标准输出和标准错误重定向到<code>/dev/null</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 第一个是标准输出，第二个是标准错误</span></span><br><span class="line">grep <span class="string">"foo"</span> bar &gt; /dev/null 2&gt; /dev/null</span><br><span class="line"></span><br><span class="line"><span class="comment"># 或者可以这样：</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># grep "name" test_lazy.cpp 2&gt;&amp;1 &gt; /dev/null</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> [ <span class="string">"$?"</span> -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"found foo in bar"</span></span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"not found foo in bar"</span></span><br><span class="line"><span class="keyword">fi</span></span><br></pre></td></tr></table></figure><h2 id="globbing"><a href="#globbing" class="headerlink" title="globbing"></a>globbing</h2><ul><li>任意字符：<code>*</code></li><li>单个字符：<code>?</code></li><li>使用<code>{}</code>给定可选元素的集合。</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a.&#123;hpp,cpp&#125;  =&gt; a.hpp a.cpp</span><br><span class="line">a&#123;,0,1,2&#125;   =&gt; a a0 a1 a2</span><br><span class="line"><span class="comment"># 支持多层级</span></span><br><span class="line">touch proj&#123;1,2&#125;/&#123;a,b&#125;.txt</span><br><span class="line"><span class="comment"># 还支持range</span></span><br><span class="line">touch proj&#123;1,2&#125;/&#123;a..g&#125;.txt</span><br></pre></td></tr></table></figure><h2 id="bash-中的函数"><a href="#bash-中的函数" class="headerlink" title="bash 中的函数"></a>bash 中的函数</h2><ul><li>如何写函数</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="title">mycd</span></span> () &#123;</span><br><span class="line">    <span class="built_in">cd</span> <span class="variable">$1</span></span><br><span class="line">    <span class="built_in">pwd</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ul><li>如何在bash中导入脚本中的函数</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">source</span> your_bash_script.sh</span><br><span class="line"><span class="comment"># then use the function defined in the bash script</span></span><br><span class="line"><span class="comment"># 你可以这样理解：from your_bash_script import *</span></span><br></pre></td></tr></table></figure><h2 id="for-loop"><a href="#for-loop" class="headerlink" title="for-loop"></a>for-loop</h2><h3 id="遍历给定的元素序列"><a href="#遍历给定的元素序列" class="headerlink" title="遍历给定的元素序列"></a>遍历给定的元素序列</h3><p>使用<code>for item in xxx; do yyy; done</code>来遍历给定的序列，并施加具体操作于序列元素：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1 2 3</span><br><span class="line"><span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$i</span>"</span></span><br><span class="line"><span class="keyword">done</span></span><br><span class="line"><span class="comment"># welcome 1</span></span><br><span class="line"><span class="comment"># welcome 2</span></span><br><span class="line"><span class="comment"># welcome 3</span></span><br></pre></td></tr></table></figure><p>注意，列表元素是通过空格来隔离的。如果这样写</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> 1, 2, 3</span><br></pre></td></tr></table></figure><p>那么最终输出也是<code>welcome 1, welcome 2, welcome 3</code></p><p>还可以使用for-range的方法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> &#123;1..3&#125;</span><br></pre></td></tr></table></figure><h3 id="c-style-for-loop"><a href="#c-style-for-loop" class="headerlink" title="c-style for-loop"></a>c-style for-loop</h3><p>也可以像C语言那样使用for-loop：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (( Exp1; Exp2; Exp3)); <span class="keyword">do</span> xxx; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>例如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (( c=1; c&lt;=3; c++ )); <span class="keyword">do</span> <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span>; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><p>还可以使用这种风格构造无穷循环，<code>for (( ; ; )); do xxx; done</code>。</p><h3 id="break-continue"><a href="#break-continue" class="headerlink" title="break / continue"></a>break / continue</h3><p>当满足一定条件时，使用<code>break</code>退出循环，或使用<code>continue</code>继续循环。</p><h3 id="while"><a href="#while" class="headerlink" title="while"></a>while</h3><p>除了for-loop，还可以使用<code>while</code>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">while</span> CONDITION; <span class="keyword">do</span> xxx; <span class="keyword">done</span></span><br></pre></td></tr></table></figure><h3 id="until"><a href="#until" class="headerlink" title="until"></a>until</h3><p><code>until</code>和<code>while</code>的用法一致，不同点在于：</p><ul><li><code>while</code>是CONDITION为<code>true</code>执行，当<code>false</code>是退出循环</li><li><code>until</code>是CONDITION为<code>false</code>执行，当<code>true</code>时退出循环</li></ul><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c=1</span><br><span class="line">until [ <span class="variable">$c</span> -gt 3 ]; <span class="keyword">do</span></span><br><span class="line">  <span class="built_in">echo</span> <span class="string">"welcome <span class="variable">$c</span>"</span></span><br><span class="line">  ((c++))</span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h2 id="数学表达式"><a href="#数学表达式" class="headerlink" title="数学表达式"></a>数学表达式</h2><p>在上面for-loop中，已经看到了我们使用<code>((exp))</code>的形式进行数学表达式运算。一般来说，在bash shell中进行数学表达式的运算可以采用：</p><ul><li>使用<code>expr</code>，如<code>c=$(expr 1 + 1); echo $c</code>，注意操作数与操作符之间都是有空格的。</li><li>使用<code>let</code>，如<code>c=1; let c=$c+1; echo $c</code>，注意操作数与操作符之间没有空格。</li><li>使用双括号<code>(())</code>，就像上面看到的那样：<code>c=1; echo $((c += 1)); echo $c</code>。这时候，操作数与操作符之间的空格可有可无。</li></ul><p>最后一种双括号可能更为常用，支持的操作符：<code>+/-/++/--/*/%/**</code>，也支持逻辑运算符：<code>&gt;=/&lt;=/&gt;/&lt;/==/!=/!/||/&amp;&amp;</code>。</p><p>如果希望进行浮点数运算，bash本身是不支持的，可以使用<code>bc</code>命令，将表达式作为字符串传入就可以了：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">echo</span> <span class="string">"1.0+2.0"</span> | bc</span><br><span class="line">c=$(r=1.5;<span class="built_in">echo</span> <span class="string">"<span class="variable">$r</span> + 2.5"</span>|bc); <span class="built_in">echo</span> <span class="variable">$c</span></span><br></pre></td></tr></table></figure><h2 id="调试工具"><a href="#调试工具" class="headerlink" title="调试工具"></a>调试工具</h2><p>shellcheck可以用来帮助静态分析shell脚本。用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">shellcheck your_bash_script.sh</span><br></pre></td></tr></table></figure><p>可以去网站上试用：<a href="https://www.shellcheck.net/#" target="_blank" rel="noopener">Shellcheck</a></p><h2 id="几个有用的命令"><a href="#几个有用的命令" class="headerlink" title="几个有用的命令"></a>几个有用的命令</h2><p>这里列出一些常用的命令工具，都是和查找有关。更多内容，可以通过<code>man</code>或者<code>tldr</code>查看。</p><h3 id="查找文件-find"><a href="#查找文件-find" class="headerlink" title="查找文件 find"></a>查找文件 find</h3><p>最常用的用法：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 递归地查找当前目录及子目录下所有的python文件</span></span><br><span class="line">find . -name=<span class="string">"*.py"</span></span><br><span class="line"><span class="comment"># -type d 表示过滤结果为所有目录</span></span><br><span class="line"><span class="comment"># -type f 表示过滤结果为所有文件</span></span><br><span class="line">find . -name=<span class="string">"test"</span> -<span class="built_in">type</span> d</span><br><span class="line"><span class="comment"># Find all files modified in the last day</span></span><br><span class="line">find . -mtime -1</span><br><span class="line"><span class="comment"># Find all zip files with size in range 500k to 10M</span></span><br><span class="line">find . -size +500k -size -10M -name <span class="string">'*.tar.gz'</span></span><br></pre></td></tr></table></figure><p><code>find</code>还可以通过<code>-exec</code>来接后续处理，如：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Delete all files with .tmp extension</span></span><br><span class="line"><span class="comment"># 注意最后的 \</span></span><br><span class="line">find . -name <span class="string">'*.tmp'</span> -<span class="built_in">exec</span> rm &#123;&#125; \;</span><br><span class="line"><span class="comment"># Find all PNG files and convert them to JPG</span></span><br><span class="line">find . -name <span class="string">'*.png'</span> -<span class="built_in">exec</span> convert &#123;&#125; &#123;.&#125;.jpg \;</span><br></pre></td></tr></table></figure><p>你也可以用<code>fd</code>作为<code>find</code>的改进版。具体用法可以参考<a href="https://github.com/sharkdp/fd" target="_blank" rel="noopener">fd</a>，这里不再多说了。</p><h3 id="locate"><a href="#locate" class="headerlink" title="locate"></a>locate</h3><p>如果你想按照名字去查找文件，还可以试试<code>locate</code>。一个简单的比较：</p><ul><li><code>locate</code>只支持按名字查找，<code>find</code>可以更加多样</li><li><code>locate</code>通过周期性更新的database来查找，时效性不如<code>find</code></li><li><code>locate</code>使用更简单，默认会查找所有符合要求的文件，<code>find</code>一般是查找给定路径下的文件</li></ul><p>由于上述原因，我一般是使用<code>locate</code>查找系统自带的某个lib等文件。比如有时候我可能不知道<code>libcudart.so</code>在哪里，这时候就可以通过<code>locate libcudart.so</code>来查找。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">locate libcudart.so | grep <span class="string">"/usr"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># /usr/local/cuda-10.0/doc/man/man7/libcudart.so.7</span></span><br><span class="line"><span class="comment"># /usr/local/cuda-10.0/lib64/libcudart.so</span></span><br><span class="line"><span class="comment"># ...</span></span><br></pre></td></tr></table></figure><h3 id="在文件中查找字符串-grep"><a href="#在文件中查找字符串-grep" class="headerlink" title="在文件中查找字符串 grep"></a>在文件中查找字符串 grep</h3><p><code>grep</code> 用来在文件中正则匹配字符串，比如某个变量或函数定义啥的。<code>grep</code>命令很有用，在后续课程中还会着重介绍。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在文件中查找xxx，并打印其所在的行</span></span><br><span class="line">grep xxx file</span><br><span class="line"></span><br><span class="line"><span class="comment"># 在所有文件中递归地查找</span></span><br><span class="line">grep -R xxx .</span><br></pre></td></tr></table></figure><p>常用的一些flag，可以是<code>-C +number</code>（用来显示match的context，number是行数），<code>-v</code>是反转（不包含所给pattern的行）</p><p>和<code>find</code>一样，<code>grep</code>也有一些更好用的替代品，如<code>rg</code>，<code>ag</code>，<code>ack</code>等。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Find all python files where I used the requests library</span></span><br><span class="line">rg -t py <span class="string">'import requests'</span></span><br><span class="line"><span class="comment"># Find all files (including hidden files) without a shebang line</span></span><br><span class="line">rg -u --files-without-match <span class="string">"^#!"</span></span><br><span class="line"><span class="comment"># Find all matches of foo and print the following 5 lines</span></span><br><span class="line">rg foo -A 5</span><br><span class="line"><span class="comment"># Print statistics of matches (# of matched lines and files )</span></span><br><span class="line">rg --stats PATTERN</span><br></pre></td></tr></table></figure><h3 id="查找历史命令-history"><a href="#查找历史命令-history" class="headerlink" title="查找历史命令 history"></a>查找历史命令 history</h3><p><code>history</code>可以打印历史的shell命令，和<code>grep</code>配合能够找到历史上曾经用过的某给定命令。不过这个我在使用<code>zsh</code>的时候，一般是通过光标上下键来联想查找的。</p><p>一个有用的工具：<code>fzf</code>（which means 模糊查找）。</p><p>另外，这里讲师推荐了一款基于历史命令的自动补全（看lecture时候觉得很酷）。如果你和我一样使用<code>zsh</code>，可以参考这个插件：<a href="https://github.com/zsh-users/zsh-autosuggestions" target="_blank" rel="noopener">zsh-autosuggestions</a>。</p><h3 id="关于目录"><a href="#关于目录" class="headerlink" title="关于目录"></a>关于目录</h3><p>因为shell环境下没有GUI，所以查看一个目录内的内容，包括跳转目录都很不方便。对此也有一些好用的工具：</p><ul><li>查看目录内容：<code>tree</code>（最经典），<code>broot</code>，<code>nnn</code>，<code>ranger</code></li><li>跳转目录：<code>autojump</code>（在用），<code>fasd</code></li></ul><h2 id="课后习题"><a href="#课后习题" class="headerlink" title="课后习题"></a>课后习题</h2><h3 id="关于ls的用法"><a href="#关于ls的用法" class="headerlink" title="关于ls的用法"></a>关于ls的用法</h3><ul><li>Includes all files, including hidden files：<code>ls -al</code></li><li>Sizes are listed in human readable format (e.g. 454M instead of 454279954)：<code>ls -lh</code></li><li>Files are ordered by recency：<code>ls -lt</code></li><li>Output is colorized：<code>ls -l --color</code> （zsh自动colorized，所以这个没有验证）</li></ul><h3 id="bash函数"><a href="#bash函数" class="headerlink" title="bash函数"></a>bash函数</h3><p>Write bash functions <code>marco</code> and <code>polo</code> that do the following. Whenever you execute <code>marco</code> the current working directory should be saved in some manner, then when you execute <code>polo</code>, no matter what directory you are in, <code>polo</code> should cd you back to the directory where you executed <code>marco</code>. For ease of debugging you can write the code in a file <code>marco.sh</code> and (re)load the definitions to your shell by executing <code>source marco.sh</code>.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用文件存储要cd的path</span></span><br><span class="line"><span class="function"><span class="title">macro</span></span> () &#123;</span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"<span class="variable">$(pwd)</span>"</span> &gt; <span class="variable">$HOME</span>/.macro.history</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="title">polo</span></span> () &#123;</span><br><span class="line">    <span class="built_in">cd</span> <span class="string">"<span class="variable">$(cat "$HOME/.macro.history")</span>"</span> || <span class="built_in">exit</span> 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="循环和程序返回值判断"><a href="#循环和程序返回值判断" class="headerlink" title="循环和程序返回值判断"></a>循环和程序返回值判断</h3><p>Say you have a command that fails rarely. In order to debug it you need to capture its output but it can be time consuming to get a failure run. Write a bash script that runs the following script until it fails and captures its standard output and error streams to files and prints everything at the end. Bonus points if you can also report how many runs it took for the script to fail.</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ((i=1; ; i++)); <span class="keyword">do</span></span><br><span class="line">  <span class="comment"># save the script to `fail_rarely.sh`</span></span><br><span class="line">  ./fail_rarely.sh 2&amp;&gt; out.log</span><br><span class="line">  <span class="keyword">if</span> [ $? -ne 0 ]; <span class="keyword">then</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"fail after run <span class="variable">$i</span> times"</span></span><br><span class="line">    <span class="built_in">echo</span> <span class="string">"stdout and stderr message:"</span></span><br><span class="line">    cat out.log</span><br><span class="line">    <span class="built_in">break</span></span><br><span class="line">  <span class="keyword">fi</span></span><br><span class="line"><span class="keyword">done</span></span><br></pre></td></tr></table></figure><h3 id="xargs和管道"><a href="#xargs和管道" class="headerlink" title="xargs和管道"></a>xargs和管道</h3><p>As we covered in lecture find’s <code>-exec</code> can be very powerful for performing operations over the files we are searching for. However, what if we want to do something with all the files, like creating a zip file? As you have seen so far commands will take input from both arguments and STDIN. When piping commands, we are connecting STDOUT to STDIN, but some commands like tar take inputs from arguments. To bridge this disconnect there’s the <code>xargs</code> command which will execute a command using STDIN as arguments. For example <code>ls | xargs rm</code> will delete the files in the current directory.</p><p>Your task is to write a command that recursively finds all HTML files in the folder and makes a zip with them. Note that your command should work even if the files have spaces (hint: check <code>-d</code> flag for <code>xargs</code>)</p><h4 id="xargs"><a href="#xargs" class="headerlink" title="xargs"></a>xargs</h4><p>先来看下<code>xargs</code>和管道的区别。这里已经给了一个例子：<code>ls | xargs rm</code>。由于<code>rm</code>命令比较危险，所以下面会换成<code>cat</code>（删除文件变成了打印文件内容）。</p><p>为什么不能用管道呢，比如<code>ls | cat</code>。我们先建立一个空目录作为playground：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">mkdir <span class="built_in">test</span></span><br><span class="line"><span class="built_in">cd</span> <span class="built_in">test</span></span><br><span class="line"><span class="built_in">echo</span> <span class="string">"simgple test"</span> &gt; a.txt</span><br></pre></td></tr></table></figure><p>执行<code>ls | cat</code>，会发现它只是把当前目录下的所有文件名打印了出来，并没有打印<code>a.txt</code>的内容：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls | cat</span><br><span class="line"><span class="comment"># a.txt</span></span><br></pre></td></tr></table></figure><p>这是因为管道只是把STDOUT作为<code>cat</code>的STDIN。在linux中，STDOUT和STDIN是两个特殊的文件，<code>ls</code>将把它的输出结果写入到STDOUT中，同时我们就会在屏幕上看到对应输出。而<code>cat</code>从STDIN中接受输入。当没有管道时，由用户输入并写入STDIN。由于管道，<code>cat</code>将直接从STDOUT中读取。也就是<code>ls</code>的输出，也就是当前目录下的文件列表。拆解后想当于下面：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls &gt; stdout_ls</span><br><span class="line">cat &lt; stdout_ls</span><br></pre></td></tr></table></figure><p>所以，如果我们想要打印<code>a.txt</code>的内容，管道就不够用了。也就是上面说的，我们要把<code>ls</code>的输出作为<code>cat</code>的参数。这时候需要使用<code>xargs</code>：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">ls | xargs cat</span><br><span class="line"><span class="comment"># simple test</span></span><br></pre></td></tr></table></figure><h4 id="准备"><a href="#准备" class="headerlink" title="准备"></a>准备</h4><p>先准备一些测试文件</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mkdir htmls</span><br><span class="line"><span class="built_in">cd</span> htmls</span><br><span class="line">mkdir htmls/&#123;1..3&#125;</span><br><span class="line">touch htmls/1/1.html</span><br><span class="line">touch htmls/2/2\ 2.html</span><br><span class="line">touch htmls/root.html</span><br></pre></td></tr></table></figure><h4 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h4><p>题目说明中的<code>-d</code>没找到，在<a href="https://www.tecmint.com/xargs-command-examples/" target="_blank" rel="noopener">12 Practical Examples of Linux Xargs Command for Beginners</a>找到了如下用法，使用<br><code>-print0</code>和<code>-0</code>（是数字<code>0</code>）配合，具体可以参考<code>man xargs</code>中的内容。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-0      Change xargs to expect NUL (``\0'') characters as separators, instead of spaces and newlines.  </span></span><br><span class="line"><span class="comment">#        This is expected to be used in concert with the -print0 function in find(1).</span></span><br><span class="line"></span><br><span class="line">find htmls -name <span class="string">"*.html"</span> -print0 | xargs -0 tar vcf html.zip</span><br></pre></td></tr></table></figure><h3 id="命令组合"><a href="#命令组合" class="headerlink" title="命令组合"></a>命令组合</h3><p>Write a command or script to recursively find the most recently modified file in a directory. More generally, can you list all files by recency?</p><p>首先递归地列出当前目录下的所有文件，再使用<code>ls -lt</code>将其按照时间排序。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">find -L . -<span class="built_in">type</span> f -print0 | xargs -0 ls -lt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 如果只需要最新的那个，使用 head命令只打印第一行</span></span><br><span class="line">find -L . -<span class="built_in">type</span> f -print0 | xargs -0 ls -lt | head -1</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;工欲善其事，必先利其器。&lt;a href=&quot;https://missing.csail.mit.edu/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MIT Missing Semester&lt;/a&gt;就是这样一门课。在这门课中，不会讲到多少理论知识，也不会告诉你如何写代码，而是会向你介绍诸如shell，git等常用工具的使用。这些工具其实自己在学习工作中或多或少都有接触，不过还是有一些点是漏掉的。所以，一起来和MIT的这些牛人们重新熟悉下这些工具吧！&lt;/p&gt;
&lt;p&gt;这篇博客，包括后续的几篇，是我个人在过课程lecture的时候随手记下的自己之前不太清楚的点，可能并不适合阅读到这篇文章的你。如果有时间，还是建议去课程网站上自己过一遍。&lt;/p&gt;
&lt;p&gt;这里我跳过了第一节课，直接从bash shell开始。&lt;/p&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
      <category term="bash" scheme="https://xmfbit.github.io/tags/bash/"/>
    
  </entry>
  
  <entry>
    <title>Debian9 编译Caffe的一个坑</title>
    <link href="https://xmfbit.github.io/2019/11/24/caffe-compiling-debian9/"/>
    <id>https://xmfbit.github.io/2019/11/24/caffe-compiling-debian9/</id>
    <published>2019-11-24T19:12:48.000Z</published>
    <updated>2020-05-02T07:17:54.448Z</updated>
    
    <content type="html"><![CDATA[<p>记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">In file included from /usr/local/cuda/include/cuda_runtime.h:120:0,</span><br><span class="line">                 from &lt;command-line&gt;:0:</span><br><span class="line">/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &quot;&quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER</span><br><span class="line">_BUILD__ instead.&quot;&quot; is not valid in preprocessor expressions</span><br><span class="line"> #define __CUDACC_VER__ &quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&quot;</span><br></pre></td></tr></table></figure><p>如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。</p><p>解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个<a href="https://github.com/NVIDIA/caffe/issues/408" target="_blank" rel="noopener">github issue</a>，修改<code>include/caffe/common.hpp</code>即可。</p><a id="more"></a><p><img src="/img/fix_caffe_for_boost_CUDACC_VER_error.png" alt="diff修改"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;记录一个编译Caffe的坑。环境，Debian 9 + GCC 6.3.0，出现的问题：&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;In file included from /usr/local/cuda/include/cuda_runtime.h:120:0,&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;                 from &amp;lt;command-line&amp;gt;:0:&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;/usr/local/cuda/include/crt/common_functions.h:74:24: error: token &amp;quot;&amp;quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;_BUILD__ instead.&amp;quot;&amp;quot; is not valid in preprocessor expressions&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt; #define __CUDACC_VER__ &amp;quot;__CUDACC_VER__ is no longer supported.  Use __CUDACC_VER_MAJOR__, __CUDACC_VER_MINOR__, and __CUDACC_VER_BUILD__ instead.&amp;quot;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;如果你和我一样，自从从Github clone Caffe后很长时间没有与master合并过，就有可能出现这个问题。&lt;/p&gt;
&lt;p&gt;解决方法：这个问题应该是和boost有关，最初我看到的解决方法是将boost升级到1.65.1。不过感觉好麻烦，后来找到了这个&lt;a href=&quot;https://github.com/NVIDIA/caffe/issues/408&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;github issue&lt;/a&gt;，修改&lt;code&gt;include/caffe/common.hpp&lt;/code&gt;即可。&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>论文 - MetaPruning：Meta Learning for Automatic Neural Network Channel Pruning</title>
    <link href="https://xmfbit.github.io/2019/10/26/paper-meta-pruning/"/>
    <id>https://xmfbit.github.io/2019/10/26/paper-meta-pruning/</id>
    <published>2019-10-26T00:33:37.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：<a href="https://github.com/liuzechun/MetaPruning" target="_blank" rel="noopener">MetaPruning</a>。</p><p>这篇文章首发于<a href="https://wemp.app/accounts/fd027dce-bcd1-4eaf-9e64-88bffd7ca8a2" target="_blank" rel="noopener">Paper Weekly公众号</a>，欢迎关注。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>模型剪枝是一种能够减少模型大小和计算量的方法。模型剪枝一般可以分为三个步骤：</p><ul><li>训练一个参数量较多的大网络</li><li>将不重要的权重参数剪掉</li><li>剪枝后的小网络做fine tune</li></ul><p>其中第二步是模型剪枝中的关键。有很多paper围绕“怎么判断权重是否重要”以及“如何剪枝”等问题进行讨论。困扰模型剪枝落地的一个问题就是剪枝比例的确定。传统的剪枝方法常常需要人工layer by layer地去确定每层的剪枝比例，然后进行fine tune，用起来很耗时，而且很不方便。不过最近的<a href="https://arxiv.org/abs/1810.05270" target="_blank" rel="noopener">Rethinking the Value of Network Pruning</a>指出，剪枝后的权重并不重要，对于channel pruning来说，更重要的是找到剪枝后的网络结构，具体来说就是每层留下的channel数量。受这个发现启发，文章提出可以用一个PruningNet，对于给定的剪枝网络，自动生成weight，无需进行retrain，然后评测剪枝网络在验证集上的性能，从而选出最优的网络结构。</p><p>具体来说，PruningNet的输入是剪枝后的网络结构，必须首先对网络结构进行编码，转换为coding vector。这里可以直接用剪枝后网络每层的channel数来编码。在搜索剪枝网络的时候，我们可以尝试各种coding vector，用PruningNet生成剪枝后的网络权重。网络结构和权重都有了，就可以去评测网络的性能。进而用进化算法搜索最优的coding vector，也就是最优的剪枝结构。在用进化算法搜索的时候，可以使用自定义的目标函数，包括将网络的accuracy，latency，FLOPS等考虑进来。</p><p><img src="/img/paper_metapruning_pruningnet.jpg" alt="PruningNet的训练和使用"></p><h2 id="PruningNet"><a href="#PruningNet" class="headerlink" title="PruningNet"></a>PruningNet</h2><p>从上一小节已经可以知道，PruningNet是整个算法的关键。那么怎么才能找到这样一个“神奇网络”呢？</p><p>先做一下符号约定，使用$c_i$表示剪枝之后第$i$层的channel数量，$l$为网络的层数，$W$表示剪枝后网络的权重。那么PruningNet的输入输出如下所示：</p><p>$W = \text{PruningNet}(c_1, c_2, \dots, c_l)$</p><h3 id="训练"><a href="#训练" class="headerlink" title="训练"></a>训练</h3><p>先结合下图看一下forward部分。PruningNet是由$l$个PruningBlock组成的，每个PruningBlock是一个两层的MLP。首先看图b，编码着网络结构信息的coding vector输入到当前block后，输出经过Reshape，成了一个Weight Matrix。注意哦，这里的WeightMatrix是固定大小的（也就是未剪枝的原始Weight shape大小），和剪枝网络结构无关。再看图a，因为要对网络进行剪枝，所以WeightMatrix要进行Crop。对应到图b，可以看到，Crop是在两个维度上进行的。首先，由于上一层也进行了剪枝，所以input channel数变少了；其次，由于当前层进行了剪枝，所以output channel数变少了。这样经过Crop，就生成了剪枝后的网络weight。我们再输入一个mini batch的训练图片，就可以得到剪枝后的网络的loss。</p><p><img src="/img/paper_metapruning_pruningnet_forward.jpg" alt="PruningNet train forward"></p><p>在backward部分，我们不更新剪枝后网络的权重，而是更新PruningNet的权重。由于上面的操作都是可微分的，所以直接用链式法则传过去就行。如果你使用PyTorch等支持自动微分的框架，这是很容易的。</p><p>下图所示是训练过程的整个PruningNet（左侧）和剪枝后网络（右侧，即PrunedNet）。训练过程中的coding vector在状态空间里随机采样，随机选取每层的channel数量。</p><p>PS：和原始论文相比，下图和上图顺序是颠倒的。这里从底向上介绍了PruningNet的训练，而论文则是自顶向下。</p><p><img src="/img/paper_metapruning_whole_meta_learning.jpg" alt="整个PruningNet"></p><h3 id="搜索"><a href="#搜索" class="headerlink" title="搜索"></a>搜索</h3><p>训练好PruningNet后，就可以用它来进行搜索了！我们只需要输入某个coding vector，PruningNet就会为我们生成对应每层的WeightMatrix。别忘了coding vector是编码的网络结构，现在又有了weight，我们就可以在验证集上测试网络的性能了。进而，可以使用进化算法等优化方法去搜索最优的coding vector。当我们得到了最优结构的剪枝网络后，再from scratch地训练它。</p><p>进化算法这里不再赘述，很多优化的书中包括网上都有资料。这里把整个算法流程贴出来：</p><p><img src="/img/paper_metapruning_evaluation_algorithm.jpg" alt="进化算法流程"></p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在ImageNet上用MobileNet和ResNet进行了实验。训练PruningNet用了$\frac{1}{4}$的原模型的epochs。数据增强使用常见的标准流程，输入image大小为$224\times 224$。</p><p>将原始ImageNet的训练集做分割，每个类别选50张组成sub-validation（共计50000），其余作为sub-training。在训练时，我们使用sub-training训练PruningNet。在搜索时，使用sub-validation评估剪枝网络的性能。不过，还要注意，在搜索时，使用20000张sub-training中的图片重新计算BatchNorm layer中的running mean和running variance。</p><h3 id="shortcut剪枝"><a href="#shortcut剪枝" class="headerlink" title="shortcut剪枝"></a>shortcut剪枝</h3><p>在进行模型剪枝时，一个比较难处理的问题是ResNet中的shortcut结构。因为最后有一个element-wise的相加操作，必须保证两路feature map是严格shape相同的，所以不能随意剪枝，否则会造成channel不匹配。下面对几种论文中用到的网络结构分别讨论。</p><h4 id="MobileNet-v1"><a href="#MobileNet-v1" class="headerlink" title="MobileNet-v1"></a>MobileNet-v1</h4><p>MobileNet-v1是没有shortcut结构的。我们为每个conv layer都配上相应的PruningBlock——一个两层的MLP。PruningNet的输入coding vector中的元素是剪枝后每层的channel数量。而输入第$i$个PruningBlock的是一个2D vector，由归一化的第$i-1$层和第$i$层的剪枝比例构成。这部分可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv1/training/mobilenet_v1.py#L15" target="_blank" rel="noopener">MetaPruning</a>来看。注意第$1$个conv layer的输入是1D vector，因为它是第一个被剪枝的layer。在训练时，coding vector的搜索空间被以一定步长划分为grid，采样就是在这些格点上进行的。</p><h4 id="MobileNet-v2"><a href="#MobileNet-v2" class="headerlink" title="MobileNet-v2"></a>MobileNet-v2</h4><p>MobileNet-v2引入了类似ResNet的shortcut结构，这种resnet block必须统一看待。具体来说，对于没有在resnet block中的conv，处理方法如MobileNet-v1。对每个resnet block，配上一个相应的PruningBlock。由于每个resnet block中只有一个中间层（$3\times 3$的conv），所以输出第$i$个PruningBlock的是一个3D vector，由归一化的第$i-1$个resnet block，第$i$个resnet block和中间conv层的剪枝比例构成。其他设置和MobileNet-v1相同。这里可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/mobilenetv2/training/mobilenet_v2.py#L109" target="_blank" rel="noopener">MetaPruning</a>来看。</p><h4 id="ResNet"><a href="#ResNet" class="headerlink" title="ResNet"></a>ResNet</h4><p>处理方法如MobileNet-v2所示。可以结合代码<a href="https://github.com/liuzechun/MetaPruning/blob/master/resnet/training/resnet.py#L75" target="_blank" rel="noopener">MetaPruning</a>来看。</p><h3 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h3><p>在相近FLOPS情况下，和MobileNet论文中改变ratio参数得到的模型比较，MetaPruning得到的模型accuracy更高。尤其是压缩比例更大时，该方法更有优势。</p><p><img src="/img/paper_metapruning_compare_with_mobilenet_baseline.jpg" alt="MobileNet baseline比较"></p><p>和其他剪枝方法（如<a href="https://arxiv.org/abs/1802.03494" target="_blank" rel="noopener">AMC</a>）等方法比较，该方法也得到了SOTA的结果。MetaPruning方法能够以一种统一的方法处理ResNet中的shortcut结构，并且不需要人工调整太多的参数。</p><p><img src="/img/paper_metapruning_compare_with_other_pruning_automl.jpg" alt="和其他剪枝方法比较"></p><p>上面的比较都是基于理论FLOPS，现在更多人在关注网络在实际硬件上的latency怎么样。文章对此也进行了讨论。如何测试网络的latency？当然可以每个网络都实际跑一下，不过有些麻烦。基于每个layer的inference时间是互相独立的这个假设，作者首先构造了各个layer inference latency的查找表（参见论文<a href="https://arxiv.org/abs/1812.03443" target="_blank" rel="noopener">Fbnet: Hardware-aware efficient convnet design via differentiable neural architecture search</a>），以此来估计实际网络的latency。作者这里和MobileNet baseline做了比较，结果也证明了该方法更优。</p><p><img src="/img/paper_metapruning_latency_compare_with_mobilenet_baseline.jpg" alt="latency比较"></p><h3 id="PruningNet结果分析"><a href="#PruningNet结果分析" class="headerlink" title="PruningNet结果分析"></a>PruningNet结果分析</h3><p>此外，作者还对PruningNet的预测结果进行可视化，试图找出一些可解释性，并找出剪枝参数的一些规律。</p><ul><li>down-sampling的部分PruningNet倾向于保留更多的channel，如MobileNet-v2 block中间的那个conv</li><li>优先剪浅层layer的channel，FLOPS约束太强剪深层的channel，但可能会造成网络accuracy下降比较多</li></ul><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>这篇文章从“剪枝后的weight作用不大”的现象出发，将剪枝和NAS结合，提出了PruningNet为剪枝后的网络预测weight，避免了网络的retrain，从而可以快速衡量剪枝网络的性能。并在编码网络信息的coding vector状态空间进行搜索，找到给定约束条件下的最优网络结构，在ImageNet数据集和ResNet/MobileNet-v1/v2上取得了比之前剪枝算法更好的效果。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇文章来自于旷视。旷视内部有一个基础模型组，孙剑老师也是很看好NAS相关的技术，相信这篇文章无论从学术上还是工程落地上都有可以让人借鉴的地方。回到文章本身，模型剪枝算法能够减少模型计算量，实现模型压缩和加速的目的，但是模型剪枝过程中确定剪枝比例等参数的过程实在让人头痛。这篇文章提出了PruningNet的概念，自动为剪枝后的模型生成权重，从而绕过了费时的retrain步骤。并且能够和进化算法等搜索方法结合，通过搜索编码network的coding vector，自动地根据所给约束搜索剪枝后的网络结构。和AutoML技术相比，这种方法并不是从头搜索，而是从已有的大模型出发，从而缩小了搜索空间，节省了搜索算力和时间。个人觉得这种剪枝和NAS结合的方法，应该会在以后吸引越来越多人的注意。这篇文章的代码已经开源在了Github：&lt;a href=&quot;https://github.com/liuzechun/MetaPruning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;MetaPruning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章首发于&lt;a href=&quot;https://wemp.app/accounts/fd027dce-bcd1-4eaf-9e64-88bffd7ca8a2&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Paper Weekly公众号&lt;/a&gt;，欢迎关注。&lt;/p&gt;
    
    </summary>
    
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Bag of Tricks for Image Classification with Convolutional Neural Networks</title>
    <link href="https://xmfbit.github.io/2019/07/06/bag-of-tricks-for-image-cls/"/>
    <id>https://xmfbit.github.io/2019/07/06/bag-of-tricks-for-image-cls/</id>
    <published>2019-07-06T13:52:45.000Z</published>
    <updated>2020-05-02T07:17:54.448Z</updated>
    
    <content type="html"><![CDATA[<p>这是<a href="https://arxiv.org/abs/1812.01187" target="_blank" rel="noopener">Bag of Tricks for Image Classification with Convolutional Neural Networks</a>的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。</p><p><img src="/img/bag_of_tricks_no_silver_bullet.jpeg" alt="软工里面没有银弹，数据科学同样这样"></p><a id="more"></a><h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这篇文章主要讨论了训练图片分类模型的tricks，包括data augmentation，（lr，batch size等）超参设置，模型架构微调和模型蒸馏等技术。可以在增加少许计算量的情况下，把ResNet-50的top 1 acc提升4个点，从而打败许多后起之秀。Talk is cheap, show me the code. 论文讨论的方法对应代码，都已经在GluonCV中开源，所以建议在阅读论文的时候，对照<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py" target="_blank" rel="noopener">代码</a>进行学习。</p><p><img src="/img/bag_of_tricks_resnet50_overperform_others.jpg" alt="ResNet-50的效果提升"></p><h2 id="Baseline-Training"><a href="#Baseline-Training" class="headerlink" title="Baseline Training"></a>Baseline Training</h2><p>这里介绍了一些（已经不算trick的）训练ResNet-50可以注意的地方。使用这些方法，应该可以复现论文中给出的结果。</p><h3 id="Data-Argumentation"><a href="#Data-Argumentation" class="headerlink" title="Data Argumentation"></a>Data Argumentation</h3><p>这里都是老生常谈了，可以直接参看代码<a href="https://github.com/dmlc/gluon-cv/blob/master/scripts/classification/imagenet/train_imagenet.py#L203" target="_blank" rel="noopener">gluon cv/image classification</a>。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">jitter_param = <span class="number">0.4</span></span><br><span class="line">lighting_param = <span class="number">0.1</span></span><br><span class="line">mean_rgb = [<span class="number">123.68</span>, <span class="number">116.779</span>, <span class="number">103.939</span>]</span><br><span class="line">std_rgb = [<span class="number">58.393</span>, <span class="number">57.12</span>, <span class="number">57.375</span>]</span><br><span class="line">train_data = mx.io.ImageRecordIter(</span><br><span class="line">    path_imgrec         = rec_train,</span><br><span class="line">    path_imgidx         = rec_train_idx,</span><br><span class="line">    preprocess_threads  = num_workers,</span><br><span class="line">    shuffle             = <span class="literal">True</span>,</span><br><span class="line">    batch_size          = batch_size,</span><br><span class="line">    data_shape          = (<span class="number">3</span>, input_size, input_size),</span><br><span class="line">    mean_r              = mean_rgb[<span class="number">0</span>],</span><br><span class="line">    mean_g              = mean_rgb[<span class="number">1</span>],</span><br><span class="line">    mean_b              = mean_rgb[<span class="number">2</span>],</span><br><span class="line">    std_r               = std_rgb[<span class="number">0</span>],</span><br><span class="line">    std_g               = std_rgb[<span class="number">1</span>],</span><br><span class="line">    std_b               = std_rgb[<span class="number">2</span>],</span><br><span class="line">    rand_mirror         = <span class="literal">True</span>,</span><br><span class="line">    random_resized_crop = <span class="literal">True</span>,</span><br><span class="line">    max_aspect_ratio    = <span class="number">4.</span> / <span class="number">3.</span>,</span><br><span class="line">    min_aspect_ratio    = <span class="number">3.</span> / <span class="number">4.</span>,</span><br><span class="line">    max_random_area     = <span class="number">1</span>,</span><br><span class="line">    min_random_area     = <span class="number">0.08</span>,</span><br><span class="line">    brightness          = jitter_param,</span><br><span class="line">    saturation          = jitter_param,</span><br><span class="line">    contrast            = jitter_param,</span><br><span class="line">    pca_noise           = lighting_param,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h3 id="参数初始化"><a href="#参数初始化" class="headerlink" title="参数初始化"></a>参数初始化</h3><ul><li>使用Xavier初始化卷积层和全连接层的权重，也就是$w\sim \mathcal{U}(-a, a)$，其中$a = \sqrt{6/(d_{in} + d_{out})}$，$d$是输入和输出的channel size。偏置项初始化为$0$。</li><li>BatchNorm的$\gamma$初始化为$1$，偏置项为$0$。</li></ul><h3 id="训练参数"><a href="#训练参数" class="headerlink" title="训练参数"></a>训练参数</h3><p>8卡V100，batch size = 256，使用NAG梯度下降，lr从0.1，在30，60，90epoch处除以10。</p><p>使用上述设置，得到的ResNet-50模型比原始论文更好，不过Inception-V3（输入为$229\times 229$大小）和MobileNet稍差于原始论文。</p><h2 id="更快地训练"><a href="#更快地训练" class="headerlink" title="更快地训练"></a>更快地训练</h2><p>主要讨论使用低精度（FP16）和大batch size对训练的影响。</p><h3 id="大batch-size"><a href="#大batch-size" class="headerlink" title="大batch size"></a>大batch size</h3><p>大的batch size经常会导致模型的val acc降低（一个简单的解释是，大batch size造成iteration次数减少，导致模型效果变差。当然，实际训练中，大batch size常常搭配较大的lr，所以问题并不是这么简单），可以考虑使用下面的方法解决这个问题。</p><h4 id="（成比例）提高lr"><a href="#（成比例）提高lr" class="headerlink" title="（成比例）提高lr"></a>（成比例）提高lr</h4><p>上面说的iteration次数减少是一个方面。另一个考虑是大的batch size会造成对梯度的估计方差变小，我们可以乘上一个较大的lr，让方差的不确定性增大一些。一个经验之谈是，lr随着batch size成比例扩大。比如在训练ResNet-50的时候，He给出的在$B = 256$时，lr取为$0.1$。那么如果$B = 512$，那么lr也相应扩大为$0.2$。</p><h4 id="lr-warm-up"><a href="#lr-warm-up" class="headerlink" title="lr warm up"></a>lr warm up</h4><p>如果lr初始设置的很大，可能会带来数值不稳定。因为刚开始的时候权重是随机初始化的，gradient也比较大。可以给lr做warm up，也就是开始若干个迭代用较小的lr，等训练稳定了再用回那个大的lr。一种方法是线性warm up，也就是在warm up阶段，lr是线性地从0涨到给定的那个大lr。</p><h4 id="设置-gamma-0"><a href="#设置-gamma-0" class="headerlink" title="设置$\gamma = 0$"></a>设置$\gamma = 0$</h4><p>这个操作比较新奇，在初始阶段，BN的$\beta$参数是设置为$0$的。如果我们再设置$\gamma = 0$，说明BN的输出就是$0$了。这是什么操作？！</p><p>作者指出，可以在ResNet这种有by-pass的结构中使用这个trick。在ResNet block的最后一层，我们经常做$y = x + res(x)$，可以考虑将res这一路的最后一个BN层的$\gamma$参数设置为0。这时候，相当于只有输入$x$传到后面，相当于减少了网络的层深。之后的训练中，$\gamma$会逐渐变大，也就逐渐恢复了res通路。</p><p>这种方法也是试图解决网络训练初始阶段不稳定的问题。不过这个操作还是挺骚的。。。类似的方法（利用BN层的$\gamma$参数）也见到过被用在模型剪枝上，如Net Sliming等方法。可以参见博客中的相关文章讨论。</p><h4 id="weight-decay"><a href="#weight-decay" class="headerlink" title="weight decay"></a>weight decay</h4><p>给weight加上L2 norm来做weight decay，是缓解网络过拟合的标准解决办法之一。不过，最好只对conv和fc的kernel做，而不要对它们的bias，BN的$\gamma$和$\beta$做。</p><p>上面的方法，在batch size不大于2K的时候，应该是够用了的。</p><h3 id="低精度"><a href="#低精度" class="headerlink" title="低精度"></a>低精度</h3><p>很多新GPU都加入了FP16的硬件支持，例如V100上使用FP16比FP32，训练能够加速$2$到$3$倍。FP16的问题是表示范围变小了，同时分辨率变小。对应地会造成两个问题，溢出和无法更新（梯度过小，不到FP16的最小表示）。一种解决办法是使用FP16来做forward和backward，但是在FP32上更新梯度（防止梯度过小）。同时给loss乘上一个系数，让它更好地契合FP16能表示的数据范围。</p><p>这里简要介绍下FP16精度的相关内容。关于Nvidia GPU FP16的更多信息，可以参考<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Nvidia文档混合精度训练</a>。</p><h4 id="FP16数据表示"><a href="#FP16数据表示" class="headerlink" title="FP16数据表示"></a>FP16数据表示</h4><p>FP16，顾名思义，就是使用16个bit表示浮点数。具体编码方式上，和FP32基本一致，只不过位数有了缩水。</p><blockquote><p>IEEE 754 standard defines the following 16-bit half-precision floating point format: 1 sign bit, 5 exponent bits, and 10 fractional bits.</p></blockquote><p>TODO: FP32和FP16的比较</p><h4 id="FP16-in-MXNet"><a href="#FP16-in-MXNet" class="headerlink" title="FP16 in MXNet"></a>FP16 in MXNet</h4><p>在MXNet中，使用混合精度训练还是挺简单的。具体可以参考<a href="https://mxnet.incubator.apache.org/versions/master/faq/float16.html" target="_blank" rel="noopener">Mixed precision training using float16</a></p><p>下面是使用gluon训练时候要注意的几个地方：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## optimizer 开启混合精度选项</span></span><br><span class="line"><span class="comment">## 这会使optimizer为参数保存一份FP32拷贝，在上面进行梯度的更新，</span></span><br><span class="line"><span class="comment">## 防止梯度过小无法更新FP16</span></span><br><span class="line"><span class="keyword">if</span> opt.dtype != <span class="string">'float32'</span>:</span><br><span class="line">    optimizer_params[<span class="string">'multi_precision'</span>] = <span class="literal">True</span></span><br><span class="line"><span class="comment">## net cast到给定的数值精度</span></span><br><span class="line">net = get_model(model_name, **kwargs)</span><br><span class="line">net.cast(opt.dtype)</span><br><span class="line"><span class="comment">## 训练过程中，将输入也cast到指定精度</span></span><br><span class="line"><span class="keyword">while</span> in_training:</span><br><span class="line">    <span class="comment">## blablabla</span></span><br><span class="line">    outputs = [net(X.astype(opt.dtype, copy=<span class="literal">False</span>)) <span class="keyword">for</span> X <span class="keyword">in</span> data]</span><br><span class="line">    <span class="comment">## 计算loss也把label cast到指定精度</span></span><br></pre></td></tr></table></figure><p>使用MXNet老的symbolic接口时候，因为静态图一旦写好就固定了，所以我们需要在建图的时候，考虑FP16精度。</p><ul><li>在原始输入node后面接一个<code>cast</code> op，将FP32转成FP16。</li><li>最好在<code>SoftmaxOutput</code>之前，插入一个<code>cast</code> op，将FP16转回FP32，以便有更高的精度。</li><li><code>optimizer</code>打开<code>multi_precision</code>开关，这里和上面gluon是一致的。</li></ul><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 建图</span></span><br><span class="line">data = mx.sym.Variable(name=<span class="string">"data"</span>)</span><br><span class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</span><br><span class="line">    data = mx.sym.Cast(data=data, dtype=np.float16)</span><br><span class="line"><span class="comment"># ... the rest of the network</span></span><br><span class="line">net_out = net(data)</span><br><span class="line"><span class="keyword">if</span> dtype == <span class="string">'float16'</span>:</span><br><span class="line">    net_out = mx.sym.Cast(data=net_out, dtype=np.float32)</span><br><span class="line">output = mx.sym.SoftmaxOutput(data=net_out, name=<span class="string">'softmax'</span>)</span><br><span class="line"><span class="comment">## 优化器设置</span></span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>, multi_precision=<span class="literal">True</span>, lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><p>下面有几条额外的建议：</p><ul><li>FP16加速主要来源于新GPU上的Tensor Core计算$D = A * B + C$这种运算，且它们的维度是$8$的倍数。所以如果不满足$8$倍数这个条件，FP16的计算速度可能不会很快，或者说和FP32相比没多少优势。尤其是当你在CIFAR10这种输入图片size比较小的数据集上训练的时候。</li><li>针对上面这种情况，你可以使用<code>nvprof</code>工具来check是否Tensor Core被使用了，那些名字里面带有<code>s884cudnn</code>的操作就是了。</li><li>确保data io和preprocessing不要成为瓶颈，不然面对这些扯后腿的地方，FP16男默女泪。</li><li>batch size最好设置为8的倍数，2的幂次是坠吼的。</li><li>如果GPU memory还算充足，可以设置<code>MXNET_CUDNN_AUTOTUNE_DEFAULT = 2</code>，来让MXNet有更多的测试来选用最快的卷积算法，代价就是更多的显存占用。</li><li>最好为BatchNorm和SoftmaxOutput使用FP32精度。Gluon里面这些都是自动的，MXNet中BN层是自动的，但是SoftmaxOutput需要自己设置一下，见上。</li></ul><h4 id="loss-scaling"><a href="#loss-scaling" class="headerlink" title="loss scaling"></a>loss scaling</h4><p>再说一下上面提到的loss scaling。</p><p>为啥要做loss scaling呢？主要是由于FP16的精度比较差，而能够表示的较大的数对于CNN网络来说又基本用不到（虽然说FP16的表示范围相比FP32已经缩水不少了），所以可能出现这样一种情形，loss对FP16 weight或activation求梯度，梯度太小，以至于FP16无法表示。那其实我们可以给loss乘上一个系数，放大gradient，以便FP16能够表示。在梯度更新之前，再把这个梯度scale回去，就可以了。如下图所示。</p><p><img src="/img/bag_of_tricks_fp16_range_dismatch.jpg" alt="FP16和FP32的range不匹配"></p><p>使用gluon或MXNet设置loss scaling的方法如下：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## gluon</span></span><br><span class="line">loss = gluon.loss.SoftmaxCrossEntropyLoss(weight=<span class="number">128</span>)</span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</span><br><span class="line">                                multi_precision=<span class="literal">True</span>,</span><br><span class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</span><br><span class="line"><span class="comment">## mxnet</span></span><br><span class="line">mxnet.sym.SoftmaxOutput(other_args, grad_scale=<span class="number">128.0</span>)</span><br><span class="line">optimizer = mx.optimizer.create(<span class="string">'sgd'</span>,</span><br><span class="line">                                multi_precision=<span class="literal">True</span>,</span><br><span class="line">                                rescale_grad=<span class="number">1.0</span>/<span class="number">128</span>)</span><br></pre></td></tr></table></figure><p>经验来看，对于Multibox SSD, R-CNN, bigLSTM and Seq2seq这些任务，loss scaling是比较有必要的。这里有个疑问，loss scaling应该是在训练过程中不断变化的，但上面的使用都是直接把loss scaling写死了（gluon还好，再手动给loss乘上一个因子），那如何修改loss scaling呢？后面指出可以使用constant的loss scaling（一般取2的幂次64，128等），但是不知道实际训练会不会有问题。<a href="https://docs.nvidia.com/deeplearning/sdk/mixed-precision-training/index.html" target="_blank" rel="noopener">Nvidia guide</a>中给出的建议是：</p><blockquote><p>If you encounter precision problems, it is beneficial to scale the loss up by 128, and scale the application of the gradients down by 128.</p></blockquote><p>当然，最好的办法是自己看一下FP32 gradient的分布。</p><p>当当当。。。说了这么多，那么具体加速效果如何呢？使用batch size = $1024$，和batch size = $256$的baseline相比，从下表可知，三种不同的网络结构，分别加速了$1.6X$到$3X$，而且acc还涨了一些。</p><p><img src="/img/bag_of_tricks_accelarate_training.jpg" alt="加速效果"></p><p>具体的acc影响的ablation实验如下。可以看到，只是使用lr线性增大的情况下，大（batch size的）网络稍逊于小（batch size的）网络。不过当使用上面几个技术综合来看的时候，大小网络的性能差异已经抹去了，而且大网络的训练速度更快。</p><p><img src="/img/bag_of_tricks_ablation_of_accelarate_train.jpg" alt="ablation实验结果"></p><h2 id="更好的网络"><a href="#更好的网络" class="headerlink" title="更好的网络"></a>更好的网络</h2><p>TODO: 未完待续</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这是&lt;a href=&quot;https://arxiv.org/abs/1812.01187&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Bag of Tricks for Image Classification with Convolutional Neural Networks&lt;/a&gt;的笔记。这篇文章躺在阅读列表里面很久了，里面的技术之前也用了一些。最近趁着做SOTA模型的训练，把论文整体读了一下，记录在这里。这篇文章总结的仍然是在通用学术数据集上的tricks。对于实际工作中遇到的训练任务，仍然是要结合问题本身来改进模型和训练算法。毕竟，没有银弹。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/bag_of_tricks_no_silver_bullet.jpeg&quot; alt=&quot;软工里面没有银弹，数据科学同样这样&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Hello TVM</title>
    <link href="https://xmfbit.github.io/2019/06/29/tvm-helloworld/"/>
    <id>https://xmfbit.github.io/2019/06/29/tvm-helloworld/</id>
    <published>2019-06-29T13:55:43.000Z</published>
    <updated>2020-05-02T07:17:54.456Z</updated>
    
    <content type="html"><![CDATA[<p>TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！)</p><blockquote><p>TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.</p></blockquote><p>compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。</p><p>我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM!</p><p><img src="/img/tvm_introduction.jpg" alt="TVM概念图"></p><a id="more"></a><h2 id="背景介绍"><a href="#背景介绍" class="headerlink" title="背景介绍"></a>背景介绍</h2><p>随着深度学习逐渐从研究所的“伊甸园”迅速在工业界的铺开，摆在大家面前的问题是如何将深度学习模型部署到目标硬件平台上，能够多快好省地完成前向计算，从而提供更好的用户体验，<del>同时为老板省钱，还能减少碳排放来造福子孙</del>。</p><p>和单纯做研究相比，在工业界我们主要遇到了两个问题：</p><ul><li>深度学习框架实在是太$^{\text{TM}}$多了。caffe / mxnet / tensorflow / pytorch训练出来的模型都彼此有不同的分发格式。如果你和我一样，做过不同框架的TensorRT的部署，我想你会懂的。。。</li><li>GPU实在是太$^{\text{TM}}$贵了。深度学习春风吹满地，老黄股票真争气。另一方面，一些嵌入式平台没有使用GPU的条件。同时一些人也开始在做FPGA/ASIC的深度学习加速卡。如何将深度学习模型部署适配到多样的硬件平台上？</li></ul><p>为了解决第一个问题，TVM内部实现了自己的IR，可以将上面这些主流深度学习框架的模型转换为统一的内部表示，以便后续处理。若想要详细了解，可以看下NNVM这篇博客：<a href="https://tvm.ai/2017/10/06/nnvm-compiler-announcement" target="_blank" rel="noopener">NNVM Compiler: Open Compiler for AI Frameworks</a>。这张图应该能够说明NNVM在TVM中起到的作用。</p><p><img src="/img/tvm_hello_nnvm_as_a_bridge.jpg" alt="NNVM在TVM中的作用"></p><p>为了解决第二个问题，TVM内部有多重机制来做优化。其中一个特点是，使用机器学习（结合专家知识）的方法，通过在目标硬件上跑大量trial，来获得该硬件上相关运算（例如卷积）的最优实现。这使得TVM能够做到快速为新型硬件或新的op做优化。我们知道，在GPU上我们站在Nvidia内部专家的肩膀上，使用CUDA / CUDNN / CUBLAS编程。但相比于Conv / Pooling等Nvidia已经优化的很好了的op，我们自己写的op很可能效率不高。或者在新的硬件上，没有类似CUDA的生态，如何对网络进行调优？TVM这种基于机器学习的方法给出了一个可行的方案。我们只需给定参数的搜索空间（少量的人类专家知识），就可以将剩下的工作交给TVM。如果对此感兴趣，可以阅读TVM中关于AutoTuner的介绍和tutorial：<a href="https://docs.tvm.ai/tutorials/autotvm/tune_nnvm_arm.html" target="_blank" rel="noopener">Auto-tuning a convolutional network for ARM CPU</a>。</p><h2 id="编译"><a href="#编译" class="headerlink" title="编译"></a>编译</h2><p>我的环境为Debian 8，CUDA 9。</p><h3 id="准备代码"><a href="#准备代码" class="headerlink" title="准备代码"></a>准备代码</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">git <span class="built_in">clone</span> https://github.com/dmlc/tvm.git</span><br><span class="line"><span class="built_in">cd</span> tvm</span><br><span class="line">git checkout e22b5802</span><br><span class="line">git submodule update --init --recursive</span><br></pre></td></tr></table></figure><h3 id="config文件"><a href="#config文件" class="headerlink" title="config文件"></a>config文件</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> tvm</span><br><span class="line">mkdir build</span><br><span class="line">cp ../cmake/config.cmake ./build</span><br><span class="line"><span class="built_in">cd</span> build</span><br></pre></td></tr></table></figure><p>编辑config文件，打开CUDA / BLAS / cuBLAS / CUDNN的开关。注意下LLVM的开关。LLVM可以从这个页面<a href="http://releases.llvm.org/download.html" target="_blank" rel="noopener">LLVM Download</a>下载，我之前就已经下载好，版本为7.0。如果你像我一样是Debian8，可以使用for Ubuntu14.04的那个版本。由于是已经编译好的二进制包，下载之后解压即可。</p><p>找到这一行，改成<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">set(USE_LLVM /path/to/llvm/bin/llvm-config)</span><br></pre></td></tr></table></figure></p><h3 id="编译-1"><a href="#编译-1" class="headerlink" title="编译"></a>编译</h3><p>这里有个坑，因为我们使用了LLVM，最好使用LLVM中的clang。否则可能导致tvm生成的代码无法二次导入。见这个讨论帖：<a href="https://discuss.tvm.ai/t/runtime-llvm-cc-create-shared-error-while-run-tune-simple-template/1037" target="_blank" rel="noopener">_cc.create_shared error while run tune_simple_template</a>。</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">export</span> LLVM=/path/to/llvm</span><br><span class="line">cmake -DCMAKE_C_COMPILER=<span class="variable">$LLVM</span>/bin/clang -DCMAKE_CXX_COMPILER=<span class="variable">$LLVM</span>/bin/clang++ ..</span><br><span class="line"><span class="comment"># 火力全开，let's rock</span></span><br><span class="line">make -j$(nproc)</span><br></pre></td></tr></table></figure><h3 id="python包安装"><a href="#python包安装" class="headerlink" title="python包安装"></a>python包安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">cd</span> /path/to/tvm</span><br><span class="line"><span class="comment"># 我一般用清华的镜像，你呢。。。</span></span><br><span class="line"><span class="built_in">export</span> THU_MIRROR=https://pypi.tuna.tsinghua.edu.cn/simple</span><br><span class="line">pip install tornado tornado psutil xgboost numpy decorator attrs  --user -i <span class="variable">$THU_MIRROR</span></span><br><span class="line"><span class="built_in">cd</span> python; python setup.py install --user; <span class="built_in">cd</span> ..</span><br><span class="line"><span class="built_in">cd</span> topi/python; python setup.py install --user; <span class="built_in">cd</span> ../..</span><br><span class="line"><span class="built_in">cd</span> nnvm/python; python setup.py install --user; <span class="built_in">cd</span> ../..</span><br></pre></td></tr></table></figure><h2 id="demo"><a href="#demo" class="headerlink" title="demo"></a>demo</h2><p>使用tvm为mxnet symbol计算图生成CUDA代码，并进行前向计算。</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> tvm</span><br><span class="line"><span class="keyword">from</span> tvm <span class="keyword">import</span> relay</span><br><span class="line"><span class="keyword">from</span> tvm.relay <span class="keyword">import</span> testing</span><br><span class="line"><span class="keyword">from</span> tvm.contrib <span class="keyword">import</span> graph_runtime</span><br><span class="line"><span class="keyword">import</span> mxnet <span class="keyword">as</span> mx</span><br><span class="line"></span><br><span class="line"><span class="comment">## load mxnet model</span></span><br><span class="line">prefix = <span class="string">'/your/mxnet/checkpoint/prefix'</span></span><br><span class="line">epoch = <span class="number">0</span></span><br><span class="line">mx_sym, arg_params, aux_params = mx.model.load_checkpoint(prefix, epoch)</span><br><span class="line"></span><br><span class="line"><span class="comment">## import model into tvm from mxnet</span></span><br><span class="line">shape_dict = &#123;<span class="string">'data'</span>: (<span class="number">1</span>, <span class="number">3</span>, <span class="number">224</span>, <span class="number">224</span>)&#125;</span><br><span class="line"><span class="comment">## tvm提供了 frontend.from_XXX 接口，从不同的框架中导入模型</span></span><br><span class="line">relay_func, relay_params = relay.frontend.from_mxnet(mx_sym, shape_dict,</span><br><span class="line">        arg_params=arg_params, aux_params=aux_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设定目标硬件为 GPU，生成TVM模型</span></span><br><span class="line"><span class="comment">## ---------------------------- </span></span><br><span class="line"><span class="comment"># graph：execution graph in json format</span></span><br><span class="line"><span class="comment"># lib: tvm module library of compiled functions for the graph on the target hardware</span></span><br><span class="line"><span class="comment"># params: parameter blobs</span></span><br><span class="line"><span class="comment">## ---------------------------</span></span><br><span class="line">target = <span class="string">'cuda'</span></span><br><span class="line"><span class="keyword">with</span> relay.build_config(opt_level=<span class="number">3</span>):</span><br><span class="line">    graph, lib, params = relay.build(relay_func, target, params=relay_params)</span><br><span class="line"></span><br><span class="line"><span class="comment"># run forward</span></span><br><span class="line"><span class="comment">## 直接使用tvm提供的cat示例图片</span></span><br><span class="line"><span class="keyword">from</span> tvm.contrib.download <span class="keyword">import</span> download_testdata</span><br><span class="line">img_url = <span class="string">'https://github.com/dmlc/mxnet.js/blob/master/data/cat.png?raw=true'</span></span><br><span class="line">img_path = download_testdata(img_url, <span class="string">'cat.png'</span>, module=<span class="string">'data'</span>)</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line">image = Image.open(img_path).resize((<span class="number">224</span>, <span class="number">224</span>))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">transform_image</span><span class="params">(im)</span>:</span></span><br><span class="line">    im = np.array(im).astype(np.float32)</span><br><span class="line">    im = np.transpose(im, [<span class="number">2</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">    im = im[np.newaxis, :]</span><br><span class="line">    <span class="keyword">return</span> im</span><br><span class="line"></span><br><span class="line">x = transform_image(image)</span><br><span class="line"></span><br><span class="line"><span class="comment"># let's go</span></span><br><span class="line">ctx = tvm.gpu(<span class="number">0</span>)</span><br><span class="line">dtype = <span class="string">'float32'</span></span><br><span class="line"><span class="comment">## 加载模型</span></span><br><span class="line">m = graph_runtime.create(graph, lib, ctx)</span><br><span class="line"><span class="comment">## set input data</span></span><br><span class="line">m.set_input(<span class="string">'data'</span>, tvm.nd.array(x.astype(dtype)))</span><br><span class="line"><span class="comment">## set input params</span></span><br><span class="line">m.set_input(**params)</span><br><span class="line">m.run()</span><br><span class="line"><span class="comment"># get output</span></span><br><span class="line">outputs = m.get_output(<span class="number">0</span>)</span><br><span class="line">top1 = np.argmax(outputs.asnumpy()[<span class="number">0</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># save model</span></span><br><span class="line"><span class="comment">## lib存为tar包文件，解压后可以发现，就是打包了动态链接库</span></span><br><span class="line">path_lib = <span class="string">'./deploy_resnet50_v2_lib.tar'</span></span><br><span class="line">lib.export_library(path_lib)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 计算图存为json文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>, <span class="string">'w'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(graph)</span><br><span class="line"><span class="comment">## 权重存为二进制文件</span></span><br><span class="line"><span class="keyword">with</span> open(<span class="string">'./deploy_params'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> f:</span><br><span class="line">    f.write(relay.save_param_dict(params))</span><br><span class="line"></span><br><span class="line"><span class="comment"># load model back</span></span><br><span class="line">loaded_json = open(<span class="string">'./deploy_resnet50_v2_graph.json'</span>).read()</span><br><span class="line">loaded_lib = tvm.module.load(path_lib)</span><br><span class="line">loaded_params = bytearray(open(<span class="string">'./deploy_params'</span>, <span class="string">'rb'</span>).read())</span><br><span class="line">module = graph_runtime.create(loaded_json, loaded_lib, ctx)</span><br><span class="line"><span class="comment">## 好了，剩下的就都一样了</span></span><br></pre></td></tr></table></figure><h2 id="最后的话"><a href="#最后的话" class="headerlink" title="最后的话"></a>最后的话</h2><p>我个人的观点，TVM是一个很有意思的项目。在深度学习模型的优化和部署上做了很多探索，在官方放出的benchmark上表现还是不错的。如果使用非GPU进行模型的部署，TVM值得一试。不过在GPU上，得益于Nvidia的CUDA生态，目前TensorRT仍然用起来更方便，综合性能更好。如果你和我一样，主要仍然在GPU上搞事情，可以密切关注TVM的发展，并尝试使用在自己的项目中，不过我觉得还是优先考虑TensorRT。<del>另一方面，TVM的代码实在是看不太懂啊。。。</del></p><h2 id="想要更多"><a href="#想要更多" class="headerlink" title="想要更多"></a>想要更多</h2><ul><li>TVM paper：<a href="https://arxiv.org/abs/1802.04799" target="_blank" rel="noopener">TVM: An Automated End-to-End Optimizing Compiler for Deep Learning</a></li><li>TVM 项目主页：<a href="https://tvm.ai/" target="_blank" rel="noopener">TVM</a></li></ul><p>后续TVM的介绍，不知道啥时候有时间再写。。。随缘吧。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;TVM 是什么？A compiler stack，graph level / operator level optimization，目的是（不同框架的）深度学习模型在不同硬件平台上提高 performance (我要更快！)&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;TVM, a compiler that takes a high-level specification of a deep learning program from existing frameworks and generates low-level optimized code for a diverse set of hardware back-ends.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;compiler比较好理解。C编译器将C代码转换为汇编，再进一步处理成CPU可以理解的机器码。TVM的compiler是指将不同前端深度学习框架训练的模型，转换为统一的中间语言表示。stack我的理解是，TVM还提供了后续处理方法，对IR进行优化（graph / operator level），并转换为目标硬件上的代码逻辑（可能会进行benchmark，反复进行上述优化），从而实现了端到端的深度学习模型部署。&lt;/p&gt;
&lt;p&gt;我刚刚接触TVM，这篇主要介绍了如何编译TVM，以及如何使用TVM加载mxnet模型，进行前向计算。Hello TVM!&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/tvm_introduction.jpg&quot; alt=&quot;TVM概念图&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="tvm" scheme="https://xmfbit.github.io/tags/tvm/"/>
    
  </entry>
  
  <entry>
    <title>重读 C++ Primer</title>
    <link href="https://xmfbit.github.io/2019/05/01/cpp-primer-review/"/>
    <id>https://xmfbit.github.io/2019/05/01/cpp-primer-review/</id>
    <published>2019-05-01T13:32:58.000Z</published>
    <updated>2020-05-02T07:17:54.448Z</updated>
    
    <content type="html"><![CDATA[<p>重读C++ Primer第五版，整理一些糊涂的语法知识点。</p><div align="center">     <img src="/img/cpp_is_terrible.jpg" width="200" height="300" alt="入门到放弃" align="center"></div><a id="more"></a><h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><p>总结一些比较容易搞乱的基础语法。</p><h3 id="const-限定说明符"><a href="#const-限定说明符" class="headerlink" title="const 限定说明符"></a>const 限定说明符</h3><ul><li><code>const</code>对象一般只在当前文件可见，如果希望在其他文件访问，在声明和定义时，均需加上<code>extern</code>关键字。</li></ul><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">extern</span> <span class="keyword">const</span> <span class="keyword">int</span> BUF_SIZE = <span class="number">100</span>;</span><br></pre></td></tr></table></figure><ul><li>顶层<code>const</code>和底层<code>const</code></li></ul><p>指针本身也是对象，所以有所谓的“常量指针”（指针本身不能赋值）和“指向常量的指针”（指针指向的那个对象不能赋值）。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">10</span>;</span><br><span class="line"><span class="comment">// 指针指向的对象不能经由指针赋值</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* p1 = &amp;a;</span><br><span class="line">*p = <span class="number">0</span>;  <span class="comment">// 错误</span></span><br><span class="line"><span class="comment">// 指针本身不能再赋值</span></span><br><span class="line"><span class="keyword">int</span>* <span class="keyword">const</span> p2 = &amp;a;</span><br><span class="line"><span class="keyword">int</span> b = <span class="number">0</span>;</span><br><span class="line">p2 = &amp;p;   <span class="comment">// 错误</span></span><br></pre></td></tr></table></figure><p>如何记住这条规则？c++中类型说明从右向左读即可。例如<code>p1</code>，其左侧首先遇到<code>int*</code>，故其是个“普通”指针（没有被<code>const</code>修饰），再往左才读到<code>const</code>，故这个指针指向的内容是常量，不能修改。<code>p2</code>同理。</p><p>把“指针本身是常量”的行为称为“顶层const”（top-level），把“指针指向内容是常量”的行为称为“底层const”（low-level）。</p><h3 id="auto-和-decltype"><a href="#auto-和-decltype" class="headerlink" title="auto 和 decltype"></a>auto 和 decltype</h3><ul><li><code>auto</code>类型推断的规则</li></ul><p>编译器推断<code>auto</code>声明变量的类型时，可能和初始值类型不一样。当初始值类型为引用时，编译器以被引用对象的类型作为<code>auto</code>的类型，除非显式指明。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">int</span>&amp; ri = i;</span><br><span class="line"><span class="comment">// type of j: int</span></span><br><span class="line"><span class="keyword">auto</span> j = ri;</span><br><span class="line"><span class="comment">// type of rj: int&amp;</span></span><br><span class="line"><span class="keyword">auto</span>&amp; rj = ri;</span><br></pre></td></tr></table></figure><p>另外，<code>auto</code>只会保留底层<code>const</code>，忽略顶层<code>const</code>，除非显式指定。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> a = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">const</span> <span class="keyword">int</span>* <span class="keyword">const</span> p = &amp;a;</span><br><span class="line"><span class="comment">// type of b: int</span></span><br><span class="line"><span class="keyword">auto</span> b = a;</span><br><span class="line"><span class="comment">// type of p1: const int*</span></span><br><span class="line"><span class="keyword">auto</span> p1 = p;</span><br><span class="line"><span class="comment">// type of p2: const int* const</span></span><br><span class="line"><span class="keyword">const</span> <span class="keyword">auto</span> p2 = p;</span><br><span class="line">p1 = &amp;b;   <span class="comment">// ok, p1 本身已经不是const的了</span></span><br><span class="line">p2 = &amp;b;   <span class="comment">// wrong! 显式指定了 p2 本身是 const</span></span><br><span class="line">*p1 = <span class="number">10</span>;  <span class="comment">// wrong! p1 保留了底层const，指向的内容仍然不可改变</span></span><br></pre></td></tr></table></figure><ul><li><code>decltype</code> 类型推断规则</li></ul><p>和<code>auto</code>不同，<code>decltype</code>保留表达式的顶层<code>const</code>和引用。</p><ol><li>如果表达式是变量，那么返回该变量的类型；</li><li>如果表达式不是纯变量，返回表达式结果的类型；</li><li>如果表达式是解引用，返回引用类型。</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">int</span> i = <span class="number">42</span>, *p = &amp;i, &amp;r = i;</span><br><span class="line"><span class="keyword">decltype</span>(i) j;    <span class="comment">// ok, j is a int</span></span><br><span class="line"><span class="keyword">decltype</span>(r) y;    <span class="comment">// wrong! y是引用类型，必须初始化</span></span><br><span class="line"><span class="keyword">decltype</span>(r + <span class="number">0</span>) z;  <span class="comment">// ok, r+0 返回值是int</span></span><br><span class="line"><span class="keyword">decltype</span>(*p) c;   <span class="comment">// wrong! 解引用的结果是引用，必须初始化</span></span><br></pre></td></tr></table></figure><p>有一种情况特殊，如果是春变量，但是变量名加上括号，结果将是引用。原因：变量加上括号，将会被当做表达式。而变量又可以被赋值，所以得到了引用。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dectype((i)) d;  <span class="comment">// wrong! d是引用</span></span><br></pre></td></tr></table></figure><h2 id="泛型算法"><a href="#泛型算法" class="headerlink" title="泛型算法"></a>泛型算法</h2><p>C++的标准库中实现了很多泛型算法，如<code>find</code>, <code>sort</code>等。它们大多定义在<code>&lt;algorithm&gt;</code>头文件中，一些数值相关的定义在<code>&lt;numeric&gt;</code>中。通过“迭代器”这一层抽象，泛型算法可以不关心所操作数据实际储存的容器，不过仍然受制于实际数据类型。例如<code>find</code>中，为了比较当前元素是否为所求值，要求元素类型实现<code>==</code>运算。好在这些算法大多支持自定义操作。</p><h3 id="迭代器"><a href="#迭代器" class="headerlink" title="迭代器"></a>迭代器</h3><p>在标准库的<code>&lt;iterator&gt;</code>中，定义了如下几种通用迭代器。</p><ul><li>插入迭代器</li></ul><p>插入器是一个迭代器的适配器，接受一个容器，生成一个用于该容器的迭代器，能够实现向该容器插入元素。插入迭代器有三种，区别在于插入元素的位置：</p><ol><li><code>back_inserter</code>，创建一个使用<code>push_back</code>插入的迭代器</li><li><code>front_inserter</code>，创建一个使用<code>push_front</code>插入的迭代器</li><li><code>inserter</code>，创建一个使用<code>insert</code>的迭代器，在给定的迭代器前面插入元素</li></ol><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;iterator&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="meta-keyword">include</span> <span class="meta-string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> <span class="built_in">std</span>;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用back_inserter插入数据</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b;</span><br><span class="line">    <span class="comment">// copy a -&gt; b, 动态改变b的大小</span></span><br><span class="line">    copy(begin(a), end(a), back_inserter(b));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v: b) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// b: 1, 2, 3, 4, 5</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 使用inserter，将数据插入指定位置</span></span><br><span class="line"><span class="function"><span class="keyword">int</span> <span class="title">main</span><span class="params">()</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> a[] = &#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line">    <span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; b &#123;<span class="number">6</span>,<span class="number">7</span>,<span class="number">8</span>&#125;;</span><br><span class="line">    <span class="comment">// find iter of value 8</span></span><br><span class="line">    <span class="keyword">auto</span> iter = find(b.begin(), b.end(), <span class="number">8</span>);</span><br><span class="line">    <span class="comment">// copy a -&gt; b before value 8</span></span><br><span class="line">    copy(begin(a), end(a), inserter(b, iter));</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">auto</span> v : b) &#123;</span><br><span class="line">        <span class="built_in">cout</span> &lt;&lt; v &lt;&lt; <span class="built_in">endl</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="comment">// b: 6, 7, 1, 2, 3, 4, 5, 8</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>这里要注意的是，当使用<code>front_inserter</code>时，由于插入总是在容器头部发生，所以最后的插入结果是原始数据序列的逆序。</p><ul><li>流迭代器</li></ul><p>虽然输入输出流不是容器，不过也有用于这些IO对象的迭代器：<code>istream_iterator</code>和<code>ostream_iterator</code>。这样，我们可以通过它们向对应的输入输出流读写数据，</p><p>创建输入流迭代器时，必须指定其要操作的数据类型，并将其绑定到某个流（标准输入输出流或文件流），或使用默认初始化，得到当做尾后值使用的迭代器。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function">istream_iterator&lt;<span class="keyword">int</span>&gt; <span class="title">in_iter</span><span class="params">(<span class="built_in">cin</span>)</span></span>;</span><br><span class="line">istream_iterator&lt;<span class="keyword">int</span>&gt; in_eof;</span><br><span class="line"><span class="comment">// 使用迭代器构建vector</span></span><br><span class="line"><span class="function"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; <span class="title">values</span><span class="params">(in_iter, in_eof)</span></span>;</span><br></pre></td></tr></table></figure><p>创建输出流迭代器时，必须指定其要操作的数据类型，并向其绑定到某个流，还可以传入第二个参数，类型是C风格的字符串（字符串字面常量或指向<code>\0</code>结尾的字符数组指针），表示在输出数据之后，还会输出此字符串。</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v&#123;<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>&#125;;</span><br><span class="line"><span class="comment">// 输出：1       2       3       4       5 </span></span><br><span class="line">copy(v.begin(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">"\t"</span>));</span><br></pre></td></tr></table></figure><ul><li>反向迭代器</li></ul><p>顾名思义，反向迭代器的迭代顺序和正常的迭代器是相反的。使用<code>rbegin</code>和<code>rend</code>可以获得绑定在该容器的反向迭代器。不过<code>forward_list</code>和流对象，由于没有同时实现<code>++</code>和<code>--</code>，所以没有反向迭代器。</p><p>反向迭代器常常用来在容器中查找最后一个满足条件的元素。这时候要注意，如果继续使用该迭代器，顺序仍然是反向的。如果需要正向迭代器，可以使用<code>.base()</code>方法得到对应的正向迭代器。不过要注意，正向迭代器和反向迭代器的位置会不一样哦~</p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// 找到数组中最后一个5,并将其后数字打印出来</span></span><br><span class="line"><span class="built_in">vector</span>&lt;<span class="keyword">int</span>&gt; v &#123;<span class="number">10</span>, <span class="number">5</span>, <span class="number">4</span>, <span class="number">5</span>, <span class="number">1</span>, <span class="number">2</span>&#125;;</span><br><span class="line"><span class="keyword">auto</span> iter = find(v.rbegin(), v.rend(), <span class="number">5</span>);</span><br><span class="line"><span class="comment">// 输出：5,4,5,10,</span></span><br><span class="line">copy(iter, v.rend(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</span><br><span class="line"><span class="built_in">cout</span> &lt;&lt; <span class="string">"\n"</span>;</span><br><span class="line"><span class="comment">// 输出：1,2, 注意并没有输出5</span></span><br><span class="line">copy(iter.base(), v.end(), ostream_iterator&lt;<span class="keyword">int</span>&gt;(<span class="built_in">cout</span>, <span class="string">","</span>));</span><br></pre></td></tr></table></figure><h2 id="未完待续"><a href="#未完待续" class="headerlink" title="未完待续"></a>未完待续</h2><p>拖延症发作。。。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;重读C++ Primer第五版，整理一些糊涂的语法知识点。&lt;/p&gt;
&lt;div align=&quot;center&quot;&gt;    
 &lt;img src=&quot;/img/cpp_is_terrible.jpg&quot; width=&quot;200&quot; height=&quot;300&quot; alt=&quot;入门到放弃&quot; align=&quot;center&quot;&gt;
&lt;/div&gt;
    
    </summary>
    
    
      <category term="cpp" scheme="https://xmfbit.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>YOLO Caffe模型转换BN的坑</title>
    <link href="https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/"/>
    <id>https://xmfbit.github.io/2019/03/09/darknet-caffe-converter/</id>
    <published>2019-03-09T12:51:18.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。</p><a id="more"></a><h1 id="Darknet中BN的计算"><a href="#Darknet中BN的计算" class="headerlink" title="Darknet中BN的计算"></a>Darknet中BN的计算</h1><p>以CPU代码为例，在Darknet中，BN做normalization的操作如下，<a href="https://github.com/pjreddie/darknet/blob/master/src/blas.c#L147" target="_blank" rel="noopener">normalize_cpu</a></p><figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">void</span> <span class="title">normalize_cpu</span><span class="params">(<span class="keyword">float</span> *x, <span class="keyword">float</span> *mean, <span class="keyword">float</span> *variance, <span class="keyword">int</span> batch, <span class="keyword">int</span> filters, <span class="keyword">int</span> spatial)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">    <span class="keyword">int</span> b, f, i;</span><br><span class="line">    <span class="keyword">for</span>(b = <span class="number">0</span>; b &lt; batch; ++b)&#123;</span><br><span class="line">        <span class="keyword">for</span>(f = <span class="number">0</span>; f &lt; filters; ++f)&#123;</span><br><span class="line">            <span class="keyword">for</span>(i = <span class="number">0</span>; i &lt; spatial; ++i)&#123;</span><br><span class="line">                <span class="keyword">int</span> index = b*filters*spatial + f*spatial + i;</span><br><span class="line">                x[index] = (x[index] - mean[f])/(<span class="built_in">sqrt</span>(variance[f]) + <span class="number">.000001</span>f);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>可以看到，Darknet中的BN计算如下：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2} + \epsilon}</script><p>而且，$\epsilon$参数是固定的，为$1\times 10^{-6}$。</p><h1 id="问题和解决"><a href="#问题和解决" class="headerlink" title="问题和解决"></a>问题和解决</h1><p>然而，在Caffe（以及大部分其他框架）中，$\epsilon$的位置是在根号里面的，也就是：</p><script type="math/tex; mode=display">\hat{x} = \frac{x - \mu}{\sqrt{\sigma^2 + \epsilon}}</script><p>另外，查看<code>caffe.proto</code>可以知道，Caffe默认的$\epsilon$值为$1\times 10^{-5}$。</p><p>所以，在转换为caffe prototxt时，需要设置<code>batch_norm_param</code>如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">batch_norm_param &#123;</span><br><span class="line">  use_global_stats: true</span><br><span class="line">  eps: 1e-06</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>另外，需要重新求解$\sigma^2$，按照layer输出要相等的等量关系，可以求得：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">convert_running_var</span><span class="params">(var, eps=DARKNET_EPS)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.square(np.sqrt(var) + eps) - eps</span><br></pre></td></tr></table></figure><p>这里调整之后，转换后的Caffe模型和原始Darknet模型的输出误差已经是$1\times 10^{-7}$量级，可以认为转换成功。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;YOLO虽好，但是Darknet框架实在是小众，有必要在Inference阶段将其转换为其他框架，以便后续统一部署和管理。Caffe作为小巧灵活的老资格框架，使用灵活，方便魔改，所以尝试将Darknet训练的YOLO模型转换为Caffe。这里简单记录下YOLO V3 原始Darknet模型转换为Caffe模型过程中的一个坑。&lt;/p&gt;
    
    </summary>
    
    
      <category term="caffe" scheme="https://xmfbit.github.io/tags/caffe/"/>
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
  </entry>
  
  <entry>
    <title>MacOS Mojave更新之后一定要做这几件事！</title>
    <link href="https://xmfbit.github.io/2018/10/27/mac-update-mojave/"/>
    <id>https://xmfbit.github.io/2018/10/27/mac-update-mojave/</id>
    <published>2018-10-27T14:57:12.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。<br><a id="more"></a></p><h2 id="Git等工具"><a href="#Git等工具" class="headerlink" title="Git等工具"></a>Git等工具</h2><p>试图使用<code>git</code>是出现了如下错误：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git clone xx.git</span><br><span class="line">xcrun: error: invalid active developer path (/Library/Developer/CommandLineTools), missing xcrun at: /Library/Developer/CommandLineTools/usr/bin/xcrun</span><br></pre></td></tr></table></figure><p>解决办法参考<a href="https://apple.stackexchange.com/questions/254380/macos-mojave-invalid-active-developer-path" target="_blank" rel="noopener">macOS Mojave: invalid active developer path</a>中的最高赞回答：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">xcode-select --install</span><br></pre></td></tr></table></figure></p><h2 id="osxfuse"><a href="#osxfuse" class="headerlink" title="osxfuse"></a>osxfuse</h2><p>参考Github讨论帖<a href="https://github.com/osxfuse/osxfuse/issues/542" target="_blank" rel="noopener">osxfuse not compatible with MacOS Mojave</a>，从官网下载最新的3.8.2版本安装即可。</p><h2 id="VSCode等编辑器字体变“瘦”"><a href="#VSCode等编辑器字体变“瘦”" class="headerlink" title="VSCode等编辑器字体变“瘦”"></a>VSCode等编辑器字体变“瘦”</h2><p>更新之后，发现VSCode编辑器中的字体变得“很瘦”，不美观。执行下面的命令，并重启机器，应该可以恢复。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">defaults write -g CGFontRenderingFontSmoothingDisabled -bool NO</span><br></pre></td></tr></table></figure></p><h2 id="Mos-Caffine-IINA-等APP"><a href="#Mos-Caffine-IINA-等APP" class="headerlink" title="Mos Caffine IINA 等APP"></a>Mos Caffine IINA 等APP</h2><p>Mos可以平滑Mac上外接鼠标的滚动，并调整鼠标滚动方向和Windows相同。更新后发现Mos失灵。这应该是和新版本中更强的权限管理有关，解决办法是在”安全隐私设置” -&gt; “辅助功能”中，先把Mos的勾勾去掉，然后重新勾选。Caffine同样的操作。</p><p>IINA是一款Mac上的播放器软件，是我在Mac上的默认播放器。更新后点击媒体文件，发现只是弹出IINA软件的界面，却没有自动播放。解决办法是在媒体文件上右键，在打开方式中重新选择IINA，并勾选默认打开方式选项。</p><p>更新新系统后，遇到的坑暂时就这么多。希望能够帮助到需要的人。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;很奇怪，对于手机上的APP，我一般能不升级就不升级；但是对于PC上的软件或操作系统更新，则是能升级就升级。。在将手中的MacOS更新到最新版本Mojave后，发现了一些需要手动调节的问题，记录在这里，原谅我标题党的画风。。。&lt;br&gt;
    
    </summary>
    
    
      <category term="tool" scheme="https://xmfbit.github.io/tags/tool/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Rethinking The Value of Network Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/22/paper-rethinking-the-value-of-network-pruning/</id>
    <published>2018-10-22T22:25:42.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://openreview.net/forum?id=rJlnB3C5Ym" target="_blank" rel="noopener">这篇文章</a>是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考<a href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/">论文总结 - 模型剪枝 Model Pruning</a>。</p><p>这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。</p><p>然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：<a href="https://github.com/Eric-mingjie/rethinking-network-pruning" target="_blank" rel="noopener">Eric-mingjie/rethinking-network-pruning</a>。</p><a id="more"></a><h2 id="FLOP的计算"><a href="#FLOP的计算" class="headerlink" title="FLOP的计算"></a>FLOP的计算</h2><p>代码中有关于PyTorch模型的FLOPs的计算，见<a href="https://github.com/Eric-mingjie/rethinking-network-pruning/blob/master/imagenet/l1-norm-pruning/compute_flops.py" target="_blank" rel="noopener">compute_flops.py</a>。可以很方便地应用到自己的代码中。</p><h2 id="ThiNet的实现"><a href="#ThiNet的实现" class="headerlink" title="ThiNet的实现"></a>ThiNet的实现</h2><h2 id="实验比较"><a href="#实验比较" class="headerlink" title="实验比较"></a>实验比较</h2><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><p>几个仍然有疑问的地方：</p><ol><li><p>作者已经证明在ImageNet/CIFAR等样本分布均衡的数据集上的结论，如果样本分布不均衡呢？有三种思路有待验证：</p><ul><li>prune模型需要从大模型处继承权重，然后直接在不均衡数据集上训练即可；</li><li>prune模型不需要从大模型处继承权重， 但是需要先在ImageNet数据集上训练，然后再在不均衡数据集上训练；</li><li>prune模型直接在不均衡数据集上训练（以我的经验，这种思路应该是不work的）</li></ul></li><li><p>prune前的大模型权重不重要，结构重要，这是本文的结论之一。自动搜索树的prune算法可以看做是模型结构搜索，但是大模型给出了搜索空间的一个很好的初始点。这个初始点是否是任务无关的？也就是说，对A任务有效的小模型，是否在B任务上也是很work的？</p></li><li><p>现在的网络搜索中应用了强化学习/遗传算法等方法，这些方法怎么能够和prune结合？ECCV 2018中HanSong和He Yihui发表了AMC方法。</p></li></ol><p>总之，作者用自己辛勤的实验，给我们指出了一个”可能的”（毕竟文章还没被接收）误区，但是仍然有很多乌云漂浮在上面，需要更多的实验。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;a href=&quot;https://openreview.net/forum?id=rJlnB3C5Ym&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;这篇文章&lt;/a&gt;是ICLR 2019的投稿文章，最近也引发了大家的注意。在我的博客中，已经对此做过简单的介绍，请参考&lt;a href=&quot;https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/&quot;&gt;论文总结 - 模型剪枝 Model Pruning&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;这篇文章的主要观点在于想纠正人们之前的认识误区。当然这个认识误区和DL的发展是密不可分的。DL中最先提出的AlexNet是一个很大的模型。后面的研究者虽然也在不断发明新的网络结构（如inception，Global Pooling，ResNet等）来获得参数更少更强大的模型，但模型的size总还是很大。既然研究社区是从这样的“大”模型出发的，那当面对工程上需要小模型以便在手机等移动设备上使用时，很自然的一条路就是去除大模型中已有的参数从而得到小模型。也是很自然的，我们需要保留大模型中“有用的”那些参数，让小模型以此为基础进行fine tune，补偿因为去除参数而导致的模型性能下降。&lt;/p&gt;
&lt;p&gt;然而，自然的想法就是合理的么？这篇文章对此提出了质疑。这篇论文的主要思路已经在上面贴出的博文链接中说过了。这篇文章主要是结合作者开源的代码对论文进行梳理：&lt;a href=&quot;https://github.com/Eric-mingjie/rethinking-network-pruning&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Eric-mingjie/rethinking-network-pruning&lt;/a&gt;。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文总结 - 模型剪枝 Model Pruning</title>
    <link href="https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/"/>
    <id>https://xmfbit.github.io/2018/10/03/paper-summary-model-pruning/</id>
    <published>2018-10-03T16:31:07.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。</p><p><img src="/img/paper-summary-model-pruning-joke.jpg" alt="剪枝的学问"></p><a id="more"></a><h2 id="Deep-Compression-Han-Song"><a href="#Deep-Compression-Han-Song" class="headerlink" title="Deep Compression, Han Song"></a>Deep Compression, Han Song</h2><p>抛去LeCun等人在90年代初的几篇论文，HanSong是这个领域的先行者。发表了一系列关于模型压缩的论文。其中NIPS 2015上的这篇<a href="https://arxiv.org/abs/1506.02626" target="_blank" rel="noopener">Learning both weights and connections for efficient neural network</a>着重讨论了对模型进行剪枝的方法。这篇论文之前我已经写过了<a href="https://xmfbit.github.io/2018/03/14/paper-network-prune-hansong/">阅读总结</a>，比较详细。</p><p>概括来说，作者提出的主要观点包括，L1 norm作为neuron是否重要的metric，train -&gt; pruning -&gt; retrain三阶段方法以及iteratively pruning。需要注意的是，作者的方法只能得到非结构化的稀疏，对于作者的专用硬件EIE可能会很有帮助。但是如果想要在通用GPU或CPU上用这种方法做加速，是不太现实的。</p><h2 id="SSL，WenWei"><a href="#SSL，WenWei" class="headerlink" title="SSL，WenWei"></a>SSL，WenWei</h2><p>既然非结构化稀疏对现有的通用GPU/CPU不友好，那么可以考虑构造结构化的稀疏。将Conv中的某个filter或filter的某个方形区域甚至是某个layer直接去掉，应该是可以获得加速效果的。WenWei<a href="https://arxiv.org/abs/1608.03665" target="_blank" rel="noopener">论文Learning Structured Sparsity in Deep Neural Networks</a>发表在NIPS 2016上，介绍了如何使用LASSO，给损失函数加入相应的惩罚，进行结构化稀疏。这篇论文之前也已经写过博客，可以参考<a href="https://xmfbit.github.io/2018/02/24/paper-ssl-dnn/">博客文章</a>。</p><p>概括来说，作者引入LASSO正则惩罚项，通过不同的具体形式，构造了对不同结构化稀疏的损失函数。</p><h2 id="L1-norm-Filter-Pruning，Li-Hao"><a href="#L1-norm-Filter-Pruning，Li-Hao" class="headerlink" title="L1-norm Filter Pruning，Li Hao"></a>L1-norm Filter Pruning，Li Hao</h2><p>在通用GPU/CPU上，加速效果最好的还是整个Filter直接去掉。作者发表在ICLR 2017上的<a href="https://arxiv.org/abs/1608.08710" target="_blank" rel="noopener">论文Pruning Filters for Efficient ConvNets</a>提出了一种简单的对卷积层的filter进行剪枝的方法。</p><p>这篇论文真的很简单。。。主要观点就是通过Filter的L1 norm来判断这个filter是否重要。人为设定剪枝比例后，将该层不重要的那些filter直接去掉，并进行fine tune。在确定剪枝比例的时候，假定每个layer都是互相独立的，分别对其在不同剪枝比例下进行剪枝，并评估模型在验证集上的表现，做sensitivity分析，然后确定合理的剪枝比例。在实现的时候要注意，第$i$个layer中的第$j$个filter被去除，会导致其输出的feature map中的第$j$个channel缺失，所以要相应调整后续的BN层和Conv层的对应channel上的参数。</p><p>另外，实现起来还有一些细节，这些可以参见原始论文。提一点，在对ResNet这种有旁路结构的网络进行剪枝时，每个block中的最后一个conv不太好处理。因为它的输出要与旁路做加和运算。如果channel数量不匹配，是没法做的。作者在这里的处理方法是，听identity那一路的。如果那一路确定了剪枝后剩余的index是多少，那么$\mathcal{F}(x)$那一路的最后那个conv也这样剪枝。</p><p>这里给出一张在ImageNet上做sensitivity analysis的图表。需要对每个待剪枝的layer进行类似的分析。</p><p><img src="/img/paper-model-pruning-filter-pruning-sensitivity-results.png" alt="sensitivity分析"></p><h2 id="Automated-Gradual-Pruning-Gupta"><a href="#Automated-Gradual-Pruning-Gupta" class="headerlink" title="Automated Gradual Pruning, Gupta"></a>Automated Gradual Pruning, Gupta</h2><p>这篇文章发表在NIPS 2017的一个关于移动设备的workshop上，名字很有意思（这些人起名字为什么都这么熟练啊）：<a href="https://arxiv.org/abs/1710.01878" target="_blank" rel="noopener">To prune, or not to prune: exploring the efficacy of pruning for model compression</a>。TensorFlow的repo中已经有了对应的实现（亲儿子。。）：<a href="https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/model_pruning" target="_blank" rel="noopener">Model pruning: Training tensorflow models to have masked connections</a>。哈姆雷特不能回答的问题，作者的答案则是Yes。</p><p><img src="/img/paper-model-pruning-why-so-baixue.jpg" alt="为什么你们起名字这么熟练啊"></p><p>这篇文章主要有两个贡献。一是比较了large模型经过prune之后得到的large-sparse模型和相似memory footprint但是compact-small模型的性能，得出结论：对于很多网络结构（CNN，stacked LSTM, seq-to-seq LSTM）等，都是前者更好。具体的数据参考论文。</p><p>二是提出了一个渐进的自动调节的pruning策略。首先，作者也着眼于非结构化稀疏。同时和上面几篇文章一样，作者也使用绝对值大小作为衡量importance的标准，作者提出，sparsity可以按照下式自动调节：</p><script type="math/tex; mode=display">s_t = s_f + (s_i-s_f)(1-\frac{t-t_0}{n\Delta t})^3 \quad \text{for}\quad t \in \{t_0, t_0+\Delta t,\dots,t_0+n\Delta t\}</script><p>其中，$s_i$是初始剪枝比例，一般为$0$。$s_f$为最终的剪枝比例，开始剪枝的迭代次数为$t_0$，剪枝间隔为$\Delta t$，共进行$n$次。</p><h2 id="Net-Sliming-Liu-Zhuang-amp-Huang-Gao"><a href="#Net-Sliming-Liu-Zhuang-amp-Huang-Gao" class="headerlink" title="Net Sliming, Liu Zhuang &amp; Huang Gao"></a>Net Sliming, Liu Zhuang &amp; Huang Gao</h2><p>这篇文章<a href="https://arxiv.org/abs/1708.06519" target="_blank" rel="noopener">Learning Efficient Convolutional Networks through Network Slimming</a>发表在ICCV 2017，利用CNN网络中的必备组件——BN层中的gamma参数，实现端到端地学习剪枝参数，决定某个layer中该去除掉哪些channel。作者中有DenseNet的作者——姚班学生刘壮和康奈尔大学博士后黄高。代码已经开源：<a href="https://github.com/liuzhuang13/slimming" target="_blank" rel="noopener">liuzhuang13/slimming</a>。</p><p>作者的主要贡献是提出可以使用BN层的gamma参数，标志其前面的conv输出的feature map的某个channel是否重要，相应地，也是conv参数中的那个filter是否重要。</p><p>首先，需要给BN的gamma参数加上L1 正则惩罚训练模型，新的损失函数变为$L= \sum_{(x,y)}l(f(x, W), y) + \lambda \sum_{\gamma \in \Gamma}g(\gamma)$。</p><p>接着将该网络中的所有gamma进行排序，根据人为给出的剪枝比例，去掉那些gamma很小的channel，也就是对应的filter。最后进行finetune。这个过程可以反复多次，得到更好的效果。如下所示：<br><img src="/img/paper-model-pruning-net-sliming-procedure.png" alt="Net Sliming的大致流程"></p><p>还是上面遇到过的问题，如果处理ResNet或者DenseNet Feature map会多路输出的问题。这里作者提出使用一个”channel selection layer”，统一对该feature map的输出进行处理，只选择没有被mask掉的那些channel输出。具体实现可以参见开源代码<a href="https://github.com/Eric-mingjie/network-slimming/blob/master/models/channel_selection.py#L6" target="_blank" rel="noopener">channel selection layer</a>：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">channel_selection</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Select channels from the output of BatchNorm2d layer. It should be put directly after BatchNorm2d layer.</span></span><br><span class="line"><span class="string">    The output shape of this layer is determined by the number of 1 in `self.indexes`.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, num_channels)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize the `indexes` with all one vector with the length same as the number of channels.</span></span><br><span class="line"><span class="string">        During pruning, the places in `indexes` which correpond to the channels to be pruned will be set to 0.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        super(channel_selection, self).__init__()</span><br><span class="line">        self.indexes = nn.Parameter(torch.ones(num_channels))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_tensor)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Parameter</span></span><br><span class="line"><span class="string">        ---------</span></span><br><span class="line"><span class="string">        input_tensor: (N,C,H,W). It should be the output of BatchNorm2d layer.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        selected_index = np.squeeze(np.argwhere(self.indexes.data.cpu().numpy()))</span><br><span class="line">        <span class="keyword">if</span> selected_index.size == <span class="number">1</span>:</span><br><span class="line">            selected_index = np.resize(selected_index, (<span class="number">1</span>,))</span><br><span class="line">        output = input_tensor[:, selected_index, :, :]</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure><p>略微解释一下：在开始加入L1正则，惩罚gamma的时候，相当于identity变换；当确定剪枝参数后，相应index会被置为$0$，被mask掉，这样输出就没有这个channel了。后面的几路都可以用这个共同的输出。</p><h2 id="AutoPruner-Wu-Jianxin"><a href="#AutoPruner-Wu-Jianxin" class="headerlink" title="AutoPruner, Wu Jianxin"></a>AutoPruner, Wu Jianxin</h2><p>这篇文章<a href="https://arxiv.org/abs/1805.08941" target="_blank" rel="noopener">AutoPruner: An End-to-End Trainable Filter Pruning Method for Efficient Deep Model Inference</a>是南大Wu Jianxin组新进发的文章，还没有投稿到任何学术会议或期刊，只是挂在了Arvix上，应该是还不够完善。他们还有一篇文章ThiNet：<a href="https://arxiv.org/abs/1707.06342" target="_blank" rel="noopener">ThiNet: A Filter Level Pruning Method for Deep Neural Network Compression</a>发表在ICCV 2017上。</p><p>这篇文章的主要贡献是提出了一种端到端的模型剪枝方法，如下图所示。为第$i$个Conv输出加上一个旁路，输入为其输出的Feature map，依次经过Batch-wise Pooling -&gt; FC -&gt; scaled sigmoid的变换，按channel输出取值在$[0,1]$范围的向量作为mask，与Feature map做积，mask掉相应的channel。通过学习FC的参数，就可以得到适当的mask，判断该剪掉第$i$个Conv的哪个filter。其中，scaled sigmoid变换是指$y = \sigma(\alpha x)$。通过训练过程中不断调大$\alpha$，就可以控制sigmoid的“硬度”，最终实现$0-1$门的效果。<br><img src="/img/paper-summary-autopruner-arch.png" alt="AutoPruner框图"></p><p>构造损失函数$\mathcal{L} = \mathcal{L}_{\text{cross-entropy}} + \lambda \Vert \frac{\Vert v \Vert_1}{C} - r \Vert_2^2$。其中，$v$是sigmoid输出的mask，$C$为输出的channel数量，$r$为目标稀疏度。</p><p>不过在具体的细节上，作者表示要注意的东西很多。主要是FC层的初始化和几个超参数的处理。作者在论文中提出了相应想法：</p><ul><li>FC层初始化权重为$0$均值，方差为$10\sqrt{\frac{2}{n}}$的高斯分布，其中$n = C\times H \times W$。</li><li>上述$\alpha$的控制，如何增长$\alpha$。作者设计了一套if-else的规则。</li><li>上述损失函数中的比例$\lambda$，作者使用了$\lambda = 100 \vert r_b - r\vert$的自适应调节方法。</li></ul><p><img src="/img/paper-summary-model-compression-autopruner-alg.png" alt="AutoPruner Alg"></p><h2 id="Rethinking-Net-Pruning-匿名"><a href="#Rethinking-Net-Pruning-匿名" class="headerlink" title="Rethinking Net Pruning, 匿名"></a>Rethinking Net Pruning, 匿名</h2><p>这篇文章<a href="https://openreview.net/pdf?id=rJlnB3C5Ym" target="_blank" rel="noopener">Rethinking the Value of Network Pruning</a>有意思了。严格说来，它还在ICLR 2019的匿名评审阶段，并没有被接收。不过这篇文章的炮口已经瞄准了之前提出的好几个model pruning方法，对它们的结果提出了质疑。上面的链接中，也有被diss的方法之一的作者He Yihui和本文作者的交流。</p><p>之前的剪枝算法大多考虑两个问题：</p><ol><li>怎么求得一个高效的剪枝模型结构，如何确定剪枝方式和剪枝比例：在哪里剪，剪多少</li><li>剪枝模型的参数求取：如何保留原始模型中重要的weight，对进行补偿，使得accuracy等性能指标回复到原始模型</li></ol><p>而本文的作者check了六种SOA的工作，发现：在剪枝算法得到的模型上进行finetune，只比相同结构，但是使用random初始化权重的网络performance好了一点点，甚至有的时候还不如。作者的结论是：</p><ol><li>训练一个over parameter的model对最终得到一个efficient的小模型不是必要的</li><li>为了得到剪枝后的小模型，求取大模型中的important参数其实并不打紧</li><li>剪枝得到的结构，相比求得的weight，更重要。所以不如将剪枝算法看做是网络结构搜索的一种特例。</li></ol><p>作者立了两个论点来打：</p><ol><li>要先训练一个over-parameter的大模型，然后在其基础上剪枝。因为大模型有更强大的表达能力。</li><li>剪枝之后的网络结构和权重都很重要，是剪枝模型finetune的基础。</li></ol><p>作者试图通过实验证明，很多剪枝方法并没有他们声称的那么有效，很多时候，无需剪枝之后的权重，而是直接随机初始化并训练，就能达到这些论文中的剪枝方法的效果。当然，这些论文并不是一无是处。作者提出，是剪枝之后的结构更重要。这些剪枝方法可以看做是网络结构的搜索。</p><p>论文的其他部分就是对几种现有方法的实验和diss。我还没有细看，如果后续这篇论文得到了接收，再做总结吧~夹带一些私货，基于几篇论文的实现经验和在真实数据集上的测试，这篇文章的看法我是同意的。</p><p>更新：这篇文章的作者原来正是Net Sliming的作者Liu Zhuang和Huang Gao，那实验和结论应该是很有保障的。最近这篇文章确实也引起了大家的注意，值得好好看一看。</p><h2 id="其他论文等资源"><a href="#其他论文等资源" class="headerlink" title="其他论文等资源"></a>其他论文等资源</h2><ul><li><a href="https://nervanasystems.github.io/distiller/index.html" target="_blank" rel="noopener">Distiller</a>：一个使用PyTorch实现的剪枝工具包</li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;模型剪枝是常用的模型压缩方法之一。这篇是最近看的模型剪枝相关论文的总结。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-summary-model-pruning-joke.jpg&quot; alt=&quot;剪枝的学问&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>VIM安装YouCompleteMe和Jedi进行自动补全</title>
    <link href="https://xmfbit.github.io/2018/10/02/vim-you-complete-me/"/>
    <id>https://xmfbit.github.io/2018/10/02/vim-you-complete-me/</id>
    <published>2018-10-02T22:30:54.000Z</published>
    <updated>2020-05-02T07:17:54.456Z</updated>
    
    <content type="html"><![CDATA[<p>这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~</p><p><img src="/img/vim-config-demo.png" alt="我的VIM"></p><a id="more"></a><h2 id="使用Jedi和YouCompleteMe配置Vim"><a href="#使用Jedi和YouCompleteMe配置Vim" class="headerlink" title="使用Jedi和YouCompleteMe配置Vim"></a>使用Jedi和YouCompleteMe配置Vim</h2><p>在远程开发机上调试代码时，我的习惯是大型项目使用sshfs将其镜像到本地，然后使用VSCode打开编辑。VSCode中有终端可以方便的ssh到远端开发机，我将”CTRL+`”配置成了编辑器和终端之间的切换快捷键。加上vim插件，就可以实现不用鼠标，不离开当前编辑环境进行代码编写和调试了。</p><p>然而，如果是想在开发机上写一段小的代码，上述方法就显得太麻烦了。</p><h2 id="编译Vim"><a href="#编译Vim" class="headerlink" title="编译Vim"></a>编译Vim</h2><p>编译Vim，注意我们要设定其安装目录为anaconda下的bin目录：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">./configure --with-features=huge --<span class="built_in">enable</span>-multibyte --<span class="built_in">enable</span>-pythoninterp=yes --with-python-config-dir=/path/to/anaconda/bin/python-config --<span class="built_in">enable</span>-gui=gtk2 --prefix=/path/to/anaconda</span><br></pre></td></tr></table></figure><p>编译并安装：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">make -j4 VIMRUNTIMEDIR=/path/to/anaconda/share/vim/vim81</span><br><span class="line">make install</span><br></pre></td></tr></table></figure></p><p>安装后，可以查看vim的version进行确认。安装没有问题，会提示刚才编译的版本信息。<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">vim --version</span><br></pre></td></tr></table></figure></p><p>使用Vundle管理插件，这个没有什么问题，直接按照README提示即可，见：<a href="https://github.com/VundleVim/Vundle.vim" target="_blank" rel="noopener">Vundle@Github</a>。</p><p>使用Vundle进行插件管理，只需要以下面的形式指明插件目录或Github仓库名称，进入vim后，在Normal状态，输入<code>:PluginInstall</code>即可。</p><h2 id="Jedi"><a href="#Jedi" class="headerlink" title="Jedi"></a>Jedi</h2><p>首先需要安装jedi的python包：<br><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install jedi</span><br></pre></td></tr></table></figure></p><p>使用Vbudle安装<a href="https://github.com/davidhalter/jedi-vim" target="_blank" rel="noopener">jedi-vim</a>，并在<code>.vimrc</code>中添加以下内容。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">let g:jedi#force_py_version=2.7</span><br></pre></td></tr></table></figure></p><h2 id="YouCompleteMe"><a href="#YouCompleteMe" class="headerlink" title="YouCompleteMe"></a>YouCompleteMe</h2><p>使用Vundle安装<a href="https://github.com/Valloric/YouCompleteMe#ubuntu-linux-x64" target="_blank" rel="noopener">YouCompleteMe</a>。</p><p>之后，进入目录<code>.vim/bundle/YouCompleteMe</code>，执行<code>./install.py</code>。如果需要C++支持，执行<code>./install.py --clang-completer</code>。</p><p>但是，其中遇到了问题，找不到Python.h文件。使用<code>locate Python.h</code>，明确该文件确实存在，且其位于<code>/path/to/anaconda/include/python2.7</code>后，手动修改CMakeLists.txt，指定该文件目录位置即可。</p><p>修改这个：<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/CMakeLists.txt</code><br>和<br><code>.vim/bundle/YouCompleteMe/third_party/ycmd/cpp/ycm/CMakeLists.txt</code>，向其中添加：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">set</span>( CMAKE_CXX_FLAGS <span class="string">"<span class="variable">$&#123;CMAKE_CXX_FLAGS&#125;</span> -I/path/to/anaconda/include/python2.7"</span> )</span><br></pre></td></tr></table></figure><p>强行指定头文件包含目录。</p><h2 id="括号自动补全"><a href="#括号自动补全" class="headerlink" title="括号自动补全"></a>括号自动补全</h2><p>虽然SO上有人指出可以直接通过设置<code>.vimrc</code>的方法实现，不过还是直接用现成的插件吧。推荐使用<a href="https://github.com/jiangmiao/auto-pairs" target="_blank" rel="noopener">jiangmiao/auto-pairs</a>。可以按照README的说明进行安装。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;这篇主要记录自己尝试编译Anaconda + VIM并安装Jedi和YouCompleteMe自动补全插件的过程。踩了一些坑，不过最后还是装上了。给VIM装上了Dracula主题，有点小清新的感觉~&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/vim-config-demo.png&quot; alt=&quot;我的VIM&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="vim" scheme="https://xmfbit.github.io/tags/vim/"/>
    
      <category term="tools" scheme="https://xmfbit.github.io/tags/tools/"/>
    
  </entry>
  
  <entry>
    <title>MXNet fit介绍</title>
    <link href="https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/"/>
    <id>https://xmfbit.github.io/2018/10/02/mxnet-fit-usage/</id>
    <published>2018-10-02T22:11:15.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>在MXNet中，<code>Module</code>提供了训练模型的方便接口。使用<code>symbol</code>将计算图建好之后，用<code>Module</code>包装一下，就可以通过<code>fit()</code>方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用<code>fit()</code>接口就不太合适。这里把<code>fit()</code>代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。</p><p>其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个<a href="https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks" target="_blank" rel="noopener">Slide分享</a>，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。</p><p><img src="/img/differences-of-deep-learning-frameworks-22-638.jpg" alt="Differences of Deep Learning Frameworks"></p><a id="more"></a><h2 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h2><p>首先，需要将数据绑定到计算图上，并初始化模型的参数，并初始化求解器。这些是求解模型必不可少的。</p><p>其次，还会建立训练的metric，方便我们掌握训练进程和当前模型在训练任务的表现。</p><p>这些是在为后续迭代进行梯度下降更新做准备。</p><h2 id="迭代更新"><a href="#迭代更新" class="headerlink" title="迭代更新"></a>迭代更新</h2><p>使用SGD进行训练的时候，我们需要不停地从数据迭代器中获取包含data和label的batch，并将其feed到网络模型中。进行forward computing后进行bp，获得梯度，并根据具体的优化方法（SGD, SGD with momentum, RMSprop等）进行参数更新。</p><p>这部分可以抽成：<br><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># in an epoch</span></span><br><span class="line"><span class="keyword">while</span> <span class="keyword">not</span> end_epoch:</span><br><span class="line">    batch = next(train_iter)</span><br><span class="line">    m.forward_backward(batch)</span><br><span class="line">    m.update()</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        next_batch = next(data_iter)</span><br><span class="line">        m.prepare(next_batch)</span><br><span class="line">    <span class="keyword">except</span> StopIteration:</span><br><span class="line">        end_epoch = <span class="literal">True</span></span><br></pre></td></tr></table></figure></p><h2 id="metric"><a href="#metric" class="headerlink" title="metric"></a>metric</h2><p>在训练的时候，观察输出的各种metric是必不可少的。我们对训练过程的把握就是通过metric给出的信息。通常在分类任务中常用到的metric有Accuracy，TopK-Accuracy以及交叉熵损失等，这些已经在MXNet中有了现成的实现。而在<code>fit</code>中，调用了<code>m.update_metric(eval_metric, data_batch.label)</code>实现。这里的<code>eval_metric</code>就是我们指定的metric，而<code>label</code>是batch提供的label。注意，在MXNet中，label一般都是以<code>list</code>的形式给出（对应于多任务学习），也就是说这里的label是<code>list of NDArray</code>。当自己魔改的时候要注意。</p><h2 id="logging"><a href="#logging" class="headerlink" title="logging"></a>logging</h2><p>计算了eval_metric等信息，我们需要将其在屏幕上打印出来。MXNet中可以通过callback实现。另外，保存模型checkpoint这样的功能也是通过callback实现的。一种常用的场景是每过若干个batch，做一次logging，打印当前的metric信息，如交叉熵损失降到多少了，准确率提高到多少了等。MXNet会将以下信息打包成<code>BatchEndParam</code>类型（其实是一个自定义的<code>namedtuple</code>）的变量，包括当前epoch，当前迭代次数，评估的metric。如果你需要更多的信息或者更自由的logging监控，也可以参考代码自己实现。</p><p>我们以常用的<code>Speedometer</code>看一下如何使用这些信息，其功能如下，将训练的速度和metric打印出来。</p><blockquote><p>Logs training speed and evaluation metrics periodically</p></blockquote><p>PS:这里有个隐藏的坑。MXNet中的<code>Speedometer</code>每回调一次，会把<code>metric</code>的内容清除。这在训练的时候当然没问题。但是如果是在validation上跑，就会有问题了。这样最终得到的只是最后一个回调周期那些batch的metric，而不是整个验证集上的。如果在<code>fit</code>方法中传入了<code>eval_batch_end_callback</code>参数就要注意这个问题了。解决办法一是在<code>Speedometer</code>实例初始化时传入<code>auto_reset=False</code>，另一种干脆就不要加这个参数，默认为<code>None</code>好了。同样的问题也发生在调用<code>Module.score()</code>方法来获取模型在验证集上metric的时候。</p><p>可以在<code>Speedometer</code>代码中寻找下面这几行，会更清楚：</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> param.eval_metric <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">    name_value = param.eval_metric.get_name_value()</span><br><span class="line">    <span class="keyword">if</span> self.auto_reset:</span><br><span class="line">        param.eval_metric.reset()</span><br></pre></td></tr></table></figure><h2 id="在验证集上测试"><a href="#在验证集上测试" class="headerlink" title="在验证集上测试"></a>在验证集上测试</h2><p>当在训练集上跑过一个epoch后，如果提供了验证集的迭代器，会在验证集上对模型进行测试。这里，MXNet直接封装了<code>score()</code>方法。在<code>score</code>中，基本流程和<code>fit()</code>相同，只是我们只需要forward computing即可。</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><p>用了一段时间的MXNet，给我的最大的感觉是MXNet就像一个写计算图的前端，提供了很方便的python接口生成静态图，以及很多“可插拔”的插件（虽然可能不是很全，更像是一份guide而不是拿来即用的tool），如上文中的metric等，使其更适合做成流程化的基础DL平台，供给更上层方便地配置使用。缺点就是隐藏了比较多的实现细节（当然，你完全可以从代码中自己学习，比如从<code>fit()</code>代码了解神经网络的大致训练流程）。至于MXNet宣扬的诸如速度快，图优化，省计算资源等优点，因为我没有过数据对比，就不说了。</p><p>缺点就是写图的时候有时不太灵活（可能也是我写的看的还比较少），即使是和TensorFlow这种同为静态图的DL框架比。另外，貌似MXNet中很多东西都没有跟上最新的论文等，比如Cosine的learning rate decay就没有。Model Zoo也比较少(gluon可能会好一点，Gluon-CV和Gluon-NLP貌似是在搞一些论文复现的工作)。对开发来讲，很多东西都需要阅读代码才能知道是怎么回事，只是读文档的话容易踩坑。</p><p>说到这里，感觉MXNet的python训练接口（包括module，optimizer，metric等）更像是一份example代码，是在教你怎么去用MXNet，而不像一个灵活地强大的工具箱。当然，很多东西不能得兼，希望MXNet越来越好。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;在MXNet中，&lt;code&gt;Module&lt;/code&gt;提供了训练模型的方便接口。使用&lt;code&gt;symbol&lt;/code&gt;将计算图建好之后，用&lt;code&gt;Module&lt;/code&gt;包装一下，就可以通过&lt;code&gt;fit()&lt;/code&gt;方法对其进行训练。当然，官方提供的接口一般只适合用来训练分类任务，如果是其他任务（如detection, segmentation等），单纯使用&lt;code&gt;fit()&lt;/code&gt;接口就不太合适。这里把&lt;code&gt;fit()&lt;/code&gt;代码梳理一下，也是为了后续方便在其基础上实现扩展，更好地用在自己的任务。&lt;/p&gt;
&lt;p&gt;其实如果看开源代码数量的话，MXNet已经显得式微，远不如TensorFlow，PyTorch也早已经后来居上。不过据了解，很多公司内部都有基于MXNet自研的框架或平台工具。下面这张图来自LinkedIn上的一个&lt;a href=&quot;https://www.slideshare.net/beam2d/differences-of-deep-learning-frameworks&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Slide分享&lt;/a&gt;，姑且把它贴在下面，算是当前流行框架的一个比较（应该可以把Torch换成PyTorch）。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/differences-of-deep-learning-frameworks-22-638.jpg&quot; alt=&quot;Differences of Deep Learning Frameworks&quot;&gt;&lt;/p&gt;
    
    </summary>
    
    
      <category term="mxnet" scheme="https://xmfbit.github.io/tags/mxnet/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Like What You Like - Knowledge Distill via Neuron Selectivity Transfer</title>
    <link href="https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/"/>
    <id>https://xmfbit.github.io/2018/10/02/paper-knowledge-transfer-neural-selectivity-transfer/</id>
    <published>2018-10-02T21:32:05.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>好长时间没有写博客了，国庆假期把最近看的东西整理一下。<a href="https://arxiv.org/abs/1707.01219" target="_blank" rel="noopener">Like What You Like: Knowledge Distill via Neuron Selectivity Transfer</a>这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。</p><p><img src="/img/paper-nst-kt-like-what-you-like.gif" alt="jump if you jump"></p><p>另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：<a href="https://arxiv.org/abs/1707.01213" target="_blank" rel="noopener">Data-driven sparse structure selection for deep neural networks</a>。</p><p>另另外，其实这两篇文章挂在Arxiv的时间很接近，<a href="https://www.zhihu.com/question/62068158" target="_blank" rel="noopener">知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？</a>有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。</p><a id="more"></a><h2 id="Motivation"><a href="#Motivation" class="headerlink" title="Motivation"></a>Motivation</h2><p>KD的一个痛点在于其只适用于softmax分类问题。这样，对于Detection。Segmentation等一系列问题，没有办法应用。另一个问题在于当分类类别数较少时，KD效果不理想。这个问题比较好理解，假设我们面对一个二分类问题，那么我们并不关心类间的similarity，而是尽可能把两类分开即可。同时，这篇文章的实验部分也验证了这个猜想：当分类问题分类类别数较多时，使用KD能够取得best的结果。</p><p>作者联想到，我们是否可以把CNN中某个中间层输出的feature map利用起来呢？Student输出的feature map要和Teacher的相似，相当于是Student学习到了Teacher提取特征的能力。在CNN中，每个filter都是在和一个feature map上的patch做卷积得到输出，很多个filter都做卷积运算，就得到了feature（map）。另外，当filter和patch有相似的结构时，得到的激活比较大。举个例子，如果filter是Sobel算子，那么当输入image是边缘的时候，得到的响应是最大的。filter学习出来的是输入中的某些模式。当模式匹配上时，激活。这里也可以参考一些对CNN中filter做可视化的研究。</p><p>顺着上面的思路，有人提出了Attention Transfer的方法，可以参见这篇文章：<a href="https://arxiv.org/abs/1612.03928" target="_blank" rel="noopener">Improving the Performance of Convolutional Neural Networks via Attention Transfer</a>。而在NST这篇文章中，作者引入了新的损失函数，用于衡量Student和Teacher对相同输入的激活Feature map的不同，可以说除了下面要介绍的数学概念以外，没有什么难理解的地方。整个训练的网络结构如下所示：<br><img src="/img/paper-nst-student-and-teacher.png" alt="NST知识蒸馏的整体框图结构"></p><h2 id="Maximum-Mean-Discrepancy"><a href="#Maximum-Mean-Discrepancy" class="headerlink" title="Maximum Mean Discrepancy"></a>Maximum Mean Discrepancy</h2><p>MMD 是用来衡量sampled data之间分布差异的距离量度。如果有两个不同的分布$p$和$q$，以及从两个分布中采样得到的Data set$\mathcal{X}$和$\mathcal{Y}$。那么MMD距离如下：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \Vert \frac{1}{N}\sum_{i=1}^{N}\phi(x^i) - \frac{1}{M}\sum_{j=1}^{M}\phi(y^j) \Vert_2^2</script><p>其中，$\phi$表示某个mapping function。变形之后（内积打开括号），可以得到：</p><script type="math/tex; mode=display">\mathcal{L}(\mathcal{X}, \mathcal{Y}) = \frac{1}{N^2}\sum_{i=1}^{N}\sum_{j=1}^{N}k(x^i, x^j) + \frac{1}{M^2}\sum_{i=1}^{M}\sum_{j=1}^{M}k(y^i, y^j) - \frac{2}{MN}\sum_{i=1}^{N}\sum_{j=1}^{M}k(x^i, y^j)</script><p>其中，$k$是某个kernel function，$k(x, y) = \phi(x)^{T}\phi(y)$。</p><p>我们可以使用MMD来衡量Student模型和Teacher模型中间输出的激活feature map的相似程度。通过优化这个损失函数，使得S的输出分布接近T。通过引入MMD，将NST loss定义如下，下标$S$表示Student的输出，$T$表示Teacher的输出。第一项$\mathcal{H}$是指由样本类别标签计算的CrossEntropy Loss。第二项即为上述的MMD Loss。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{H}(y, p_S) + \frac{\lambda}{2}\mathcal{L}_{MMD}(F_T, F_S)</script><p>注意，为了确保后一项有意义，需要保证$F_T$和$F_S$有相同长度。具体来说，对于网络中间输出的feature map，我们将每个channel上的$HW$维的feature vector作为分布$\mathcal{X}$的一个采样。按照作者的设定，我们需要保证S和T对应的feature map在spatial dimension上必须一样大。如果不一样，可以使用插值方法进行扩展。</p><p>为了不受相对幅值大小的影响，需要对feature vector做normalization。</p><p>对于kernal的选择，作者提出了三种可行方案：线性，多项式和高斯核。在后续通过实验对比了它们的性能。</p><h2 id="和其他方法的关联"><a href="#和其他方法的关联" class="headerlink" title="和其他方法的关联"></a>和其他方法的关联</h2><p>如果使用线性核函数，也就是$\phi$是一个identity mapping，那么MMD就成了直接比较两个样本分布质心的距离。这时候，和上文提到的AT方法的一种形式是类似的。（这个我觉得有点强行扯关系。。。）</p><p>如果使用二次多项式核函数，可以得到，$\mathcal{L}_{MMD}(F_T, F_S) = \Vert G_T - G_S\Vert_F^2$。其中，$G \in \mathbb{R}^{HW\times HW}$为Gram矩阵，其中的元素$g_{ij} = (f^i)^Tf^j$。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者在CIFAR10，ImageNet等数据集上进行了实验。Student均使用Inception-BN网络，Teacher分别使用了ResNet-1001和ResNet-101。一些具体的参数设置参考论文即可。</p><p>下面是CIFAR10上的结果。可以看到，单一方法下，CIFAR10分类NST效果最好，CIFAR100分类KD最好。组合方法中，KD+NST最好。<br><img src="/img/paper-nst-cifar10-results.png" alt="CIFAR10 结果"></p><p>下面是ImageNet上的结果。KD+NST的组合仍然是效果最好的。<br><img src="/img/paper-nst-imagenet-results.png" alt="ImageNet结果"></p><p>作者还对NST前后，Student和Teacher的输出Feature map做了聚类，发现NST确实能够使得S的输出去接近T的输出分布。如下图所示：<br><img src="/img/paper-nst-visulization-teacher-student-feature-map.png" alt="NST减小了T和S的激活feature map的distance"></p><p>此外，作者还实验了在Detection任务上的表现。在PASCAL VOC2007数据集上基于Faster RCNN方法进行了实验。backbone网络仍然是Inception BN，从<code>4b</code>layer获取feature map，此时stide为16。</p><p><img src="/img/paper-nst-pascal-voc-results.png" alt="PASCAL VOC结果"></p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;好长时间没有写博客了，国庆假期把最近看的东西整理一下。&lt;a href=&quot;https://arxiv.org/abs/1707.01219&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Like What You Like: Knowledge Distill via Neuron Selectivity Transfer&lt;/a&gt;这篇文章是图森的工作，在Knowledge Distilling基础上做出了改进Neural Selectivity Transfer，使用KD + NST方法能够取得SOTA的结果。PS：DL领域的论文名字真的是百花齐放。。。Like what you like。。。感受一下。。。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/paper-nst-kt-like-what-you-like.gif&quot; alt=&quot;jump if you jump&quot;&gt;&lt;/p&gt;
&lt;p&gt;另外，这篇论文的作者Wang Naiyan大神和Huang Zehao在今年的ECCV 2018上还有一篇论文发表，同样是模型压缩，但是使用了剪枝方法，有兴趣可以关注一下：&lt;a href=&quot;https://arxiv.org/abs/1707.01213&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Data-driven sparse structure selection for deep neural networks&lt;/a&gt;。&lt;/p&gt;
&lt;p&gt;另另外，其实这两篇文章挂在Arxiv的时间很接近，&lt;a href=&quot;https://www.zhihu.com/question/62068158&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;知乎的讨论帖：如何评价图森科技连发的三篇关于深度模型压缩的文章？&lt;/a&gt;有相关回答，可以看一下。DL/CV方法论文实在太多了，感觉Naiyan大神和图森的工作还是很值得信赖的，值得去follow。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
  <entry>
    <title>论文 - Distilling the Knowledge in a Neural Network</title>
    <link href="https://xmfbit.github.io/2018/06/07/knowledge-distilling/"/>
    <id>https://xmfbit.github.io/2018/06/07/knowledge-distilling/</id>
    <published>2018-06-07T21:56:12.000Z</published>
    <updated>2020-05-02T07:17:54.452Z</updated>
    
    <content type="html"><![CDATA[<p>知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文<a href="https://arxiv.org/abs/1503.02531" target="_blank" rel="noopener">Distilling the Knowledge in a Neural Network</a>。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。</p><a id="more"></a><h2 id="背景"><a href="#背景" class="headerlink" title="背景"></a>背景</h2><p>这里我将Wang Naiyang在知乎相关问题的<a href="https://www.zhihu.com/question/50519680/answer/136363665" target="_blank" rel="noopener">回答</a>粘贴如下，将KD方法的motivation讲的很清楚。图森也发了论文对KD进行了改进，下篇笔记总结。</p><blockquote><p>Knowledge Distill是一种简单弥补分类问题监督信号不足的办法。传统的分类问题，模型的目标是将输入的特征映射到输出空间的一个点上，例如在著名的Imagenet比赛中，就是要将所有可能的输入图片映射到输出空间的1000个点上。这么做的话这1000个点中的每一个点是一个one hot编码的类别信息。这样一个label能提供的监督信息只有log(class)这么多bit。然而在KD中，我们可以使用teacher model对于每个样本输出一个连续的label分布，这样可以利用的监督信息就远比one hot的多了。另外一个角度的理解，大家可以想象如果只有label这样的一个目标的话，那么这个模型的目标就是把训练样本中每一类的样本强制映射到同一个点上，这样其实对于训练很有帮助的类内variance和类间distance就损失掉了。然而使用teacher model的输出可以恢复出这方面的信息。具体的举例就像是paper中讲的， 猫和狗的距离比猫和桌子要近，同时如果一个动物确实长得像猫又像狗，那么它是可以给两类都提供监督。综上所述，KD的核心思想在于”打散”原来压缩到了一个点的监督信息，让student模型的输出尽量match teacher模型的输出分布。其实要达到这个目标其实不一定使用teacher model，在数据标注或者采集的时候本身保留的不确定信息也可以帮助模型的训练。</p></blockquote><h2 id="蒸馏"><a href="#蒸馏" class="headerlink" title="蒸馏"></a>蒸馏</h2><p>这篇论文很好阅读。论文中实现蒸馏是靠soften softmax prob实现的。在分类任务中，常常使用交叉熵作为损失函数，使用one-hot编码的标注好的类别标签${1,2,\dots,K}$作为target，如下所示：</p><script type="math/tex; mode=display">\mathcal{L} = -\sum_{i=1}^{K}t_i\log p_i</script><p>作者指出，粗暴地使用one-hot编码丢失了类间和类内关于相似性的额外信息。举个例子，在手写数字识别时，$2$和$3$就长得很像。但是使用上述方法，完全没有考虑到这种相似性。对于已经训练好的模型，当识别数字$2$时，很有可能它给出的概率是：数字$2$为$0.99$，数字$3$为$10^{-2}$，数字$7$为$10^{-4}$。如何能够利用训练好的Teacher模型给出的这种信息呢？</p><p>可以使用带温度的softmax函数。对于softmax的输入（下文统一称为logit），我们按照下式给出输出：</p><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>其中，当$T = 1$时，就是普通的softmax变换。这里令$T &gt; 1$，就得到了软化的softmax。（这个很好理解，除以一个比$1$大的数，相当于被squash了，线性的sqush被指数放大，差距就不会这么大了）。OK，有了这个东西，我们将Teacher网络和Student的最后充当分类器的那个全连接层的输出都做这个处理。</p><p>对Teacher网络的logit如此处理，得到的就是soft target。相比于one-hot的ground truth或softmax的prob输出，这个软化之后的target能够提供更多的类别间和类内信息。<br>可以对待训练的Student网络也如此处理，这样就得到了另外一个“交叉熵”损失：</p><script type="math/tex; mode=display">\mathcal{L}_{soft}=-\sum_{i=1}^{K}p_i\log q_i</script><p>其中，$p_i$为Teacher模型给出的soft target，$q_i$为Student模型给出的soft output。作者发现，最好的方式是做一个multi task learning，将上面这个损失函数和真正的交叉熵损失加权相加。相应地，我们将其称为hard target。</p><script type="math/tex; mode=display">\mathcal{L} = \mathcal{L}_{hard} + \lambda \mathcal{L}_{soft}</script><p>其中，$\mathcal{L}_{hard}$是分类问题中经典的交叉熵损失。由于做softened softmax计算时，需要除以$T$，导致soft target关联的梯度幅值被缩小了$T^2$倍，所以有必要在$\lambda$中预先考虑到$T^2$这个因子。</p><p>PS:这里有一篇地平线烫叔关于多任务中loss函数设计的回答：<a href="https://www.zhihu.com/question/268105631/answer/335246543" target="_blank" rel="noopener">神经网络中，设计loss function有哪些技巧? - Alan Huang的回答 - 知乎</a>。</p><h2 id="实现"><a href="#实现" class="headerlink" title="实现"></a>实现</h2><p>这里给出一个开源的MXNet的实现:<a href="https://github.com/TuSimple/neuron-selectivity-transfer/blob/master/symbol/transfer.py#L4" target="_blank" rel="noopener">kd loss by mxnet</a>。MXNet中的<code>SoftmaxOutput</code>不仅能直接支持one-hot编码类型的array作为label输入，甚至label的<code>dtype</code>也可以不是整型！</p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kd</span><span class="params">(student_hard_logits, teacher_hard_logits, temperature, weight_lambda, prefix)</span>:</span></span><br><span class="line">    student_soft_logits = student_hard_logits / temperature</span><br><span class="line">    teacher_soft_logits = teacher_hard_logits / temperature</span><br><span class="line">    teacher_soft_labels = mx.symbol.SoftmaxActivation(teacher_soft_logits,</span><br><span class="line">        name=<span class="string">"teacher%s_soft_labels"</span> % prefix)</span><br><span class="line">    kd_loss = mx.symbol.SoftmaxOutput(data=student_soft_logits, label=teacher_soft_labels,</span><br><span class="line">                                      grad_scale=weight_lambda, name=<span class="string">"%skd_loss"</span> % prefix)</span><br><span class="line">    <span class="keyword">return</span> kd_loss</span><br></pre></td></tr></table></figure><h2 id="matching-logit是特例"><a href="#matching-logit是特例" class="headerlink" title="matching logit是特例"></a>matching logit是特例</h2><p>（这部分没什么用，练习推导了一下交叉熵损失的梯度计算）</p><p>在Hinton之前，有学者提出可以匹配Teacher和Student输出的logit，Hinton指出这是本文方法在一定假设下的近似。为了和论文中的符号相同，下面我们使用$C$表示soft target带来的loss，Teacher和Student第$i$个神经元输出的logit分别为$v_i$和$z_i$，输出的softened softmax分别为$p_i$和$q_i$。那么我们有：</p><script type="math/tex; mode=display">C = -\sum_{j=1}^{C}p_j \log q_j</script><p>而且，</p><script type="math/tex; mode=display">p_i = \frac{\exp(v_i/T)}{\sum_j \exp(v_j/T)}</script><script type="math/tex; mode=display">q_i = \frac{\exp(z_i/T)}{\sum_j \exp(z_j/T)}</script><p>让我们暂时忽略$T$（最后我们乘上$\frac{1}{T}$即可），我们有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} = -\sum_{j=1}^{K}p_j\frac{1}{q_j}\frac{\partial q_j}{\partial z_i}</script><p>分情况讨论，当$i = j$时，有：</p><script type="math/tex; mode=display">\frac{\partial q_j}{\partial z_i} = q_i (1-q_i)</script><p>当$i \neq j$时，有：</p><script type="math/tex; mode=display">\begin{aligned}\frac{\partial q_j}{\partial z_i} &= \frac{-e^{z_i}e^{z_j}}{(\sum_k e^{z_k})^2}  \\&=-q_iq_j\end{aligned}</script><p>这样，我们有：</p><script type="math/tex; mode=display">\begin{aligned} \frac{\partial C}{\partial z_i} &= - p_i\frac{1}{q_i}q_i(1-q_i) + \sum_{j=1, j\neq i}^{K}p_j\frac{1}{q_j}q_iq_j  \\&= -p_i + p_iq_i + \sum_{j=1, j\neq i}^K p_jq_i \\&= q_i -p_i\end{aligned}</script><p>当然，其实上面的推导过程只不过是重复了一遍one-hot编码的交叉熵损失的计算。</p><p>这样，如果我们假设logit是零均值的，也就是说$\sum_j z_j = \sum_j v_j = 0$，那么有：</p><script type="math/tex; mode=display">\frac{\partial C}{\partial z_i} \sim \frac{1}{NT^2}(z_i - v_i)</script><p>所以说，MSE下进行logit的匹配，是本文方法的一个特例。</p><h2 id="实验"><a href="#实验" class="headerlink" title="实验"></a>实验</h2><p>作者使用了MNIST进行图片分类的实验，一个有趣的地方在于（和论文前半部分举的$2$和$3$识别的例子呼应），作者在数据集中有意地去除了标签为$3$的样本。没有KD的student网络不能识别测试时候提供的$3$，有KD的student网络能够识别一些$3$（虽然它从来没有在训练样本中出现过！）。后面，作者在语音识别和一个Google内部的很大的图像分类数据集（JFT dataset）上做了实验，</p><h2 id="附"><a href="#附" class="headerlink" title="附"></a>附</h2><ul><li>知乎上关于soft target的讨论，有Wang Naiyan和Zhou Bolei的分析：<a href="https://www.zhihu.com/question/50519680" target="_blank" rel="noopener">如何理解soft target这一做法？</a></li></ul>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;知识蒸馏（Knowledge Distilling）是模型压缩的一种方法，是指利用已经训练的一个较复杂的Teacher模型，指导一个较轻量的Student模型训练，从而在减小模型大小和计算资源的同时，尽量保持原Teacher模型的准确率的方法。这种方法受到大家的注意，主要是由于Hinton的论文&lt;a href=&quot;https://arxiv.org/abs/1503.02531&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Distilling the Knowledge in a Neural Network&lt;/a&gt;。这篇博客做一总结。后续还会有KD方法的改进相关论文的心得介绍。&lt;/p&gt;
    
    </summary>
    
    
      <category term="deep learning" scheme="https://xmfbit.github.io/tags/deep-learning/"/>
    
      <category term="paper" scheme="https://xmfbit.github.io/tags/paper/"/>
    
      <category term="model compression" scheme="https://xmfbit.github.io/tags/model-compression/"/>
    
  </entry>
  
</feed>
